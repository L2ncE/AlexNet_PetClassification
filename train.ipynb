{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "557cb615-1c2e-43db-8a86-17dbbc4fe43f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] ME(42716:139713944598336,MainProcess):2023-06-23-16:44:14.810.928 [mindspore/dataset/engine/datasets.py:1145] Dataset is shuffled before split.\n",
      "Warning: unknown JFIF revision number 0.00\n",
      "Warning: unknown JFIF revision number 0.00\n",
      "Warning: unknown JFIF revision number 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 step: 1, loss is 4.604930877685547\n",
      "epoch: 1 step: 2, loss is 4.604280471801758\n",
      "epoch: 1 step: 3, loss is 4.603142738342285\n",
      "epoch: 1 step: 4, loss is 4.60131311416626\n",
      "epoch: 1 step: 5, loss is 4.599898815155029\n",
      "epoch: 1 step: 6, loss is 4.5969085693359375\n",
      "epoch: 1 step: 7, loss is 4.594311714172363\n",
      "epoch: 1 step: 8, loss is 4.591119766235352\n",
      "epoch: 1 step: 9, loss is 4.588165760040283\n",
      "epoch: 1 step: 10, loss is 4.584085464477539\n",
      "epoch: 1 step: 11, loss is 4.580217361450195\n",
      "epoch: 1 step: 12, loss is 4.576235771179199\n",
      "epoch: 1 step: 13, loss is 4.572023391723633\n",
      "epoch: 1 step: 14, loss is 4.567639350891113\n",
      "epoch: 1 step: 15, loss is 4.563488483428955\n",
      "epoch: 1 step: 16, loss is 4.558172225952148\n",
      "epoch: 1 step: 17, loss is 4.554418563842773\n",
      "epoch: 1 step: 18, loss is 4.549302101135254\n",
      "epoch: 1 step: 19, loss is 4.543544769287109\n",
      "epoch: 1 step: 20, loss is 4.540306091308594\n",
      "epoch: 1 step: 21, loss is 4.534407615661621\n",
      "epoch: 1 step: 22, loss is 4.528200626373291\n",
      "epoch: 1 step: 23, loss is 4.523693084716797\n",
      "epoch: 1 step: 24, loss is 4.517543315887451\n",
      "epoch: 1 step: 25, loss is 4.5121588706970215\n",
      "epoch: 1 step: 26, loss is 4.506768703460693\n",
      "epoch: 1 step: 27, loss is 4.501678943634033\n",
      "epoch: 1 step: 28, loss is 4.496762752532959\n",
      "epoch: 1 step: 29, loss is 4.493980884552002\n",
      "epoch: 1 step: 30, loss is 4.485960006713867\n",
      "epoch: 1 step: 31, loss is 4.481928825378418\n",
      "epoch: 1 step: 32, loss is 4.473363399505615\n",
      "epoch: 1 step: 33, loss is 4.465959548950195\n",
      "epoch: 1 step: 34, loss is 4.463160514831543\n",
      "epoch: 1 step: 35, loss is 4.457555770874023\n",
      "epoch: 1 step: 36, loss is 4.451664924621582\n",
      "epoch: 1 step: 37, loss is 4.447016716003418\n",
      "epoch: 1 step: 38, loss is 4.435876846313477\n",
      "epoch: 1 step: 39, loss is 4.437229633331299\n",
      "epoch: 1 step: 40, loss is 4.423810958862305\n",
      "epoch: 1 step: 41, loss is 4.42018461227417\n",
      "epoch: 1 step: 42, loss is 4.417471885681152\n",
      "epoch: 1 step: 43, loss is 4.41228723526001\n",
      "epoch: 1 step: 44, loss is 4.402584075927734\n",
      "epoch: 1 step: 45, loss is 4.403199672698975\n",
      "epoch: 1 step: 46, loss is 4.39702844619751\n",
      "epoch: 1 step: 47, loss is 4.384336471557617\n",
      "epoch: 1 step: 48, loss is 4.379086971282959\n",
      "epoch: 1 step: 49, loss is 4.380406379699707\n",
      "epoch: 1 step: 50, loss is 4.362746715545654\n",
      "epoch: 1 step: 51, loss is 4.368355751037598\n",
      "epoch: 1 step: 52, loss is 4.3578901290893555\n",
      "epoch: 1 step: 53, loss is 4.347789287567139\n",
      "epoch: 1 step: 54, loss is 4.3420915603637695\n",
      "epoch: 1 step: 55, loss is 4.340313911437988\n",
      "epoch: 1 step: 56, loss is 4.338809013366699\n",
      "epoch: 1 step: 57, loss is 4.328291416168213\n",
      "epoch: 1 step: 58, loss is 4.322944641113281\n",
      "epoch: 1 step: 59, loss is 4.3199462890625\n",
      "epoch: 1 step: 60, loss is 4.304566860198975\n",
      "epoch: 1 step: 61, loss is 4.299489498138428\n",
      "epoch: 1 step: 62, loss is 4.292943000793457\n",
      "epoch: 1 step: 63, loss is 4.2914862632751465\n",
      "epoch: 1 step: 64, loss is 4.285301208496094\n",
      "epoch: 1 step: 65, loss is 4.263949394226074\n",
      "epoch: 1 step: 66, loss is 4.272219657897949\n",
      "epoch: 1 step: 67, loss is 4.254332065582275\n",
      "epoch: 1 step: 68, loss is 4.266754150390625\n",
      "epoch: 1 step: 69, loss is 4.266144752502441\n",
      "epoch: 1 step: 70, loss is 4.254082679748535\n",
      "epoch: 1 step: 71, loss is 4.2482805252075195\n",
      "epoch: 1 step: 72, loss is 4.247623443603516\n",
      "epoch: 1 step: 73, loss is 4.2462382316589355\n",
      "epoch: 1 step: 74, loss is 4.228265762329102\n",
      "epoch: 1 step: 75, loss is 4.223451614379883\n",
      "epoch: 1 step: 76, loss is 4.208759307861328\n",
      "epoch: 1 step: 77, loss is 4.197024345397949\n",
      "epoch: 1 step: 78, loss is 4.20658016204834\n",
      "epoch: 1 step: 79, loss is 4.179567337036133\n",
      "epoch: 1 step: 80, loss is 4.189593315124512\n",
      "epoch: 1 step: 81, loss is 4.159529209136963\n",
      "epoch: 1 step: 82, loss is 4.1707539558410645\n",
      "epoch: 1 step: 83, loss is 4.154119491577148\n",
      "epoch: 1 step: 84, loss is 4.151923656463623\n",
      "epoch: 1 step: 85, loss is 4.135105133056641\n",
      "epoch: 1 step: 86, loss is 4.155521392822266\n",
      "epoch: 1 step: 87, loss is 4.133345127105713\n",
      "epoch: 1 step: 88, loss is 4.125319957733154\n",
      "epoch: 1 step: 89, loss is 4.140089988708496\n",
      "epoch: 1 step: 90, loss is 4.1234283447265625\n",
      "epoch: 1 step: 91, loss is 4.113231658935547\n",
      "epoch: 1 step: 92, loss is 4.091352462768555\n",
      "epoch: 1 step: 93, loss is 4.098297119140625\n",
      "epoch: 1 step: 94, loss is 4.081357002258301\n",
      "epoch: 1 step: 95, loss is 4.077291488647461\n",
      "epoch: 1 step: 96, loss is 4.07703971862793\n",
      "epoch: 1 step: 97, loss is 4.058829307556152\n",
      "epoch: 1 step: 98, loss is 4.049444198608398\n",
      "epoch: 1 step: 99, loss is 4.037881374359131\n",
      "epoch: 1 step: 100, loss is 4.043091773986816\n",
      "epoch: 1 step: 101, loss is 4.013830661773682\n",
      "epoch: 1 step: 102, loss is 4.01658296585083\n",
      "epoch: 1 step: 103, loss is 4.0195417404174805\n",
      "epoch: 1 step: 104, loss is 3.997077465057373\n",
      "epoch: 1 step: 105, loss is 4.005906581878662\n",
      "epoch: 1 step: 106, loss is 4.013946056365967\n",
      "epoch: 1 step: 107, loss is 3.987032413482666\n",
      "epoch: 1 step: 108, loss is 3.9676127433776855\n",
      "epoch: 1 step: 109, loss is 3.999269485473633\n",
      "epoch: 1 step: 110, loss is 3.959792375564575\n",
      "epoch: 1 step: 111, loss is 3.962217092514038\n",
      "epoch: 1 step: 112, loss is 3.918518304824829\n",
      "epoch: 1 step: 113, loss is 3.9190797805786133\n",
      "epoch: 1 step: 114, loss is 3.91257643699646\n",
      "epoch: 1 step: 115, loss is 3.9057857990264893\n",
      "epoch: 1 step: 116, loss is 3.9096007347106934\n",
      "epoch: 1 step: 117, loss is 3.873784303665161\n",
      "epoch: 1 step: 118, loss is 3.8709628582000732\n",
      "epoch: 1 step: 119, loss is 3.848604679107666\n",
      "epoch: 1 step: 120, loss is 3.8376948833465576\n",
      "epoch: 1 step: 121, loss is 3.810770273208618\n",
      "epoch: 1 step: 122, loss is 3.7816007137298584\n",
      "epoch: 1 step: 123, loss is 3.8076629638671875\n",
      "epoch: 1 step: 124, loss is 3.7554430961608887\n",
      "epoch: 1 step: 125, loss is 3.730588674545288\n",
      "epoch: 1 step: 126, loss is 3.694655418395996\n",
      "epoch: 1 step: 127, loss is 3.6808948516845703\n",
      "epoch: 1 step: 128, loss is 3.6518473625183105\n",
      "epoch: 1 step: 129, loss is 3.5838820934295654\n",
      "epoch: 1 step: 130, loss is 3.5447661876678467\n",
      "epoch: 1 step: 131, loss is 3.5067927837371826\n",
      "epoch: 1 step: 132, loss is 3.4367520809173584\n",
      "epoch: 1 step: 133, loss is 3.182626247406006\n",
      "epoch: 1 step: 134, loss is 3.199277400970459\n",
      "epoch: 1 step: 135, loss is 2.868408203125\n",
      "epoch: 1 step: 136, loss is 2.6970653533935547\n",
      "epoch: 1 step: 137, loss is 2.0798263549804688\n",
      "epoch: 1 step: 138, loss is 1.7633297443389893\n",
      "epoch: 1 step: 139, loss is 1.3108041286468506\n",
      "epoch: 1 step: 140, loss is 1.1073853969573975\n",
      "epoch: 1 step: 141, loss is 0.8537968397140503\n",
      "epoch: 1 step: 142, loss is 0.6548473238945007\n",
      "epoch: 1 step: 143, loss is 0.7771131992340088\n",
      "epoch: 1 step: 144, loss is 0.635431706905365\n",
      "epoch: 1 step: 145, loss is 1.142167568206787\n",
      "epoch: 1 step: 146, loss is 0.768082320690155\n",
      "epoch: 1 step: 147, loss is 3.304879665374756\n",
      "epoch: 1 step: 148, loss is 1.1056828498840332\n",
      "epoch: 1 step: 149, loss is 1.6216646432876587\n",
      "epoch: 1 step: 150, loss is 4.127776145935059\n",
      "epoch: 1 step: 151, loss is 4.4062066078186035\n",
      "epoch: 1 step: 152, loss is 1.7826683521270752\n",
      "epoch: 1 step: 153, loss is 3.5167958736419678\n",
      "epoch: 1 step: 154, loss is 2.8549394607543945\n",
      "epoch: 1 step: 155, loss is 1.8917709589004517\n",
      "epoch: 1 step: 156, loss is 1.563247561454773\n",
      "epoch: 1 step: 157, loss is 1.4552412033081055\n",
      "epoch: 1 step: 158, loss is 1.9389288425445557\n",
      "epoch: 1 step: 159, loss is 2.2111804485321045\n",
      "epoch: 1 step: 160, loss is 2.676206588745117\n",
      "epoch: 1 step: 161, loss is 2.772653579711914\n",
      "epoch: 1 step: 162, loss is 2.692730188369751\n",
      "epoch: 1 step: 163, loss is 2.6282615661621094\n",
      "epoch: 1 step: 164, loss is 2.618382215499878\n",
      "epoch: 1 step: 165, loss is 2.530332326889038\n",
      "epoch: 1 step: 166, loss is 1.8247002363204956\n",
      "epoch: 1 step: 167, loss is 1.8248271942138672\n",
      "epoch: 1 step: 168, loss is 1.1392399072647095\n",
      "epoch: 1 step: 169, loss is 0.8013460636138916\n",
      "epoch: 1 step: 170, loss is 0.7688471674919128\n",
      "epoch: 1 step: 171, loss is 0.7709141969680786\n",
      "epoch: 1 step: 172, loss is 0.8023240566253662\n",
      "epoch: 1 step: 173, loss is 0.7012113928794861\n",
      "epoch: 1 step: 174, loss is 0.8102220892906189\n",
      "epoch: 1 step: 175, loss is 0.7074053287506104\n",
      "epoch: 1 step: 176, loss is 0.8604157567024231\n",
      "epoch: 1 step: 177, loss is 0.756858766078949\n",
      "epoch: 1 step: 178, loss is 0.7429742217063904\n",
      "epoch: 1 step: 179, loss is 0.5436261892318726\n",
      "epoch: 1 step: 180, loss is 1.2905064821243286\n",
      "epoch: 1 step: 181, loss is 0.9247366786003113\n",
      "epoch: 1 step: 182, loss is 0.9259362816810608\n",
      "epoch: 1 step: 183, loss is 1.5257288217544556\n",
      "epoch: 1 step: 184, loss is 0.49779292941093445\n",
      "epoch: 1 step: 185, loss is 0.709342360496521\n",
      "epoch: 1 step: 186, loss is 0.7477593421936035\n",
      "epoch: 1 step: 187, loss is 0.8095166683197021\n",
      "epoch: 1 step: 188, loss is 0.7711019515991211\n",
      "epoch: 1 step: 189, loss is 0.7701617479324341\n",
      "epoch: 1 step: 190, loss is 0.9152256846427917\n",
      "epoch: 1 step: 191, loss is 1.4017773866653442\n",
      "epoch: 1 step: 192, loss is 0.6999199986457825\n",
      "epoch: 1 step: 193, loss is 1.146560549736023\n",
      "epoch: 1 step: 194, loss is 0.36353039741516113\n",
      "epoch: 1 step: 195, loss is 1.572392463684082\n",
      "epoch: 1 step: 196, loss is 0.5738829970359802\n",
      "epoch: 1 step: 197, loss is 0.7153007388114929\n",
      "epoch: 1 step: 198, loss is 0.9814692139625549\n",
      "epoch: 1 step: 199, loss is 0.8515934944152832\n",
      "epoch: 1 step: 200, loss is 0.6749454736709595\n",
      "epoch: 1 step: 201, loss is 0.8108445405960083\n",
      "epoch: 1 step: 202, loss is 0.7068953514099121\n",
      "epoch: 1 step: 203, loss is 0.6758936047554016\n",
      "epoch: 1 step: 204, loss is 0.8612676858901978\n",
      "epoch: 1 step: 205, loss is 0.8938072919845581\n",
      "epoch: 1 step: 206, loss is 0.6963213086128235\n",
      "epoch: 1 step: 207, loss is 0.7073832154273987\n",
      "epoch: 1 step: 208, loss is 0.8929511308670044\n",
      "epoch: 1 step: 209, loss is 0.6892979145050049\n",
      "epoch: 1 step: 210, loss is 0.7539886236190796\n",
      "epoch: 1 step: 211, loss is 0.6138352751731873\n",
      "epoch: 1 step: 212, loss is 1.0686572790145874\n",
      "epoch: 1 step: 213, loss is 0.6231570243835449\n",
      "epoch: 1 step: 214, loss is 1.2666877508163452\n",
      "epoch: 1 step: 215, loss is 0.855688214302063\n",
      "epoch: 1 step: 216, loss is 0.9069885611534119\n",
      "epoch: 1 step: 217, loss is 0.6817284822463989\n",
      "epoch: 1 step: 218, loss is 0.7032801508903503\n",
      "epoch: 1 step: 219, loss is 0.7242337465286255\n",
      "epoch: 1 step: 220, loss is 0.7397947907447815\n",
      "epoch: 1 step: 221, loss is 0.8368004560470581\n",
      "epoch: 1 step: 222, loss is 0.8783067464828491\n",
      "epoch: 1 step: 223, loss is 0.7886918783187866\n",
      "epoch: 1 step: 224, loss is 0.611675500869751\n",
      "epoch: 1 step: 225, loss is 0.8044086694717407\n",
      "epoch: 1 step: 226, loss is 0.7584661245346069\n",
      "epoch: 1 step: 227, loss is 0.7796897888183594\n",
      "epoch: 1 step: 228, loss is 0.878533661365509\n",
      "epoch: 1 step: 229, loss is 0.6982030868530273\n",
      "epoch: 1 step: 230, loss is 0.8528091311454773\n",
      "epoch: 1 step: 231, loss is 0.6859697699546814\n",
      "epoch: 1 step: 232, loss is 0.6603555679321289\n",
      "epoch: 1 step: 233, loss is 0.7076534628868103\n",
      "epoch: 1 step: 234, loss is 0.7012009024620056\n",
      "epoch: 1 step: 235, loss is 0.662254273891449\n",
      "epoch: 1 step: 236, loss is 0.6919817924499512\n",
      "epoch: 1 step: 237, loss is 0.769171953201294\n",
      "epoch: 1 step: 238, loss is 0.6533631086349487\n",
      "epoch: 1 step: 239, loss is 0.5080283880233765\n",
      "epoch: 1 step: 240, loss is 0.7371870279312134\n",
      "epoch: 1 step: 241, loss is 0.7598512768745422\n",
      "epoch: 1 step: 242, loss is 0.8418869376182556\n",
      "epoch: 1 step: 243, loss is 0.717155396938324\n",
      "epoch: 1 step: 244, loss is 0.6461529731750488\n",
      "epoch: 1 step: 245, loss is 0.6942523717880249\n",
      "epoch: 1 step: 246, loss is 0.6767619848251343\n",
      "epoch: 1 step: 247, loss is 0.6946171522140503\n",
      "epoch: 1 step: 248, loss is 0.6508160829544067\n",
      "epoch: 1 step: 249, loss is 0.8610028624534607\n",
      "epoch: 1 step: 250, loss is 0.7094875574111938\n",
      "epoch: 1 step: 251, loss is 0.7027592658996582\n",
      "epoch: 1 step: 252, loss is 0.7323785424232483\n",
      "epoch: 1 step: 253, loss is 0.8302112221717834\n",
      "epoch: 1 step: 254, loss is 0.7083672881126404\n",
      "epoch: 1 step: 255, loss is 0.6565631031990051\n",
      "epoch: 1 step: 256, loss is 0.6989253759384155\n",
      "epoch: 1 step: 257, loss is 0.6622664332389832\n",
      "epoch: 1 step: 258, loss is 0.6329353451728821\n",
      "epoch: 1 step: 259, loss is 0.7663166522979736\n",
      "epoch: 1 step: 260, loss is 0.661977231502533\n",
      "epoch: 1 step: 261, loss is 0.7063429951667786\n",
      "epoch: 1 step: 262, loss is 0.6692994832992554\n",
      "epoch: 1 step: 263, loss is 0.7044321298599243\n",
      "epoch: 1 step: 264, loss is 0.6941196322441101\n",
      "epoch: 1 step: 265, loss is 0.7288273572921753\n",
      "epoch: 1 step: 266, loss is 0.6929709911346436\n",
      "epoch: 1 step: 267, loss is 0.6495154500007629\n",
      "epoch: 1 step: 268, loss is 0.6584604978561401\n",
      "epoch: 1 step: 269, loss is 0.8175076246261597\n",
      "epoch: 1 step: 270, loss is 0.5676738023757935\n",
      "epoch: 1 step: 271, loss is 0.7343456745147705\n",
      "epoch: 1 step: 272, loss is 0.9966244101524353\n",
      "epoch: 1 step: 273, loss is 0.8299778699874878\n",
      "epoch: 1 step: 274, loss is 0.7002705931663513\n",
      "epoch: 1 step: 275, loss is 0.6969716548919678\n",
      "epoch: 1 step: 276, loss is 0.6991685628890991\n",
      "epoch: 1 step: 277, loss is 0.602300763130188\n",
      "epoch: 1 step: 278, loss is 0.8638503551483154\n",
      "epoch: 1 step: 279, loss is 0.812045693397522\n",
      "epoch: 1 step: 280, loss is 0.9506008625030518\n",
      "epoch: 1 step: 281, loss is 0.7187395691871643\n",
      "epoch: 1 step: 282, loss is 0.6700180172920227\n",
      "epoch: 1 step: 283, loss is 0.7164696455001831\n",
      "epoch: 1 step: 284, loss is 0.6721163392066956\n",
      "epoch: 1 step: 285, loss is 0.6437274813652039\n",
      "epoch: 1 step: 286, loss is 0.603464663028717\n",
      "epoch: 1 step: 287, loss is 0.9384502172470093\n",
      "epoch: 1 step: 288, loss is 0.9826580286026001\n",
      "epoch: 1 step: 289, loss is 0.5599987506866455\n",
      "epoch: 1 step: 290, loss is 0.719567596912384\n",
      "epoch: 1 step: 291, loss is 0.991815984249115\n",
      "epoch: 1 step: 292, loss is 0.7357813715934753\n",
      "epoch: 1 step: 293, loss is 0.75070720911026\n",
      "epoch: 1 step: 294, loss is 0.7304529547691345\n",
      "epoch: 1 step: 295, loss is 0.6779228448867798\n",
      "epoch: 1 step: 296, loss is 0.6048648953437805\n",
      "epoch: 1 step: 297, loss is 0.6583018898963928\n",
      "epoch: 1 step: 298, loss is 0.8783062100410461\n",
      "epoch: 1 step: 299, loss is 0.7017525434494019\n",
      "epoch: 1 step: 300, loss is 1.1977903842926025\n",
      "epoch: 1 step: 301, loss is 1.0177537202835083\n",
      "epoch: 1 step: 302, loss is 0.8329246044158936\n",
      "epoch: 1 step: 303, loss is 0.6847345232963562\n",
      "epoch: 1 step: 304, loss is 0.7506779432296753\n",
      "epoch: 1 step: 305, loss is 0.7592480778694153\n",
      "epoch: 1 step: 306, loss is 0.7681426405906677\n",
      "epoch: 1 step: 307, loss is 0.7143388390541077\n",
      "epoch: 1 step: 308, loss is 0.6685361862182617\n",
      "epoch: 1 step: 309, loss is 0.7169509530067444\n",
      "epoch: 1 step: 310, loss is 0.8039190173149109\n",
      "epoch: 1 step: 311, loss is 0.651634931564331\n",
      "epoch: 1 step: 312, loss is 0.7490300536155701\n",
      "epoch: 1 step: 313, loss is 0.7821354269981384\n",
      "epoch: 1 step: 314, loss is 0.8482765555381775\n",
      "epoch: 1 step: 315, loss is 0.6320202946662903\n",
      "epoch: 1 step: 316, loss is 0.5979323387145996\n",
      "epoch: 1 step: 317, loss is 0.7812954783439636\n",
      "epoch: 1 step: 318, loss is 0.6657150387763977\n",
      "epoch: 1 step: 319, loss is 0.7029876112937927\n",
      "epoch: 1 step: 320, loss is 0.7026966214179993\n",
      "epoch: 1 step: 321, loss is 0.7010704874992371\n",
      "epoch: 1 step: 322, loss is 0.6842939853668213\n",
      "epoch: 1 step: 323, loss is 0.6665714383125305\n",
      "epoch: 1 step: 324, loss is 0.7055590748786926\n",
      "epoch: 1 step: 325, loss is 0.7612804174423218\n",
      "epoch: 1 step: 326, loss is 0.6719927191734314\n",
      "epoch: 1 step: 327, loss is 0.6456413865089417\n",
      "epoch: 1 step: 328, loss is 0.6576357483863831\n",
      "epoch: 1 step: 329, loss is 0.6984487771987915\n",
      "epoch: 1 step: 330, loss is 0.8604322671890259\n",
      "epoch: 1 step: 331, loss is 0.6528492569923401\n",
      "epoch: 1 step: 332, loss is 0.7147166132926941\n",
      "epoch: 1 step: 333, loss is 0.6678823828697205\n",
      "epoch: 1 step: 334, loss is 0.7279731631278992\n",
      "epoch: 1 step: 335, loss is 0.6851001381874084\n",
      "epoch: 1 step: 336, loss is 0.7201547026634216\n",
      "epoch: 1 step: 337, loss is 0.7637210488319397\n",
      "epoch: 1 step: 338, loss is 0.7061968445777893\n",
      "epoch: 1 step: 339, loss is 0.6750548481941223\n",
      "epoch: 1 step: 340, loss is 0.7370128631591797\n",
      "epoch: 1 step: 341, loss is 0.7281200885772705\n",
      "epoch: 1 step: 342, loss is 0.6912771463394165\n",
      "epoch: 1 step: 343, loss is 0.6771147847175598\n",
      "epoch: 1 step: 344, loss is 0.6617511510848999\n",
      "epoch: 1 step: 345, loss is 0.8082246780395508\n",
      "epoch: 1 step: 346, loss is 0.7303241491317749\n",
      "epoch: 1 step: 347, loss is 0.6664318442344666\n",
      "epoch: 1 step: 348, loss is 0.6589562296867371\n",
      "epoch: 1 step: 349, loss is 0.8640249967575073\n",
      "epoch: 1 step: 350, loss is 0.9501762986183167\n",
      "epoch: 1 step: 351, loss is 0.6795669198036194\n",
      "epoch: 1 step: 352, loss is 0.673488199710846\n",
      "epoch: 1 step: 353, loss is 0.5264251232147217\n",
      "epoch: 1 step: 354, loss is 0.8762480616569519\n",
      "epoch: 1 step: 355, loss is 0.8157055377960205\n",
      "epoch: 1 step: 356, loss is 0.7043138742446899\n",
      "epoch: 1 step: 357, loss is 0.7077602744102478\n",
      "epoch: 1 step: 358, loss is 0.762130856513977\n",
      "epoch: 1 step: 359, loss is 0.672385036945343\n",
      "epoch: 1 step: 360, loss is 0.709800660610199\n",
      "epoch: 1 step: 361, loss is 0.7336616516113281\n",
      "epoch: 1 step: 362, loss is 0.6320569515228271\n",
      "epoch: 1 step: 363, loss is 0.7838199138641357\n",
      "epoch: 1 step: 364, loss is 0.8709867000579834\n",
      "epoch: 1 step: 365, loss is 0.6742459535598755\n",
      "epoch: 1 step: 366, loss is 0.6554628014564514\n",
      "epoch: 1 step: 367, loss is 0.7557861804962158\n",
      "epoch: 1 step: 368, loss is 0.7023995518684387\n",
      "epoch: 1 step: 369, loss is 0.7084707021713257\n",
      "epoch: 1 step: 370, loss is 0.6668018698692322\n",
      "epoch: 1 step: 371, loss is 0.7018256783485413\n",
      "epoch: 1 step: 372, loss is 0.6710731387138367\n",
      "epoch: 1 step: 373, loss is 0.7288172245025635\n",
      "epoch: 1 step: 374, loss is 0.6936210989952087\n",
      "epoch: 1 step: 375, loss is 0.7098013162612915\n",
      "epoch: 1 step: 376, loss is 0.728032112121582\n",
      "epoch: 1 step: 377, loss is 0.7057759761810303\n",
      "epoch: 1 step: 378, loss is 0.6508313417434692\n",
      "epoch: 1 step: 379, loss is 0.6951438784599304\n",
      "epoch: 1 step: 380, loss is 0.735987663269043\n",
      "epoch: 1 step: 381, loss is 0.669257640838623\n",
      "epoch: 1 step: 382, loss is 0.7932159900665283\n",
      "epoch: 1 step: 383, loss is 0.6546088457107544\n",
      "epoch: 1 step: 384, loss is 0.7174753546714783\n",
      "epoch: 1 step: 385, loss is 0.6375072598457336\n",
      "epoch: 1 step: 386, loss is 0.6413872241973877\n",
      "epoch: 1 step: 387, loss is 0.6444220542907715\n",
      "epoch: 1 step: 388, loss is 0.6568989157676697\n",
      "epoch: 1 step: 389, loss is 0.6580239534378052\n",
      "epoch: 1 step: 390, loss is 0.6834770441055298\n",
      "epoch: 1 step: 391, loss is 0.9123016595840454\n",
      "epoch: 1 step: 392, loss is 0.6833080649375916\n",
      "epoch: 1 step: 393, loss is 0.7442262172698975\n",
      "epoch: 1 step: 394, loss is 0.6942716240882874\n",
      "epoch: 1 step: 395, loss is 0.7138797044754028\n",
      "epoch: 1 step: 396, loss is 0.7587561011314392\n",
      "epoch: 1 step: 397, loss is 0.8292289972305298\n",
      "epoch: 1 step: 398, loss is 0.6430755257606506\n",
      "epoch: 1 step: 399, loss is 0.7000512480735779\n",
      "epoch: 1 step: 400, loss is 0.6744621992111206\n",
      "epoch: 1 step: 401, loss is 0.6763767004013062\n",
      "epoch: 1 step: 402, loss is 0.7408083081245422\n",
      "epoch: 1 step: 403, loss is 0.6630690693855286\n",
      "epoch: 1 step: 404, loss is 0.8425776958465576\n",
      "epoch: 1 step: 405, loss is 0.6033603549003601\n",
      "epoch: 1 step: 406, loss is 0.8295056223869324\n",
      "epoch: 1 step: 407, loss is 0.9265716671943665\n",
      "epoch: 1 step: 408, loss is 0.7286667823791504\n",
      "epoch: 1 step: 409, loss is 0.7425326108932495\n",
      "epoch: 1 step: 410, loss is 0.6814038753509521\n",
      "epoch: 1 step: 411, loss is 0.7310687303543091\n",
      "epoch: 1 step: 412, loss is 0.7590875029563904\n",
      "epoch: 1 step: 413, loss is 0.641444742679596\n",
      "epoch: 1 step: 414, loss is 0.6653188467025757\n",
      "epoch: 1 step: 415, loss is 0.9620760679244995\n",
      "epoch: 1 step: 416, loss is 0.6462420225143433\n",
      "epoch: 1 step: 417, loss is 0.7217398285865784\n",
      "epoch: 1 step: 418, loss is 0.6900104880332947\n",
      "epoch: 1 step: 419, loss is 0.6899361610412598\n",
      "epoch: 1 step: 420, loss is 0.7210215926170349\n",
      "epoch: 1 step: 421, loss is 0.6249561905860901\n",
      "epoch: 1 step: 422, loss is 0.6652584075927734\n",
      "epoch: 1 step: 423, loss is 0.717107892036438\n",
      "epoch: 1 step: 424, loss is 0.7894288301467896\n",
      "epoch: 1 step: 425, loss is 0.722186267375946\n",
      "epoch: 1 step: 426, loss is 0.6595175862312317\n",
      "epoch: 1 step: 427, loss is 0.818010687828064\n",
      "epoch: 1 step: 428, loss is 0.8616593480110168\n",
      "epoch: 1 step: 429, loss is 0.6494951844215393\n",
      "epoch: 1 step: 430, loss is 0.7014682292938232\n",
      "epoch: 1 step: 431, loss is 0.7339428663253784\n",
      "epoch: 1 step: 432, loss is 0.6961535811424255\n",
      "epoch: 1 step: 433, loss is 0.7019906044006348\n",
      "epoch: 1 step: 434, loss is 0.6892336010932922\n",
      "epoch: 1 step: 435, loss is 0.7029152512550354\n",
      "epoch: 1 step: 436, loss is 0.7730445265769958\n",
      "epoch: 1 step: 437, loss is 0.6984364986419678\n",
      "epoch: 1 step: 438, loss is 0.7137588858604431\n",
      "epoch: 1 step: 439, loss is 0.8170480132102966\n",
      "epoch: 1 step: 440, loss is 0.633547842502594\n",
      "epoch: 1 step: 441, loss is 0.6856055855751038\n",
      "epoch: 1 step: 442, loss is 0.6686435341835022\n",
      "epoch: 1 step: 443, loss is 0.7183515429496765\n",
      "epoch: 1 step: 444, loss is 0.6624516248703003\n",
      "epoch: 1 step: 445, loss is 0.6984765529632568\n",
      "epoch: 1 step: 446, loss is 0.8566117286682129\n",
      "epoch: 1 step: 447, loss is 0.8289427161216736\n",
      "epoch: 1 step: 448, loss is 0.7049579620361328\n",
      "epoch: 1 step: 449, loss is 0.6653079390525818\n",
      "epoch: 1 step: 450, loss is 0.7644953727722168\n",
      "epoch: 1 step: 451, loss is 0.6559107899665833\n",
      "epoch: 1 step: 452, loss is 0.7161818742752075\n",
      "epoch: 1 step: 453, loss is 0.6747475862503052\n",
      "epoch: 1 step: 454, loss is 0.6799088716506958\n",
      "epoch: 1 step: 455, loss is 0.7289469242095947\n",
      "epoch: 1 step: 456, loss is 0.8305554986000061\n",
      "epoch: 1 step: 457, loss is 0.5740058422088623\n",
      "epoch: 1 step: 458, loss is 0.7146991491317749\n",
      "epoch: 1 step: 459, loss is 0.6897265315055847\n",
      "epoch: 1 step: 460, loss is 0.6866270899772644\n",
      "epoch: 1 step: 461, loss is 0.6194123029708862\n",
      "epoch: 1 step: 462, loss is 0.7404666543006897\n",
      "epoch: 1 step: 463, loss is 0.7222959399223328\n",
      "epoch: 1 step: 464, loss is 0.8214577436447144\n",
      "epoch: 1 step: 465, loss is 0.7648006677627563\n",
      "epoch: 1 step: 466, loss is 0.6851659417152405\n",
      "epoch: 1 step: 467, loss is 0.6779444813728333\n",
      "epoch: 1 step: 468, loss is 0.7336820960044861\n",
      "epoch: 1 step: 469, loss is 0.6363059282302856\n",
      "epoch: 1 step: 470, loss is 0.7458488941192627\n",
      "epoch: 1 step: 471, loss is 0.7810940146446228\n",
      "epoch: 1 step: 472, loss is 0.722034215927124\n",
      "epoch: 1 step: 473, loss is 0.7567632794380188\n",
      "epoch: 1 step: 474, loss is 0.8438369035720825\n",
      "epoch: 1 step: 475, loss is 0.681496262550354\n",
      "epoch: 1 step: 476, loss is 0.7502819299697876\n",
      "epoch: 1 step: 477, loss is 0.6752878427505493\n",
      "epoch: 1 step: 478, loss is 0.7042737007141113\n",
      "epoch: 1 step: 479, loss is 0.7170816659927368\n",
      "epoch: 1 step: 480, loss is 0.6018252968788147\n",
      "epoch: 1 step: 481, loss is 0.8849036693572998\n",
      "epoch: 1 step: 482, loss is 0.8095070123672485\n",
      "epoch: 1 step: 483, loss is 0.7811493277549744\n",
      "epoch: 1 step: 484, loss is 0.6962105631828308\n",
      "epoch: 1 step: 485, loss is 0.7073890566825867\n",
      "epoch: 1 step: 486, loss is 0.6980593204498291\n",
      "epoch: 1 step: 487, loss is 0.650348424911499\n",
      "epoch: 1 step: 488, loss is 0.824345588684082\n",
      "epoch: 1 step: 489, loss is 0.6739673614501953\n",
      "epoch: 1 step: 490, loss is 0.8411835432052612\n",
      "epoch: 1 step: 491, loss is 0.7387404441833496\n",
      "epoch: 1 step: 492, loss is 0.7802642583847046\n",
      "epoch: 1 step: 493, loss is 0.6140488982200623\n",
      "epoch: 1 step: 494, loss is 0.702121913433075\n",
      "epoch: 1 step: 495, loss is 0.7120702862739563\n",
      "epoch: 1 step: 496, loss is 0.7485523819923401\n",
      "epoch: 1 step: 497, loss is 0.7250257730484009\n",
      "epoch: 1 step: 498, loss is 0.6679412126541138\n",
      "epoch: 1 step: 499, loss is 0.8657246828079224\n",
      "epoch: 1 step: 500, loss is 0.6257795095443726\n",
      "epoch: 1 step: 501, loss is 0.6355046629905701\n",
      "epoch: 1 step: 502, loss is 0.5311902761459351\n",
      "epoch: 1 step: 503, loss is 0.7942692637443542\n",
      "epoch: 1 step: 504, loss is 0.9106884002685547\n",
      "epoch: 1 step: 505, loss is 1.018829345703125\n",
      "epoch: 1 step: 506, loss is 0.6816084980964661\n",
      "epoch: 1 step: 507, loss is 0.5825760960578918\n",
      "epoch: 1 step: 508, loss is 0.5476899743080139\n",
      "epoch: 1 step: 509, loss is 0.6861622333526611\n",
      "epoch: 1 step: 510, loss is 0.7430180907249451\n",
      "epoch: 1 step: 511, loss is 0.6722269058227539\n",
      "epoch: 1 step: 512, loss is 0.8062023520469666\n",
      "epoch: 1 step: 513, loss is 0.7055141925811768\n",
      "epoch: 1 step: 514, loss is 0.7318085432052612\n",
      "epoch: 1 step: 515, loss is 0.6799165606498718\n",
      "epoch: 1 step: 516, loss is 0.706736147403717\n",
      "epoch: 1 step: 517, loss is 0.6920675039291382\n",
      "epoch: 1 step: 518, loss is 0.751568078994751\n",
      "epoch: 1 step: 519, loss is 0.6587076783180237\n",
      "epoch: 1 step: 520, loss is 0.8762940764427185\n",
      "epoch: 1 step: 521, loss is 0.6469528079032898\n",
      "epoch: 1 step: 522, loss is 0.674028754234314\n",
      "epoch: 1 step: 523, loss is 0.7744230031967163\n",
      "epoch: 1 step: 524, loss is 0.7273899912834167\n",
      "epoch: 1 step: 525, loss is 0.6959407925605774\n",
      "epoch: 1 step: 526, loss is 0.6835242509841919\n",
      "epoch: 1 step: 527, loss is 0.6678444743156433\n",
      "epoch: 1 step: 528, loss is 0.7890467643737793\n",
      "epoch: 1 step: 529, loss is 0.7573953866958618\n",
      "epoch: 1 step: 530, loss is 0.6441060304641724\n",
      "epoch: 1 step: 531, loss is 0.7537107467651367\n",
      "epoch: 1 step: 532, loss is 0.6973928809165955\n",
      "epoch: 1 step: 533, loss is 0.7190638184547424\n",
      "epoch: 1 step: 534, loss is 0.6773074269294739\n",
      "epoch: 1 step: 535, loss is 0.6869417428970337\n",
      "epoch: 1 step: 536, loss is 0.6635653972625732\n",
      "epoch: 1 step: 537, loss is 0.6592361330986023\n",
      "epoch: 1 step: 538, loss is 0.6655515432357788\n",
      "epoch: 1 step: 539, loss is 0.8463445901870728\n",
      "epoch: 1 step: 540, loss is 0.674072265625\n",
      "epoch: 1 step: 541, loss is 0.8356231451034546\n",
      "epoch: 1 step: 542, loss is 0.7614985108375549\n",
      "epoch: 1 step: 543, loss is 0.7344233989715576\n",
      "epoch: 1 step: 544, loss is 0.7123041152954102\n",
      "epoch: 1 step: 545, loss is 0.668475329875946\n",
      "epoch: 1 step: 546, loss is 0.7174252867698669\n",
      "epoch: 1 step: 547, loss is 0.7140905261039734\n",
      "epoch: 1 step: 548, loss is 0.7084529995918274\n",
      "epoch: 1 step: 549, loss is 0.5868868231773376\n",
      "epoch: 1 step: 550, loss is 0.8191061615943909\n",
      "epoch: 1 step: 551, loss is 0.858220100402832\n",
      "epoch: 1 step: 552, loss is 0.5835989713668823\n",
      "epoch: 1 step: 553, loss is 0.7004299163818359\n",
      "epoch: 1 step: 554, loss is 0.618495523929596\n",
      "epoch: 1 step: 555, loss is 0.7012472152709961\n",
      "epoch: 1 step: 556, loss is 0.7288448214530945\n",
      "epoch: 1 step: 557, loss is 0.6938348412513733\n",
      "epoch: 1 step: 558, loss is 0.7015101909637451\n",
      "epoch: 1 step: 559, loss is 0.6689728498458862\n",
      "epoch: 1 step: 560, loss is 0.6825202703475952\n",
      "epoch: 1 step: 561, loss is 0.6843059659004211\n",
      "epoch: 1 step: 562, loss is 0.7736417651176453\n",
      "epoch: 1 step: 563, loss is 0.7218198776245117\n",
      "epoch: 1 step: 564, loss is 0.6783342957496643\n",
      "epoch: 1 step: 565, loss is 0.7393494844436646\n",
      "epoch: 1 step: 566, loss is 0.7240429520606995\n",
      "epoch: 1 step: 567, loss is 0.6672167181968689\n",
      "epoch: 1 step: 568, loss is 0.7074822783470154\n",
      "epoch: 1 step: 569, loss is 0.7815983295440674\n",
      "epoch: 1 step: 570, loss is 0.7091975212097168\n",
      "epoch: 1 step: 571, loss is 0.5022181868553162\n",
      "epoch: 1 step: 572, loss is 0.8919098973274231\n",
      "epoch: 1 step: 573, loss is 0.6754168272018433\n",
      "epoch: 1 step: 574, loss is 0.783396303653717\n",
      "epoch: 1 step: 575, loss is 0.5906481742858887\n",
      "epoch: 1 step: 576, loss is 0.8260959982872009\n",
      "epoch: 1 step: 577, loss is 0.9124594330787659\n",
      "epoch: 1 step: 578, loss is 0.6949028968811035\n",
      "epoch: 1 step: 579, loss is 0.7161986231803894\n",
      "epoch: 1 step: 580, loss is 0.7563641667366028\n",
      "epoch: 1 step: 581, loss is 0.8631442785263062\n",
      "epoch: 1 step: 582, loss is 0.7556370496749878\n",
      "epoch: 1 step: 583, loss is 0.7613516449928284\n",
      "epoch: 1 step: 584, loss is 0.7780779600143433\n",
      "epoch: 1 step: 585, loss is 0.6494396924972534\n",
      "epoch: 1 step: 586, loss is 0.7161429524421692\n",
      "epoch: 1 step: 587, loss is 0.5712428092956543\n",
      "epoch: 1 step: 588, loss is 0.8591054081916809\n",
      "epoch: 1 step: 589, loss is 0.7235321402549744\n",
      "epoch: 1 step: 590, loss is 0.7363543510437012\n",
      "epoch: 1 step: 591, loss is 0.7031717896461487\n",
      "epoch: 1 step: 592, loss is 0.6829116344451904\n",
      "epoch: 1 step: 593, loss is 0.6700308918952942\n",
      "epoch: 1 step: 594, loss is 0.6978782415390015\n",
      "epoch: 1 step: 595, loss is 0.83763188123703\n",
      "epoch: 1 step: 596, loss is 0.7775817513465881\n",
      "epoch: 1 step: 597, loss is 0.7830075025558472\n",
      "epoch: 1 step: 598, loss is 0.763256847858429\n",
      "epoch: 1 step: 599, loss is 0.7735875844955444\n",
      "epoch: 1 step: 600, loss is 0.7954089641571045\n",
      "epoch: 1 step: 601, loss is 0.5859075784683228\n",
      "epoch: 1 step: 602, loss is 0.6457902789115906\n",
      "epoch: 1 step: 603, loss is 0.6691065430641174\n",
      "epoch: 1 step: 604, loss is 0.779186487197876\n",
      "epoch: 1 step: 605, loss is 0.662187933921814\n",
      "epoch: 1 step: 606, loss is 0.7229437828063965\n",
      "epoch: 1 step: 607, loss is 0.6061825752258301\n",
      "epoch: 1 step: 608, loss is 0.7724398970603943\n",
      "epoch: 1 step: 609, loss is 0.6938077211380005\n",
      "epoch: 1 step: 610, loss is 0.7046313285827637\n",
      "epoch: 1 step: 611, loss is 0.7255847454071045\n",
      "epoch: 1 step: 612, loss is 0.6938626170158386\n",
      "epoch: 1 step: 613, loss is 0.6644906997680664\n",
      "epoch: 1 step: 614, loss is 0.6390430927276611\n",
      "epoch: 1 step: 615, loss is 0.6095718741416931\n",
      "epoch: 1 step: 616, loss is 0.7035120129585266\n",
      "epoch: 1 step: 617, loss is 0.306695818901062\n",
      "epoch: 1 step: 618, loss is 1.3029512166976929\n",
      "epoch: 1 step: 619, loss is 0.9023047089576721\n",
      "epoch: 1 step: 620, loss is 0.6070308089256287\n",
      "epoch: 1 step: 621, loss is 0.5711228251457214\n",
      "epoch: 1 step: 622, loss is 1.1165269613265991\n",
      "epoch: 1 step: 623, loss is 0.7557805180549622\n",
      "epoch: 1 step: 624, loss is 0.7545735836029053\n",
      "epoch: 1 step: 625, loss is 0.6855190396308899\n",
      "epoch: 1 step: 626, loss is 0.7389593124389648\n",
      "epoch: 1 step: 627, loss is 0.6676599383354187\n",
      "epoch: 1 step: 628, loss is 0.7876838445663452\n",
      "epoch: 1 step: 629, loss is 0.5442364811897278\n",
      "epoch: 1 step: 630, loss is 0.7637819647789001\n",
      "epoch: 1 step: 631, loss is 0.5804076194763184\n",
      "epoch: 1 step: 632, loss is 0.730450451374054\n",
      "epoch: 1 step: 633, loss is 0.7515398263931274\n",
      "epoch: 1 step: 634, loss is 0.7516121864318848\n",
      "epoch: 1 step: 635, loss is 1.2073678970336914\n",
      "epoch: 1 step: 636, loss is 0.7594048380851746\n",
      "epoch: 1 step: 637, loss is 0.7594229578971863\n",
      "epoch: 1 step: 638, loss is 0.6774744391441345\n",
      "epoch: 1 step: 639, loss is 0.6923453211784363\n",
      "epoch: 1 step: 640, loss is 0.6804304122924805\n",
      "epoch: 1 step: 641, loss is 0.6917896866798401\n",
      "epoch: 1 step: 642, loss is 0.7214285731315613\n",
      "epoch: 1 step: 643, loss is 0.7132645845413208\n",
      "epoch: 1 step: 644, loss is 0.699390172958374\n",
      "epoch: 1 step: 645, loss is 0.6723405718803406\n",
      "epoch: 1 step: 646, loss is 0.6759589314460754\n",
      "epoch: 1 step: 647, loss is 0.7462915778160095\n",
      "epoch: 1 step: 648, loss is 0.6765217781066895\n",
      "epoch: 1 step: 649, loss is 0.6683257818222046\n",
      "epoch: 1 step: 650, loss is 0.7232837080955505\n",
      "epoch: 1 step: 651, loss is 0.7361419796943665\n",
      "epoch: 1 step: 652, loss is 0.6630768775939941\n",
      "epoch: 1 step: 653, loss is 0.693329393863678\n",
      "epoch: 1 step: 654, loss is 0.6709952354431152\n",
      "epoch: 1 step: 655, loss is 0.7240356802940369\n",
      "epoch: 1 step: 656, loss is 0.6842256784439087\n",
      "epoch: 1 step: 657, loss is 0.6652918457984924\n",
      "epoch: 1 step: 658, loss is 0.7773534059524536\n",
      "epoch: 1 step: 659, loss is 0.6510279774665833\n",
      "epoch: 1 step: 660, loss is 0.6916829347610474\n",
      "epoch: 1 step: 661, loss is 0.6738802194595337\n",
      "epoch: 1 step: 662, loss is 0.6838188767433167\n",
      "epoch: 1 step: 663, loss is 0.7177702784538269\n",
      "epoch: 1 step: 664, loss is 0.719495952129364\n",
      "epoch: 1 step: 665, loss is 0.6592226624488831\n",
      "epoch: 1 step: 666, loss is 0.6754310131072998\n",
      "epoch: 1 step: 667, loss is 0.806047260761261\n",
      "epoch: 1 step: 668, loss is 0.7060800194740295\n",
      "epoch: 1 step: 669, loss is 0.7771828174591064\n",
      "epoch: 1 step: 670, loss is 0.8255965113639832\n",
      "epoch: 1 step: 671, loss is 0.7415745258331299\n",
      "epoch: 1 step: 672, loss is 0.7443087100982666\n",
      "epoch: 1 step: 673, loss is 0.6987313628196716\n",
      "epoch: 1 step: 674, loss is 0.7220005989074707\n",
      "epoch: 1 step: 675, loss is 0.6204487681388855\n",
      "epoch: 1 step: 676, loss is 0.6607037782669067\n",
      "epoch: 1 step: 677, loss is 0.6654446125030518\n",
      "epoch: 1 step: 678, loss is 0.8345496654510498\n",
      "epoch: 1 step: 679, loss is 0.8649468421936035\n",
      "epoch: 1 step: 680, loss is 1.0640016794204712\n",
      "epoch: 1 step: 681, loss is 0.6710200309753418\n",
      "epoch: 1 step: 682, loss is 0.6344200968742371\n",
      "epoch: 1 step: 683, loss is 0.7013857364654541\n",
      "epoch: 1 step: 684, loss is 0.6858894228935242\n",
      "epoch: 1 step: 685, loss is 0.6968349814414978\n",
      "epoch: 1 step: 686, loss is 0.7034105658531189\n",
      "epoch: 1 step: 687, loss is 0.6849778294563293\n",
      "epoch: 1 step: 688, loss is 0.6964603066444397\n",
      "epoch: 1 step: 689, loss is 0.7133919596672058\n",
      "epoch: 1 step: 690, loss is 0.7156533002853394\n",
      "epoch: 1 step: 691, loss is 0.6725257039070129\n",
      "epoch: 1 step: 692, loss is 0.7357532978057861\n",
      "epoch: 1 step: 693, loss is 0.721349835395813\n",
      "epoch: 1 step: 694, loss is 0.712847113609314\n",
      "epoch: 1 step: 695, loss is 0.6866499185562134\n",
      "epoch: 1 step: 696, loss is 0.7570339441299438\n",
      "epoch: 1 step: 697, loss is 0.6998721957206726\n",
      "epoch: 1 step: 698, loss is 0.6834080815315247\n",
      "epoch: 1 step: 699, loss is 0.6779268383979797\n",
      "epoch: 1 step: 700, loss is 0.6457530856132507\n",
      "epoch: 1 step: 701, loss is 0.7312667965888977\n",
      "epoch: 1 step: 702, loss is 0.6870981454849243\n",
      "epoch: 1 step: 703, loss is 0.7399647235870361\n",
      "epoch: 1 step: 704, loss is 0.6883378028869629\n",
      "epoch: 1 step: 705, loss is 0.6216745972633362\n",
      "epoch: 1 step: 706, loss is 0.6271198987960815\n",
      "epoch: 1 step: 707, loss is 0.6011012196540833\n",
      "epoch: 1 step: 708, loss is 0.7939441800117493\n",
      "epoch: 1 step: 709, loss is 0.5288426876068115\n",
      "epoch: 1 step: 710, loss is 0.6032090187072754\n",
      "epoch: 1 step: 711, loss is 0.893475353717804\n",
      "epoch: 1 step: 712, loss is 0.8663368225097656\n",
      "epoch: 1 step: 713, loss is 0.6690077185630798\n",
      "epoch: 1 step: 714, loss is 0.7360825538635254\n",
      "epoch: 1 step: 715, loss is 0.657262921333313\n",
      "epoch: 1 step: 716, loss is 0.6686218976974487\n",
      "epoch: 1 step: 717, loss is 0.6805985569953918\n",
      "epoch: 1 step: 718, loss is 0.706700325012207\n",
      "epoch: 1 step: 719, loss is 0.6769808530807495\n",
      "epoch: 1 step: 720, loss is 0.7563628554344177\n",
      "epoch: 1 step: 721, loss is 0.7139517664909363\n",
      "epoch: 1 step: 722, loss is 0.6789870858192444\n",
      "epoch: 1 step: 723, loss is 0.6848547458648682\n",
      "epoch: 1 step: 724, loss is 0.6549571752548218\n",
      "epoch: 1 step: 725, loss is 0.7608153223991394\n",
      "epoch: 1 step: 726, loss is 0.8398130536079407\n",
      "epoch: 1 step: 727, loss is 0.5638481974601746\n",
      "epoch: 1 step: 728, loss is 0.7964814901351929\n",
      "epoch: 1 step: 729, loss is 0.6917716860771179\n",
      "epoch: 1 step: 730, loss is 0.7126163244247437\n",
      "epoch: 1 step: 731, loss is 0.6862938404083252\n",
      "epoch: 1 step: 732, loss is 0.6054927110671997\n",
      "epoch: 1 step: 733, loss is 0.7613052725791931\n",
      "epoch: 1 step: 734, loss is 0.7034521102905273\n",
      "epoch: 1 step: 735, loss is 0.7285062670707703\n",
      "epoch: 1 step: 736, loss is 0.7484207153320312\n",
      "epoch: 1 step: 737, loss is 0.7876563668251038\n",
      "epoch: 1 step: 738, loss is 0.7740691304206848\n",
      "epoch: 1 step: 739, loss is 0.6428937315940857\n",
      "epoch: 1 step: 740, loss is 0.6682565808296204\n",
      "epoch: 1 step: 741, loss is 0.7433128356933594\n",
      "epoch: 1 step: 742, loss is 0.6959797739982605\n",
      "epoch: 1 step: 743, loss is 0.7180962562561035\n",
      "epoch: 1 step: 744, loss is 0.7057666182518005\n",
      "epoch: 1 step: 745, loss is 0.6549543738365173\n",
      "epoch: 1 step: 746, loss is 0.7543886303901672\n",
      "epoch: 1 step: 747, loss is 0.6900927424430847\n",
      "epoch: 1 step: 748, loss is 0.5922741293907166\n",
      "epoch: 1 step: 749, loss is 0.7405349612236023\n",
      "epoch: 1 step: 750, loss is 0.7381783723831177\n",
      "epoch: 1 step: 751, loss is 0.7401283383369446\n",
      "epoch: 1 step: 752, loss is 0.6804629564285278\n",
      "epoch: 1 step: 753, loss is 0.6591196060180664\n",
      "epoch: 1 step: 754, loss is 0.7264400124549866\n",
      "epoch: 1 step: 755, loss is 0.6807463765144348\n",
      "epoch: 1 step: 756, loss is 0.7560850381851196\n",
      "epoch: 1 step: 757, loss is 0.6893157362937927\n",
      "epoch: 1 step: 758, loss is 0.7990731596946716\n",
      "epoch: 1 step: 759, loss is 0.6543876528739929\n",
      "epoch: 1 step: 760, loss is 0.6578259468078613\n",
      "epoch: 1 step: 761, loss is 0.6060057282447815\n",
      "epoch: 1 step: 762, loss is 0.8536421656608582\n",
      "epoch: 1 step: 763, loss is 0.6672192811965942\n",
      "epoch: 1 step: 764, loss is 0.6904407739639282\n",
      "epoch: 1 step: 765, loss is 0.7187662124633789\n",
      "epoch: 1 step: 766, loss is 0.7268169522285461\n",
      "epoch: 1 step: 767, loss is 0.6260350942611694\n",
      "epoch: 1 step: 768, loss is 0.865915060043335\n",
      "epoch: 1 step: 769, loss is 0.9262105226516724\n",
      "epoch: 1 step: 770, loss is 0.7850832343101501\n",
      "epoch: 1 step: 771, loss is 0.6526181697845459\n",
      "epoch: 1 step: 772, loss is 0.7278667688369751\n",
      "epoch: 1 step: 773, loss is 0.6651688814163208\n",
      "epoch: 1 step: 774, loss is 0.6757951378822327\n",
      "epoch: 1 step: 775, loss is 0.71160489320755\n",
      "epoch: 1 step: 776, loss is 0.6546727418899536\n",
      "epoch: 1 step: 777, loss is 0.7522814869880676\n",
      "epoch: 1 step: 778, loss is 0.7077133655548096\n",
      "epoch: 1 step: 779, loss is 0.6843372583389282\n",
      "epoch: 1 step: 780, loss is 0.6821808815002441\n",
      "epoch: 1 step: 781, loss is 0.6815842986106873\n",
      "epoch: 1 step: 782, loss is 0.8197216987609863\n",
      "epoch: 1 step: 783, loss is 0.6977851390838623\n",
      "epoch: 1 step: 784, loss is 0.8257490396499634\n",
      "epoch: 1 step: 785, loss is 0.6388660669326782\n",
      "epoch: 1 step: 786, loss is 0.6574376821517944\n",
      "epoch: 1 step: 787, loss is 0.7250220775604248\n",
      "epoch: 1 step: 788, loss is 0.7109934687614441\n",
      "epoch: 1 step: 789, loss is 0.6907222270965576\n",
      "epoch: 1 step: 790, loss is 0.6911368370056152\n",
      "epoch: 1 step: 791, loss is 0.7480365633964539\n",
      "epoch: 1 step: 792, loss is 0.8020829558372498\n",
      "epoch: 1 step: 793, loss is 0.7694889903068542\n",
      "epoch: 1 step: 794, loss is 0.7511709928512573\n",
      "epoch: 1 step: 795, loss is 0.7502333521842957\n",
      "epoch: 1 step: 796, loss is 0.6823105216026306\n",
      "epoch: 1 step: 797, loss is 0.7449565529823303\n",
      "epoch: 1 step: 798, loss is 0.7165588140487671\n",
      "epoch: 1 step: 799, loss is 0.788878321647644\n",
      "epoch: 1 step: 800, loss is 0.7877737879753113\n",
      "epoch: 1 step: 801, loss is 0.627120316028595\n",
      "epoch: 1 step: 802, loss is 0.7758841514587402\n",
      "epoch: 1 step: 803, loss is 0.7394062876701355\n",
      "epoch: 1 step: 804, loss is 0.7684549689292908\n",
      "epoch: 1 step: 805, loss is 0.6999675631523132\n",
      "epoch: 1 step: 806, loss is 0.6846440434455872\n",
      "epoch: 1 step: 807, loss is 0.6917791366577148\n",
      "epoch: 1 step: 808, loss is 0.7117602229118347\n",
      "epoch: 1 step: 809, loss is 0.7296677827835083\n",
      "epoch: 1 step: 810, loss is 0.7650659084320068\n",
      "epoch: 1 step: 811, loss is 0.6631144285202026\n",
      "epoch: 1 step: 812, loss is 0.6297802925109863\n",
      "epoch: 1 step: 813, loss is 0.7408210039138794\n",
      "epoch: 1 step: 814, loss is 0.6176625490188599\n",
      "epoch: 1 step: 815, loss is 0.7348978519439697\n",
      "epoch: 1 step: 816, loss is 0.7262523174285889\n",
      "epoch: 1 step: 817, loss is 0.6432908773422241\n",
      "epoch: 1 step: 818, loss is 0.8440670371055603\n",
      "epoch: 1 step: 819, loss is 0.776806116104126\n",
      "epoch: 1 step: 820, loss is 0.7324414253234863\n",
      "epoch: 1 step: 821, loss is 0.7277319431304932\n",
      "epoch: 1 step: 822, loss is 0.6598487496376038\n",
      "epoch: 1 step: 823, loss is 0.6989285945892334\n",
      "epoch: 1 step: 824, loss is 0.6962273716926575\n",
      "epoch: 1 step: 825, loss is 0.7095187902450562\n",
      "epoch: 1 step: 826, loss is 0.6929889917373657\n",
      "epoch: 1 step: 827, loss is 0.8265902400016785\n",
      "epoch: 1 step: 828, loss is 0.659318208694458\n",
      "epoch: 1 step: 829, loss is 0.7106783390045166\n",
      "epoch: 1 step: 830, loss is 0.6600633859634399\n",
      "epoch: 1 step: 831, loss is 0.6373266577720642\n",
      "epoch: 1 step: 832, loss is 0.6543059349060059\n",
      "epoch: 1 step: 833, loss is 0.6463180780410767\n",
      "epoch: 1 step: 834, loss is 0.7154814600944519\n",
      "epoch: 1 step: 835, loss is 0.7103720903396606\n",
      "epoch: 1 step: 836, loss is 0.5919196009635925\n",
      "epoch: 1 step: 837, loss is 0.7909801006317139\n",
      "epoch: 1 step: 838, loss is 0.5377771258354187\n",
      "epoch: 1 step: 839, loss is 0.9409052133560181\n",
      "epoch: 1 step: 840, loss is 0.9629126787185669\n",
      "epoch: 1 step: 841, loss is 0.6813971400260925\n",
      "epoch: 1 step: 842, loss is 0.725908637046814\n",
      "epoch: 1 step: 843, loss is 0.6389241218566895\n",
      "epoch: 1 step: 844, loss is 0.6654725074768066\n",
      "epoch: 1 step: 845, loss is 0.6547232270240784\n",
      "epoch: 1 step: 846, loss is 0.5212889313697815\n",
      "epoch: 1 step: 847, loss is 0.5834775567054749\n",
      "epoch: 1 step: 848, loss is 0.7509170770645142\n",
      "epoch: 1 step: 849, loss is 1.0470068454742432\n",
      "epoch: 1 step: 850, loss is 1.1277326345443726\n",
      "epoch: 1 step: 851, loss is 1.0487064123153687\n",
      "epoch: 1 step: 852, loss is 1.118525743484497\n",
      "epoch: 1 step: 853, loss is 0.8612616658210754\n",
      "epoch: 1 step: 854, loss is 0.4964389503002167\n",
      "epoch: 1 step: 855, loss is 0.784314751625061\n",
      "epoch: 1 step: 856, loss is 0.727077841758728\n",
      "epoch: 1 step: 857, loss is 0.7737796902656555\n",
      "epoch: 1 step: 858, loss is 0.8021118640899658\n",
      "epoch: 1 step: 859, loss is 0.7766036987304688\n",
      "epoch: 1 step: 860, loss is 0.7402569651603699\n",
      "epoch: 1 step: 861, loss is 0.8030300736427307\n",
      "epoch: 1 step: 862, loss is 0.7288340330123901\n",
      "epoch: 1 step: 863, loss is 0.5984016060829163\n",
      "epoch: 1 step: 864, loss is 0.7422305941581726\n",
      "epoch: 1 step: 865, loss is 0.6222652196884155\n",
      "epoch: 1 step: 866, loss is 0.8651042580604553\n",
      "epoch: 1 step: 867, loss is 0.8147046566009521\n",
      "epoch: 1 step: 868, loss is 0.7116411924362183\n",
      "epoch: 1 step: 869, loss is 0.7706754803657532\n",
      "epoch: 1 step: 870, loss is 0.6155543923377991\n",
      "epoch: 1 step: 871, loss is 0.7083740830421448\n",
      "epoch: 1 step: 872, loss is 0.695067822933197\n",
      "epoch: 1 step: 873, loss is 0.7061174511909485\n",
      "epoch: 1 step: 874, loss is 0.6603596210479736\n",
      "epoch: 1 step: 875, loss is 0.7279289364814758\n",
      "epoch: 1 step: 876, loss is 0.7013647556304932\n",
      "epoch: 1 step: 877, loss is 0.6638068556785583\n",
      "epoch: 1 step: 878, loss is 0.700278639793396\n",
      "epoch: 1 step: 879, loss is 0.7896146774291992\n",
      "epoch: 1 step: 880, loss is 0.6621005535125732\n",
      "epoch: 1 step: 881, loss is 0.657857358455658\n",
      "epoch: 1 step: 882, loss is 0.7436173558235168\n",
      "epoch: 1 step: 883, loss is 0.6725724935531616\n",
      "epoch: 1 step: 884, loss is 0.6864303350448608\n",
      "epoch: 1 step: 885, loss is 0.6764556765556335\n",
      "epoch: 1 step: 886, loss is 0.7718451023101807\n",
      "epoch: 1 step: 887, loss is 0.859147846698761\n",
      "epoch: 1 step: 888, loss is 0.7529152631759644\n",
      "epoch: 1 step: 889, loss is 0.6501339673995972\n",
      "epoch: 1 step: 890, loss is 0.6964750289916992\n",
      "epoch: 1 step: 891, loss is 0.683688759803772\n",
      "epoch: 1 step: 892, loss is 0.6985553503036499\n",
      "epoch: 1 step: 893, loss is 0.7625551223754883\n",
      "epoch: 1 step: 894, loss is 0.7335109114646912\n",
      "epoch: 1 step: 895, loss is 0.6827303767204285\n",
      "epoch: 1 step: 896, loss is 0.7106098532676697\n",
      "epoch: 1 step: 897, loss is 0.7041622400283813\n",
      "epoch: 1 step: 898, loss is 0.6251362562179565\n",
      "epoch: 1 step: 899, loss is 0.6976949572563171\n",
      "epoch: 1 step: 900, loss is 0.9732933640480042\n",
      "epoch: 1 step: 901, loss is 0.5538350343704224\n",
      "epoch: 1 step: 902, loss is 1.034088373184204\n",
      "epoch: 1 step: 903, loss is 0.6872321367263794\n",
      "epoch: 1 step: 904, loss is 0.8079062700271606\n",
      "epoch: 1 step: 905, loss is 0.520004391670227\n",
      "epoch: 1 step: 906, loss is 0.9091348052024841\n",
      "epoch: 1 step: 907, loss is 0.7595379948616028\n",
      "epoch: 1 step: 908, loss is 0.6433634757995605\n",
      "epoch: 1 step: 909, loss is 0.6831355094909668\n",
      "epoch: 1 step: 910, loss is 0.7264494299888611\n",
      "epoch: 1 step: 911, loss is 0.6589741706848145\n",
      "epoch: 1 step: 912, loss is 0.6958218216896057\n",
      "epoch: 1 step: 913, loss is 0.7149680852890015\n",
      "epoch: 1 step: 914, loss is 0.7612252235412598\n",
      "epoch: 1 step: 915, loss is 0.6995989084243774\n",
      "epoch: 1 step: 916, loss is 0.6464042663574219\n",
      "epoch: 1 step: 917, loss is 0.6958994269371033\n",
      "epoch: 1 step: 918, loss is 0.6466190814971924\n",
      "epoch: 1 step: 919, loss is 0.6429202556610107\n",
      "epoch: 1 step: 920, loss is 0.7616506814956665\n",
      "epoch: 1 step: 921, loss is 0.7230254411697388\n",
      "epoch: 1 step: 922, loss is 0.6272962093353271\n",
      "epoch: 1 step: 923, loss is 0.719745397567749\n",
      "epoch: 1 step: 924, loss is 0.739273190498352\n",
      "epoch: 1 step: 925, loss is 0.6688330769538879\n",
      "epoch: 1 step: 926, loss is 0.7124607563018799\n",
      "epoch: 1 step: 927, loss is 0.6180494427680969\n",
      "epoch: 1 step: 928, loss is 0.6438624262809753\n",
      "epoch: 1 step: 929, loss is 0.6798373460769653\n",
      "epoch: 1 step: 930, loss is 0.713871955871582\n",
      "epoch: 1 step: 931, loss is 0.7736706733703613\n",
      "epoch: 1 step: 932, loss is 0.6743954420089722\n",
      "epoch: 1 step: 933, loss is 0.7376587390899658\n",
      "epoch: 1 step: 934, loss is 0.6353987455368042\n",
      "epoch: 1 step: 935, loss is 0.686776876449585\n",
      "epoch: 1 step: 936, loss is 0.6898025274276733\n",
      "epoch: 1 step: 937, loss is 0.6291336417198181\n",
      "epoch: 1 step: 938, loss is 0.7682740092277527\n",
      "epoch: 1 step: 939, loss is 0.7304901480674744\n",
      "epoch: 1 step: 940, loss is 0.6946262121200562\n",
      "epoch: 1 step: 941, loss is 0.6815521717071533\n",
      "epoch: 1 step: 942, loss is 0.6291378736495972\n",
      "epoch: 1 step: 943, loss is 0.6877287030220032\n",
      "epoch: 1 step: 944, loss is 0.5764158368110657\n",
      "epoch: 1 step: 945, loss is 0.6901100277900696\n",
      "epoch: 1 step: 946, loss is 0.6492870450019836\n",
      "epoch: 1 step: 947, loss is 0.8503894805908203\n",
      "epoch: 1 step: 948, loss is 0.7454584240913391\n",
      "epoch: 1 step: 949, loss is 0.6406368017196655\n",
      "epoch: 1 step: 950, loss is 0.6649604439735413\n",
      "epoch: 1 step: 951, loss is 0.6689485907554626\n",
      "epoch: 1 step: 952, loss is 0.8330129384994507\n",
      "epoch: 1 step: 953, loss is 0.7027778029441833\n",
      "epoch: 1 step: 954, loss is 0.7766767144203186\n",
      "epoch: 1 step: 955, loss is 0.732567310333252\n",
      "epoch: 1 step: 956, loss is 0.8018779158592224\n",
      "epoch: 1 step: 957, loss is 0.6689382791519165\n",
      "epoch: 1 step: 958, loss is 0.6159411072731018\n",
      "epoch: 1 step: 959, loss is 0.5981032252311707\n",
      "epoch: 1 step: 960, loss is 0.6424335241317749\n",
      "epoch: 1 step: 961, loss is 0.6573013067245483\n",
      "epoch: 1 step: 962, loss is 0.9282602071762085\n",
      "epoch: 1 step: 963, loss is 0.4602245092391968\n",
      "epoch: 1 step: 964, loss is 1.1394178867340088\n",
      "epoch: 1 step: 965, loss is 0.7817392945289612\n",
      "epoch: 1 step: 966, loss is 0.7312818765640259\n",
      "epoch: 1 step: 967, loss is 0.7775985598564148\n",
      "epoch: 1 step: 968, loss is 0.781934916973114\n",
      "epoch: 1 step: 969, loss is 0.6625381112098694\n",
      "epoch: 1 step: 970, loss is 0.6881812810897827\n",
      "epoch: 1 step: 971, loss is 0.7367990612983704\n",
      "epoch: 1 step: 972, loss is 0.5967729687690735\n",
      "epoch: 1 step: 973, loss is 0.6263630390167236\n",
      "epoch: 1 step: 974, loss is 0.8701440095901489\n",
      "epoch: 1 step: 975, loss is 0.6883413791656494\n",
      "epoch: 1 step: 976, loss is 0.8347904086112976\n",
      "epoch: 1 step: 977, loss is 0.7923685908317566\n",
      "epoch: 1 step: 978, loss is 0.8658207058906555\n",
      "epoch: 1 step: 979, loss is 0.6728965044021606\n",
      "epoch: 1 step: 980, loss is 0.6279574036598206\n",
      "epoch: 1 step: 981, loss is 0.8008941411972046\n",
      "epoch: 1 step: 982, loss is 0.6769731640815735\n",
      "epoch: 1 step: 983, loss is 0.6923890113830566\n",
      "epoch: 1 step: 984, loss is 0.7137019038200378\n",
      "epoch: 1 step: 985, loss is 0.7167205810546875\n",
      "epoch: 1 step: 986, loss is 0.7279223203659058\n",
      "epoch: 1 step: 987, loss is 0.6711554527282715\n",
      "epoch: 1 step: 988, loss is 0.7313374280929565\n",
      "epoch: 1 step: 989, loss is 0.7867425680160522\n",
      "epoch: 1 step: 990, loss is 0.766828179359436\n",
      "epoch: 1 step: 991, loss is 0.7603048086166382\n",
      "epoch: 1 step: 992, loss is 0.6347277760505676\n",
      "epoch: 1 step: 993, loss is 0.6853206753730774\n",
      "epoch: 1 step: 994, loss is 0.6934705376625061\n",
      "epoch: 1 step: 995, loss is 0.7250460386276245\n",
      "epoch: 1 step: 996, loss is 0.7090126276016235\n",
      "epoch: 1 step: 997, loss is 0.7079327702522278\n",
      "epoch: 1 step: 998, loss is 0.6940585970878601\n",
      "epoch: 1 step: 999, loss is 0.6349045038223267\n",
      "epoch: 1 step: 1000, loss is 0.7491726875305176\n",
      "epoch: 1 step: 1001, loss is 0.6703435182571411\n",
      "epoch: 1 step: 1002, loss is 0.7624504566192627\n",
      "epoch: 1 step: 1003, loss is 0.8240220546722412\n",
      "epoch: 1 step: 1004, loss is 0.6528091430664062\n",
      "epoch: 1 step: 1005, loss is 0.6060318946838379\n",
      "epoch: 1 step: 1006, loss is 0.7177798748016357\n",
      "epoch: 1 step: 1007, loss is 0.6958242654800415\n",
      "epoch: 1 step: 1008, loss is 0.618647575378418\n",
      "epoch: 1 step: 1009, loss is 0.7254506349563599\n",
      "epoch: 1 step: 1010, loss is 0.7080168128013611\n",
      "epoch: 1 step: 1011, loss is 0.6626412272453308\n",
      "epoch: 1 step: 1012, loss is 0.6757150888442993\n",
      "epoch: 1 step: 1013, loss is 0.7035119533538818\n",
      "epoch: 1 step: 1014, loss is 0.6953946948051453\n",
      "epoch: 1 step: 1015, loss is 0.6825304627418518\n",
      "epoch: 1 step: 1016, loss is 0.5959137678146362\n",
      "epoch: 1 step: 1017, loss is 0.6496542692184448\n",
      "epoch: 1 step: 1018, loss is 0.7542279958724976\n",
      "epoch: 1 step: 1019, loss is 0.7018804550170898\n",
      "epoch: 1 step: 1020, loss is 0.6737245321273804\n",
      "epoch: 1 step: 1021, loss is 0.7618798613548279\n",
      "epoch: 1 step: 1022, loss is 0.6749012470245361\n",
      "epoch: 1 step: 1023, loss is 0.7134717702865601\n",
      "epoch: 1 step: 1024, loss is 0.6782817244529724\n",
      "epoch: 1 step: 1025, loss is 0.698142409324646\n",
      "epoch: 1 step: 1026, loss is 0.6575486660003662\n",
      "epoch: 1 step: 1027, loss is 0.7329552173614502\n",
      "epoch: 1 step: 1028, loss is 0.7063218355178833\n",
      "epoch: 1 step: 1029, loss is 0.6491543054580688\n",
      "epoch: 1 step: 1030, loss is 0.702032208442688\n",
      "epoch: 1 step: 1031, loss is 0.644456148147583\n",
      "epoch: 1 step: 1032, loss is 0.7635344862937927\n",
      "epoch: 1 step: 1033, loss is 0.6633687615394592\n",
      "epoch: 1 step: 1034, loss is 0.6428658962249756\n",
      "epoch: 1 step: 1035, loss is 0.7845945358276367\n",
      "epoch: 1 step: 1036, loss is 0.7224456071853638\n",
      "epoch: 1 step: 1037, loss is 0.7002173662185669\n",
      "epoch: 1 step: 1038, loss is 0.7920730710029602\n",
      "epoch: 1 step: 1039, loss is 0.6748279333114624\n",
      "epoch: 1 step: 1040, loss is 0.6860858201980591\n",
      "epoch: 1 step: 1041, loss is 0.6849492788314819\n",
      "epoch: 1 step: 1042, loss is 0.8120426535606384\n",
      "epoch: 1 step: 1043, loss is 0.5988561511039734\n",
      "epoch: 1 step: 1044, loss is 0.739952564239502\n",
      "epoch: 1 step: 1045, loss is 0.6830460429191589\n",
      "epoch: 1 step: 1046, loss is 0.6575887799263\n",
      "epoch: 1 step: 1047, loss is 0.7474139332771301\n",
      "epoch: 1 step: 1048, loss is 0.6467682719230652\n",
      "epoch: 1 step: 1049, loss is 0.7176849246025085\n",
      "epoch: 1 step: 1050, loss is 0.7730652093887329\n",
      "epoch: 1 step: 1051, loss is 0.6351314783096313\n",
      "epoch: 1 step: 1052, loss is 0.6284418702125549\n",
      "epoch: 1 step: 1053, loss is 0.6410333514213562\n",
      "epoch: 1 step: 1054, loss is 0.6954044103622437\n",
      "epoch: 1 step: 1055, loss is 0.7216150164604187\n",
      "epoch: 1 step: 1056, loss is 0.6881124377250671\n",
      "epoch: 1 step: 1057, loss is 0.7365918159484863\n",
      "epoch: 1 step: 1058, loss is 0.6874091029167175\n",
      "epoch: 1 step: 1059, loss is 0.6935625076293945\n",
      "epoch: 1 step: 1060, loss is 0.651968777179718\n",
      "epoch: 1 step: 1061, loss is 0.7200977206230164\n",
      "epoch: 1 step: 1062, loss is 0.6734430193901062\n",
      "epoch: 1 step: 1063, loss is 0.5990142226219177\n",
      "epoch: 1 step: 1064, loss is 0.627599835395813\n",
      "epoch: 1 step: 1065, loss is 0.576965868473053\n",
      "epoch: 1 step: 1066, loss is 0.6884069442749023\n",
      "epoch: 1 step: 1067, loss is 0.7729782462120056\n",
      "epoch: 1 step: 1068, loss is 0.9406306147575378\n",
      "epoch: 1 step: 1069, loss is 0.6371327638626099\n",
      "epoch: 1 step: 1070, loss is 0.6138807535171509\n",
      "epoch: 1 step: 1071, loss is 0.6930726766586304\n",
      "epoch: 1 step: 1072, loss is 0.682752251625061\n",
      "epoch: 1 step: 1073, loss is 0.6281108260154724\n",
      "epoch: 1 step: 1074, loss is 0.7476705312728882\n",
      "epoch: 1 step: 1075, loss is 0.711380660533905\n",
      "epoch: 1 step: 1076, loss is 0.7073220014572144\n",
      "epoch: 1 step: 1077, loss is 0.7764604091644287\n",
      "epoch: 1 step: 1078, loss is 0.7098318934440613\n",
      "epoch: 1 step: 1079, loss is 0.7836159467697144\n",
      "epoch: 1 step: 1080, loss is 0.7972098588943481\n",
      "epoch: 1 step: 1081, loss is 0.8105802536010742\n",
      "epoch: 1 step: 1082, loss is 0.7640153169631958\n",
      "epoch: 1 step: 1083, loss is 0.624058187007904\n",
      "epoch: 1 step: 1084, loss is 0.7409170269966125\n",
      "epoch: 1 step: 1085, loss is 0.6994355320930481\n",
      "epoch: 1 step: 1086, loss is 0.7238008975982666\n",
      "epoch: 1 step: 1087, loss is 0.7138804197311401\n",
      "epoch: 1 step: 1088, loss is 0.6686497330665588\n",
      "epoch: 1 step: 1089, loss is 0.6678096055984497\n",
      "epoch: 1 step: 1090, loss is 0.7514338493347168\n",
      "epoch: 1 step: 1091, loss is 0.6988154053688049\n",
      "epoch: 1 step: 1092, loss is 0.6492210626602173\n",
      "epoch: 1 step: 1093, loss is 0.6758459210395813\n",
      "epoch: 1 step: 1094, loss is 0.6857367753982544\n",
      "epoch: 1 step: 1095, loss is 0.6919299960136414\n",
      "epoch: 1 step: 1096, loss is 0.7013778686523438\n",
      "epoch: 1 step: 1097, loss is 0.6909926533699036\n",
      "epoch: 1 step: 1098, loss is 0.6900988221168518\n",
      "epoch: 1 step: 1099, loss is 0.7044734954833984\n",
      "epoch: 1 step: 1100, loss is 0.6957544088363647\n",
      "epoch: 1 step: 1101, loss is 0.6974188089370728\n",
      "epoch: 1 step: 1102, loss is 0.6772417426109314\n",
      "epoch: 1 step: 1103, loss is 0.6305832862854004\n",
      "epoch: 1 step: 1104, loss is 0.6938058137893677\n",
      "epoch: 1 step: 1105, loss is 0.7466146945953369\n",
      "epoch: 1 step: 1106, loss is 0.688072681427002\n",
      "epoch: 1 step: 1107, loss is 0.8154212832450867\n",
      "epoch: 1 step: 1108, loss is 0.6701176166534424\n",
      "epoch: 1 step: 1109, loss is 0.6774689555168152\n",
      "epoch: 1 step: 1110, loss is 0.6325691342353821\n",
      "epoch: 1 step: 1111, loss is 0.6858000755310059\n",
      "epoch: 1 step: 1112, loss is 0.6354072690010071\n",
      "epoch: 1 step: 1113, loss is 0.6332945823669434\n",
      "epoch: 1 step: 1114, loss is 0.7708055377006531\n",
      "epoch: 1 step: 1115, loss is 0.7575265169143677\n",
      "epoch: 1 step: 1116, loss is 0.6502089500427246\n",
      "epoch: 1 step: 1117, loss is 0.7042679786682129\n",
      "epoch: 1 step: 1118, loss is 0.7079126834869385\n",
      "epoch: 1 step: 1119, loss is 0.7554588317871094\n",
      "epoch: 1 step: 1120, loss is 0.6997422575950623\n",
      "epoch: 1 step: 1121, loss is 0.719405472278595\n",
      "epoch: 1 step: 1122, loss is 0.7263349890708923\n",
      "epoch: 1 step: 1123, loss is 0.7291756868362427\n",
      "epoch: 1 step: 1124, loss is 0.6986628174781799\n",
      "epoch: 1 step: 1125, loss is 0.7181462049484253\n",
      "epoch: 1 step: 1126, loss is 0.6357345581054688\n",
      "epoch: 1 step: 1127, loss is 0.6903216242790222\n",
      "epoch: 1 step: 1128, loss is 0.6554126143455505\n",
      "epoch: 1 step: 1129, loss is 0.9378972053527832\n",
      "epoch: 1 step: 1130, loss is 0.7347862124443054\n",
      "epoch: 1 step: 1131, loss is 0.7302160263061523\n",
      "epoch: 1 step: 1132, loss is 0.7162072658538818\n",
      "epoch: 1 step: 1133, loss is 0.6302165985107422\n",
      "epoch: 1 step: 1134, loss is 0.7388290166854858\n",
      "epoch: 1 step: 1135, loss is 0.7830608487129211\n",
      "epoch: 1 step: 1136, loss is 0.5890520811080933\n",
      "epoch: 1 step: 1137, loss is 0.6456702351570129\n",
      "epoch: 1 step: 1138, loss is 0.8527493476867676\n",
      "epoch: 1 step: 1139, loss is 0.6752939224243164\n",
      "epoch: 1 step: 1140, loss is 0.7572677731513977\n",
      "epoch: 1 step: 1141, loss is 0.7441769242286682\n",
      "epoch: 1 step: 1142, loss is 0.5969930291175842\n",
      "epoch: 1 step: 1143, loss is 0.7308441996574402\n",
      "epoch: 1 step: 1144, loss is 0.5387740135192871\n",
      "epoch: 1 step: 1145, loss is 0.7626879215240479\n",
      "epoch: 1 step: 1146, loss is 0.5306113958358765\n",
      "epoch: 1 step: 1147, loss is 0.7926366925239563\n",
      "epoch: 1 step: 1148, loss is 0.8288049697875977\n",
      "epoch: 1 step: 1149, loss is 0.7896708250045776\n",
      "epoch: 1 step: 1150, loss is 1.022472620010376\n",
      "epoch: 1 step: 1151, loss is 0.7742113471031189\n",
      "epoch: 1 step: 1152, loss is 0.7677157521247864\n",
      "epoch: 1 step: 1153, loss is 0.6148528456687927\n",
      "epoch: 1 step: 1154, loss is 0.7632625699043274\n",
      "epoch: 1 step: 1155, loss is 0.733964204788208\n",
      "epoch: 1 step: 1156, loss is 0.7565737962722778\n",
      "epoch: 1 step: 1157, loss is 0.7432846426963806\n",
      "epoch: 1 step: 1158, loss is 0.7340043187141418\n",
      "epoch: 1 step: 1159, loss is 0.6653137803077698\n",
      "epoch: 1 step: 1160, loss is 0.8525509834289551\n",
      "epoch: 1 step: 1161, loss is 0.8259814381599426\n",
      "epoch: 1 step: 1162, loss is 0.6568815112113953\n",
      "epoch: 1 step: 1163, loss is 0.6445538997650146\n",
      "epoch: 1 step: 1164, loss is 0.6663161516189575\n",
      "epoch: 1 step: 1165, loss is 0.7313509583473206\n",
      "epoch: 1 step: 1166, loss is 0.8844438195228577\n",
      "epoch: 1 step: 1167, loss is 0.7282634973526001\n",
      "epoch: 1 step: 1168, loss is 0.7589589953422546\n",
      "epoch: 1 step: 1169, loss is 0.5929747223854065\n",
      "epoch: 1 step: 1170, loss is 0.5773754715919495\n",
      "epoch: 1 step: 1171, loss is 0.7577944993972778\n",
      "epoch: 1 step: 1172, loss is 0.6613564491271973\n",
      "epoch: 1 step: 1173, loss is 0.6679028272628784\n",
      "epoch: 1 step: 1174, loss is 0.6665727496147156\n",
      "epoch: 1 step: 1175, loss is 0.6334529519081116\n",
      "epoch: 1 step: 1176, loss is 0.6353499889373779\n",
      "epoch: 1 step: 1177, loss is 0.773348331451416\n",
      "epoch: 1 step: 1178, loss is 0.717658519744873\n",
      "epoch: 1 step: 1179, loss is 0.8419987559318542\n",
      "epoch: 1 step: 1180, loss is 0.8569240570068359\n",
      "epoch: 1 step: 1181, loss is 0.7915701866149902\n",
      "epoch: 1 step: 1182, loss is 0.7253652215003967\n",
      "epoch: 1 step: 1183, loss is 0.7051905989646912\n",
      "epoch: 1 step: 1184, loss is 0.6434904336929321\n",
      "epoch: 1 step: 1185, loss is 0.7855973839759827\n",
      "epoch: 1 step: 1186, loss is 1.0542761087417603\n",
      "epoch: 1 step: 1187, loss is 0.6421664953231812\n",
      "epoch: 1 step: 1188, loss is 0.9297590255737305\n",
      "epoch: 1 step: 1189, loss is 0.679224967956543\n",
      "epoch: 1 step: 1190, loss is 0.7361570596694946\n",
      "epoch: 1 step: 1191, loss is 0.7730351090431213\n",
      "epoch: 1 step: 1192, loss is 0.6751299500465393\n",
      "epoch: 1 step: 1193, loss is 0.6894102692604065\n",
      "epoch: 1 step: 1194, loss is 0.7643592953681946\n",
      "epoch: 1 step: 1195, loss is 0.616414487361908\n",
      "epoch: 1 step: 1196, loss is 0.5889595150947571\n",
      "epoch: 1 step: 1197, loss is 0.7177098989486694\n",
      "epoch: 1 step: 1198, loss is 0.8061243295669556\n",
      "epoch: 1 step: 1199, loss is 0.6786720156669617\n",
      "epoch: 1 step: 1200, loss is 0.7995194792747498\n",
      "epoch: 1 step: 1201, loss is 0.5367687940597534\n",
      "epoch: 1 step: 1202, loss is 0.6614550948143005\n",
      "epoch: 1 step: 1203, loss is 0.6486947536468506\n",
      "epoch: 1 step: 1204, loss is 0.645551860332489\n",
      "epoch: 1 step: 1205, loss is 0.53984534740448\n",
      "epoch: 1 step: 1206, loss is 0.7060519456863403\n",
      "epoch: 1 step: 1207, loss is 0.6219662427902222\n",
      "epoch: 1 step: 1208, loss is 0.866940438747406\n",
      "epoch: 1 step: 1209, loss is 0.804015040397644\n",
      "epoch: 1 step: 1210, loss is 0.8007559776306152\n",
      "epoch: 1 step: 1211, loss is 0.7232721447944641\n",
      "epoch: 1 step: 1212, loss is 0.706116259098053\n",
      "epoch: 1 step: 1213, loss is 0.7032991647720337\n",
      "epoch: 1 step: 1214, loss is 0.7364828586578369\n",
      "epoch: 1 step: 1215, loss is 0.6758975982666016\n",
      "epoch: 1 step: 1216, loss is 0.5397859215736389\n",
      "epoch: 1 step: 1217, loss is 0.8330134749412537\n",
      "epoch: 1 step: 1218, loss is 0.6912345886230469\n",
      "epoch: 1 step: 1219, loss is 0.6127158403396606\n",
      "epoch: 1 step: 1220, loss is 0.8580237627029419\n",
      "epoch: 1 step: 1221, loss is 0.9303127527236938\n",
      "epoch: 1 step: 1222, loss is 0.704025387763977\n",
      "epoch: 1 step: 1223, loss is 0.7627976536750793\n",
      "epoch: 1 step: 1224, loss is 0.7243213057518005\n",
      "epoch: 1 step: 1225, loss is 0.685610294342041\n",
      "epoch: 1 step: 1226, loss is 0.6336047649383545\n",
      "epoch: 1 step: 1227, loss is 0.6121779680252075\n",
      "epoch: 1 step: 1228, loss is 0.6404061913490295\n",
      "epoch: 1 step: 1229, loss is 0.5761843323707581\n",
      "epoch: 1 step: 1230, loss is 1.0875405073165894\n",
      "epoch: 1 step: 1231, loss is 0.7932763695716858\n",
      "epoch: 1 step: 1232, loss is 0.5774456858634949\n",
      "epoch: 1 step: 1233, loss is 0.7032163739204407\n",
      "epoch: 1 step: 1234, loss is 0.7600405812263489\n",
      "epoch: 1 step: 1235, loss is 0.834865391254425\n",
      "epoch: 1 step: 1236, loss is 0.7648608088493347\n",
      "epoch: 1 step: 1237, loss is 0.8912391066551208\n",
      "epoch: 1 step: 1238, loss is 0.6380444765090942\n",
      "epoch: 1 step: 1239, loss is 0.7888885736465454\n",
      "epoch: 1 step: 1240, loss is 0.7520461082458496\n",
      "epoch: 1 step: 1241, loss is 0.7352369427680969\n",
      "epoch: 1 step: 1242, loss is 0.7036764621734619\n",
      "epoch: 1 step: 1243, loss is 0.7531514167785645\n",
      "epoch: 1 step: 1244, loss is 0.7085230946540833\n",
      "epoch: 1 step: 1245, loss is 0.7928373217582703\n",
      "epoch: 1 step: 1246, loss is 0.7432428598403931\n",
      "epoch: 1 step: 1247, loss is 0.668895959854126\n",
      "epoch: 1 step: 1248, loss is 0.6641218662261963\n",
      "epoch: 1 step: 1249, loss is 0.6136277914047241\n",
      "epoch: 1 step: 1250, loss is 0.6315001249313354\n",
      "epoch: 1 step: 1251, loss is 0.6364462971687317\n",
      "epoch: 1 step: 1252, loss is 0.708478569984436\n",
      "epoch: 1 step: 1253, loss is 0.789063572883606\n",
      "epoch: 1 step: 1254, loss is 0.9183644652366638\n",
      "epoch: 1 step: 1255, loss is 0.8282376527786255\n",
      "epoch: 1 step: 1256, loss is 0.6685347557067871\n",
      "epoch: 1 step: 1257, loss is 0.7214230895042419\n",
      "epoch: 1 step: 1258, loss is 0.657142698764801\n",
      "epoch: 1 step: 1259, loss is 0.7410725951194763\n",
      "epoch: 1 step: 1260, loss is 0.6859470009803772\n",
      "epoch: 1 step: 1261, loss is 0.7572064995765686\n",
      "epoch: 1 step: 1262, loss is 0.6233051419258118\n",
      "epoch: 1 step: 1263, loss is 0.7543724179267883\n",
      "epoch: 1 step: 1264, loss is 0.7400230169296265\n",
      "epoch: 1 step: 1265, loss is 0.7351163625717163\n",
      "epoch: 1 step: 1266, loss is 0.6962538361549377\n",
      "epoch: 1 step: 1267, loss is 0.6471221446990967\n",
      "epoch: 1 step: 1268, loss is 0.6836480498313904\n",
      "epoch: 1 step: 1269, loss is 0.6713248491287231\n",
      "epoch: 1 step: 1270, loss is 0.6593433618545532\n",
      "epoch: 1 step: 1271, loss is 0.7527957558631897\n",
      "epoch: 1 step: 1272, loss is 0.7166375517845154\n",
      "epoch: 1 step: 1273, loss is 0.6349424719810486\n",
      "epoch: 1 step: 1274, loss is 0.7408310770988464\n",
      "epoch: 1 step: 1275, loss is 0.593733012676239\n",
      "epoch: 1 step: 1276, loss is 0.6335104703903198\n",
      "epoch: 1 step: 1277, loss is 0.6678799390792847\n",
      "epoch: 1 step: 1278, loss is 0.6482089161872864\n",
      "epoch: 1 step: 1279, loss is 0.8400607109069824\n",
      "epoch: 1 step: 1280, loss is 0.6431765556335449\n",
      "epoch: 1 step: 1281, loss is 0.7444058060646057\n",
      "epoch: 1 step: 1282, loss is 0.6626213192939758\n",
      "epoch: 1 step: 1283, loss is 0.6821521520614624\n",
      "epoch: 1 step: 1284, loss is 0.7358556389808655\n",
      "epoch: 1 step: 1285, loss is 0.6339084506034851\n",
      "epoch: 1 step: 1286, loss is 0.5995657444000244\n",
      "epoch: 1 step: 1287, loss is 0.6931517720222473\n",
      "epoch: 1 step: 1288, loss is 0.6560765504837036\n",
      "epoch: 1 step: 1289, loss is 0.8733444809913635\n",
      "epoch: 1 step: 1290, loss is 0.6611475348472595\n",
      "epoch: 1 step: 1291, loss is 0.7087874412536621\n",
      "epoch: 1 step: 1292, loss is 0.8207936882972717\n",
      "epoch: 1 step: 1293, loss is 0.681067705154419\n",
      "epoch: 1 step: 1294, loss is 0.6366825103759766\n",
      "epoch: 1 step: 1295, loss is 0.6709696054458618\n",
      "epoch: 1 step: 1296, loss is 0.6244751811027527\n",
      "epoch: 1 step: 1297, loss is 0.7696784734725952\n",
      "epoch: 1 step: 1298, loss is 1.0245808362960815\n",
      "epoch: 1 step: 1299, loss is 0.6578423976898193\n",
      "epoch: 1 step: 1300, loss is 0.5573249459266663\n",
      "epoch: 1 step: 1301, loss is 0.7752498984336853\n",
      "epoch: 1 step: 1302, loss is 0.888400137424469\n",
      "epoch: 1 step: 1303, loss is 0.5955093502998352\n",
      "epoch: 1 step: 1304, loss is 0.7112860083580017\n",
      "epoch: 1 step: 1305, loss is 0.7108219861984253\n",
      "epoch: 1 step: 1306, loss is 0.7809377908706665\n",
      "epoch: 1 step: 1307, loss is 0.6448214054107666\n",
      "epoch: 1 step: 1308, loss is 0.6771185398101807\n",
      "epoch: 1 step: 1309, loss is 0.6514577269554138\n",
      "epoch: 1 step: 1310, loss is 0.7194485664367676\n",
      "epoch: 1 step: 1311, loss is 0.7075681090354919\n",
      "epoch: 1 step: 1312, loss is 0.6277226209640503\n",
      "epoch: 1 step: 1313, loss is 0.6980643272399902\n",
      "epoch: 1 step: 1314, loss is 0.6363543272018433\n",
      "epoch: 1 step: 1315, loss is 0.648476243019104\n",
      "epoch: 1 step: 1316, loss is 0.5871829986572266\n",
      "epoch: 1 step: 1317, loss is 0.895049512386322\n",
      "epoch: 1 step: 1318, loss is 0.8149259090423584\n",
      "epoch: 1 step: 1319, loss is 0.7735400199890137\n",
      "epoch: 1 step: 1320, loss is 0.6106152534484863\n",
      "epoch: 1 step: 1321, loss is 0.5828736424446106\n",
      "epoch: 1 step: 1322, loss is 0.7126351594924927\n",
      "epoch: 1 step: 1323, loss is 0.8580154776573181\n",
      "epoch: 1 step: 1324, loss is 0.8883327841758728\n",
      "epoch: 1 step: 1325, loss is 0.9195448160171509\n",
      "epoch: 1 step: 1326, loss is 0.748388409614563\n",
      "epoch: 1 step: 1327, loss is 0.7453460693359375\n",
      "epoch: 1 step: 1328, loss is 0.7099468111991882\n",
      "epoch: 1 step: 1329, loss is 0.7242476344108582\n",
      "epoch: 1 step: 1330, loss is 0.7053219079971313\n",
      "epoch: 1 step: 1331, loss is 0.6472625136375427\n",
      "epoch: 1 step: 1332, loss is 0.9104008674621582\n",
      "epoch: 1 step: 1333, loss is 0.7479699850082397\n",
      "epoch: 1 step: 1334, loss is 0.6100226640701294\n",
      "epoch: 1 step: 1335, loss is 0.8973562717437744\n",
      "epoch: 1 step: 1336, loss is 0.6067411303520203\n",
      "epoch: 1 step: 1337, loss is 0.8302143216133118\n",
      "epoch: 1 step: 1338, loss is 0.600968062877655\n",
      "epoch: 1 step: 1339, loss is 0.6616759300231934\n",
      "epoch: 1 step: 1340, loss is 0.7356956005096436\n",
      "epoch: 1 step: 1341, loss is 0.6612708568572998\n",
      "epoch: 1 step: 1342, loss is 0.6757084131240845\n",
      "epoch: 1 step: 1343, loss is 0.7037994265556335\n",
      "epoch: 1 step: 1344, loss is 0.8263451457023621\n",
      "epoch: 1 step: 1345, loss is 0.6647359132766724\n",
      "epoch: 1 step: 1346, loss is 0.7988348007202148\n",
      "epoch: 1 step: 1347, loss is 0.8813206553459167\n",
      "epoch: 1 step: 1348, loss is 0.6944385170936584\n",
      "epoch: 1 step: 1349, loss is 0.7350319027900696\n",
      "epoch: 1 step: 1350, loss is 0.7052084803581238\n",
      "epoch: 1 step: 1351, loss is 0.7223880887031555\n",
      "epoch: 1 step: 1352, loss is 0.6951701045036316\n",
      "epoch: 1 step: 1353, loss is 0.7355847954750061\n",
      "epoch: 1 step: 1354, loss is 0.6584551334381104\n",
      "epoch: 1 step: 1355, loss is 0.6960912942886353\n",
      "epoch: 1 step: 1356, loss is 0.7311724424362183\n",
      "epoch: 1 step: 1357, loss is 0.6684460043907166\n",
      "epoch: 1 step: 1358, loss is 0.6941887736320496\n",
      "epoch: 1 step: 1359, loss is 0.7386608123779297\n",
      "epoch: 1 step: 1360, loss is 0.6114861965179443\n",
      "epoch: 1 step: 1361, loss is 0.7014986276626587\n",
      "epoch: 1 step: 1362, loss is 0.6506679654121399\n",
      "epoch: 1 step: 1363, loss is 0.7422019243240356\n",
      "epoch: 1 step: 1364, loss is 0.8351004123687744\n",
      "epoch: 1 step: 1365, loss is 0.9241448640823364\n",
      "epoch: 1 step: 1366, loss is 0.680303692817688\n",
      "epoch: 1 step: 1367, loss is 0.8244264125823975\n",
      "epoch: 1 step: 1368, loss is 0.88392174243927\n",
      "epoch: 1 step: 1369, loss is 0.6733762621879578\n",
      "epoch: 1 step: 1370, loss is 0.630081832408905\n",
      "epoch: 1 step: 1371, loss is 0.8166614174842834\n",
      "epoch: 1 step: 1372, loss is 0.8629196286201477\n",
      "epoch: 1 step: 1373, loss is 0.6007519960403442\n",
      "epoch: 1 step: 1374, loss is 0.838528037071228\n",
      "epoch: 1 step: 1375, loss is 0.7809192538261414\n",
      "epoch: 1 step: 1376, loss is 0.8335258960723877\n",
      "epoch: 1 step: 1377, loss is 0.7651122808456421\n",
      "epoch: 1 step: 1378, loss is 0.7280818819999695\n",
      "epoch: 1 step: 1379, loss is 0.6811773777008057\n",
      "epoch: 1 step: 1380, loss is 0.6788196563720703\n",
      "epoch: 1 step: 1381, loss is 0.7389546632766724\n",
      "epoch: 1 step: 1382, loss is 0.6712796092033386\n",
      "epoch: 1 step: 1383, loss is 0.7191314697265625\n",
      "epoch: 1 step: 1384, loss is 0.8061583638191223\n",
      "epoch: 1 step: 1385, loss is 0.6466018557548523\n",
      "epoch: 1 step: 1386, loss is 0.717849850654602\n",
      "epoch: 1 step: 1387, loss is 0.7641668915748596\n",
      "epoch: 1 step: 1388, loss is 0.7251911759376526\n",
      "epoch: 1 step: 1389, loss is 0.696411669254303\n",
      "epoch: 1 step: 1390, loss is 0.7115548253059387\n",
      "epoch: 1 step: 1391, loss is 0.6813660860061646\n",
      "epoch: 1 step: 1392, loss is 0.7022949457168579\n",
      "epoch: 1 step: 1393, loss is 0.743889331817627\n",
      "epoch: 1 step: 1394, loss is 0.6542696356773376\n",
      "epoch: 1 step: 1395, loss is 0.6708943247795105\n",
      "epoch: 1 step: 1396, loss is 0.6805582046508789\n",
      "epoch: 1 step: 1397, loss is 0.7008688449859619\n",
      "epoch: 1 step: 1398, loss is 0.6522740721702576\n",
      "epoch: 1 step: 1399, loss is 0.7979249954223633\n",
      "epoch: 1 step: 1400, loss is 0.6263613700866699\n",
      "epoch: 1 step: 1401, loss is 0.8167649507522583\n",
      "epoch: 1 step: 1402, loss is 0.6617425680160522\n",
      "epoch: 1 step: 1403, loss is 0.6379689574241638\n",
      "epoch: 1 step: 1404, loss is 0.6587694883346558\n",
      "epoch: 1 step: 1405, loss is 0.6152039766311646\n",
      "epoch: 1 step: 1406, loss is 0.7041996121406555\n",
      "epoch: 1 step: 1407, loss is 0.775135338306427\n",
      "epoch: 1 step: 1408, loss is 0.6398876309394836\n",
      "epoch: 1 step: 1409, loss is 0.7486452460289001\n",
      "epoch: 1 step: 1410, loss is 0.7045391201972961\n",
      "epoch: 1 step: 1411, loss is 0.6019654870033264\n",
      "epoch: 1 step: 1412, loss is 0.6995814442634583\n",
      "epoch: 1 step: 1413, loss is 0.7152712941169739\n",
      "epoch: 1 step: 1414, loss is 0.6814510226249695\n",
      "epoch: 1 step: 1415, loss is 0.7772320508956909\n",
      "epoch: 1 step: 1416, loss is 0.737463116645813\n",
      "epoch: 1 step: 1417, loss is 0.6214737296104431\n",
      "epoch: 1 step: 1418, loss is 0.6526533365249634\n",
      "epoch: 1 step: 1419, loss is 0.6808022260665894\n",
      "epoch: 1 step: 1420, loss is 0.6925956010818481\n",
      "epoch: 1 step: 1421, loss is 0.6952522397041321\n",
      "epoch: 1 step: 1422, loss is 0.6408992409706116\n",
      "epoch: 1 step: 1423, loss is 0.682202160358429\n",
      "epoch: 1 step: 1424, loss is 0.6963070631027222\n",
      "epoch: 1 step: 1425, loss is 0.7160369157791138\n",
      "epoch: 1 step: 1426, loss is 0.656708836555481\n",
      "epoch: 1 step: 1427, loss is 0.6531711220741272\n",
      "epoch: 1 step: 1428, loss is 0.6240329146385193\n",
      "epoch: 1 step: 1429, loss is 0.7000393271446228\n",
      "epoch: 1 step: 1430, loss is 0.65785813331604\n",
      "epoch: 1 step: 1431, loss is 0.7583226561546326\n",
      "epoch: 1 step: 1432, loss is 0.6485774517059326\n",
      "epoch: 1 step: 1433, loss is 0.6849035620689392\n",
      "epoch: 1 step: 1434, loss is 0.6494998335838318\n",
      "epoch: 1 step: 1435, loss is 0.69461989402771\n",
      "epoch: 1 step: 1436, loss is 0.6355839371681213\n",
      "epoch: 1 step: 1437, loss is 0.6954305768013\n",
      "epoch: 1 step: 1438, loss is 0.7033350467681885\n",
      "epoch: 1 step: 1439, loss is 0.7544334530830383\n",
      "epoch: 1 step: 1440, loss is 0.5391923189163208\n",
      "epoch: 1 step: 1441, loss is 0.6274251937866211\n",
      "epoch: 1 step: 1442, loss is 0.9688886404037476\n",
      "epoch: 1 step: 1443, loss is 0.6502743363380432\n",
      "epoch: 1 step: 1444, loss is 0.8865930438041687\n",
      "epoch: 1 step: 1445, loss is 0.7078638076782227\n",
      "epoch: 1 step: 1446, loss is 0.5434006452560425\n",
      "epoch: 1 step: 1447, loss is 0.7742302417755127\n",
      "epoch: 1 step: 1448, loss is 0.6843639612197876\n",
      "epoch: 1 step: 1449, loss is 0.6331807374954224\n",
      "epoch: 1 step: 1450, loss is 0.6298744678497314\n",
      "epoch: 1 step: 1451, loss is 0.6542810201644897\n",
      "epoch: 1 step: 1452, loss is 0.6750105023384094\n",
      "epoch: 1 step: 1453, loss is 0.6381272673606873\n",
      "epoch: 1 step: 1454, loss is 0.7135069966316223\n",
      "epoch: 1 step: 1455, loss is 0.7440825700759888\n",
      "epoch: 1 step: 1456, loss is 0.7905083298683167\n",
      "epoch: 1 step: 1457, loss is 0.7016972303390503\n",
      "epoch: 1 step: 1458, loss is 0.7652878761291504\n",
      "epoch: 1 step: 1459, loss is 0.5970160961151123\n",
      "epoch: 1 step: 1460, loss is 0.7069350481033325\n",
      "epoch: 1 step: 1461, loss is 0.6364214420318604\n",
      "epoch: 1 step: 1462, loss is 0.7747912406921387\n",
      "epoch: 1 step: 1463, loss is 0.6834346055984497\n",
      "epoch: 1 step: 1464, loss is 0.6259183287620544\n",
      "epoch: 1 step: 1465, loss is 0.6529358625411987\n",
      "epoch: 1 step: 1466, loss is 0.7819594740867615\n",
      "epoch: 1 step: 1467, loss is 0.6617236137390137\n",
      "epoch: 1 step: 1468, loss is 0.6624430418014526\n",
      "epoch: 1 step: 1469, loss is 0.820317804813385\n",
      "epoch: 1 step: 1470, loss is 0.6323563456535339\n",
      "epoch: 1 step: 1471, loss is 0.7089787721633911\n",
      "epoch: 1 step: 1472, loss is 0.6589505672454834\n",
      "epoch: 1 step: 1473, loss is 0.692681074142456\n",
      "epoch: 1 step: 1474, loss is 0.6417714953422546\n",
      "epoch: 1 step: 1475, loss is 0.6202303767204285\n",
      "epoch: 1 step: 1476, loss is 0.6808207035064697\n",
      "epoch: 1 step: 1477, loss is 0.6130416393280029\n",
      "epoch: 1 step: 1478, loss is 0.6418771147727966\n",
      "epoch: 1 step: 1479, loss is 0.7275735139846802\n",
      "epoch: 1 step: 1480, loss is 0.6659436821937561\n",
      "epoch: 1 step: 1481, loss is 0.580476701259613\n",
      "epoch: 1 step: 1482, loss is 0.8879484534263611\n",
      "epoch: 1 step: 1483, loss is 0.7330120801925659\n",
      "epoch: 1 step: 1484, loss is 0.7507758736610413\n",
      "epoch: 1 step: 1485, loss is 0.6408388614654541\n",
      "epoch: 1 step: 1486, loss is 0.7016697525978088\n",
      "epoch: 1 step: 1487, loss is 0.6857101321220398\n",
      "epoch: 1 step: 1488, loss is 0.7148705124855042\n",
      "epoch: 1 step: 1489, loss is 0.7949069738388062\n",
      "epoch: 1 step: 1490, loss is 0.7155791521072388\n",
      "epoch: 1 step: 1491, loss is 0.7719863057136536\n",
      "epoch: 1 step: 1492, loss is 0.571707546710968\n",
      "epoch: 1 step: 1493, loss is 0.5774189829826355\n",
      "epoch: 1 step: 1494, loss is 0.7479609251022339\n",
      "epoch: 1 step: 1495, loss is 0.5723973512649536\n",
      "epoch: 1 step: 1496, loss is 0.6866662502288818\n",
      "epoch: 1 step: 1497, loss is 0.7535527944564819\n",
      "epoch: 1 step: 1498, loss is 0.5888783931732178\n",
      "epoch: 1 step: 1499, loss is 0.7107166051864624\n",
      "epoch: 1 step: 1500, loss is 0.6408156752586365\n",
      "epoch: 1 step: 1501, loss is 0.6917769908905029\n",
      "epoch: 1 step: 1502, loss is 0.5980887413024902\n",
      "epoch: 1 step: 1503, loss is 0.6217527389526367\n",
      "epoch: 1 step: 1504, loss is 0.5655716061592102\n",
      "epoch: 1 step: 1505, loss is 0.5735977292060852\n",
      "epoch: 1 step: 1506, loss is 0.48906010389328003\n",
      "epoch: 1 step: 1507, loss is 0.6557501554489136\n",
      "epoch: 1 step: 1508, loss is 0.634826123714447\n",
      "epoch: 1 step: 1509, loss is 0.692971408367157\n",
      "epoch: 1 step: 1510, loss is 0.6678485870361328\n",
      "epoch: 1 step: 1511, loss is 0.963145911693573\n",
      "epoch: 1 step: 1512, loss is 0.5465840101242065\n",
      "epoch: 1 step: 1513, loss is 0.7568433880805969\n",
      "epoch: 1 step: 1514, loss is 0.6508791446685791\n",
      "epoch: 1 step: 1515, loss is 0.7375258803367615\n",
      "epoch: 1 step: 1516, loss is 0.5891417264938354\n",
      "epoch: 1 step: 1517, loss is 0.7664921283721924\n",
      "epoch: 1 step: 1518, loss is 0.5380516648292542\n",
      "epoch: 1 step: 1519, loss is 0.9410280585289001\n",
      "epoch: 1 step: 1520, loss is 0.61763596534729\n",
      "epoch: 1 step: 1521, loss is 1.0819638967514038\n",
      "epoch: 1 step: 1522, loss is 0.9105450510978699\n",
      "epoch: 1 step: 1523, loss is 0.876695990562439\n",
      "epoch: 1 step: 1524, loss is 0.6030036211013794\n",
      "epoch: 1 step: 1525, loss is 0.7102964520454407\n",
      "epoch: 1 step: 1526, loss is 0.6521944999694824\n",
      "epoch: 1 step: 1527, loss is 0.7973546981811523\n",
      "epoch: 1 step: 1528, loss is 0.6726236343383789\n",
      "epoch: 1 step: 1529, loss is 0.6442697048187256\n",
      "epoch: 1 step: 1530, loss is 0.6356752514839172\n",
      "epoch: 1 step: 1531, loss is 0.6866019368171692\n",
      "epoch: 1 step: 1532, loss is 0.7184656262397766\n",
      "epoch: 1 step: 1533, loss is 0.7103753089904785\n",
      "epoch: 1 step: 1534, loss is 0.6985476016998291\n",
      "epoch: 1 step: 1535, loss is 0.7252815365791321\n",
      "epoch: 1 step: 1536, loss is 0.7009098529815674\n",
      "epoch: 1 step: 1537, loss is 0.6783810257911682\n",
      "epoch: 1 step: 1538, loss is 0.7957396507263184\n",
      "epoch: 1 step: 1539, loss is 0.7434753775596619\n",
      "epoch: 1 step: 1540, loss is 0.6615660190582275\n",
      "epoch: 1 step: 1541, loss is 0.6797075271606445\n",
      "epoch: 1 step: 1542, loss is 0.7036743760108948\n",
      "epoch: 1 step: 1543, loss is 0.6915887594223022\n",
      "epoch: 1 step: 1544, loss is 0.6947394013404846\n",
      "epoch: 1 step: 1545, loss is 0.6155093312263489\n",
      "epoch: 1 step: 1546, loss is 0.700141191482544\n",
      "epoch: 1 step: 1547, loss is 0.7024505138397217\n",
      "epoch: 1 step: 1548, loss is 0.7868353724479675\n",
      "epoch: 1 step: 1549, loss is 0.8440009355545044\n",
      "epoch: 1 step: 1550, loss is 0.6917564272880554\n",
      "epoch: 1 step: 1551, loss is 0.6323052644729614\n",
      "epoch: 1 step: 1552, loss is 0.7706365585327148\n",
      "epoch: 1 step: 1553, loss is 0.6619992256164551\n",
      "epoch: 1 step: 1554, loss is 0.6969566345214844\n",
      "epoch: 1 step: 1555, loss is 0.6155647039413452\n",
      "epoch: 1 step: 1556, loss is 0.5995496511459351\n",
      "epoch: 1 step: 1557, loss is 0.6806191205978394\n",
      "epoch: 1 step: 1558, loss is 0.7194778919219971\n",
      "epoch: 1 step: 1559, loss is 0.7417142391204834\n",
      "epoch: 1 step: 1560, loss is 0.7908865213394165\n",
      "epoch: 1 step: 1561, loss is 0.8592458963394165\n",
      "epoch: 1 step: 1562, loss is 0.590614914894104\n",
      "epoch: 1 step: 1563, loss is 0.6629312634468079\n",
      "epoch: 1 step: 1564, loss is 0.6694105863571167\n",
      "epoch: 1 step: 1565, loss is 0.6324345469474792\n",
      "epoch: 1 step: 1566, loss is 0.7632320523262024\n",
      "epoch: 1 step: 1567, loss is 0.7633870244026184\n",
      "epoch: 1 step: 1568, loss is 0.6568938493728638\n",
      "epoch: 1 step: 1569, loss is 0.6723294258117676\n",
      "epoch: 1 step: 1570, loss is 0.6919131278991699\n",
      "epoch: 1 step: 1571, loss is 0.6711318492889404\n",
      "epoch: 1 step: 1572, loss is 0.6150548458099365\n",
      "epoch: 1 step: 1573, loss is 0.7625198364257812\n",
      "epoch: 1 step: 1574, loss is 0.6847559809684753\n",
      "epoch: 1 step: 1575, loss is 0.826696515083313\n",
      "epoch: 1 step: 1576, loss is 0.7367303967475891\n",
      "epoch: 1 step: 1577, loss is 0.650521457195282\n",
      "epoch: 1 step: 1578, loss is 0.7414884567260742\n",
      "epoch: 1 step: 1579, loss is 0.83062744140625\n",
      "epoch: 1 step: 1580, loss is 0.6471052169799805\n",
      "epoch: 1 step: 1581, loss is 0.7057738900184631\n",
      "epoch: 1 step: 1582, loss is 0.6651369333267212\n",
      "epoch: 1 step: 1583, loss is 0.6831626892089844\n",
      "epoch: 1 step: 1584, loss is 0.6725427508354187\n",
      "epoch: 1 step: 1585, loss is 0.6386249661445618\n",
      "epoch: 1 step: 1586, loss is 0.6564853191375732\n",
      "epoch: 1 step: 1587, loss is 0.6501049995422363\n",
      "epoch: 1 step: 1588, loss is 0.6401810646057129\n",
      "epoch: 1 step: 1589, loss is 0.7321271300315857\n",
      "epoch: 1 step: 1590, loss is 0.5588206648826599\n",
      "epoch: 1 step: 1591, loss is 0.8686429858207703\n",
      "epoch: 1 step: 1592, loss is 0.9349220991134644\n",
      "epoch: 1 step: 1593, loss is 0.9882370829582214\n",
      "epoch: 1 step: 1594, loss is 0.8010170459747314\n",
      "epoch: 1 step: 1595, loss is 0.6233029961585999\n",
      "epoch: 1 step: 1596, loss is 0.6851242780685425\n",
      "epoch: 1 step: 1597, loss is 0.6636965870857239\n",
      "epoch: 1 step: 1598, loss is 0.8154894113540649\n",
      "epoch: 1 step: 1599, loss is 0.8535255193710327\n",
      "epoch: 1 step: 1600, loss is 0.5314264893531799\n",
      "epoch: 1 step: 1601, loss is 0.7207460403442383\n",
      "epoch: 1 step: 1602, loss is 0.6243187189102173\n",
      "epoch: 1 step: 1603, loss is 0.7544344663619995\n",
      "epoch: 1 step: 1604, loss is 0.6858742237091064\n",
      "epoch: 1 step: 1605, loss is 0.48498696088790894\n",
      "epoch: 1 step: 1606, loss is 0.8315427899360657\n",
      "epoch: 1 step: 1607, loss is 0.7033325433731079\n",
      "epoch: 1 step: 1608, loss is 0.8451516628265381\n",
      "epoch: 1 step: 1609, loss is 0.8462221026420593\n",
      "epoch: 1 step: 1610, loss is 0.7501057982444763\n",
      "epoch: 1 step: 1611, loss is 0.7275521755218506\n",
      "epoch: 1 step: 1612, loss is 0.6834675073623657\n",
      "epoch: 1 step: 1613, loss is 0.7012184858322144\n",
      "epoch: 1 step: 1614, loss is 0.6214178800582886\n",
      "epoch: 1 step: 1615, loss is 0.6061153411865234\n",
      "epoch: 1 step: 1616, loss is 0.6254228949546814\n",
      "epoch: 1 step: 1617, loss is 0.870597779750824\n",
      "epoch: 1 step: 1618, loss is 0.6887430548667908\n",
      "epoch: 1 step: 1619, loss is 0.49295952916145325\n",
      "epoch: 1 step: 1620, loss is 0.9787427186965942\n",
      "epoch: 1 step: 1621, loss is 0.7780449986457825\n",
      "epoch: 1 step: 1622, loss is 0.6805659532546997\n",
      "epoch: 1 step: 1623, loss is 0.7359768748283386\n",
      "epoch: 1 step: 1624, loss is 0.5761022567749023\n",
      "epoch: 1 step: 1625, loss is 0.7530888319015503\n",
      "epoch: 1 step: 1626, loss is 0.5718638300895691\n",
      "epoch: 1 step: 1627, loss is 0.6804856061935425\n",
      "epoch: 1 step: 1628, loss is 0.7034459114074707\n",
      "epoch: 1 step: 1629, loss is 0.6758419275283813\n",
      "epoch: 1 step: 1630, loss is 0.7057086229324341\n",
      "epoch: 1 step: 1631, loss is 0.7350120544433594\n",
      "epoch: 1 step: 1632, loss is 0.6535821557044983\n",
      "epoch: 1 step: 1633, loss is 0.6288841366767883\n",
      "epoch: 1 step: 1634, loss is 0.6900891065597534\n",
      "epoch: 1 step: 1635, loss is 0.7402936220169067\n",
      "epoch: 1 step: 1636, loss is 0.7308914065361023\n",
      "epoch: 1 step: 1637, loss is 0.7109431028366089\n",
      "epoch: 1 step: 1638, loss is 0.6731916666030884\n",
      "epoch: 1 step: 1639, loss is 0.6746674180030823\n",
      "epoch: 1 step: 1640, loss is 0.6182070970535278\n",
      "epoch: 1 step: 1641, loss is 0.6559423208236694\n",
      "epoch: 1 step: 1642, loss is 0.6410719752311707\n",
      "epoch: 1 step: 1643, loss is 0.6123616099357605\n",
      "epoch: 1 step: 1644, loss is 0.6418747305870056\n",
      "epoch: 1 step: 1645, loss is 0.619329035282135\n",
      "epoch: 1 step: 1646, loss is 0.9066108465194702\n",
      "epoch: 1 step: 1647, loss is 0.558101236820221\n",
      "epoch: 1 step: 1648, loss is 0.8135194182395935\n",
      "epoch: 1 step: 1649, loss is 0.7034302949905396\n",
      "epoch: 1 step: 1650, loss is 0.43524375557899475\n",
      "epoch: 1 step: 1651, loss is 0.8319397568702698\n",
      "epoch: 1 step: 1652, loss is 0.8373745679855347\n",
      "epoch: 1 step: 1653, loss is 0.5195626020431519\n",
      "epoch: 1 step: 1654, loss is 0.8083076477050781\n",
      "epoch: 1 step: 1655, loss is 0.6830219030380249\n",
      "epoch: 1 step: 1656, loss is 0.6371171474456787\n",
      "epoch: 1 step: 1657, loss is 0.7904273867607117\n",
      "epoch: 1 step: 1658, loss is 0.5881665349006653\n",
      "epoch: 1 step: 1659, loss is 0.5984815359115601\n",
      "epoch: 1 step: 1660, loss is 0.7155460119247437\n",
      "epoch: 1 step: 1661, loss is 0.8880878686904907\n",
      "epoch: 1 step: 1662, loss is 0.6907077431678772\n",
      "epoch: 1 step: 1663, loss is 0.7200302481651306\n",
      "epoch: 1 step: 1664, loss is 0.7129932045936584\n",
      "epoch: 1 step: 1665, loss is 0.6817023754119873\n",
      "epoch: 1 step: 1666, loss is 0.6877043843269348\n",
      "epoch: 1 step: 1667, loss is 0.7043285965919495\n",
      "epoch: 1 step: 1668, loss is 0.6781051754951477\n",
      "epoch: 1 step: 1669, loss is 0.680433988571167\n",
      "epoch: 1 step: 1670, loss is 0.6123923659324646\n",
      "epoch: 1 step: 1671, loss is 0.679111659526825\n",
      "epoch: 1 step: 1672, loss is 0.6983193159103394\n",
      "epoch: 1 step: 1673, loss is 0.6070490479469299\n",
      "epoch: 1 step: 1674, loss is 0.7554134726524353\n",
      "epoch: 1 step: 1675, loss is 0.7348178029060364\n",
      "epoch: 1 step: 1676, loss is 0.6690360903739929\n",
      "epoch: 1 step: 1677, loss is 0.6375501155853271\n",
      "epoch: 1 step: 1678, loss is 0.7016662955284119\n",
      "epoch: 1 step: 1679, loss is 0.683595597743988\n",
      "epoch: 1 step: 1680, loss is 0.6822540760040283\n",
      "epoch: 1 step: 1681, loss is 0.6490305066108704\n",
      "epoch: 1 step: 1682, loss is 0.8218774795532227\n",
      "epoch: 1 step: 1683, loss is 0.6366316080093384\n",
      "epoch: 1 step: 1684, loss is 0.6601951122283936\n",
      "epoch: 1 step: 1685, loss is 0.758373498916626\n",
      "epoch: 1 step: 1686, loss is 0.704655110836029\n",
      "epoch: 1 step: 1687, loss is 0.7243326902389526\n",
      "epoch: 1 step: 1688, loss is 0.7472226619720459\n",
      "epoch: 1 step: 1689, loss is 0.6971883177757263\n",
      "epoch: 1 step: 1690, loss is 0.6161441802978516\n",
      "epoch: 1 step: 1691, loss is 0.6940549612045288\n",
      "epoch: 1 step: 1692, loss is 0.6641016602516174\n",
      "epoch: 1 step: 1693, loss is 0.58177250623703\n",
      "epoch: 1 step: 1694, loss is 0.7736760973930359\n",
      "epoch: 1 step: 1695, loss is 0.6651771068572998\n",
      "epoch: 1 step: 1696, loss is 0.7687267065048218\n",
      "epoch: 1 step: 1697, loss is 0.6628228425979614\n",
      "epoch: 1 step: 1698, loss is 0.7223803997039795\n",
      "epoch: 1 step: 1699, loss is 0.6199362874031067\n",
      "epoch: 1 step: 1700, loss is 0.6289581060409546\n",
      "epoch: 1 step: 1701, loss is 0.7400755882263184\n",
      "epoch: 1 step: 1702, loss is 0.7170636653900146\n",
      "epoch: 1 step: 1703, loss is 0.6662812829017639\n",
      "epoch: 1 step: 1704, loss is 0.7253849506378174\n",
      "epoch: 1 step: 1705, loss is 0.6256897449493408\n",
      "epoch: 1 step: 1706, loss is 0.6868243217468262\n",
      "epoch: 1 step: 1707, loss is 0.7270423173904419\n",
      "epoch: 1 step: 1708, loss is 0.5845413208007812\n",
      "epoch: 1 step: 1709, loss is 0.7189173698425293\n",
      "epoch: 1 step: 1710, loss is 0.7991592884063721\n",
      "epoch: 1 step: 1711, loss is 0.6344704031944275\n",
      "epoch: 1 step: 1712, loss is 0.736854612827301\n",
      "epoch: 1 step: 1713, loss is 0.6034832000732422\n",
      "epoch: 1 step: 1714, loss is 0.633703887462616\n",
      "epoch: 1 step: 1715, loss is 0.8159853219985962\n",
      "epoch: 1 step: 1716, loss is 0.5322878956794739\n",
      "epoch: 1 step: 1717, loss is 0.6161153316497803\n",
      "epoch: 1 step: 1718, loss is 0.7025616765022278\n",
      "epoch: 1 step: 1719, loss is 0.6837847828865051\n",
      "epoch: 1 step: 1720, loss is 0.7836040258407593\n",
      "epoch: 1 step: 1721, loss is 0.8190375566482544\n",
      "epoch: 1 step: 1722, loss is 0.7601369023323059\n",
      "epoch: 1 step: 1723, loss is 0.7114431262016296\n",
      "epoch: 1 step: 1724, loss is 0.5965008735656738\n",
      "epoch: 1 step: 1725, loss is 0.580418586730957\n",
      "epoch: 1 step: 1726, loss is 0.6942301988601685\n",
      "epoch: 1 step: 1727, loss is 0.7065809369087219\n",
      "epoch: 1 step: 1728, loss is 0.7785501480102539\n",
      "epoch: 1 step: 1729, loss is 0.8156449794769287\n",
      "epoch: 1 step: 1730, loss is 0.657057523727417\n",
      "epoch: 1 step: 1731, loss is 0.5688784718513489\n",
      "epoch: 1 step: 1732, loss is 0.8219425678253174\n",
      "epoch: 1 step: 1733, loss is 0.6297882795333862\n",
      "epoch: 1 step: 1734, loss is 0.6000041961669922\n",
      "epoch: 1 step: 1735, loss is 0.6583524346351624\n",
      "epoch: 1 step: 1736, loss is 0.7864447832107544\n",
      "epoch: 1 step: 1737, loss is 0.6462156772613525\n",
      "epoch: 1 step: 1738, loss is 0.659039318561554\n",
      "epoch: 1 step: 1739, loss is 0.7795428037643433\n",
      "epoch: 1 step: 1740, loss is 0.6866090297698975\n",
      "epoch: 1 step: 1741, loss is 0.6070843935012817\n",
      "epoch: 1 step: 1742, loss is 0.7549383640289307\n",
      "epoch: 1 step: 1743, loss is 0.6434853672981262\n",
      "epoch: 1 step: 1744, loss is 0.6200115084648132\n",
      "epoch: 1 step: 1745, loss is 0.7537462711334229\n",
      "epoch: 1 step: 1746, loss is 0.768355667591095\n",
      "epoch: 1 step: 1747, loss is 0.7326687574386597\n",
      "epoch: 1 step: 1748, loss is 0.7176380753517151\n",
      "epoch: 1 step: 1749, loss is 0.6977432370185852\n",
      "epoch: 1 step: 1750, loss is 0.6231415271759033\n",
      "epoch: 1 step: 1751, loss is 0.6367385387420654\n",
      "epoch: 1 step: 1752, loss is 0.6447199583053589\n",
      "epoch: 1 step: 1753, loss is 0.7180384993553162\n",
      "epoch: 1 step: 1754, loss is 0.7151907086372375\n",
      "epoch: 1 step: 1755, loss is 0.6493712663650513\n",
      "epoch: 1 step: 1756, loss is 0.6323740482330322\n",
      "epoch: 1 step: 1757, loss is 0.7546753287315369\n",
      "epoch: 1 step: 1758, loss is 0.5742202997207642\n",
      "epoch: 1 step: 1759, loss is 0.5635370016098022\n",
      "epoch: 1 step: 1760, loss is 0.7959052920341492\n",
      "epoch: 1 step: 1761, loss is 0.7080807089805603\n",
      "epoch: 1 step: 1762, loss is 0.5794018507003784\n",
      "epoch: 1 step: 1763, loss is 0.6848115921020508\n",
      "epoch: 1 step: 1764, loss is 0.6661343574523926\n",
      "epoch: 1 step: 1765, loss is 0.6980937123298645\n",
      "epoch: 1 step: 1766, loss is 0.776025652885437\n",
      "epoch: 1 step: 1767, loss is 0.7779675126075745\n",
      "epoch: 1 step: 1768, loss is 0.642707109451294\n",
      "epoch: 1 step: 1769, loss is 0.6557790040969849\n",
      "epoch: 1 step: 1770, loss is 0.6678249835968018\n",
      "epoch: 1 step: 1771, loss is 0.6094126105308533\n",
      "epoch: 1 step: 1772, loss is 0.7265089154243469\n",
      "epoch: 1 step: 1773, loss is 0.6569015979766846\n",
      "epoch: 1 step: 1774, loss is 0.5694136023521423\n",
      "epoch: 1 step: 1775, loss is 0.8033754825592041\n",
      "epoch: 1 step: 1776, loss is 0.8450415730476379\n",
      "epoch: 1 step: 1777, loss is 0.9056395292282104\n",
      "epoch: 1 step: 1778, loss is 0.8515892028808594\n",
      "epoch: 1 step: 1779, loss is 0.7248756289482117\n",
      "epoch: 1 step: 1780, loss is 0.6431810259819031\n",
      "epoch: 1 step: 1781, loss is 0.6321948766708374\n",
      "epoch: 1 step: 1782, loss is 0.6166497468948364\n",
      "epoch: 1 step: 1783, loss is 0.7025434374809265\n",
      "epoch: 1 step: 1784, loss is 0.6221819519996643\n",
      "epoch: 1 step: 1785, loss is 0.6425738334655762\n",
      "epoch: 1 step: 1786, loss is 0.7689292430877686\n",
      "epoch: 1 step: 1787, loss is 0.8127937912940979\n",
      "epoch: 1 step: 1788, loss is 0.6371276378631592\n",
      "epoch: 1 step: 1789, loss is 0.7489558458328247\n",
      "epoch: 1 step: 1790, loss is 0.7585190534591675\n",
      "epoch: 1 step: 1791, loss is 0.6824225187301636\n",
      "epoch: 1 step: 1792, loss is 0.6594475507736206\n",
      "epoch: 1 step: 1793, loss is 0.7797607779502869\n",
      "epoch: 1 step: 1794, loss is 0.7554214000701904\n",
      "epoch: 1 step: 1795, loss is 0.6229293346405029\n",
      "epoch: 1 step: 1796, loss is 0.6932734847068787\n",
      "epoch: 1 step: 1797, loss is 0.6806310415267944\n",
      "epoch: 1 step: 1798, loss is 0.708911657333374\n",
      "epoch: 1 step: 1799, loss is 0.6781883835792542\n",
      "epoch: 1 step: 1800, loss is 0.6994215846061707\n",
      "epoch: 1 step: 1801, loss is 0.6999474763870239\n",
      "epoch: 1 step: 1802, loss is 0.8683288097381592\n",
      "epoch: 1 step: 1803, loss is 0.6034788489341736\n",
      "epoch: 1 step: 1804, loss is 0.6643558740615845\n",
      "epoch: 1 step: 1805, loss is 0.6797067523002625\n",
      "epoch: 1 step: 1806, loss is 0.633411705493927\n",
      "epoch: 1 step: 1807, loss is 0.6740634441375732\n",
      "epoch: 1 step: 1808, loss is 0.721526026725769\n",
      "epoch: 1 step: 1809, loss is 0.6269108057022095\n",
      "epoch: 1 step: 1810, loss is 0.8128544092178345\n",
      "epoch: 1 step: 1811, loss is 0.6660569310188293\n",
      "epoch: 1 step: 1812, loss is 0.6643058061599731\n",
      "epoch: 1 step: 1813, loss is 0.7098512649536133\n",
      "epoch: 1 step: 1814, loss is 0.5971531867980957\n",
      "epoch: 1 step: 1815, loss is 0.6522731184959412\n",
      "epoch: 1 step: 1816, loss is 0.669495165348053\n",
      "epoch: 1 step: 1817, loss is 0.6162986159324646\n",
      "epoch: 1 step: 1818, loss is 0.7164644002914429\n",
      "epoch: 1 step: 1819, loss is 0.658151388168335\n",
      "epoch: 1 step: 1820, loss is 0.6551621556282043\n",
      "epoch: 1 step: 1821, loss is 0.8553006649017334\n",
      "epoch: 1 step: 1822, loss is 0.6450616717338562\n",
      "epoch: 1 step: 1823, loss is 0.6829304099082947\n",
      "epoch: 1 step: 1824, loss is 0.678500235080719\n",
      "epoch: 1 step: 1825, loss is 0.688032865524292\n",
      "epoch: 1 step: 1826, loss is 0.6861085295677185\n",
      "epoch: 1 step: 1827, loss is 0.638958215713501\n",
      "epoch: 1 step: 1828, loss is 0.662438154220581\n",
      "epoch: 1 step: 1829, loss is 0.8584805727005005\n",
      "epoch: 1 step: 1830, loss is 0.46634840965270996\n",
      "epoch: 1 step: 1831, loss is 0.58275306224823\n",
      "epoch: 1 step: 1832, loss is 0.6969131231307983\n",
      "epoch: 1 step: 1833, loss is 0.6068909764289856\n",
      "epoch: 1 step: 1834, loss is 0.47958216071128845\n",
      "epoch: 1 step: 1835, loss is 0.7176128625869751\n",
      "epoch: 1 step: 1836, loss is 0.974155068397522\n",
      "epoch: 1 step: 1837, loss is 0.6943665146827698\n",
      "epoch: 1 step: 1838, loss is 0.6835167407989502\n",
      "epoch: 1 step: 1839, loss is 0.7428962588310242\n",
      "epoch: 1 step: 1840, loss is 0.4409753680229187\n",
      "epoch: 1 step: 1841, loss is 0.5196405649185181\n",
      "epoch: 1 step: 1842, loss is 0.7219585180282593\n",
      "epoch: 1 step: 1843, loss is 0.6906707286834717\n",
      "epoch: 1 step: 1844, loss is 0.8647984862327576\n",
      "epoch: 1 step: 1845, loss is 0.7944906949996948\n",
      "epoch: 1 step: 1846, loss is 0.7230342626571655\n",
      "epoch: 1 step: 1847, loss is 0.6252928376197815\n",
      "epoch: 1 step: 1848, loss is 0.6978555917739868\n",
      "epoch: 1 step: 1849, loss is 0.5209444761276245\n",
      "epoch: 1 step: 1850, loss is 0.8707719445228577\n",
      "epoch: 1 step: 1851, loss is 0.67377108335495\n",
      "epoch: 1 step: 1852, loss is 0.6469311118125916\n",
      "epoch: 1 step: 1853, loss is 0.9698605537414551\n",
      "epoch: 1 step: 1854, loss is 0.7151407599449158\n",
      "epoch: 1 step: 1855, loss is 0.8069770336151123\n",
      "epoch: 1 step: 1856, loss is 0.7839127779006958\n",
      "epoch: 1 step: 1857, loss is 0.6926817893981934\n",
      "epoch: 1 step: 1858, loss is 0.6933377385139465\n",
      "epoch: 1 step: 1859, loss is 0.5861543416976929\n",
      "epoch: 1 step: 1860, loss is 0.8108693361282349\n",
      "epoch: 1 step: 1861, loss is 0.7703304886817932\n",
      "epoch: 1 step: 1862, loss is 0.7170524001121521\n",
      "epoch: 1 step: 1863, loss is 0.8039933443069458\n",
      "epoch: 1 step: 1864, loss is 0.7151511907577515\n",
      "epoch: 1 step: 1865, loss is 0.8367557525634766\n",
      "epoch: 1 step: 1866, loss is 0.5969280004501343\n",
      "epoch: 1 step: 1867, loss is 0.6429852247238159\n",
      "epoch: 1 step: 1868, loss is 0.7148019075393677\n",
      "epoch: 1 step: 1869, loss is 0.6362947821617126\n",
      "epoch: 1 step: 1870, loss is 0.6214148998260498\n",
      "epoch: 1 step: 1871, loss is 0.7021850347518921\n",
      "epoch: 1 step: 1872, loss is 0.6665960550308228\n",
      "epoch: 1 step: 1873, loss is 0.6275842189788818\n",
      "epoch: 1 step: 1874, loss is 0.7281596064567566\n",
      "epoch: 1 step: 1875, loss is 0.7237644195556641\n",
      "epoch: 1 step: 1876, loss is 0.6475109457969666\n",
      "epoch: 1 step: 1877, loss is 0.6724866628646851\n",
      "epoch: 1 step: 1878, loss is 0.759097158908844\n",
      "epoch: 1 step: 1879, loss is 0.6329857707023621\n",
      "epoch: 1 step: 1880, loss is 0.7231555581092834\n",
      "epoch: 1 step: 1881, loss is 0.6911231875419617\n",
      "epoch: 1 step: 1882, loss is 0.6922814249992371\n",
      "epoch: 1 step: 1883, loss is 0.7374366521835327\n",
      "epoch: 1 step: 1884, loss is 0.7129876017570496\n",
      "epoch: 1 step: 1885, loss is 0.7065370678901672\n",
      "epoch: 1 step: 1886, loss is 0.710680365562439\n",
      "epoch: 1 step: 1887, loss is 0.6156312227249146\n",
      "epoch: 1 step: 1888, loss is 0.6420566439628601\n",
      "epoch: 1 step: 1889, loss is 0.723419189453125\n",
      "epoch: 1 step: 1890, loss is 0.7637927532196045\n",
      "epoch: 1 step: 1891, loss is 0.667201042175293\n",
      "epoch: 1 step: 1892, loss is 0.5214089751243591\n",
      "epoch: 1 step: 1893, loss is 0.5766602158546448\n",
      "epoch: 1 step: 1894, loss is 0.8080064058303833\n",
      "epoch: 1 step: 1895, loss is 0.623579740524292\n",
      "epoch: 1 step: 1896, loss is 0.6733418703079224\n",
      "epoch: 1 step: 1897, loss is 0.6878080368041992\n",
      "epoch: 1 step: 1898, loss is 0.5137949585914612\n",
      "epoch: 1 step: 1899, loss is 0.7852369546890259\n",
      "epoch: 1 step: 1900, loss is 0.6354398131370544\n",
      "epoch: 1 step: 1901, loss is 0.614045262336731\n",
      "epoch: 1 step: 1902, loss is 0.5559236407279968\n",
      "epoch: 1 step: 1903, loss is 0.6362388134002686\n",
      "epoch: 1 step: 1904, loss is 0.7298411130905151\n",
      "epoch: 1 step: 1905, loss is 0.6430965065956116\n",
      "epoch: 1 step: 1906, loss is 0.7220299243927002\n",
      "epoch: 1 step: 1907, loss is 0.6648885607719421\n",
      "epoch: 1 step: 1908, loss is 0.7043612599372864\n",
      "epoch: 1 step: 1909, loss is 0.6494570374488831\n",
      "epoch: 1 step: 1910, loss is 0.5700681805610657\n",
      "epoch: 1 step: 1911, loss is 0.8346500396728516\n",
      "epoch: 1 step: 1912, loss is 0.8805151581764221\n",
      "epoch: 1 step: 1913, loss is 0.6745940446853638\n",
      "epoch: 1 step: 1914, loss is 0.6249681711196899\n",
      "epoch: 1 step: 1915, loss is 0.7128104567527771\n",
      "epoch: 1 step: 1916, loss is 0.6691142916679382\n",
      "epoch: 1 step: 1917, loss is 0.6833583116531372\n",
      "epoch: 1 step: 1918, loss is 0.6747453212738037\n",
      "epoch: 1 step: 1919, loss is 0.7206103801727295\n",
      "epoch: 1 step: 1920, loss is 0.4894408583641052\n",
      "epoch: 1 step: 1921, loss is 0.6299830675125122\n",
      "epoch: 1 step: 1922, loss is 0.7207269668579102\n",
      "epoch: 1 step: 1923, loss is 0.6811606884002686\n",
      "epoch: 1 step: 1924, loss is 0.6312420964241028\n",
      "epoch: 1 step: 1925, loss is 0.6645722985267639\n",
      "epoch: 1 step: 1926, loss is 0.8412537574768066\n",
      "epoch: 1 step: 1927, loss is 0.5215787887573242\n",
      "epoch: 1 step: 1928, loss is 0.5675820112228394\n",
      "epoch: 1 step: 1929, loss is 0.5628542304039001\n",
      "epoch: 1 step: 1930, loss is 0.6500474810600281\n",
      "epoch: 1 step: 1931, loss is 0.7108675837516785\n",
      "epoch: 1 step: 1932, loss is 0.770803689956665\n",
      "epoch: 1 step: 1933, loss is 0.808736264705658\n",
      "epoch: 1 step: 1934, loss is 0.8325851559638977\n",
      "epoch: 1 step: 1935, loss is 0.717146635055542\n",
      "epoch: 1 step: 1936, loss is 0.6005719304084778\n",
      "epoch: 1 step: 1937, loss is 0.7134934663772583\n",
      "epoch: 1 step: 1938, loss is 0.575615406036377\n",
      "epoch: 1 step: 1939, loss is 0.6074919104576111\n",
      "epoch: 1 step: 1940, loss is 0.6542963981628418\n",
      "epoch: 1 step: 1941, loss is 0.8758655786514282\n",
      "epoch: 1 step: 1942, loss is 0.7445914149284363\n",
      "epoch: 1 step: 1943, loss is 0.5473013520240784\n",
      "epoch: 1 step: 1944, loss is 0.8017150163650513\n",
      "epoch: 1 step: 1945, loss is 0.7883321642875671\n",
      "epoch: 1 step: 1946, loss is 0.8388606309890747\n",
      "epoch: 1 step: 1947, loss is 0.665327250957489\n",
      "epoch: 1 step: 1948, loss is 0.7063698768615723\n",
      "epoch: 1 step: 1949, loss is 0.6085482239723206\n",
      "epoch: 1 step: 1950, loss is 0.6123172044754028\n",
      "epoch: 1 step: 1951, loss is 0.6227161288261414\n",
      "epoch: 1 step: 1952, loss is 0.8156832456588745\n",
      "epoch: 1 step: 1953, loss is 0.8685157895088196\n",
      "epoch: 1 step: 1954, loss is 0.6267978549003601\n",
      "epoch: 1 step: 1955, loss is 0.49076709151268005\n",
      "epoch: 1 step: 1956, loss is 0.7606402039527893\n",
      "epoch: 1 step: 1957, loss is 0.945642352104187\n",
      "epoch: 1 step: 1958, loss is 0.9430561661720276\n",
      "epoch: 1 step: 1959, loss is 0.867052435874939\n",
      "epoch: 1 step: 1960, loss is 0.681178629398346\n",
      "epoch: 1 step: 1961, loss is 0.7160778045654297\n",
      "epoch: 1 step: 1962, loss is 0.6109880208969116\n",
      "epoch: 1 step: 1963, loss is 0.7438976168632507\n",
      "epoch: 1 step: 1964, loss is 0.6314017176628113\n",
      "epoch: 1 step: 1965, loss is 0.6395934224128723\n",
      "epoch: 1 step: 1966, loss is 0.8086646199226379\n",
      "epoch: 1 step: 1967, loss is 0.7715893387794495\n",
      "epoch: 1 step: 1968, loss is 0.7671935558319092\n",
      "epoch: 1 step: 1969, loss is 0.7555890083312988\n",
      "epoch: 1 step: 1970, loss is 0.7226191759109497\n",
      "epoch: 1 step: 1971, loss is 0.757060706615448\n",
      "epoch: 1 step: 1972, loss is 0.7041593194007874\n",
      "epoch: 1 step: 1973, loss is 0.7215485572814941\n",
      "epoch: 1 step: 1974, loss is 0.5989242196083069\n",
      "epoch: 1 step: 1975, loss is 0.6599444150924683\n",
      "epoch: 1 step: 1976, loss is 0.7128311395645142\n",
      "epoch: 1 step: 1977, loss is 0.6929212212562561\n",
      "epoch: 1 step: 1978, loss is 0.778758704662323\n",
      "epoch: 1 step: 1979, loss is 0.6411228775978088\n",
      "epoch: 1 step: 1980, loss is 0.7279300689697266\n",
      "epoch: 1 step: 1981, loss is 0.7620102763175964\n",
      "epoch: 1 step: 1982, loss is 0.7216992378234863\n",
      "epoch: 1 step: 1983, loss is 0.7799277901649475\n",
      "epoch: 1 step: 1984, loss is 0.6162225604057312\n",
      "epoch: 1 step: 1985, loss is 0.6388972401618958\n",
      "epoch: 1 step: 1986, loss is 0.801429271697998\n",
      "epoch: 1 step: 1987, loss is 0.7375754714012146\n",
      "epoch: 1 step: 1988, loss is 0.6919422745704651\n",
      "epoch: 1 step: 1989, loss is 0.7673877477645874\n",
      "epoch: 1 step: 1990, loss is 0.7109400033950806\n",
      "epoch: 1 step: 1991, loss is 0.6948363184928894\n",
      "epoch: 1 step: 1992, loss is 0.6056472063064575\n",
      "epoch: 1 step: 1993, loss is 0.576336145401001\n",
      "epoch: 1 step: 1994, loss is 0.6933975219726562\n",
      "epoch: 1 step: 1995, loss is 0.7881532907485962\n",
      "epoch: 1 step: 1996, loss is 0.6354000568389893\n",
      "epoch: 1 step: 1997, loss is 0.6607151627540588\n",
      "epoch: 1 step: 1998, loss is 0.5871514081954956\n",
      "epoch: 1 step: 1999, loss is 0.7023892402648926\n",
      "epoch: 1 step: 2000, loss is 0.6938554048538208\n",
      "epoch: 1 step: 2001, loss is 0.5581040382385254\n",
      "epoch: 1 step: 2002, loss is 0.6395092010498047\n",
      "epoch: 1 step: 2003, loss is 0.6756654381752014\n",
      "epoch: 1 step: 2004, loss is 0.677404522895813\n",
      "epoch: 1 step: 2005, loss is 0.6913300156593323\n",
      "epoch: 1 step: 2006, loss is 0.6997102499008179\n",
      "epoch: 1 step: 2007, loss is 0.7008628249168396\n",
      "epoch: 1 step: 2008, loss is 0.6408194303512573\n",
      "epoch: 1 step: 2009, loss is 0.4741409420967102\n",
      "epoch: 1 step: 2010, loss is 0.6297792792320251\n",
      "epoch: 1 step: 2011, loss is 0.47778037190437317\n",
      "epoch: 1 step: 2012, loss is 0.5800548195838928\n",
      "epoch: 1 step: 2013, loss is 0.8314238786697388\n",
      "epoch: 1 step: 2014, loss is 1.0293048620224\n",
      "epoch: 1 step: 2015, loss is 0.9188206791877747\n",
      "epoch: 1 step: 2016, loss is 0.7543573379516602\n",
      "epoch: 1 step: 2017, loss is 0.5374720096588135\n",
      "epoch: 1 step: 2018, loss is 0.7425413131713867\n",
      "epoch: 1 step: 2019, loss is 0.7603800892829895\n",
      "epoch: 1 step: 2020, loss is 0.650000274181366\n",
      "epoch: 1 step: 2021, loss is 0.6918109655380249\n",
      "epoch: 1 step: 2022, loss is 0.8724716305732727\n",
      "epoch: 1 step: 2023, loss is 0.5233124494552612\n",
      "epoch: 1 step: 2024, loss is 0.5279145240783691\n",
      "epoch: 1 step: 2025, loss is 0.7095004916191101\n",
      "epoch: 1 step: 2026, loss is 0.5529428720474243\n",
      "epoch: 1 step: 2027, loss is 0.6570295095443726\n",
      "epoch: 1 step: 2028, loss is 0.8139467835426331\n",
      "epoch: 1 step: 2029, loss is 0.6614973545074463\n",
      "epoch: 1 step: 2030, loss is 0.66497403383255\n",
      "epoch: 1 step: 2031, loss is 0.6027144193649292\n",
      "epoch: 1 step: 2032, loss is 0.5446784496307373\n",
      "epoch: 1 step: 2033, loss is 0.858375072479248\n",
      "epoch: 1 step: 2034, loss is 0.6771728992462158\n",
      "epoch: 1 step: 2035, loss is 0.5987448692321777\n",
      "epoch: 1 step: 2036, loss is 0.7158089876174927\n",
      "epoch: 1 step: 2037, loss is 0.7727184295654297\n",
      "epoch: 1 step: 2038, loss is 0.5745424032211304\n",
      "epoch: 1 step: 2039, loss is 0.57437664270401\n",
      "epoch: 1 step: 2040, loss is 0.6162535548210144\n",
      "epoch: 1 step: 2041, loss is 0.6920795440673828\n",
      "epoch: 1 step: 2042, loss is 0.7389910221099854\n",
      "epoch: 1 step: 2043, loss is 0.6798275709152222\n",
      "epoch: 1 step: 2044, loss is 0.631344199180603\n",
      "epoch: 1 step: 2045, loss is 0.7574028372764587\n",
      "epoch: 1 step: 2046, loss is 0.6894384026527405\n",
      "epoch: 1 step: 2047, loss is 0.6395371556282043\n",
      "epoch: 1 step: 2048, loss is 0.6539065837860107\n",
      "epoch: 1 step: 2049, loss is 0.6500473022460938\n",
      "epoch: 1 step: 2050, loss is 1.15206778049469\n",
      "epoch: 1 step: 2051, loss is 0.679886519908905\n",
      "epoch: 1 step: 2052, loss is 0.7859043478965759\n",
      "epoch: 1 step: 2053, loss is 0.8969629406929016\n",
      "epoch: 1 step: 2054, loss is 0.4565817713737488\n",
      "epoch: 1 step: 2055, loss is 0.6907033324241638\n",
      "epoch: 1 step: 2056, loss is 0.686988115310669\n",
      "epoch: 1 step: 2057, loss is 0.6666828393936157\n",
      "epoch: 1 step: 2058, loss is 0.7001156210899353\n",
      "epoch: 1 step: 2059, loss is 0.6382192969322205\n",
      "epoch: 1 step: 2060, loss is 0.6741607785224915\n",
      "epoch: 1 step: 2061, loss is 0.658850908279419\n",
      "epoch: 1 step: 2062, loss is 0.7432668209075928\n",
      "epoch: 1 step: 2063, loss is 0.6916318535804749\n",
      "epoch: 1 step: 2064, loss is 0.6094855666160583\n",
      "epoch: 1 step: 2065, loss is 0.7699885368347168\n",
      "epoch: 1 step: 2066, loss is 0.5837146639823914\n",
      "epoch: 1 step: 2067, loss is 0.6723149418830872\n",
      "epoch: 1 step: 2068, loss is 0.643936812877655\n",
      "epoch: 1 step: 2069, loss is 0.7255144715309143\n",
      "epoch: 1 step: 2070, loss is 0.7262357473373413\n",
      "epoch: 1 step: 2071, loss is 0.6533316969871521\n",
      "epoch: 1 step: 2072, loss is 0.6577091217041016\n",
      "epoch: 1 step: 2073, loss is 0.725070059299469\n",
      "epoch: 1 step: 2074, loss is 0.6021689176559448\n",
      "epoch: 1 step: 2075, loss is 0.6813449859619141\n",
      "epoch: 1 step: 2076, loss is 0.8992207050323486\n",
      "epoch: 1 step: 2077, loss is 0.6809640526771545\n",
      "epoch: 1 step: 2078, loss is 0.7452457547187805\n",
      "epoch: 1 step: 2079, loss is 0.6795725226402283\n",
      "epoch: 1 step: 2080, loss is 0.6420421600341797\n",
      "epoch: 1 step: 2081, loss is 0.7238564491271973\n",
      "epoch: 1 step: 2082, loss is 0.7889507412910461\n",
      "epoch: 1 step: 2083, loss is 0.6981068253517151\n",
      "epoch: 1 step: 2084, loss is 0.7834212183952332\n",
      "epoch: 1 step: 2085, loss is 0.5460069179534912\n",
      "epoch: 1 step: 2086, loss is 0.7329862713813782\n",
      "epoch: 1 step: 2087, loss is 0.6732386350631714\n",
      "epoch: 1 step: 2088, loss is 0.6086734533309937\n",
      "epoch: 1 step: 2089, loss is 0.7015220522880554\n",
      "epoch: 1 step: 2090, loss is 0.6253833174705505\n",
      "epoch: 1 step: 2091, loss is 0.7310012578964233\n",
      "epoch: 1 step: 2092, loss is 0.8023339509963989\n",
      "epoch: 1 step: 2093, loss is 0.6766918301582336\n",
      "epoch: 1 step: 2094, loss is 0.7162989377975464\n",
      "epoch: 1 step: 2095, loss is 0.6133418083190918\n",
      "epoch: 1 step: 2096, loss is 0.6214151382446289\n",
      "epoch: 1 step: 2097, loss is 0.6743493676185608\n",
      "epoch: 1 step: 2098, loss is 0.772950291633606\n",
      "epoch: 1 step: 2099, loss is 0.7326747179031372\n",
      "epoch: 1 step: 2100, loss is 0.8111194968223572\n",
      "epoch: 1 step: 2101, loss is 0.587620735168457\n",
      "epoch: 1 step: 2102, loss is 0.7921424508094788\n",
      "epoch: 1 step: 2103, loss is 0.6416609287261963\n",
      "epoch: 1 step: 2104, loss is 0.6636949777603149\n",
      "epoch: 1 step: 2105, loss is 0.6375564932823181\n",
      "epoch: 1 step: 2106, loss is 0.7532340884208679\n",
      "epoch: 1 step: 2107, loss is 0.7932685017585754\n",
      "epoch: 1 step: 2108, loss is 0.7563202381134033\n",
      "epoch: 1 step: 2109, loss is 0.6383332014083862\n",
      "epoch: 1 step: 2110, loss is 0.7634286284446716\n",
      "epoch: 1 step: 2111, loss is 0.6698198318481445\n",
      "epoch: 1 step: 2112, loss is 0.6100097894668579\n",
      "epoch: 1 step: 2113, loss is 0.8176136016845703\n",
      "epoch: 1 step: 2114, loss is 0.6811231374740601\n",
      "epoch: 1 step: 2115, loss is 0.6663783192634583\n",
      "epoch: 1 step: 2116, loss is 0.6683216094970703\n",
      "epoch: 1 step: 2117, loss is 0.6755726337432861\n",
      "epoch: 1 step: 2118, loss is 0.8171778321266174\n",
      "epoch: 1 step: 2119, loss is 0.7050500512123108\n",
      "epoch: 1 step: 2120, loss is 0.6972024440765381\n",
      "epoch: 1 step: 2121, loss is 0.7066207528114319\n",
      "epoch: 1 step: 2122, loss is 0.8875534534454346\n",
      "epoch: 1 step: 2123, loss is 0.7219575643539429\n",
      "epoch: 1 step: 2124, loss is 0.6059114933013916\n",
      "epoch: 1 step: 2125, loss is 0.6798098087310791\n",
      "epoch: 1 step: 2126, loss is 0.7151092886924744\n",
      "epoch: 1 step: 2127, loss is 0.6698416471481323\n",
      "epoch: 1 step: 2128, loss is 0.6459876894950867\n",
      "epoch: 1 step: 2129, loss is 0.6441668272018433\n",
      "epoch: 1 step: 2130, loss is 0.548171877861023\n",
      "epoch: 1 step: 2131, loss is 0.80534428358078\n",
      "epoch: 1 step: 2132, loss is 0.6261921525001526\n",
      "epoch: 1 step: 2133, loss is 0.7284272909164429\n",
      "epoch: 1 step: 2134, loss is 0.8209540843963623\n",
      "epoch: 1 step: 2135, loss is 0.5670059323310852\n",
      "epoch: 1 step: 2136, loss is 0.6730183959007263\n",
      "epoch: 1 step: 2137, loss is 0.5927726030349731\n",
      "epoch: 1 step: 2138, loss is 0.5979844927787781\n",
      "epoch: 1 step: 2139, loss is 0.9344989061355591\n",
      "epoch: 1 step: 2140, loss is 0.7531930804252625\n",
      "epoch: 1 step: 2141, loss is 0.7806183695793152\n",
      "epoch: 1 step: 2142, loss is 0.626000165939331\n",
      "epoch: 1 step: 2143, loss is 0.7066962718963623\n",
      "epoch: 1 step: 2144, loss is 0.6274596452713013\n",
      "epoch: 1 step: 2145, loss is 0.6219602823257446\n",
      "epoch: 1 step: 2146, loss is 0.7263330817222595\n",
      "epoch: 1 step: 2147, loss is 0.617658793926239\n",
      "epoch: 1 step: 2148, loss is 0.7673559188842773\n",
      "epoch: 1 step: 2149, loss is 0.8332216143608093\n",
      "epoch: 1 step: 2150, loss is 0.7662238478660583\n",
      "epoch: 1 step: 2151, loss is 0.6227290034294128\n",
      "epoch: 1 step: 2152, loss is 0.6270104646682739\n",
      "epoch: 1 step: 2153, loss is 0.6071322560310364\n",
      "epoch: 1 step: 2154, loss is 0.6747024655342102\n",
      "epoch: 1 step: 2155, loss is 0.756684422492981\n",
      "epoch: 1 step: 2156, loss is 0.6584475636482239\n",
      "epoch: 1 step: 2157, loss is 0.6414146423339844\n",
      "epoch: 1 step: 2158, loss is 0.8920640349388123\n",
      "epoch: 1 step: 2159, loss is 0.6603028178215027\n",
      "epoch: 1 step: 2160, loss is 0.8158438205718994\n",
      "epoch: 1 step: 2161, loss is 0.710030734539032\n",
      "epoch: 1 step: 2162, loss is 0.6273397207260132\n",
      "epoch: 1 step: 2163, loss is 0.6094218492507935\n",
      "epoch: 1 step: 2164, loss is 0.8738341331481934\n",
      "epoch: 1 step: 2165, loss is 0.698556661605835\n",
      "epoch: 1 step: 2166, loss is 0.6110776662826538\n",
      "epoch: 1 step: 2167, loss is 0.5676644444465637\n",
      "epoch: 1 step: 2168, loss is 0.6650264263153076\n",
      "epoch: 1 step: 2169, loss is 0.6576125025749207\n",
      "epoch: 1 step: 2170, loss is 0.6520354747772217\n",
      "epoch: 1 step: 2171, loss is 0.763658881187439\n",
      "epoch: 1 step: 2172, loss is 0.6563825011253357\n",
      "epoch: 1 step: 2173, loss is 0.6655616760253906\n",
      "epoch: 1 step: 2174, loss is 0.6014087200164795\n",
      "epoch: 1 step: 2175, loss is 0.5769500136375427\n",
      "epoch: 1 step: 2176, loss is 0.7162213325500488\n",
      "epoch: 1 step: 2177, loss is 0.7083868980407715\n",
      "epoch: 1 step: 2178, loss is 0.7208799123764038\n",
      "epoch: 1 step: 2179, loss is 0.708168625831604\n",
      "epoch: 1 step: 2180, loss is 0.6334397196769714\n",
      "epoch: 1 step: 2181, loss is 0.6129049062728882\n",
      "epoch: 1 step: 2182, loss is 0.6884500980377197\n",
      "epoch: 1 step: 2183, loss is 0.6359657645225525\n",
      "epoch: 1 step: 2184, loss is 0.6662946939468384\n",
      "epoch: 1 step: 2185, loss is 0.46230608224868774\n",
      "epoch: 1 step: 2186, loss is 0.7441608905792236\n",
      "epoch: 1 step: 2187, loss is 0.5906352996826172\n",
      "epoch: 1 step: 2188, loss is 0.765145480632782\n",
      "epoch: 1 step: 2189, loss is 0.6801859736442566\n",
      "epoch: 1 step: 2190, loss is 0.49539241194725037\n",
      "epoch: 1 step: 2191, loss is 0.8821878433227539\n",
      "epoch: 1 step: 2192, loss is 0.6555946469306946\n",
      "epoch: 1 step: 2193, loss is 0.689603865146637\n",
      "epoch: 1 step: 2194, loss is 0.5163987278938293\n",
      "epoch: 1 step: 2195, loss is 0.8914344310760498\n",
      "epoch: 1 step: 2196, loss is 0.7694761753082275\n",
      "epoch: 1 step: 2197, loss is 0.6592859625816345\n",
      "epoch: 1 step: 2198, loss is 0.7639822363853455\n",
      "epoch: 1 step: 2199, loss is 0.5935243964195251\n",
      "epoch: 1 step: 2200, loss is 0.6677331328392029\n",
      "epoch: 1 step: 2201, loss is 0.6687924861907959\n",
      "epoch: 1 step: 2202, loss is 0.6095981597900391\n",
      "epoch: 1 step: 2203, loss is 0.7139804363250732\n",
      "epoch: 1 step: 2204, loss is 0.8520545959472656\n",
      "epoch: 1 step: 2205, loss is 0.7133246064186096\n",
      "epoch: 1 step: 2206, loss is 0.718492329120636\n",
      "epoch: 1 step: 2207, loss is 0.6307433247566223\n",
      "epoch: 1 step: 2208, loss is 0.6126081943511963\n",
      "epoch: 1 step: 2209, loss is 0.7169997692108154\n",
      "epoch: 1 step: 2210, loss is 1.0172932147979736\n",
      "epoch: 1 step: 2211, loss is 0.8517141938209534\n",
      "epoch: 1 step: 2212, loss is 0.880998969078064\n",
      "epoch: 1 step: 2213, loss is 0.8131285309791565\n",
      "epoch: 1 step: 2214, loss is 0.7029352188110352\n",
      "epoch: 1 step: 2215, loss is 0.6126789450645447\n",
      "epoch: 1 step: 2216, loss is 0.6640723943710327\n",
      "epoch: 1 step: 2217, loss is 0.668683648109436\n",
      "epoch: 1 step: 2218, loss is 0.6828393936157227\n",
      "epoch: 1 step: 2219, loss is 0.7326675653457642\n",
      "epoch: 1 step: 2220, loss is 0.7094730734825134\n",
      "epoch: 1 step: 2221, loss is 0.6508159637451172\n",
      "epoch: 1 step: 2222, loss is 0.8620123863220215\n",
      "epoch: 1 step: 2223, loss is 0.6825582385063171\n",
      "epoch: 1 step: 2224, loss is 0.6898267269134521\n",
      "epoch: 1 step: 2225, loss is 0.6936249136924744\n",
      "epoch: 1 step: 2226, loss is 0.739971399307251\n",
      "epoch: 1 step: 2227, loss is 0.6601021885871887\n",
      "epoch: 1 step: 2228, loss is 0.8282468914985657\n",
      "epoch: 1 step: 2229, loss is 0.7948511242866516\n",
      "epoch: 1 step: 2230, loss is 0.5873463153839111\n",
      "epoch: 1 step: 2231, loss is 0.61960768699646\n",
      "epoch: 1 step: 2232, loss is 0.6525360941886902\n",
      "epoch: 1 step: 2233, loss is 0.7539636492729187\n",
      "epoch: 1 step: 2234, loss is 0.7556706070899963\n",
      "epoch: 1 step: 2235, loss is 0.7484111785888672\n",
      "epoch: 1 step: 2236, loss is 0.7592138051986694\n",
      "epoch: 1 step: 2237, loss is 0.8152400851249695\n",
      "epoch: 1 step: 2238, loss is 0.8183399438858032\n",
      "epoch: 1 step: 2239, loss is 0.7017339468002319\n",
      "epoch: 1 step: 2240, loss is 0.5943721532821655\n",
      "epoch: 1 step: 2241, loss is 0.6135447025299072\n",
      "epoch: 1 step: 2242, loss is 0.8133261799812317\n",
      "epoch: 1 step: 2243, loss is 0.7876942753791809\n",
      "epoch: 1 step: 2244, loss is 0.6100571751594543\n",
      "epoch: 1 step: 2245, loss is 0.6314733028411865\n",
      "epoch: 1 step: 2246, loss is 0.6868684887886047\n",
      "epoch: 1 step: 2247, loss is 0.859795331954956\n",
      "epoch: 1 step: 2248, loss is 0.6882538795471191\n",
      "epoch: 1 step: 2249, loss is 0.6937834620475769\n",
      "epoch: 1 step: 2250, loss is 0.6766099333763123\n",
      "epoch: 1 step: 2251, loss is 0.5744776129722595\n",
      "epoch: 1 step: 2252, loss is 0.7644960284233093\n",
      "epoch: 1 step: 2253, loss is 0.6850869655609131\n",
      "epoch: 1 step: 2254, loss is 0.6230515241622925\n",
      "epoch: 1 step: 2255, loss is 0.5909101366996765\n",
      "epoch: 1 step: 2256, loss is 0.6364637017250061\n",
      "epoch: 1 step: 2257, loss is 0.6322401762008667\n",
      "epoch: 1 step: 2258, loss is 0.7700862288475037\n",
      "epoch: 1 step: 2259, loss is 0.6532307863235474\n",
      "epoch: 1 step: 2260, loss is 0.8277354836463928\n",
      "epoch: 1 step: 2261, loss is 0.5319223999977112\n",
      "epoch: 1 step: 2262, loss is 0.8595470786094666\n",
      "epoch: 1 step: 2263, loss is 0.5980085730552673\n",
      "epoch: 1 step: 2264, loss is 0.6613261103630066\n",
      "epoch: 1 step: 2265, loss is 0.6213067770004272\n",
      "epoch: 1 step: 2266, loss is 0.6896700859069824\n",
      "epoch: 1 step: 2267, loss is 0.7054280042648315\n",
      "epoch: 1 step: 2268, loss is 0.6649909019470215\n",
      "epoch: 1 step: 2269, loss is 0.6376407146453857\n",
      "epoch: 1 step: 2270, loss is 0.7291008234024048\n",
      "epoch: 1 step: 2271, loss is 0.7766419053077698\n",
      "epoch: 1 step: 2272, loss is 0.6732314229011536\n",
      "epoch: 1 step: 2273, loss is 0.6897302269935608\n",
      "epoch: 1 step: 2274, loss is 0.7592753171920776\n",
      "epoch: 1 step: 2275, loss is 0.7187383770942688\n",
      "epoch: 1 step: 2276, loss is 0.6643064618110657\n",
      "epoch: 1 step: 2277, loss is 0.6546087265014648\n",
      "epoch: 1 step: 2278, loss is 0.7370627522468567\n",
      "epoch: 1 step: 2279, loss is 0.5685720443725586\n",
      "epoch: 1 step: 2280, loss is 0.6391652822494507\n",
      "epoch: 1 step: 2281, loss is 0.6028851270675659\n",
      "epoch: 1 step: 2282, loss is 0.6308411955833435\n",
      "epoch: 1 step: 2283, loss is 0.635676383972168\n",
      "epoch: 1 step: 2284, loss is 0.6107723712921143\n",
      "epoch: 1 step: 2285, loss is 0.7112584114074707\n",
      "epoch: 1 step: 2286, loss is 0.6741703748703003\n",
      "epoch: 1 step: 2287, loss is 0.7082860469818115\n",
      "epoch: 1 step: 2288, loss is 0.6952289938926697\n",
      "epoch: 1 step: 2289, loss is 0.6092906594276428\n",
      "epoch: 1 step: 2290, loss is 0.6130275130271912\n",
      "epoch: 1 step: 2291, loss is 0.678333580493927\n",
      "epoch: 1 step: 2292, loss is 0.6902666091918945\n",
      "epoch: 1 step: 2293, loss is 0.6940970420837402\n",
      "epoch: 1 step: 2294, loss is 0.7055065035820007\n",
      "epoch: 1 step: 2295, loss is 0.5390565991401672\n",
      "epoch: 1 step: 2296, loss is 0.7056835293769836\n",
      "epoch: 1 step: 2297, loss is 0.9615800976753235\n",
      "epoch: 1 step: 2298, loss is 0.6918259263038635\n",
      "epoch: 1 step: 2299, loss is 0.6596198081970215\n",
      "epoch: 1 step: 2300, loss is 0.8541374206542969\n",
      "epoch: 1 step: 2301, loss is 0.5108385682106018\n",
      "epoch: 1 step: 2302, loss is 0.6360154151916504\n",
      "epoch: 1 step: 2303, loss is 0.5796161890029907\n",
      "epoch: 1 step: 2304, loss is 0.6094404458999634\n",
      "epoch: 1 step: 2305, loss is 0.8057996034622192\n",
      "epoch: 1 step: 2306, loss is 0.6707411408424377\n",
      "epoch: 1 step: 2307, loss is 0.6840255856513977\n",
      "epoch: 1 step: 2308, loss is 0.7250354290008545\n",
      "epoch: 1 step: 2309, loss is 0.6001732349395752\n",
      "epoch: 1 step: 2310, loss is 0.6090580821037292\n",
      "epoch: 1 step: 2311, loss is 0.6736420392990112\n",
      "epoch: 1 step: 2312, loss is 0.7340405583381653\n",
      "epoch: 1 step: 2313, loss is 0.7034827470779419\n",
      "epoch: 1 step: 2314, loss is 0.5451790690422058\n",
      "epoch: 1 step: 2315, loss is 0.8960829377174377\n",
      "epoch: 1 step: 2316, loss is 0.558975100517273\n",
      "epoch: 1 step: 2317, loss is 0.7200854420661926\n",
      "epoch: 1 step: 2318, loss is 0.6050431728363037\n",
      "epoch: 1 step: 2319, loss is 0.690921425819397\n",
      "epoch: 1 step: 2320, loss is 0.6652332544326782\n",
      "epoch: 1 step: 2321, loss is 0.9774326086044312\n",
      "epoch: 1 step: 2322, loss is 0.5711172819137573\n",
      "epoch: 1 step: 2323, loss is 0.5597811937332153\n",
      "epoch: 1 step: 2324, loss is 0.5696544647216797\n",
      "epoch: 1 step: 2325, loss is 0.5556847453117371\n",
      "epoch: 1 step: 2326, loss is 0.7014219164848328\n",
      "epoch: 1 step: 2327, loss is 0.7170050144195557\n",
      "epoch: 1 step: 2328, loss is 0.8928797841072083\n",
      "epoch: 1 step: 2329, loss is 0.6477320790290833\n",
      "epoch: 1 step: 2330, loss is 0.6795823574066162\n",
      "epoch: 1 step: 2331, loss is 0.59758061170578\n",
      "epoch: 1 step: 2332, loss is 0.573752224445343\n",
      "epoch: 1 step: 2333, loss is 0.5321218967437744\n",
      "epoch: 1 step: 2334, loss is 0.8448472023010254\n",
      "epoch: 1 step: 2335, loss is 0.6051497459411621\n",
      "epoch: 1 step: 2336, loss is 0.6082038879394531\n",
      "epoch: 1 step: 2337, loss is 0.6950090527534485\n",
      "epoch: 1 step: 2338, loss is 0.8649593591690063\n",
      "epoch: 1 step: 2339, loss is 0.7422390580177307\n",
      "epoch: 1 step: 2340, loss is 0.7116541266441345\n",
      "epoch: 1 step: 2341, loss is 0.67131507396698\n",
      "epoch: 1 step: 2342, loss is 0.628478467464447\n",
      "epoch: 1 step: 2343, loss is 0.602988600730896\n",
      "epoch: 1 step: 2344, loss is 0.6122027039527893\n",
      "epoch: 1 step: 2345, loss is 0.7631779909133911\n",
      "epoch: 1 step: 2346, loss is 0.6211132407188416\n",
      "epoch: 1 step: 2347, loss is 0.8546820282936096\n",
      "epoch: 1 step: 2348, loss is 0.5872831344604492\n",
      "epoch: 1 step: 2349, loss is 0.4649466276168823\n",
      "epoch: 1 step: 2350, loss is 0.6456721425056458\n",
      "epoch: 1 step: 2351, loss is 0.946171760559082\n",
      "epoch: 1 step: 2352, loss is 0.7554463744163513\n",
      "epoch: 1 step: 2353, loss is 0.6657744646072388\n",
      "epoch: 1 step: 2354, loss is 0.548039972782135\n",
      "epoch: 1 step: 2355, loss is 0.7522236108779907\n",
      "epoch: 1 step: 2356, loss is 0.5989030003547668\n",
      "epoch: 1 step: 2357, loss is 0.6355155110359192\n",
      "epoch: 1 step: 2358, loss is 0.7017841339111328\n",
      "epoch: 1 step: 2359, loss is 0.6383006572723389\n",
      "epoch: 1 step: 2360, loss is 0.7060909867286682\n",
      "epoch: 1 step: 2361, loss is 0.6817170977592468\n",
      "epoch: 1 step: 2362, loss is 0.6443377733230591\n",
      "epoch: 1 step: 2363, loss is 0.6368902921676636\n",
      "epoch: 1 step: 2364, loss is 0.5395641326904297\n",
      "epoch: 1 step: 2365, loss is 0.8213870525360107\n",
      "epoch: 1 step: 2366, loss is 0.8783119320869446\n",
      "epoch: 1 step: 2367, loss is 0.69536292552948\n",
      "epoch: 1 step: 2368, loss is 0.4637209177017212\n",
      "epoch: 1 step: 2369, loss is 0.6626745462417603\n",
      "epoch: 1 step: 2370, loss is 0.6249098777770996\n",
      "epoch: 1 step: 2371, loss is 0.7480461597442627\n",
      "epoch: 1 step: 2372, loss is 0.7878841757774353\n",
      "epoch: 1 step: 2373, loss is 0.7654314041137695\n",
      "epoch: 1 step: 2374, loss is 0.7013753056526184\n",
      "epoch: 1 step: 2375, loss is 0.6659321188926697\n",
      "epoch: 1 step: 2376, loss is 0.6591428518295288\n",
      "epoch: 1 step: 2377, loss is 0.7338404655456543\n",
      "epoch: 1 step: 2378, loss is 0.7721636295318604\n",
      "epoch: 1 step: 2379, loss is 0.678395688533783\n",
      "epoch: 1 step: 2380, loss is 0.5927708148956299\n",
      "epoch: 1 step: 2381, loss is 0.7539767026901245\n",
      "epoch: 1 step: 2382, loss is 0.6809131503105164\n",
      "epoch: 1 step: 2383, loss is 0.6427772641181946\n",
      "epoch: 1 step: 2384, loss is 0.7694271206855774\n",
      "epoch: 1 step: 2385, loss is 0.6607793569564819\n",
      "epoch: 1 step: 2386, loss is 0.5374592542648315\n",
      "epoch: 1 step: 2387, loss is 0.5949485301971436\n",
      "epoch: 1 step: 2388, loss is 0.6191920638084412\n",
      "epoch: 1 step: 2389, loss is 0.725193440914154\n",
      "epoch: 1 step: 2390, loss is 0.6946324110031128\n",
      "epoch: 1 step: 2391, loss is 0.6294772624969482\n",
      "epoch: 1 step: 2392, loss is 0.664939820766449\n",
      "epoch: 1 step: 2393, loss is 0.822213351726532\n",
      "epoch: 1 step: 2394, loss is 0.5062090158462524\n",
      "epoch: 1 step: 2395, loss is 0.45908641815185547\n",
      "epoch: 1 step: 2396, loss is 0.7667872309684753\n",
      "epoch: 1 step: 2397, loss is 0.8073385953903198\n",
      "epoch: 1 step: 2398, loss is 0.8389986753463745\n",
      "epoch: 1 step: 2399, loss is 0.5765877962112427\n",
      "epoch: 1 step: 2400, loss is 0.6679434180259705\n",
      "epoch: 1 step: 2401, loss is 0.605446994304657\n",
      "epoch: 1 step: 2402, loss is 0.6372793912887573\n",
      "epoch: 1 step: 2403, loss is 0.4941639006137848\n",
      "epoch: 1 step: 2404, loss is 0.8145495653152466\n",
      "epoch: 1 step: 2405, loss is 0.6183792948722839\n",
      "epoch: 1 step: 2406, loss is 0.6018869280815125\n",
      "epoch: 1 step: 2407, loss is 0.7361886501312256\n",
      "epoch: 1 step: 2408, loss is 0.8573296070098877\n",
      "epoch: 1 step: 2409, loss is 0.6816364526748657\n",
      "epoch: 1 step: 2410, loss is 0.7097978591918945\n",
      "epoch: 1 step: 2411, loss is 0.7072732448577881\n",
      "epoch: 1 step: 2412, loss is 0.6156068444252014\n",
      "epoch: 1 step: 2413, loss is 0.48101505637168884\n",
      "epoch: 1 step: 2414, loss is 0.6102284789085388\n",
      "epoch: 1 step: 2415, loss is 0.6474709510803223\n",
      "epoch: 1 step: 2416, loss is 0.6510938405990601\n",
      "epoch: 1 step: 2417, loss is 0.949070394039154\n",
      "epoch: 1 step: 2418, loss is 0.6310731172561646\n",
      "epoch: 1 step: 2419, loss is 0.6409883499145508\n",
      "epoch: 1 step: 2420, loss is 0.7332520484924316\n",
      "epoch: 1 step: 2421, loss is 0.5886171460151672\n",
      "epoch: 1 step: 2422, loss is 0.5855768322944641\n",
      "epoch: 1 step: 2423, loss is 0.7589293122291565\n",
      "epoch: 1 step: 2424, loss is 0.6189755201339722\n",
      "epoch: 1 step: 2425, loss is 0.7439849376678467\n",
      "epoch: 1 step: 2426, loss is 0.6126859784126282\n",
      "epoch: 1 step: 2427, loss is 0.6654883623123169\n",
      "epoch: 1 step: 2428, loss is 0.7334015965461731\n",
      "epoch: 1 step: 2429, loss is 0.6065895557403564\n",
      "epoch: 1 step: 2430, loss is 0.5866371393203735\n",
      "epoch: 1 step: 2431, loss is 0.7840015292167664\n",
      "epoch: 1 step: 2432, loss is 0.7513400316238403\n",
      "epoch: 1 step: 2433, loss is 0.5376430153846741\n",
      "epoch: 1 step: 2434, loss is 0.6579999327659607\n",
      "epoch: 1 step: 2435, loss is 0.6228045225143433\n",
      "epoch: 1 step: 2436, loss is 0.6370213627815247\n",
      "epoch: 1 step: 2437, loss is 0.8328235149383545\n",
      "epoch: 1 step: 2438, loss is 0.7366526126861572\n",
      "epoch: 1 step: 2439, loss is 0.721455991268158\n",
      "epoch: 1 step: 2440, loss is 0.64836186170578\n",
      "epoch: 1 step: 2441, loss is 0.5754597783088684\n",
      "epoch: 1 step: 2442, loss is 0.6003249287605286\n",
      "epoch: 1 step: 2443, loss is 0.7045252323150635\n",
      "epoch: 1 step: 2444, loss is 0.7123779654502869\n",
      "epoch: 1 step: 2445, loss is 0.5641804933547974\n",
      "epoch: 1 step: 2446, loss is 0.7331075668334961\n",
      "epoch: 1 step: 2447, loss is 0.6991205811500549\n",
      "epoch: 1 step: 2448, loss is 0.705974280834198\n",
      "epoch: 1 step: 2449, loss is 0.6124723553657532\n",
      "epoch: 1 step: 2450, loss is 0.7939201593399048\n",
      "epoch: 1 step: 2451, loss is 0.7362639904022217\n",
      "epoch: 1 step: 2452, loss is 0.6763823628425598\n",
      "epoch: 1 step: 2453, loss is 0.6708928346633911\n",
      "epoch: 1 step: 2454, loss is 0.5946821570396423\n",
      "epoch: 1 step: 2455, loss is 0.6025318503379822\n",
      "epoch: 1 step: 2456, loss is 0.6598783731460571\n",
      "epoch: 1 step: 2457, loss is 0.6178427934646606\n",
      "epoch: 1 step: 2458, loss is 0.6597306132316589\n",
      "epoch: 1 step: 2459, loss is 0.5946361422538757\n",
      "epoch: 1 step: 2460, loss is 0.6027711033821106\n",
      "epoch: 1 step: 2461, loss is 0.6432554721832275\n",
      "epoch: 1 step: 2462, loss is 0.40760236978530884\n",
      "epoch: 1 step: 2463, loss is 0.8330304026603699\n",
      "epoch: 1 step: 2464, loss is 0.8541728854179382\n",
      "epoch: 1 step: 2465, loss is 0.7527672648429871\n",
      "epoch: 1 step: 2466, loss is 0.8034830093383789\n",
      "epoch: 1 step: 2467, loss is 0.6498938798904419\n",
      "epoch: 1 step: 2468, loss is 0.5451833605766296\n",
      "epoch: 1 step: 2469, loss is 0.6809527277946472\n",
      "epoch: 1 step: 2470, loss is 0.6670045852661133\n",
      "epoch: 1 step: 2471, loss is 0.8608906269073486\n",
      "epoch: 1 step: 2472, loss is 0.7619803547859192\n",
      "epoch: 1 step: 2473, loss is 0.767687976360321\n",
      "epoch: 1 step: 2474, loss is 0.5385510325431824\n",
      "epoch: 1 step: 2475, loss is 0.7284611463546753\n",
      "epoch: 1 step: 2476, loss is 0.9424116015434265\n",
      "epoch: 1 step: 2477, loss is 0.6929338574409485\n",
      "epoch: 1 step: 2478, loss is 0.719789445400238\n",
      "epoch: 1 step: 2479, loss is 0.6170276403427124\n",
      "epoch: 1 step: 2480, loss is 0.6495704054832458\n",
      "epoch: 1 step: 2481, loss is 0.5747848153114319\n",
      "epoch: 1 step: 2482, loss is 0.7654834389686584\n",
      "epoch: 1 step: 2483, loss is 0.7240874171257019\n",
      "epoch: 1 step: 2484, loss is 0.5488632321357727\n",
      "epoch: 1 step: 2485, loss is 0.7445870637893677\n",
      "epoch: 1 step: 2486, loss is 0.5301755666732788\n",
      "epoch: 1 step: 2487, loss is 0.5861360430717468\n",
      "epoch: 1 step: 2488, loss is 0.6372837424278259\n",
      "epoch: 1 step: 2489, loss is 0.5724217295646667\n",
      "epoch: 1 step: 2490, loss is 0.5104669332504272\n",
      "epoch: 1 step: 2491, loss is 0.7054192423820496\n",
      "epoch: 1 step: 2492, loss is 0.6377296447753906\n",
      "epoch: 1 step: 2493, loss is 0.7204419374465942\n",
      "epoch: 1 step: 2494, loss is 0.742358922958374\n",
      "epoch: 1 step: 2495, loss is 0.5616693496704102\n",
      "epoch: 1 step: 2496, loss is 0.631617546081543\n",
      "epoch: 1 step: 2497, loss is 0.5365830659866333\n",
      "epoch: 1 step: 2498, loss is 0.7180805802345276\n",
      "epoch: 1 step: 2499, loss is 0.6727999448776245\n",
      "epoch: 1 step: 2500, loss is 0.7043851613998413\n",
      "epoch: 1 step: 2501, loss is 0.7956631183624268\n",
      "epoch: 1 step: 2502, loss is 0.5183833241462708\n",
      "epoch: 1 step: 2503, loss is 0.6118906140327454\n",
      "epoch: 1 step: 2504, loss is 0.6318899393081665\n",
      "epoch: 1 step: 2505, loss is 0.4979484975337982\n",
      "epoch: 1 step: 2506, loss is 0.5255908370018005\n",
      "epoch: 1 step: 2507, loss is 0.5261852145195007\n",
      "epoch: 1 step: 2508, loss is 0.6265606880187988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: unknown JFIF revision number 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 1, loss is 0.4324832856655121\n",
      "epoch: 2 step: 2, loss is 0.807800829410553\n",
      "epoch: 2 step: 3, loss is 0.5668818950653076\n",
      "epoch: 2 step: 4, loss is 0.7725943922996521\n",
      "epoch: 2 step: 5, loss is 0.42559313774108887\n",
      "epoch: 2 step: 6, loss is 0.5771293044090271\n",
      "epoch: 2 step: 7, loss is 0.6826063990592957\n",
      "epoch: 2 step: 8, loss is 0.6983305215835571\n",
      "epoch: 2 step: 9, loss is 0.6381983757019043\n",
      "epoch: 2 step: 10, loss is 0.8669359087944031\n",
      "epoch: 2 step: 11, loss is 0.783919632434845\n",
      "epoch: 2 step: 12, loss is 0.6718325018882751\n",
      "epoch: 2 step: 13, loss is 0.7330284118652344\n",
      "epoch: 2 step: 14, loss is 0.686238169670105\n",
      "epoch: 2 step: 15, loss is 0.9054510593414307\n",
      "epoch: 2 step: 16, loss is 0.6128048896789551\n",
      "epoch: 2 step: 17, loss is 0.8461189270019531\n",
      "epoch: 2 step: 18, loss is 0.7585234642028809\n",
      "epoch: 2 step: 19, loss is 0.9070937037467957\n",
      "epoch: 2 step: 20, loss is 0.8438024520874023\n",
      "epoch: 2 step: 21, loss is 0.5411402583122253\n",
      "epoch: 2 step: 22, loss is 0.6051280498504639\n",
      "epoch: 2 step: 23, loss is 0.5936122536659241\n",
      "epoch: 2 step: 24, loss is 0.6701382398605347\n",
      "epoch: 2 step: 25, loss is 0.6170892119407654\n",
      "epoch: 2 step: 26, loss is 0.6820498704910278\n",
      "epoch: 2 step: 27, loss is 0.6273791790008545\n",
      "epoch: 2 step: 28, loss is 0.6616476774215698\n",
      "epoch: 2 step: 29, loss is 0.5654580593109131\n",
      "epoch: 2 step: 30, loss is 0.7682538032531738\n",
      "epoch: 2 step: 31, loss is 0.6123680472373962\n",
      "epoch: 2 step: 32, loss is 0.7822893261909485\n",
      "epoch: 2 step: 33, loss is 0.7132659554481506\n",
      "epoch: 2 step: 34, loss is 0.7118070721626282\n",
      "epoch: 2 step: 35, loss is 0.6064708232879639\n",
      "epoch: 2 step: 36, loss is 0.7184188365936279\n",
      "epoch: 2 step: 37, loss is 0.6855376362800598\n",
      "epoch: 2 step: 38, loss is 0.6767016053199768\n",
      "epoch: 2 step: 39, loss is 0.5746128559112549\n",
      "epoch: 2 step: 40, loss is 0.6833998560905457\n",
      "epoch: 2 step: 41, loss is 0.5665767788887024\n",
      "epoch: 2 step: 42, loss is 0.6095200777053833\n",
      "epoch: 2 step: 43, loss is 0.5387088060379028\n",
      "epoch: 2 step: 44, loss is 0.6398264765739441\n",
      "epoch: 2 step: 45, loss is 0.6284552812576294\n",
      "epoch: 2 step: 46, loss is 0.6930450797080994\n",
      "epoch: 2 step: 47, loss is 0.8055093288421631\n",
      "epoch: 2 step: 48, loss is 0.7364052534103394\n",
      "epoch: 2 step: 49, loss is 0.6980006098747253\n",
      "epoch: 2 step: 50, loss is 0.5830461382865906\n",
      "epoch: 2 step: 51, loss is 0.6566838622093201\n",
      "epoch: 2 step: 52, loss is 0.7815543413162231\n",
      "epoch: 2 step: 53, loss is 0.7665642499923706\n",
      "epoch: 2 step: 54, loss is 0.7180795073509216\n",
      "epoch: 2 step: 55, loss is 0.6034645438194275\n",
      "epoch: 2 step: 56, loss is 0.8149028420448303\n",
      "epoch: 2 step: 57, loss is 0.5809779167175293\n",
      "epoch: 2 step: 58, loss is 0.5665463209152222\n",
      "epoch: 2 step: 59, loss is 0.6350060105323792\n",
      "epoch: 2 step: 60, loss is 0.5775761008262634\n",
      "epoch: 2 step: 61, loss is 0.8600178956985474\n",
      "epoch: 2 step: 62, loss is 0.7637465596199036\n",
      "epoch: 2 step: 63, loss is 0.6676653623580933\n",
      "epoch: 2 step: 64, loss is 0.6400025486946106\n",
      "epoch: 2 step: 65, loss is 0.6285751461982727\n",
      "epoch: 2 step: 66, loss is 0.5337796807289124\n",
      "epoch: 2 step: 67, loss is 0.6347435116767883\n",
      "epoch: 2 step: 68, loss is 0.609752357006073\n",
      "epoch: 2 step: 69, loss is 0.7564952373504639\n",
      "epoch: 2 step: 70, loss is 0.584566593170166\n",
      "epoch: 2 step: 71, loss is 0.6017563939094543\n",
      "epoch: 2 step: 72, loss is 0.7091212868690491\n",
      "epoch: 2 step: 73, loss is 0.5637061595916748\n",
      "epoch: 2 step: 74, loss is 0.5573479533195496\n",
      "epoch: 2 step: 75, loss is 0.6479170322418213\n",
      "epoch: 2 step: 76, loss is 0.6943460702896118\n",
      "epoch: 2 step: 77, loss is 0.5857968926429749\n",
      "epoch: 2 step: 78, loss is 0.5102571249008179\n",
      "epoch: 2 step: 79, loss is 0.5004931092262268\n",
      "epoch: 2 step: 80, loss is 0.7620131373405457\n",
      "epoch: 2 step: 81, loss is 0.9289402365684509\n",
      "epoch: 2 step: 82, loss is 0.6054641604423523\n",
      "epoch: 2 step: 83, loss is 0.6762978434562683\n",
      "epoch: 2 step: 84, loss is 0.6151714324951172\n",
      "epoch: 2 step: 85, loss is 0.5492174029350281\n",
      "epoch: 2 step: 86, loss is 0.5174504518508911\n",
      "epoch: 2 step: 87, loss is 0.6702316403388977\n",
      "epoch: 2 step: 88, loss is 0.805181086063385\n",
      "epoch: 2 step: 89, loss is 0.5371659398078918\n",
      "epoch: 2 step: 90, loss is 0.5092007517814636\n",
      "epoch: 2 step: 91, loss is 0.6699303388595581\n",
      "epoch: 2 step: 92, loss is 1.0078486204147339\n",
      "epoch: 2 step: 93, loss is 0.49515870213508606\n",
      "epoch: 2 step: 94, loss is 0.4428912401199341\n",
      "epoch: 2 step: 95, loss is 0.5855044722557068\n",
      "epoch: 2 step: 96, loss is 0.5657139420509338\n",
      "epoch: 2 step: 97, loss is 0.612626850605011\n",
      "epoch: 2 step: 98, loss is 0.7271019220352173\n",
      "epoch: 2 step: 99, loss is 0.8235269784927368\n",
      "epoch: 2 step: 100, loss is 0.7882719039916992\n",
      "epoch: 2 step: 101, loss is 0.5888530611991882\n",
      "epoch: 2 step: 102, loss is 0.6418038606643677\n",
      "epoch: 2 step: 103, loss is 0.7090908885002136\n",
      "epoch: 2 step: 104, loss is 0.6683564186096191\n",
      "epoch: 2 step: 105, loss is 0.5946686267852783\n",
      "epoch: 2 step: 106, loss is 0.7226806282997131\n",
      "epoch: 2 step: 107, loss is 0.8316810727119446\n",
      "epoch: 2 step: 108, loss is 0.9634948372840881\n",
      "epoch: 2 step: 109, loss is 0.6969727277755737\n",
      "epoch: 2 step: 110, loss is 0.5838093161582947\n",
      "epoch: 2 step: 111, loss is 0.853005051612854\n",
      "epoch: 2 step: 112, loss is 0.747546374797821\n",
      "epoch: 2 step: 113, loss is 0.6320292949676514\n",
      "epoch: 2 step: 114, loss is 0.561090886592865\n",
      "epoch: 2 step: 115, loss is 0.768073320388794\n",
      "epoch: 2 step: 116, loss is 0.49249476194381714\n",
      "epoch: 2 step: 117, loss is 0.8785971999168396\n",
      "epoch: 2 step: 118, loss is 0.48730728030204773\n",
      "epoch: 2 step: 119, loss is 0.7610318660736084\n",
      "epoch: 2 step: 120, loss is 0.7391474843025208\n",
      "epoch: 2 step: 121, loss is 0.6764693260192871\n",
      "epoch: 2 step: 122, loss is 1.0241745710372925\n",
      "epoch: 2 step: 123, loss is 0.8528023958206177\n",
      "epoch: 2 step: 124, loss is 0.7205910086631775\n",
      "epoch: 2 step: 125, loss is 0.7071122527122498\n",
      "epoch: 2 step: 126, loss is 0.6718303561210632\n",
      "epoch: 2 step: 127, loss is 0.720549464225769\n",
      "epoch: 2 step: 128, loss is 0.6475040912628174\n",
      "epoch: 2 step: 129, loss is 0.7634118795394897\n",
      "epoch: 2 step: 130, loss is 0.5698370933532715\n",
      "epoch: 2 step: 131, loss is 0.7782611846923828\n",
      "epoch: 2 step: 132, loss is 0.685498058795929\n",
      "epoch: 2 step: 133, loss is 0.6693748831748962\n",
      "epoch: 2 step: 134, loss is 0.6177420020103455\n",
      "epoch: 2 step: 135, loss is 0.6372136473655701\n",
      "epoch: 2 step: 136, loss is 0.7117162346839905\n",
      "epoch: 2 step: 137, loss is 0.7365599870681763\n",
      "epoch: 2 step: 138, loss is 0.8165789246559143\n",
      "epoch: 2 step: 139, loss is 0.7045872211456299\n",
      "epoch: 2 step: 140, loss is 0.7330630421638489\n",
      "epoch: 2 step: 141, loss is 0.5941243171691895\n",
      "epoch: 2 step: 142, loss is 0.7437434792518616\n",
      "epoch: 2 step: 143, loss is 0.7361748218536377\n",
      "epoch: 2 step: 144, loss is 0.659170389175415\n",
      "epoch: 2 step: 145, loss is 0.7135483622550964\n",
      "epoch: 2 step: 146, loss is 0.6767096519470215\n",
      "epoch: 2 step: 147, loss is 0.6984009146690369\n",
      "epoch: 2 step: 148, loss is 0.6994516849517822\n",
      "epoch: 2 step: 149, loss is 0.4670010209083557\n",
      "epoch: 2 step: 150, loss is 0.5528152585029602\n",
      "epoch: 2 step: 151, loss is 0.6311352252960205\n",
      "epoch: 2 step: 152, loss is 0.5877165794372559\n",
      "epoch: 2 step: 153, loss is 0.7436953783035278\n",
      "epoch: 2 step: 154, loss is 0.5158930420875549\n",
      "epoch: 2 step: 155, loss is 0.9089624881744385\n",
      "epoch: 2 step: 156, loss is 0.6074515581130981\n",
      "epoch: 2 step: 157, loss is 0.6875975728034973\n",
      "epoch: 2 step: 158, loss is 0.6117929816246033\n",
      "epoch: 2 step: 159, loss is 0.5914181470870972\n",
      "epoch: 2 step: 160, loss is 0.5652227997779846\n",
      "epoch: 2 step: 161, loss is 0.6251248717308044\n",
      "epoch: 2 step: 162, loss is 0.6153849959373474\n",
      "epoch: 2 step: 163, loss is 0.8944944739341736\n",
      "epoch: 2 step: 164, loss is 0.7863323092460632\n",
      "epoch: 2 step: 165, loss is 0.8344727754592896\n",
      "epoch: 2 step: 166, loss is 0.6426361203193665\n",
      "epoch: 2 step: 167, loss is 0.6038405895233154\n",
      "epoch: 2 step: 168, loss is 0.6360592246055603\n",
      "epoch: 2 step: 169, loss is 0.8486257791519165\n",
      "epoch: 2 step: 170, loss is 0.45202940702438354\n",
      "epoch: 2 step: 171, loss is 0.6563204526901245\n",
      "epoch: 2 step: 172, loss is 0.6730710864067078\n",
      "epoch: 2 step: 173, loss is 0.4971480965614319\n",
      "epoch: 2 step: 174, loss is 0.804356575012207\n",
      "epoch: 2 step: 175, loss is 0.8260600566864014\n",
      "epoch: 2 step: 176, loss is 0.6913148164749146\n",
      "epoch: 2 step: 177, loss is 0.9086018800735474\n",
      "epoch: 2 step: 178, loss is 0.7373946309089661\n",
      "epoch: 2 step: 179, loss is 0.72398841381073\n",
      "epoch: 2 step: 180, loss is 0.8666011095046997\n",
      "epoch: 2 step: 181, loss is 0.6404588222503662\n",
      "epoch: 2 step: 182, loss is 0.7237405776977539\n",
      "epoch: 2 step: 183, loss is 0.7561233043670654\n",
      "epoch: 2 step: 184, loss is 0.8282644152641296\n",
      "epoch: 2 step: 185, loss is 0.713451087474823\n",
      "epoch: 2 step: 186, loss is 0.6112506985664368\n",
      "epoch: 2 step: 187, loss is 0.3811800181865692\n",
      "epoch: 2 step: 188, loss is 0.7704988718032837\n",
      "epoch: 2 step: 189, loss is 0.7317448258399963\n",
      "epoch: 2 step: 190, loss is 0.9217497110366821\n",
      "epoch: 2 step: 191, loss is 0.8889862298965454\n",
      "epoch: 2 step: 192, loss is 0.8203660845756531\n",
      "epoch: 2 step: 193, loss is 0.6793774962425232\n",
      "epoch: 2 step: 194, loss is 0.6474410891532898\n",
      "epoch: 2 step: 195, loss is 0.7148131132125854\n",
      "epoch: 2 step: 196, loss is 0.7258817553520203\n",
      "epoch: 2 step: 197, loss is 0.6827301979064941\n",
      "epoch: 2 step: 198, loss is 0.647714376449585\n",
      "epoch: 2 step: 199, loss is 0.6610049605369568\n",
      "epoch: 2 step: 200, loss is 0.7005520462989807\n",
      "epoch: 2 step: 201, loss is 0.7087704539299011\n",
      "epoch: 2 step: 202, loss is 0.8395594358444214\n",
      "epoch: 2 step: 203, loss is 0.85302734375\n",
      "epoch: 2 step: 204, loss is 0.7065567970275879\n",
      "epoch: 2 step: 205, loss is 0.6394540667533875\n",
      "epoch: 2 step: 206, loss is 0.6599169373512268\n",
      "epoch: 2 step: 207, loss is 0.590646505355835\n",
      "epoch: 2 step: 208, loss is 0.48720601201057434\n",
      "epoch: 2 step: 209, loss is 0.666583240032196\n",
      "epoch: 2 step: 210, loss is 0.8667388558387756\n",
      "epoch: 2 step: 211, loss is 0.607694685459137\n",
      "epoch: 2 step: 212, loss is 0.6926836967468262\n",
      "epoch: 2 step: 213, loss is 0.8184853792190552\n",
      "epoch: 2 step: 214, loss is 0.7030227780342102\n",
      "epoch: 2 step: 215, loss is 0.6627540588378906\n",
      "epoch: 2 step: 216, loss is 0.731575071811676\n",
      "epoch: 2 step: 217, loss is 0.676837682723999\n",
      "epoch: 2 step: 218, loss is 0.6526046991348267\n",
      "epoch: 2 step: 219, loss is 0.7849633693695068\n",
      "epoch: 2 step: 220, loss is 0.6449195742607117\n",
      "epoch: 2 step: 221, loss is 0.7205797433853149\n",
      "epoch: 2 step: 222, loss is 0.624397873878479\n",
      "epoch: 2 step: 223, loss is 0.6566445231437683\n",
      "epoch: 2 step: 224, loss is 0.5118961334228516\n",
      "epoch: 2 step: 225, loss is 0.5957279801368713\n",
      "epoch: 2 step: 226, loss is 0.7139061689376831\n",
      "epoch: 2 step: 227, loss is 0.7217466831207275\n",
      "epoch: 2 step: 228, loss is 0.7323764562606812\n",
      "epoch: 2 step: 229, loss is 0.6131381988525391\n",
      "epoch: 2 step: 230, loss is 0.6438817381858826\n",
      "epoch: 2 step: 231, loss is 0.7536975145339966\n",
      "epoch: 2 step: 232, loss is 0.7685497403144836\n",
      "epoch: 2 step: 233, loss is 0.5865527391433716\n",
      "epoch: 2 step: 234, loss is 0.5974044799804688\n",
      "epoch: 2 step: 235, loss is 0.7016976475715637\n",
      "epoch: 2 step: 236, loss is 0.6426056027412415\n",
      "epoch: 2 step: 237, loss is 0.7642722129821777\n",
      "epoch: 2 step: 238, loss is 0.7239739298820496\n",
      "epoch: 2 step: 239, loss is 0.6053895354270935\n",
      "epoch: 2 step: 240, loss is 0.6941257119178772\n",
      "epoch: 2 step: 241, loss is 0.7487480044364929\n",
      "epoch: 2 step: 242, loss is 0.6473120450973511\n",
      "epoch: 2 step: 243, loss is 0.7055836915969849\n",
      "epoch: 2 step: 244, loss is 0.8285555243492126\n",
      "epoch: 2 step: 245, loss is 0.7249479293823242\n",
      "epoch: 2 step: 246, loss is 0.6480273008346558\n",
      "epoch: 2 step: 247, loss is 0.6259135603904724\n",
      "epoch: 2 step: 248, loss is 0.736646294593811\n",
      "epoch: 2 step: 249, loss is 0.6220158934593201\n",
      "epoch: 2 step: 250, loss is 0.6706089973449707\n",
      "epoch: 2 step: 251, loss is 0.6821367740631104\n",
      "epoch: 2 step: 252, loss is 0.5742296576499939\n",
      "epoch: 2 step: 253, loss is 0.6975983381271362\n",
      "epoch: 2 step: 254, loss is 0.7354863286018372\n",
      "epoch: 2 step: 255, loss is 0.8062454462051392\n",
      "epoch: 2 step: 256, loss is 0.6358657479286194\n",
      "epoch: 2 step: 257, loss is 0.6563023924827576\n",
      "epoch: 2 step: 258, loss is 0.671661913394928\n",
      "epoch: 2 step: 259, loss is 0.7915031909942627\n",
      "epoch: 2 step: 260, loss is 0.5151360034942627\n",
      "epoch: 2 step: 261, loss is 0.676681637763977\n",
      "epoch: 2 step: 262, loss is 0.7784934639930725\n",
      "epoch: 2 step: 263, loss is 0.7684726715087891\n",
      "epoch: 2 step: 264, loss is 0.8153735399246216\n",
      "epoch: 2 step: 265, loss is 0.7117137908935547\n",
      "epoch: 2 step: 266, loss is 0.5534422397613525\n",
      "epoch: 2 step: 267, loss is 0.6727426052093506\n",
      "epoch: 2 step: 268, loss is 0.5789198875427246\n",
      "epoch: 2 step: 269, loss is 0.699171781539917\n",
      "epoch: 2 step: 270, loss is 0.47803235054016113\n",
      "epoch: 2 step: 271, loss is 0.6840196251869202\n",
      "epoch: 2 step: 272, loss is 0.7678535580635071\n",
      "epoch: 2 step: 273, loss is 0.8049520254135132\n",
      "epoch: 2 step: 274, loss is 0.5378316044807434\n",
      "epoch: 2 step: 275, loss is 0.6965353488922119\n",
      "epoch: 2 step: 276, loss is 0.6048842668533325\n",
      "epoch: 2 step: 277, loss is 0.6272315979003906\n",
      "epoch: 2 step: 278, loss is 0.7132844924926758\n",
      "epoch: 2 step: 279, loss is 0.7224488854408264\n",
      "epoch: 2 step: 280, loss is 0.6074699759483337\n",
      "epoch: 2 step: 281, loss is 0.527646541595459\n",
      "epoch: 2 step: 282, loss is 0.7197012305259705\n",
      "epoch: 2 step: 283, loss is 0.517024576663971\n",
      "epoch: 2 step: 284, loss is 0.5678786039352417\n",
      "epoch: 2 step: 285, loss is 0.6307527422904968\n",
      "epoch: 2 step: 286, loss is 0.5805723667144775\n",
      "epoch: 2 step: 287, loss is 0.6961843371391296\n",
      "epoch: 2 step: 288, loss is 0.7555447220802307\n",
      "epoch: 2 step: 289, loss is 0.5891958475112915\n",
      "epoch: 2 step: 290, loss is 0.5726657509803772\n",
      "epoch: 2 step: 291, loss is 0.7121703624725342\n",
      "epoch: 2 step: 292, loss is 0.7177279591560364\n",
      "epoch: 2 step: 293, loss is 0.8777669072151184\n",
      "epoch: 2 step: 294, loss is 0.576536238193512\n",
      "epoch: 2 step: 295, loss is 0.7403349280357361\n",
      "epoch: 2 step: 296, loss is 0.669594407081604\n",
      "epoch: 2 step: 297, loss is 0.8749392628669739\n",
      "epoch: 2 step: 298, loss is 0.8537454009056091\n",
      "epoch: 2 step: 299, loss is 0.5892046689987183\n",
      "epoch: 2 step: 300, loss is 0.7427953481674194\n",
      "epoch: 2 step: 301, loss is 0.5155777931213379\n",
      "epoch: 2 step: 302, loss is 0.5956547856330872\n",
      "epoch: 2 step: 303, loss is 0.5201587080955505\n",
      "epoch: 2 step: 304, loss is 0.7732236385345459\n",
      "epoch: 2 step: 305, loss is 0.6416128277778625\n",
      "epoch: 2 step: 306, loss is 0.5906909704208374\n",
      "epoch: 2 step: 307, loss is 0.5871023535728455\n",
      "epoch: 2 step: 308, loss is 0.6356397271156311\n",
      "epoch: 2 step: 309, loss is 0.6661132574081421\n",
      "epoch: 2 step: 310, loss is 0.4889618158340454\n",
      "epoch: 2 step: 311, loss is 0.5180562138557434\n",
      "epoch: 2 step: 312, loss is 0.6988228559494019\n",
      "epoch: 2 step: 313, loss is 0.49782371520996094\n",
      "epoch: 2 step: 314, loss is 0.6133173704147339\n",
      "epoch: 2 step: 315, loss is 0.7147590517997742\n",
      "epoch: 2 step: 316, loss is 0.48145753145217896\n",
      "epoch: 2 step: 317, loss is 0.38507339358329773\n",
      "epoch: 2 step: 318, loss is 0.7549336552619934\n",
      "epoch: 2 step: 319, loss is 0.7252424359321594\n",
      "epoch: 2 step: 320, loss is 0.6652237176895142\n",
      "epoch: 2 step: 321, loss is 0.6319462656974792\n",
      "epoch: 2 step: 322, loss is 1.2331066131591797\n",
      "epoch: 2 step: 323, loss is 0.44159427285194397\n",
      "epoch: 2 step: 324, loss is 0.8323912620544434\n",
      "epoch: 2 step: 325, loss is 0.5096525549888611\n",
      "epoch: 2 step: 326, loss is 0.8144053816795349\n",
      "epoch: 2 step: 327, loss is 0.5979481339454651\n",
      "epoch: 2 step: 328, loss is 0.6854623556137085\n",
      "epoch: 2 step: 329, loss is 0.5888899564743042\n",
      "epoch: 2 step: 330, loss is 0.6447269916534424\n",
      "epoch: 2 step: 331, loss is 0.6859679818153381\n",
      "epoch: 2 step: 332, loss is 0.7347574234008789\n",
      "epoch: 2 step: 333, loss is 1.0672162771224976\n",
      "epoch: 2 step: 334, loss is 0.7935623526573181\n",
      "epoch: 2 step: 335, loss is 0.4534250497817993\n",
      "epoch: 2 step: 336, loss is 0.6531970500946045\n",
      "epoch: 2 step: 337, loss is 0.4686017334461212\n",
      "epoch: 2 step: 338, loss is 0.7572067379951477\n",
      "epoch: 2 step: 339, loss is 0.6283584237098694\n",
      "epoch: 2 step: 340, loss is 0.6416689157485962\n",
      "epoch: 2 step: 341, loss is 0.6437589526176453\n",
      "epoch: 2 step: 342, loss is 0.6541816592216492\n",
      "epoch: 2 step: 343, loss is 0.5768870711326599\n",
      "epoch: 2 step: 344, loss is 0.7907654047012329\n",
      "epoch: 2 step: 345, loss is 0.6760812997817993\n",
      "epoch: 2 step: 346, loss is 0.6801038980484009\n",
      "epoch: 2 step: 347, loss is 0.6698013544082642\n",
      "epoch: 2 step: 348, loss is 0.4272816777229309\n",
      "epoch: 2 step: 349, loss is 0.6107605695724487\n",
      "epoch: 2 step: 350, loss is 0.5786285996437073\n",
      "epoch: 2 step: 351, loss is 0.7859588265419006\n",
      "epoch: 2 step: 352, loss is 0.6932697296142578\n",
      "epoch: 2 step: 353, loss is 0.6746119260787964\n",
      "epoch: 2 step: 354, loss is 0.6347718834877014\n",
      "epoch: 2 step: 355, loss is 0.515160083770752\n",
      "epoch: 2 step: 356, loss is 0.6799293160438538\n",
      "epoch: 2 step: 357, loss is 0.657671332359314\n",
      "epoch: 2 step: 358, loss is 0.8097320795059204\n",
      "epoch: 2 step: 359, loss is 0.6080623269081116\n",
      "epoch: 2 step: 360, loss is 0.656128466129303\n",
      "epoch: 2 step: 361, loss is 0.7082644701004028\n",
      "epoch: 2 step: 362, loss is 0.7719624638557434\n",
      "epoch: 2 step: 363, loss is 0.5275659561157227\n",
      "epoch: 2 step: 364, loss is 0.521964967250824\n",
      "epoch: 2 step: 365, loss is 0.7154049873352051\n",
      "epoch: 2 step: 366, loss is 0.6191992163658142\n",
      "epoch: 2 step: 367, loss is 0.4802517890930176\n",
      "epoch: 2 step: 368, loss is 0.39137476682662964\n",
      "epoch: 2 step: 369, loss is 0.5296845436096191\n",
      "epoch: 2 step: 370, loss is 0.6614552736282349\n",
      "epoch: 2 step: 371, loss is 0.7819919586181641\n",
      "epoch: 2 step: 372, loss is 0.6782113313674927\n",
      "epoch: 2 step: 373, loss is 0.5043833255767822\n",
      "epoch: 2 step: 374, loss is 0.4679659307003021\n",
      "epoch: 2 step: 375, loss is 0.8346018195152283\n",
      "epoch: 2 step: 376, loss is 0.509314775466919\n",
      "epoch: 2 step: 377, loss is 0.8415428400039673\n",
      "epoch: 2 step: 378, loss is 0.7635019421577454\n",
      "epoch: 2 step: 379, loss is 0.5458093285560608\n",
      "epoch: 2 step: 380, loss is 0.5678610801696777\n",
      "epoch: 2 step: 381, loss is 0.866797924041748\n",
      "epoch: 2 step: 382, loss is 0.7805618047714233\n",
      "epoch: 2 step: 383, loss is 1.0156452655792236\n",
      "epoch: 2 step: 384, loss is 0.6953521370887756\n",
      "epoch: 2 step: 385, loss is 0.6014574766159058\n",
      "epoch: 2 step: 386, loss is 0.6554265022277832\n",
      "epoch: 2 step: 387, loss is 0.5449687838554382\n",
      "epoch: 2 step: 388, loss is 0.44809553027153015\n",
      "epoch: 2 step: 389, loss is 0.6006878614425659\n",
      "epoch: 2 step: 390, loss is 0.5358161330223083\n",
      "epoch: 2 step: 391, loss is 0.4936721622943878\n",
      "epoch: 2 step: 392, loss is 0.7502195239067078\n",
      "epoch: 2 step: 393, loss is 0.8934688568115234\n",
      "epoch: 2 step: 394, loss is 0.6882195472717285\n",
      "epoch: 2 step: 395, loss is 0.6919775009155273\n",
      "epoch: 2 step: 396, loss is 0.5808144211769104\n",
      "epoch: 2 step: 397, loss is 0.7687653303146362\n",
      "epoch: 2 step: 398, loss is 0.594989538192749\n",
      "epoch: 2 step: 399, loss is 0.7000555992126465\n",
      "epoch: 2 step: 400, loss is 0.6582646369934082\n",
      "epoch: 2 step: 401, loss is 0.4649103581905365\n",
      "epoch: 2 step: 402, loss is 0.8724051713943481\n",
      "epoch: 2 step: 403, loss is 0.6254429817199707\n",
      "epoch: 2 step: 404, loss is 0.5126702189445496\n",
      "epoch: 2 step: 405, loss is 0.4989604949951172\n",
      "epoch: 2 step: 406, loss is 0.7197357416152954\n",
      "epoch: 2 step: 407, loss is 0.7526818513870239\n",
      "epoch: 2 step: 408, loss is 0.5576785206794739\n",
      "epoch: 2 step: 409, loss is 0.5080241560935974\n",
      "epoch: 2 step: 410, loss is 0.6256077885627747\n",
      "epoch: 2 step: 411, loss is 0.8902353644371033\n",
      "epoch: 2 step: 412, loss is 0.5390901565551758\n",
      "epoch: 2 step: 413, loss is 0.635678231716156\n",
      "epoch: 2 step: 414, loss is 0.626747190952301\n",
      "epoch: 2 step: 415, loss is 0.5812926888465881\n",
      "epoch: 2 step: 416, loss is 0.9525313973426819\n",
      "epoch: 2 step: 417, loss is 0.5714842677116394\n",
      "epoch: 2 step: 418, loss is 0.6749258041381836\n",
      "epoch: 2 step: 419, loss is 0.6161625385284424\n",
      "epoch: 2 step: 420, loss is 0.6428649425506592\n",
      "epoch: 2 step: 421, loss is 0.6674720048904419\n",
      "epoch: 2 step: 422, loss is 0.5841577053070068\n",
      "epoch: 2 step: 423, loss is 0.5591845512390137\n",
      "epoch: 2 step: 424, loss is 0.7759200930595398\n",
      "epoch: 2 step: 425, loss is 0.6414427161216736\n",
      "epoch: 2 step: 426, loss is 0.4481891691684723\n",
      "epoch: 2 step: 427, loss is 0.5643516182899475\n",
      "epoch: 2 step: 428, loss is 0.5916814208030701\n",
      "epoch: 2 step: 429, loss is 0.5427126288414001\n",
      "epoch: 2 step: 430, loss is 0.4307157099246979\n",
      "epoch: 2 step: 431, loss is 0.8661710023880005\n",
      "epoch: 2 step: 432, loss is 0.8619013428688049\n",
      "epoch: 2 step: 433, loss is 0.9368394017219543\n",
      "epoch: 2 step: 434, loss is 0.7328422665596008\n",
      "epoch: 2 step: 435, loss is 0.7287909388542175\n",
      "epoch: 2 step: 436, loss is 0.5833484530448914\n",
      "epoch: 2 step: 437, loss is 0.6712651252746582\n",
      "epoch: 2 step: 438, loss is 0.6053112745285034\n",
      "epoch: 2 step: 439, loss is 0.6156534552574158\n",
      "epoch: 2 step: 440, loss is 0.47339868545532227\n",
      "epoch: 2 step: 441, loss is 0.7465293407440186\n",
      "epoch: 2 step: 442, loss is 0.660589873790741\n",
      "epoch: 2 step: 443, loss is 0.6018680930137634\n",
      "epoch: 2 step: 444, loss is 0.6138837337493896\n",
      "epoch: 2 step: 445, loss is 0.5659408569335938\n",
      "epoch: 2 step: 446, loss is 0.7390119433403015\n",
      "epoch: 2 step: 447, loss is 0.5791313052177429\n",
      "epoch: 2 step: 448, loss is 0.5923927426338196\n",
      "epoch: 2 step: 449, loss is 0.714765191078186\n",
      "epoch: 2 step: 450, loss is 0.7063993811607361\n",
      "epoch: 2 step: 451, loss is 0.7148595452308655\n",
      "epoch: 2 step: 452, loss is 0.744948148727417\n",
      "epoch: 2 step: 453, loss is 0.7101541757583618\n",
      "epoch: 2 step: 454, loss is 0.6066389679908752\n",
      "epoch: 2 step: 455, loss is 0.545025110244751\n",
      "epoch: 2 step: 456, loss is 0.6499586701393127\n",
      "epoch: 2 step: 457, loss is 0.7094502449035645\n",
      "epoch: 2 step: 458, loss is 0.5195226669311523\n",
      "epoch: 2 step: 459, loss is 0.5816264152526855\n",
      "epoch: 2 step: 460, loss is 0.6399593949317932\n",
      "epoch: 2 step: 461, loss is 0.6736188530921936\n",
      "epoch: 2 step: 462, loss is 0.68138587474823\n",
      "epoch: 2 step: 463, loss is 0.5972511172294617\n",
      "epoch: 2 step: 464, loss is 0.5998801589012146\n",
      "epoch: 2 step: 465, loss is 0.554223895072937\n",
      "epoch: 2 step: 466, loss is 0.49626150727272034\n",
      "epoch: 2 step: 467, loss is 0.862134575843811\n",
      "epoch: 2 step: 468, loss is 0.495434045791626\n",
      "epoch: 2 step: 469, loss is 0.7412038445472717\n",
      "epoch: 2 step: 470, loss is 0.6203297972679138\n",
      "epoch: 2 step: 471, loss is 0.6641333103179932\n",
      "epoch: 2 step: 472, loss is 0.7973334193229675\n",
      "epoch: 2 step: 473, loss is 0.5306693911552429\n",
      "epoch: 2 step: 474, loss is 0.7181658744812012\n",
      "epoch: 2 step: 475, loss is 0.5670590996742249\n",
      "epoch: 2 step: 476, loss is 0.5312981009483337\n",
      "epoch: 2 step: 477, loss is 0.8512359261512756\n",
      "epoch: 2 step: 478, loss is 0.5905894637107849\n",
      "epoch: 2 step: 479, loss is 0.8203772306442261\n",
      "epoch: 2 step: 480, loss is 0.9037250280380249\n",
      "epoch: 2 step: 481, loss is 0.6036964654922485\n",
      "epoch: 2 step: 482, loss is 0.6120108366012573\n",
      "epoch: 2 step: 483, loss is 0.8176565766334534\n",
      "epoch: 2 step: 484, loss is 0.5969966650009155\n",
      "epoch: 2 step: 485, loss is 0.5501132011413574\n",
      "epoch: 2 step: 486, loss is 0.7462513446807861\n",
      "epoch: 2 step: 487, loss is 0.6923684477806091\n",
      "epoch: 2 step: 488, loss is 0.6358174681663513\n",
      "epoch: 2 step: 489, loss is 0.5286853909492493\n",
      "epoch: 2 step: 490, loss is 0.8175584077835083\n",
      "epoch: 2 step: 491, loss is 0.7243460416793823\n",
      "epoch: 2 step: 492, loss is 0.759658932685852\n",
      "epoch: 2 step: 493, loss is 0.7674989104270935\n",
      "epoch: 2 step: 494, loss is 0.5944874286651611\n",
      "epoch: 2 step: 495, loss is 0.6046335697174072\n",
      "epoch: 2 step: 496, loss is 0.606569766998291\n",
      "epoch: 2 step: 497, loss is 0.6544270515441895\n",
      "epoch: 2 step: 498, loss is 0.6954936385154724\n",
      "epoch: 2 step: 499, loss is 0.6888039708137512\n",
      "epoch: 2 step: 500, loss is 0.727189838886261\n",
      "epoch: 2 step: 501, loss is 0.8337843418121338\n",
      "epoch: 2 step: 502, loss is 0.737857460975647\n",
      "epoch: 2 step: 503, loss is 0.50576251745224\n",
      "epoch: 2 step: 504, loss is 0.7042792439460754\n",
      "epoch: 2 step: 505, loss is 0.5446397066116333\n",
      "epoch: 2 step: 506, loss is 0.5391461849212646\n",
      "epoch: 2 step: 507, loss is 0.630643367767334\n",
      "epoch: 2 step: 508, loss is 0.548646867275238\n",
      "epoch: 2 step: 509, loss is 0.5955381989479065\n",
      "epoch: 2 step: 510, loss is 0.7830866575241089\n",
      "epoch: 2 step: 511, loss is 0.8498471975326538\n",
      "epoch: 2 step: 512, loss is 0.8020979762077332\n",
      "epoch: 2 step: 513, loss is 0.6077238321304321\n",
      "epoch: 2 step: 514, loss is 0.6953258514404297\n",
      "epoch: 2 step: 515, loss is 0.8020941615104675\n",
      "epoch: 2 step: 516, loss is 0.5870070457458496\n",
      "epoch: 2 step: 517, loss is 0.5788097977638245\n",
      "epoch: 2 step: 518, loss is 0.6749099493026733\n",
      "epoch: 2 step: 519, loss is 0.6718466281890869\n",
      "epoch: 2 step: 520, loss is 0.6783050298690796\n",
      "epoch: 2 step: 521, loss is 0.7454164624214172\n",
      "epoch: 2 step: 522, loss is 0.7049605846405029\n",
      "epoch: 2 step: 523, loss is 0.7960270643234253\n",
      "epoch: 2 step: 524, loss is 0.7122097611427307\n",
      "epoch: 2 step: 525, loss is 0.5751018524169922\n",
      "epoch: 2 step: 526, loss is 0.5354928374290466\n",
      "epoch: 2 step: 527, loss is 0.6554768085479736\n",
      "epoch: 2 step: 528, loss is 0.4975663721561432\n",
      "epoch: 2 step: 529, loss is 0.6537153720855713\n",
      "epoch: 2 step: 530, loss is 0.6581942439079285\n",
      "epoch: 2 step: 531, loss is 0.6354495882987976\n",
      "epoch: 2 step: 532, loss is 0.4502805471420288\n",
      "epoch: 2 step: 533, loss is 0.6347320079803467\n",
      "epoch: 2 step: 534, loss is 0.6114050149917603\n",
      "epoch: 2 step: 535, loss is 0.6863812208175659\n",
      "epoch: 2 step: 536, loss is 0.6969566345214844\n",
      "epoch: 2 step: 537, loss is 0.5876017808914185\n",
      "epoch: 2 step: 538, loss is 0.49700915813446045\n",
      "epoch: 2 step: 539, loss is 0.8497762680053711\n",
      "epoch: 2 step: 540, loss is 0.5955837368965149\n",
      "epoch: 2 step: 541, loss is 0.6812631487846375\n",
      "epoch: 2 step: 542, loss is 0.847898006439209\n",
      "epoch: 2 step: 543, loss is 0.6764914393424988\n",
      "epoch: 2 step: 544, loss is 0.6003222465515137\n",
      "epoch: 2 step: 545, loss is 0.4530423879623413\n",
      "epoch: 2 step: 546, loss is 0.5978826284408569\n",
      "epoch: 2 step: 547, loss is 0.5015796422958374\n",
      "epoch: 2 step: 548, loss is 0.5913015604019165\n",
      "epoch: 2 step: 549, loss is 0.6898127198219299\n",
      "epoch: 2 step: 550, loss is 0.7445412278175354\n",
      "epoch: 2 step: 551, loss is 0.8293247818946838\n",
      "epoch: 2 step: 552, loss is 0.573911190032959\n",
      "epoch: 2 step: 553, loss is 0.6105539202690125\n",
      "epoch: 2 step: 554, loss is 0.5723506808280945\n",
      "epoch: 2 step: 555, loss is 0.7579617500305176\n",
      "epoch: 2 step: 556, loss is 0.7819401621818542\n",
      "epoch: 2 step: 557, loss is 0.5057069063186646\n",
      "epoch: 2 step: 558, loss is 0.5175093412399292\n",
      "epoch: 2 step: 559, loss is 0.6519327759742737\n",
      "epoch: 2 step: 560, loss is 0.6738895773887634\n",
      "epoch: 2 step: 561, loss is 0.7626060247421265\n",
      "epoch: 2 step: 562, loss is 0.396418035030365\n",
      "epoch: 2 step: 563, loss is 0.6176316142082214\n",
      "epoch: 2 step: 564, loss is 0.5949839353561401\n",
      "epoch: 2 step: 565, loss is 0.6407315731048584\n",
      "epoch: 2 step: 566, loss is 0.5897716879844666\n",
      "epoch: 2 step: 567, loss is 0.6269001960754395\n",
      "epoch: 2 step: 568, loss is 0.6222445964813232\n",
      "epoch: 2 step: 569, loss is 0.7566149830818176\n",
      "epoch: 2 step: 570, loss is 0.7845066785812378\n",
      "epoch: 2 step: 571, loss is 0.5741797685623169\n",
      "epoch: 2 step: 572, loss is 0.8044931292533875\n",
      "epoch: 2 step: 573, loss is 0.7420541048049927\n",
      "epoch: 2 step: 574, loss is 0.7173581123352051\n",
      "epoch: 2 step: 575, loss is 0.7886497974395752\n",
      "epoch: 2 step: 576, loss is 0.6419026851654053\n",
      "epoch: 2 step: 577, loss is 0.542044460773468\n",
      "epoch: 2 step: 578, loss is 0.539750337600708\n",
      "epoch: 2 step: 579, loss is 0.4738306999206543\n",
      "epoch: 2 step: 580, loss is 0.44072258472442627\n",
      "epoch: 2 step: 581, loss is 0.3874620199203491\n",
      "epoch: 2 step: 582, loss is 0.5157023668289185\n",
      "epoch: 2 step: 583, loss is 0.8277091383934021\n",
      "epoch: 2 step: 584, loss is 0.7990280985832214\n",
      "epoch: 2 step: 585, loss is 0.5303266048431396\n",
      "epoch: 2 step: 586, loss is 0.36979061365127563\n",
      "epoch: 2 step: 587, loss is 0.6453331112861633\n",
      "epoch: 2 step: 588, loss is 0.49100300669670105\n",
      "epoch: 2 step: 589, loss is 0.742816150188446\n",
      "epoch: 2 step: 590, loss is 0.5479401350021362\n",
      "epoch: 2 step: 591, loss is 0.5281684994697571\n",
      "epoch: 2 step: 592, loss is 0.6124281883239746\n",
      "epoch: 2 step: 593, loss is 1.0916686058044434\n",
      "epoch: 2 step: 594, loss is 0.5586156845092773\n",
      "epoch: 2 step: 595, loss is 0.43314120173454285\n",
      "epoch: 2 step: 596, loss is 0.8056371212005615\n",
      "epoch: 2 step: 597, loss is 0.3919169008731842\n",
      "epoch: 2 step: 598, loss is 0.8196384310722351\n",
      "epoch: 2 step: 599, loss is 0.6118850708007812\n",
      "epoch: 2 step: 600, loss is 0.6634215712547302\n",
      "epoch: 2 step: 601, loss is 0.5931636095046997\n",
      "epoch: 2 step: 602, loss is 0.6504250168800354\n",
      "epoch: 2 step: 603, loss is 0.4682570695877075\n",
      "epoch: 2 step: 604, loss is 0.5925654172897339\n",
      "epoch: 2 step: 605, loss is 0.6589686274528503\n",
      "epoch: 2 step: 606, loss is 0.6039707660675049\n",
      "epoch: 2 step: 607, loss is 0.45228078961372375\n",
      "epoch: 2 step: 608, loss is 0.7854074835777283\n",
      "epoch: 2 step: 609, loss is 0.6865645051002502\n",
      "epoch: 2 step: 610, loss is 0.45478498935699463\n",
      "epoch: 2 step: 611, loss is 0.617447555065155\n",
      "epoch: 2 step: 612, loss is 0.8169959187507629\n",
      "epoch: 2 step: 613, loss is 0.6531875133514404\n",
      "epoch: 2 step: 614, loss is 0.5445573925971985\n",
      "epoch: 2 step: 615, loss is 0.6688145995140076\n",
      "epoch: 2 step: 616, loss is 0.6095855832099915\n",
      "epoch: 2 step: 617, loss is 0.8373764157295227\n",
      "epoch: 2 step: 618, loss is 0.8911018967628479\n",
      "epoch: 2 step: 619, loss is 0.5809284448623657\n",
      "epoch: 2 step: 620, loss is 0.640052318572998\n",
      "epoch: 2 step: 621, loss is 0.5864166021347046\n",
      "epoch: 2 step: 622, loss is 0.49382421374320984\n",
      "epoch: 2 step: 623, loss is 0.7081999182701111\n",
      "epoch: 2 step: 624, loss is 0.7797327637672424\n",
      "epoch: 2 step: 625, loss is 0.5552809238433838\n",
      "epoch: 2 step: 626, loss is 0.614195704460144\n",
      "epoch: 2 step: 627, loss is 0.7313416600227356\n",
      "epoch: 2 step: 628, loss is 0.730342447757721\n",
      "epoch: 2 step: 629, loss is 0.4424425959587097\n",
      "epoch: 2 step: 630, loss is 0.49819567799568176\n",
      "epoch: 2 step: 631, loss is 0.5786129832267761\n",
      "epoch: 2 step: 632, loss is 0.6951805949211121\n",
      "epoch: 2 step: 633, loss is 0.7614715695381165\n",
      "epoch: 2 step: 634, loss is 0.7030177116394043\n",
      "epoch: 2 step: 635, loss is 0.617152750492096\n",
      "epoch: 2 step: 636, loss is 0.6127421855926514\n",
      "epoch: 2 step: 637, loss is 0.7269665598869324\n",
      "epoch: 2 step: 638, loss is 0.6867672204971313\n",
      "epoch: 2 step: 639, loss is 0.7033421397209167\n",
      "epoch: 2 step: 640, loss is 0.5784077048301697\n",
      "epoch: 2 step: 641, loss is 0.6959933042526245\n",
      "epoch: 2 step: 642, loss is 0.5800397992134094\n",
      "epoch: 2 step: 643, loss is 0.6151548027992249\n",
      "epoch: 2 step: 644, loss is 0.5848004817962646\n",
      "epoch: 2 step: 645, loss is 0.5913910865783691\n",
      "epoch: 2 step: 646, loss is 0.5014628767967224\n",
      "epoch: 2 step: 647, loss is 0.861849844455719\n",
      "epoch: 2 step: 648, loss is 0.6202618479728699\n",
      "epoch: 2 step: 649, loss is 0.6483194828033447\n",
      "epoch: 2 step: 650, loss is 0.564926028251648\n",
      "epoch: 2 step: 651, loss is 0.6580700278282166\n",
      "epoch: 2 step: 652, loss is 0.622178852558136\n",
      "epoch: 2 step: 653, loss is 0.5501295924186707\n",
      "epoch: 2 step: 654, loss is 0.5444536209106445\n",
      "epoch: 2 step: 655, loss is 0.6762204766273499\n",
      "epoch: 2 step: 656, loss is 0.4650757312774658\n",
      "epoch: 2 step: 657, loss is 0.5807060599327087\n",
      "epoch: 2 step: 658, loss is 0.6822674870491028\n",
      "epoch: 2 step: 659, loss is 0.551044762134552\n",
      "epoch: 2 step: 660, loss is 0.5246469974517822\n",
      "epoch: 2 step: 661, loss is 0.6106750965118408\n",
      "epoch: 2 step: 662, loss is 0.7532305121421814\n",
      "epoch: 2 step: 663, loss is 0.6836699843406677\n",
      "epoch: 2 step: 664, loss is 0.5991370677947998\n",
      "epoch: 2 step: 665, loss is 0.5938109159469604\n",
      "epoch: 2 step: 666, loss is 0.5414743423461914\n",
      "epoch: 2 step: 667, loss is 0.8544318675994873\n",
      "epoch: 2 step: 668, loss is 0.6434968113899231\n",
      "epoch: 2 step: 669, loss is 0.626045823097229\n",
      "epoch: 2 step: 670, loss is 0.6264929175376892\n",
      "epoch: 2 step: 671, loss is 0.7395848035812378\n",
      "epoch: 2 step: 672, loss is 0.6911892890930176\n",
      "epoch: 2 step: 673, loss is 0.5695756673812866\n",
      "epoch: 2 step: 674, loss is 0.7987865805625916\n",
      "epoch: 2 step: 675, loss is 0.6946200132369995\n",
      "epoch: 2 step: 676, loss is 0.5563313961029053\n",
      "epoch: 2 step: 677, loss is 0.5694823861122131\n",
      "epoch: 2 step: 678, loss is 0.8305149078369141\n",
      "epoch: 2 step: 679, loss is 0.5559953451156616\n",
      "epoch: 2 step: 680, loss is 0.6712963581085205\n",
      "epoch: 2 step: 681, loss is 0.5600648522377014\n",
      "epoch: 2 step: 682, loss is 0.588240921497345\n",
      "epoch: 2 step: 683, loss is 0.5991699695587158\n",
      "epoch: 2 step: 684, loss is 0.5382930040359497\n",
      "epoch: 2 step: 685, loss is 0.7055035829544067\n",
      "epoch: 2 step: 686, loss is 0.5034366250038147\n",
      "epoch: 2 step: 687, loss is 0.7398941516876221\n",
      "epoch: 2 step: 688, loss is 0.6478620767593384\n",
      "epoch: 2 step: 689, loss is 0.46566954255104065\n",
      "epoch: 2 step: 690, loss is 0.5176675319671631\n",
      "epoch: 2 step: 691, loss is 0.7522976398468018\n",
      "epoch: 2 step: 692, loss is 0.4491208493709564\n",
      "epoch: 2 step: 693, loss is 0.6270216107368469\n",
      "epoch: 2 step: 694, loss is 0.8601306676864624\n",
      "epoch: 2 step: 695, loss is 0.6881953477859497\n",
      "epoch: 2 step: 696, loss is 0.5906086564064026\n",
      "epoch: 2 step: 697, loss is 0.5070887207984924\n",
      "epoch: 2 step: 698, loss is 0.5488913655281067\n",
      "epoch: 2 step: 699, loss is 0.6143007278442383\n",
      "epoch: 2 step: 700, loss is 0.6494412422180176\n",
      "epoch: 2 step: 701, loss is 0.5240737199783325\n",
      "epoch: 2 step: 702, loss is 0.6291049718856812\n",
      "epoch: 2 step: 703, loss is 0.5710204243659973\n",
      "epoch: 2 step: 704, loss is 0.462327778339386\n",
      "epoch: 2 step: 705, loss is 0.6977990865707397\n",
      "epoch: 2 step: 706, loss is 0.6688387393951416\n",
      "epoch: 2 step: 707, loss is 0.6328446865081787\n",
      "epoch: 2 step: 708, loss is 0.6797945499420166\n",
      "epoch: 2 step: 709, loss is 0.4983936846256256\n",
      "epoch: 2 step: 710, loss is 0.6037998795509338\n",
      "epoch: 2 step: 711, loss is 0.35873571038246155\n",
      "epoch: 2 step: 712, loss is 0.7194128036499023\n",
      "epoch: 2 step: 713, loss is 0.5832072496414185\n",
      "epoch: 2 step: 714, loss is 0.7214198708534241\n",
      "epoch: 2 step: 715, loss is 0.5898140072822571\n",
      "epoch: 2 step: 716, loss is 0.4907376766204834\n",
      "epoch: 2 step: 717, loss is 0.6807624101638794\n",
      "epoch: 2 step: 718, loss is 0.46768611669540405\n",
      "epoch: 2 step: 719, loss is 0.5585391521453857\n",
      "epoch: 2 step: 720, loss is 0.9066929221153259\n",
      "epoch: 2 step: 721, loss is 0.5402419567108154\n",
      "epoch: 2 step: 722, loss is 0.5013977289199829\n",
      "epoch: 2 step: 723, loss is 0.7143170833587646\n",
      "epoch: 2 step: 724, loss is 1.2501193284988403\n",
      "epoch: 2 step: 725, loss is 0.5556245446205139\n",
      "epoch: 2 step: 726, loss is 0.8614408373832703\n",
      "epoch: 2 step: 727, loss is 0.7253175973892212\n",
      "epoch: 2 step: 728, loss is 0.7800379395484924\n",
      "epoch: 2 step: 729, loss is 0.43836718797683716\n",
      "epoch: 2 step: 730, loss is 0.3942227065563202\n",
      "epoch: 2 step: 731, loss is 0.9301343560218811\n",
      "epoch: 2 step: 732, loss is 0.8252764940261841\n",
      "epoch: 2 step: 733, loss is 0.5308027267456055\n",
      "epoch: 2 step: 734, loss is 0.586797833442688\n",
      "epoch: 2 step: 735, loss is 0.4056324064731598\n",
      "epoch: 2 step: 736, loss is 0.6536790728569031\n",
      "epoch: 2 step: 737, loss is 0.8567521572113037\n",
      "epoch: 2 step: 738, loss is 0.7676013708114624\n",
      "epoch: 2 step: 739, loss is 0.5621894598007202\n",
      "epoch: 2 step: 740, loss is 0.7133661508560181\n",
      "epoch: 2 step: 741, loss is 0.6579833626747131\n",
      "epoch: 2 step: 742, loss is 0.6978378891944885\n",
      "epoch: 2 step: 743, loss is 0.5862001180648804\n",
      "epoch: 2 step: 744, loss is 0.541500449180603\n",
      "epoch: 2 step: 745, loss is 0.7622801661491394\n",
      "epoch: 2 step: 746, loss is 0.8195695281028748\n",
      "epoch: 2 step: 747, loss is 0.5600370168685913\n",
      "epoch: 2 step: 748, loss is 0.5562724471092224\n",
      "epoch: 2 step: 749, loss is 0.5204616785049438\n",
      "epoch: 2 step: 750, loss is 0.4285673201084137\n",
      "epoch: 2 step: 751, loss is 0.6975858211517334\n",
      "epoch: 2 step: 752, loss is 0.7596060037612915\n",
      "epoch: 2 step: 753, loss is 0.6430635452270508\n",
      "epoch: 2 step: 754, loss is 0.6958112716674805\n",
      "epoch: 2 step: 755, loss is 0.5144360065460205\n",
      "epoch: 2 step: 756, loss is 0.5785198211669922\n",
      "epoch: 2 step: 757, loss is 0.5315244197845459\n",
      "epoch: 2 step: 758, loss is 0.5881822109222412\n",
      "epoch: 2 step: 759, loss is 0.8098340034484863\n",
      "epoch: 2 step: 760, loss is 0.6938777565956116\n",
      "epoch: 2 step: 761, loss is 0.9275226593017578\n",
      "epoch: 2 step: 762, loss is 0.4602982997894287\n",
      "epoch: 2 step: 763, loss is 0.7667693495750427\n",
      "epoch: 2 step: 764, loss is 0.7577424645423889\n",
      "epoch: 2 step: 765, loss is 0.47621428966522217\n",
      "epoch: 2 step: 766, loss is 0.6337317228317261\n",
      "epoch: 2 step: 767, loss is 0.7038213014602661\n",
      "epoch: 2 step: 768, loss is 0.7774926424026489\n",
      "epoch: 2 step: 769, loss is 0.4804646074771881\n",
      "epoch: 2 step: 770, loss is 0.6711952686309814\n",
      "epoch: 2 step: 771, loss is 0.6763547658920288\n",
      "epoch: 2 step: 772, loss is 0.6034093499183655\n",
      "epoch: 2 step: 773, loss is 0.6807079911231995\n",
      "epoch: 2 step: 774, loss is 0.5665430426597595\n",
      "epoch: 2 step: 775, loss is 0.5618544816970825\n",
      "epoch: 2 step: 776, loss is 0.8121844530105591\n",
      "epoch: 2 step: 777, loss is 0.6368805766105652\n",
      "epoch: 2 step: 778, loss is 0.6991578340530396\n",
      "epoch: 2 step: 779, loss is 0.7925434112548828\n",
      "epoch: 2 step: 780, loss is 0.6098991632461548\n",
      "epoch: 2 step: 781, loss is 0.6138699650764465\n",
      "epoch: 2 step: 782, loss is 0.6426084041595459\n",
      "epoch: 2 step: 783, loss is 0.5705937147140503\n",
      "epoch: 2 step: 784, loss is 0.8666344285011292\n",
      "epoch: 2 step: 785, loss is 0.7594984173774719\n",
      "epoch: 2 step: 786, loss is 0.7451412081718445\n",
      "epoch: 2 step: 787, loss is 0.6991233825683594\n",
      "epoch: 2 step: 788, loss is 0.7435675859451294\n",
      "epoch: 2 step: 789, loss is 0.6942862272262573\n",
      "epoch: 2 step: 790, loss is 0.6037357449531555\n",
      "epoch: 2 step: 791, loss is 0.5580772757530212\n",
      "epoch: 2 step: 792, loss is 0.6448948979377747\n",
      "epoch: 2 step: 793, loss is 0.5091906189918518\n",
      "epoch: 2 step: 794, loss is 0.5491055846214294\n",
      "epoch: 2 step: 795, loss is 0.7304463982582092\n",
      "epoch: 2 step: 796, loss is 0.6448116898536682\n",
      "epoch: 2 step: 797, loss is 0.6246528625488281\n",
      "epoch: 2 step: 798, loss is 0.5557584762573242\n",
      "epoch: 2 step: 799, loss is 0.6156663298606873\n",
      "epoch: 2 step: 800, loss is 0.5289695858955383\n",
      "epoch: 2 step: 801, loss is 0.6482866406440735\n",
      "epoch: 2 step: 802, loss is 0.6046963930130005\n",
      "epoch: 2 step: 803, loss is 0.5066691040992737\n",
      "epoch: 2 step: 804, loss is 0.6975387930870056\n",
      "epoch: 2 step: 805, loss is 0.43628671765327454\n",
      "epoch: 2 step: 806, loss is 0.5317243337631226\n",
      "epoch: 2 step: 807, loss is 0.6909072399139404\n",
      "epoch: 2 step: 808, loss is 0.8149107694625854\n",
      "epoch: 2 step: 809, loss is 0.9959971904754639\n",
      "epoch: 2 step: 810, loss is 0.5773343443870544\n",
      "epoch: 2 step: 811, loss is 0.6319969296455383\n",
      "epoch: 2 step: 812, loss is 0.8863773941993713\n",
      "epoch: 2 step: 813, loss is 0.7390554547309875\n",
      "epoch: 2 step: 814, loss is 0.5808324813842773\n",
      "epoch: 2 step: 815, loss is 0.5932155847549438\n",
      "epoch: 2 step: 816, loss is 0.592143177986145\n",
      "epoch: 2 step: 817, loss is 0.5909613370895386\n",
      "epoch: 2 step: 818, loss is 0.4767998158931732\n",
      "epoch: 2 step: 819, loss is 0.48811253905296326\n",
      "epoch: 2 step: 820, loss is 0.6443896889686584\n",
      "epoch: 2 step: 821, loss is 0.9180721044540405\n",
      "epoch: 2 step: 822, loss is 0.5801516175270081\n",
      "epoch: 2 step: 823, loss is 0.6137018799781799\n",
      "epoch: 2 step: 824, loss is 0.5909966826438904\n",
      "epoch: 2 step: 825, loss is 0.700946033000946\n",
      "epoch: 2 step: 826, loss is 0.6773747205734253\n",
      "epoch: 2 step: 827, loss is 0.4126863479614258\n",
      "epoch: 2 step: 828, loss is 0.5499542355537415\n",
      "epoch: 2 step: 829, loss is 0.894788384437561\n",
      "epoch: 2 step: 830, loss is 0.6625890135765076\n",
      "epoch: 2 step: 831, loss is 0.6275360584259033\n",
      "epoch: 2 step: 832, loss is 0.6916832327842712\n",
      "epoch: 2 step: 833, loss is 0.5661770105361938\n",
      "epoch: 2 step: 834, loss is 0.8073070049285889\n",
      "epoch: 2 step: 835, loss is 0.49147188663482666\n",
      "epoch: 2 step: 836, loss is 0.5006409287452698\n",
      "epoch: 2 step: 837, loss is 0.5901076793670654\n",
      "epoch: 2 step: 838, loss is 0.6913862824440002\n",
      "epoch: 2 step: 839, loss is 0.5183868408203125\n",
      "epoch: 2 step: 840, loss is 0.5700200796127319\n",
      "epoch: 2 step: 841, loss is 0.8383333086967468\n",
      "epoch: 2 step: 842, loss is 0.6546517610549927\n",
      "epoch: 2 step: 843, loss is 0.7165271043777466\n",
      "epoch: 2 step: 844, loss is 0.528765082359314\n",
      "epoch: 2 step: 845, loss is 0.7269418239593506\n",
      "epoch: 2 step: 846, loss is 0.5962390899658203\n",
      "epoch: 2 step: 847, loss is 0.6094945073127747\n",
      "epoch: 2 step: 848, loss is 0.6970587968826294\n",
      "epoch: 2 step: 849, loss is 0.427539199590683\n",
      "epoch: 2 step: 850, loss is 0.5260316729545593\n",
      "epoch: 2 step: 851, loss is 0.6064571142196655\n",
      "epoch: 2 step: 852, loss is 0.474499374628067\n",
      "epoch: 2 step: 853, loss is 0.7707581520080566\n",
      "epoch: 2 step: 854, loss is 0.7507861256599426\n",
      "epoch: 2 step: 855, loss is 0.6766073703765869\n",
      "epoch: 2 step: 856, loss is 0.5062913298606873\n",
      "epoch: 2 step: 857, loss is 0.4970241189002991\n",
      "epoch: 2 step: 858, loss is 0.47631245851516724\n",
      "epoch: 2 step: 859, loss is 0.5122599005699158\n",
      "epoch: 2 step: 860, loss is 0.4235517382621765\n",
      "epoch: 2 step: 861, loss is 0.709498405456543\n",
      "epoch: 2 step: 862, loss is 0.6849877834320068\n",
      "epoch: 2 step: 863, loss is 0.8399823904037476\n",
      "epoch: 2 step: 864, loss is 0.6205281019210815\n",
      "epoch: 2 step: 865, loss is 0.6045616269111633\n",
      "epoch: 2 step: 866, loss is 0.8773820400238037\n",
      "epoch: 2 step: 867, loss is 0.6273989677429199\n",
      "epoch: 2 step: 868, loss is 0.851166844367981\n",
      "epoch: 2 step: 869, loss is 0.6601460576057434\n",
      "epoch: 2 step: 870, loss is 0.46651366353034973\n",
      "epoch: 2 step: 871, loss is 0.6502195596694946\n",
      "epoch: 2 step: 872, loss is 0.7675685286521912\n",
      "epoch: 2 step: 873, loss is 0.3831949532032013\n",
      "epoch: 2 step: 874, loss is 0.6523069739341736\n",
      "epoch: 2 step: 875, loss is 0.5255095362663269\n",
      "epoch: 2 step: 876, loss is 0.7191823124885559\n",
      "epoch: 2 step: 877, loss is 0.6329233646392822\n",
      "epoch: 2 step: 878, loss is 0.5293788909912109\n",
      "epoch: 2 step: 879, loss is 0.7246435284614563\n",
      "epoch: 2 step: 880, loss is 0.5702990293502808\n",
      "epoch: 2 step: 881, loss is 0.5787976980209351\n",
      "epoch: 2 step: 882, loss is 0.6489010453224182\n",
      "epoch: 2 step: 883, loss is 0.7465016841888428\n",
      "epoch: 2 step: 884, loss is 0.8251088857650757\n",
      "epoch: 2 step: 885, loss is 0.5416733026504517\n",
      "epoch: 2 step: 886, loss is 0.7256981134414673\n",
      "epoch: 2 step: 887, loss is 0.46533679962158203\n",
      "epoch: 2 step: 888, loss is 0.6652611494064331\n",
      "epoch: 2 step: 889, loss is 0.642962634563446\n",
      "epoch: 2 step: 890, loss is 0.9553442001342773\n",
      "epoch: 2 step: 891, loss is 0.7304362058639526\n",
      "epoch: 2 step: 892, loss is 0.5552664399147034\n",
      "epoch: 2 step: 893, loss is 0.775425136089325\n",
      "epoch: 2 step: 894, loss is 0.6156097650527954\n",
      "epoch: 2 step: 895, loss is 0.6842061877250671\n",
      "epoch: 2 step: 896, loss is 0.5736604928970337\n",
      "epoch: 2 step: 897, loss is 0.6639797687530518\n",
      "epoch: 2 step: 898, loss is 0.626622200012207\n",
      "epoch: 2 step: 899, loss is 0.5876044631004333\n",
      "epoch: 2 step: 900, loss is 0.6683686375617981\n",
      "epoch: 2 step: 901, loss is 0.6386928558349609\n",
      "epoch: 2 step: 902, loss is 0.5549607276916504\n",
      "epoch: 2 step: 903, loss is 0.5734878778457642\n",
      "epoch: 2 step: 904, loss is 0.6702105402946472\n",
      "epoch: 2 step: 905, loss is 0.581120491027832\n",
      "epoch: 2 step: 906, loss is 0.7647328972816467\n",
      "epoch: 2 step: 907, loss is 0.5712186098098755\n",
      "epoch: 2 step: 908, loss is 0.7097154855728149\n",
      "epoch: 2 step: 909, loss is 0.49701642990112305\n",
      "epoch: 2 step: 910, loss is 0.5772277116775513\n",
      "epoch: 2 step: 911, loss is 0.527805745601654\n",
      "epoch: 2 step: 912, loss is 0.5668609142303467\n",
      "epoch: 2 step: 913, loss is 0.34693843126296997\n",
      "epoch: 2 step: 914, loss is 0.5892046093940735\n",
      "epoch: 2 step: 915, loss is 0.4798584580421448\n",
      "epoch: 2 step: 916, loss is 0.5629678964614868\n",
      "epoch: 2 step: 917, loss is 0.7773665189743042\n",
      "epoch: 2 step: 918, loss is 0.8319555521011353\n",
      "epoch: 2 step: 919, loss is 0.6131350994110107\n",
      "epoch: 2 step: 920, loss is 0.7863430380821228\n",
      "epoch: 2 step: 921, loss is 0.6278929114341736\n",
      "epoch: 2 step: 922, loss is 0.6029736399650574\n",
      "epoch: 2 step: 923, loss is 0.45043832063674927\n",
      "epoch: 2 step: 924, loss is 0.8155059814453125\n",
      "epoch: 2 step: 925, loss is 0.7273244857788086\n",
      "epoch: 2 step: 926, loss is 0.8866171836853027\n",
      "epoch: 2 step: 927, loss is 0.7141686081886292\n",
      "epoch: 2 step: 928, loss is 0.4265345633029938\n",
      "epoch: 2 step: 929, loss is 0.7821639776229858\n",
      "epoch: 2 step: 930, loss is 0.5254563093185425\n",
      "epoch: 2 step: 931, loss is 0.4641304314136505\n",
      "epoch: 2 step: 932, loss is 0.5359618067741394\n",
      "epoch: 2 step: 933, loss is 0.7267678380012512\n",
      "epoch: 2 step: 934, loss is 0.7677894234657288\n",
      "epoch: 2 step: 935, loss is 0.5905917286872864\n",
      "epoch: 2 step: 936, loss is 0.5133059620857239\n",
      "epoch: 2 step: 937, loss is 0.8355831503868103\n",
      "epoch: 2 step: 938, loss is 0.6398696303367615\n",
      "epoch: 2 step: 939, loss is 0.924514651298523\n",
      "epoch: 2 step: 940, loss is 0.5881786942481995\n",
      "epoch: 2 step: 941, loss is 0.6779757738113403\n",
      "epoch: 2 step: 942, loss is 0.8163236379623413\n",
      "epoch: 2 step: 943, loss is 0.4781123399734497\n",
      "epoch: 2 step: 944, loss is 0.8694080114364624\n",
      "epoch: 2 step: 945, loss is 0.5114735960960388\n",
      "epoch: 2 step: 946, loss is 0.47932755947113037\n",
      "epoch: 2 step: 947, loss is 0.7462082505226135\n",
      "epoch: 2 step: 948, loss is 0.445772647857666\n",
      "epoch: 2 step: 949, loss is 0.5326103568077087\n",
      "epoch: 2 step: 950, loss is 0.5152145028114319\n",
      "epoch: 2 step: 951, loss is 0.6366552114486694\n",
      "epoch: 2 step: 952, loss is 0.6818404197692871\n",
      "epoch: 2 step: 953, loss is 0.5744693279266357\n",
      "epoch: 2 step: 954, loss is 0.4235033392906189\n",
      "epoch: 2 step: 955, loss is 0.6397303342819214\n",
      "epoch: 2 step: 956, loss is 0.5333882570266724\n",
      "epoch: 2 step: 957, loss is 0.5686942934989929\n",
      "epoch: 2 step: 958, loss is 0.6261937022209167\n",
      "epoch: 2 step: 959, loss is 0.5010254383087158\n",
      "epoch: 2 step: 960, loss is 0.6891287565231323\n",
      "epoch: 2 step: 961, loss is 0.5974173545837402\n",
      "epoch: 2 step: 962, loss is 0.24558736383914948\n",
      "epoch: 2 step: 963, loss is 0.9288386106491089\n",
      "epoch: 2 step: 964, loss is 0.490393728017807\n",
      "epoch: 2 step: 965, loss is 0.800363302230835\n",
      "epoch: 2 step: 966, loss is 0.8557978868484497\n",
      "epoch: 2 step: 967, loss is 0.5097008347511292\n",
      "epoch: 2 step: 968, loss is 0.3884667754173279\n",
      "epoch: 2 step: 969, loss is 0.7714598774909973\n",
      "epoch: 2 step: 970, loss is 0.8909212946891785\n",
      "epoch: 2 step: 971, loss is 0.5335249304771423\n",
      "epoch: 2 step: 972, loss is 0.40013256669044495\n",
      "epoch: 2 step: 973, loss is 0.46107733249664307\n",
      "epoch: 2 step: 974, loss is 0.5731383562088013\n",
      "epoch: 2 step: 975, loss is 0.7257214188575745\n",
      "epoch: 2 step: 976, loss is 0.8926224112510681\n",
      "epoch: 2 step: 977, loss is 0.6566356420516968\n",
      "epoch: 2 step: 978, loss is 0.6670789122581482\n",
      "epoch: 2 step: 979, loss is 0.7168970108032227\n",
      "epoch: 2 step: 980, loss is 0.7040316462516785\n",
      "epoch: 2 step: 981, loss is 0.8574979305267334\n",
      "epoch: 2 step: 982, loss is 0.6169250011444092\n",
      "epoch: 2 step: 983, loss is 0.8444610238075256\n",
      "epoch: 2 step: 984, loss is 0.6667037010192871\n",
      "epoch: 2 step: 985, loss is 0.6317787766456604\n",
      "epoch: 2 step: 986, loss is 0.6001964807510376\n",
      "epoch: 2 step: 987, loss is 0.5816315412521362\n",
      "epoch: 2 step: 988, loss is 0.614972710609436\n",
      "epoch: 2 step: 989, loss is 0.8048108816146851\n",
      "epoch: 2 step: 990, loss is 0.7535586357116699\n",
      "epoch: 2 step: 991, loss is 0.7214468121528625\n",
      "epoch: 2 step: 992, loss is 0.6182122230529785\n",
      "epoch: 2 step: 993, loss is 0.5778575539588928\n",
      "epoch: 2 step: 994, loss is 0.6712484359741211\n",
      "epoch: 2 step: 995, loss is 0.5463587045669556\n",
      "epoch: 2 step: 996, loss is 1.012380838394165\n",
      "epoch: 2 step: 997, loss is 0.5358784198760986\n",
      "epoch: 2 step: 998, loss is 0.541033923625946\n",
      "epoch: 2 step: 999, loss is 0.8075340390205383\n",
      "epoch: 2 step: 1000, loss is 0.539706289768219\n",
      "epoch: 2 step: 1001, loss is 0.532865583896637\n",
      "epoch: 2 step: 1002, loss is 0.5089036226272583\n",
      "epoch: 2 step: 1003, loss is 0.5672265291213989\n",
      "epoch: 2 step: 1004, loss is 0.696852445602417\n",
      "epoch: 2 step: 1005, loss is 0.5640958547592163\n",
      "epoch: 2 step: 1006, loss is 0.3802929222583771\n",
      "epoch: 2 step: 1007, loss is 0.754301130771637\n",
      "epoch: 2 step: 1008, loss is 0.5045750141143799\n",
      "epoch: 2 step: 1009, loss is 0.6410157680511475\n",
      "epoch: 2 step: 1010, loss is 0.5018967390060425\n",
      "epoch: 2 step: 1011, loss is 0.47721022367477417\n",
      "epoch: 2 step: 1012, loss is 0.6391453146934509\n",
      "epoch: 2 step: 1013, loss is 0.7965458035469055\n",
      "epoch: 2 step: 1014, loss is 0.8379429578781128\n",
      "epoch: 2 step: 1015, loss is 0.6869339942932129\n",
      "epoch: 2 step: 1016, loss is 0.69352787733078\n",
      "epoch: 2 step: 1017, loss is 0.4587111473083496\n",
      "epoch: 2 step: 1018, loss is 0.6582943201065063\n",
      "epoch: 2 step: 1019, loss is 0.6805883049964905\n",
      "epoch: 2 step: 1020, loss is 0.6655662059783936\n",
      "epoch: 2 step: 1021, loss is 0.5423028469085693\n",
      "epoch: 2 step: 1022, loss is 0.6379813551902771\n",
      "epoch: 2 step: 1023, loss is 0.6462247371673584\n",
      "epoch: 2 step: 1024, loss is 0.5767276883125305\n",
      "epoch: 2 step: 1025, loss is 0.6651978492736816\n",
      "epoch: 2 step: 1026, loss is 0.684601366519928\n",
      "epoch: 2 step: 1027, loss is 0.6458716988563538\n",
      "epoch: 2 step: 1028, loss is 0.6403125524520874\n",
      "epoch: 2 step: 1029, loss is 0.6612032055854797\n",
      "epoch: 2 step: 1030, loss is 0.6422903537750244\n",
      "epoch: 2 step: 1031, loss is 0.7036897540092468\n",
      "epoch: 2 step: 1032, loss is 0.7429417967796326\n",
      "epoch: 2 step: 1033, loss is 0.7622488737106323\n",
      "epoch: 2 step: 1034, loss is 0.8015141487121582\n",
      "epoch: 2 step: 1035, loss is 0.5704824924468994\n",
      "epoch: 2 step: 1036, loss is 0.4495554268360138\n",
      "epoch: 2 step: 1037, loss is 0.535236120223999\n",
      "epoch: 2 step: 1038, loss is 0.5232284665107727\n",
      "epoch: 2 step: 1039, loss is 0.6913022994995117\n",
      "epoch: 2 step: 1040, loss is 0.5466780662536621\n",
      "epoch: 2 step: 1041, loss is 0.5308685302734375\n",
      "epoch: 2 step: 1042, loss is 0.6852666735649109\n",
      "epoch: 2 step: 1043, loss is 0.5236921906471252\n",
      "epoch: 2 step: 1044, loss is 0.4440559148788452\n",
      "epoch: 2 step: 1045, loss is 0.7391306757926941\n",
      "epoch: 2 step: 1046, loss is 0.6745135188102722\n",
      "epoch: 2 step: 1047, loss is 0.5824970006942749\n",
      "epoch: 2 step: 1048, loss is 0.47827088832855225\n",
      "epoch: 2 step: 1049, loss is 0.7676857113838196\n",
      "epoch: 2 step: 1050, loss is 0.5544235110282898\n",
      "epoch: 2 step: 1051, loss is 0.3331352472305298\n",
      "epoch: 2 step: 1052, loss is 0.8612873554229736\n",
      "epoch: 2 step: 1053, loss is 0.7948924899101257\n",
      "epoch: 2 step: 1054, loss is 0.6374553442001343\n",
      "epoch: 2 step: 1055, loss is 0.8576775789260864\n",
      "epoch: 2 step: 1056, loss is 0.5257402658462524\n",
      "epoch: 2 step: 1057, loss is 0.5805456638336182\n",
      "epoch: 2 step: 1058, loss is 0.6404306888580322\n",
      "epoch: 2 step: 1059, loss is 0.7020369172096252\n",
      "epoch: 2 step: 1060, loss is 0.7110925316810608\n",
      "epoch: 2 step: 1061, loss is 0.4517480731010437\n",
      "epoch: 2 step: 1062, loss is 0.5974934101104736\n",
      "epoch: 2 step: 1063, loss is 0.7124544382095337\n",
      "epoch: 2 step: 1064, loss is 0.7013679146766663\n",
      "epoch: 2 step: 1065, loss is 0.3612177073955536\n",
      "epoch: 2 step: 1066, loss is 0.5238085389137268\n",
      "epoch: 2 step: 1067, loss is 0.484183132648468\n",
      "epoch: 2 step: 1068, loss is 0.48176464438438416\n",
      "epoch: 2 step: 1069, loss is 0.6325840950012207\n",
      "epoch: 2 step: 1070, loss is 0.6973220705986023\n",
      "epoch: 2 step: 1071, loss is 0.5384544134140015\n",
      "epoch: 2 step: 1072, loss is 0.5422065258026123\n",
      "epoch: 2 step: 1073, loss is 0.5611781477928162\n",
      "epoch: 2 step: 1074, loss is 0.4914036989212036\n",
      "epoch: 2 step: 1075, loss is 0.4338878393173218\n",
      "epoch: 2 step: 1076, loss is 0.6362741589546204\n",
      "epoch: 2 step: 1077, loss is 0.5511762499809265\n",
      "epoch: 2 step: 1078, loss is 0.4940045177936554\n",
      "epoch: 2 step: 1079, loss is 0.7632502317428589\n",
      "epoch: 2 step: 1080, loss is 0.5303186178207397\n",
      "epoch: 2 step: 1081, loss is 0.5983755588531494\n",
      "epoch: 2 step: 1082, loss is 0.6634724736213684\n",
      "epoch: 2 step: 1083, loss is 0.5456319451332092\n",
      "epoch: 2 step: 1084, loss is 0.43613240122795105\n",
      "epoch: 2 step: 1085, loss is 0.7083840370178223\n",
      "epoch: 2 step: 1086, loss is 0.43222934007644653\n",
      "epoch: 2 step: 1087, loss is 0.5460830330848694\n",
      "epoch: 2 step: 1088, loss is 0.5239513516426086\n",
      "epoch: 2 step: 1089, loss is 0.5711974501609802\n",
      "epoch: 2 step: 1090, loss is 0.759305477142334\n",
      "epoch: 2 step: 1091, loss is 0.9019510746002197\n",
      "epoch: 2 step: 1092, loss is 0.7606539130210876\n",
      "epoch: 2 step: 1093, loss is 0.6556902527809143\n",
      "epoch: 2 step: 1094, loss is 0.33360299468040466\n",
      "epoch: 2 step: 1095, loss is 0.6174748539924622\n",
      "epoch: 2 step: 1096, loss is 0.36220332980155945\n",
      "epoch: 2 step: 1097, loss is 0.613679051399231\n",
      "epoch: 2 step: 1098, loss is 0.5644364953041077\n",
      "epoch: 2 step: 1099, loss is 0.6392828822135925\n",
      "epoch: 2 step: 1100, loss is 0.5822541117668152\n",
      "epoch: 2 step: 1101, loss is 0.6439367532730103\n",
      "epoch: 2 step: 1102, loss is 0.5218302607536316\n",
      "epoch: 2 step: 1103, loss is 0.6668038368225098\n",
      "epoch: 2 step: 1104, loss is 0.696202278137207\n",
      "epoch: 2 step: 1105, loss is 0.46163493394851685\n",
      "epoch: 2 step: 1106, loss is 0.4891910254955292\n",
      "epoch: 2 step: 1107, loss is 0.46755051612854004\n",
      "epoch: 2 step: 1108, loss is 0.6919041872024536\n",
      "epoch: 2 step: 1109, loss is 0.4974621832370758\n",
      "epoch: 2 step: 1110, loss is 0.6337977647781372\n",
      "epoch: 2 step: 1111, loss is 0.6658126711845398\n",
      "epoch: 2 step: 1112, loss is 0.46097928285598755\n",
      "epoch: 2 step: 1113, loss is 0.9450956583023071\n",
      "epoch: 2 step: 1114, loss is 0.707421064376831\n",
      "epoch: 2 step: 1115, loss is 0.4603249132633209\n",
      "epoch: 2 step: 1116, loss is 0.46035146713256836\n",
      "epoch: 2 step: 1117, loss is 0.4860670566558838\n",
      "epoch: 2 step: 1118, loss is 0.40156108140945435\n",
      "epoch: 2 step: 1119, loss is 0.6875908374786377\n",
      "epoch: 2 step: 1120, loss is 0.6008577942848206\n",
      "epoch: 2 step: 1121, loss is 0.3422655165195465\n",
      "epoch: 2 step: 1122, loss is 0.567386269569397\n",
      "epoch: 2 step: 1123, loss is 0.36895522475242615\n",
      "epoch: 2 step: 1124, loss is 0.3705134689807892\n",
      "epoch: 2 step: 1125, loss is 1.0293527841567993\n",
      "epoch: 2 step: 1126, loss is 0.7805181741714478\n",
      "epoch: 2 step: 1127, loss is 0.8568909168243408\n",
      "epoch: 2 step: 1128, loss is 0.8666470050811768\n",
      "epoch: 2 step: 1129, loss is 0.9219725728034973\n",
      "epoch: 2 step: 1130, loss is 0.33737850189208984\n",
      "epoch: 2 step: 1131, loss is 0.5926241874694824\n",
      "epoch: 2 step: 1132, loss is 0.7459437847137451\n",
      "epoch: 2 step: 1133, loss is 0.8358119130134583\n",
      "epoch: 2 step: 1134, loss is 0.530562162399292\n",
      "epoch: 2 step: 1135, loss is 0.5568037629127502\n",
      "epoch: 2 step: 1136, loss is 0.8106245398521423\n",
      "epoch: 2 step: 1137, loss is 0.7641728520393372\n",
      "epoch: 2 step: 1138, loss is 0.5857075452804565\n",
      "epoch: 2 step: 1139, loss is 0.5255924463272095\n",
      "epoch: 2 step: 1140, loss is 0.5503305196762085\n",
      "epoch: 2 step: 1141, loss is 1.397018313407898\n",
      "epoch: 2 step: 1142, loss is 0.8386030793190002\n",
      "epoch: 2 step: 1143, loss is 0.6106055974960327\n",
      "epoch: 2 step: 1144, loss is 0.4784095585346222\n",
      "epoch: 2 step: 1145, loss is 0.5989871621131897\n",
      "epoch: 2 step: 1146, loss is 0.6432132720947266\n",
      "epoch: 2 step: 1147, loss is 0.7222557067871094\n",
      "epoch: 2 step: 1148, loss is 0.6790376901626587\n",
      "epoch: 2 step: 1149, loss is 0.7121647000312805\n",
      "epoch: 2 step: 1150, loss is 0.4603636562824249\n",
      "epoch: 2 step: 1151, loss is 0.6629318594932556\n",
      "epoch: 2 step: 1152, loss is 0.636721134185791\n",
      "epoch: 2 step: 1153, loss is 0.5463215112686157\n",
      "epoch: 2 step: 1154, loss is 0.7169554829597473\n",
      "epoch: 2 step: 1155, loss is 0.8748780488967896\n",
      "epoch: 2 step: 1156, loss is 0.684776782989502\n",
      "epoch: 2 step: 1157, loss is 0.674203634262085\n",
      "epoch: 2 step: 1158, loss is 0.5878434181213379\n",
      "epoch: 2 step: 1159, loss is 0.4439576268196106\n",
      "epoch: 2 step: 1160, loss is 0.587583601474762\n",
      "epoch: 2 step: 1161, loss is 0.512729823589325\n",
      "epoch: 2 step: 1162, loss is 0.5418761968612671\n",
      "epoch: 2 step: 1163, loss is 0.5670983195304871\n",
      "epoch: 2 step: 1164, loss is 0.45020389556884766\n",
      "epoch: 2 step: 1165, loss is 0.6146911382675171\n",
      "epoch: 2 step: 1166, loss is 0.6005036234855652\n",
      "epoch: 2 step: 1167, loss is 0.6731724739074707\n",
      "epoch: 2 step: 1168, loss is 0.9531980752944946\n",
      "epoch: 2 step: 1169, loss is 0.845881462097168\n",
      "epoch: 2 step: 1170, loss is 0.5055720806121826\n",
      "epoch: 2 step: 1171, loss is 0.5078100562095642\n",
      "epoch: 2 step: 1172, loss is 0.35255199670791626\n",
      "epoch: 2 step: 1173, loss is 0.7089359760284424\n",
      "epoch: 2 step: 1174, loss is 0.5107473731040955\n",
      "epoch: 2 step: 1175, loss is 0.7232781648635864\n",
      "epoch: 2 step: 1176, loss is 1.0120770931243896\n",
      "epoch: 2 step: 1177, loss is 0.8681435585021973\n",
      "epoch: 2 step: 1178, loss is 0.39839082956314087\n",
      "epoch: 2 step: 1179, loss is 0.5608718395233154\n",
      "epoch: 2 step: 1180, loss is 0.8692066669464111\n",
      "epoch: 2 step: 1181, loss is 0.6883183717727661\n",
      "epoch: 2 step: 1182, loss is 0.543075442314148\n",
      "epoch: 2 step: 1183, loss is 0.7676929235458374\n",
      "epoch: 2 step: 1184, loss is 0.5072227716445923\n",
      "epoch: 2 step: 1185, loss is 0.6339622139930725\n",
      "epoch: 2 step: 1186, loss is 0.7671451568603516\n",
      "epoch: 2 step: 1187, loss is 0.4991644322872162\n",
      "epoch: 2 step: 1188, loss is 0.5276233553886414\n",
      "epoch: 2 step: 1189, loss is 0.6481225490570068\n",
      "epoch: 2 step: 1190, loss is 0.7766704559326172\n",
      "epoch: 2 step: 1191, loss is 0.5227457880973816\n",
      "epoch: 2 step: 1192, loss is 0.6344009041786194\n",
      "epoch: 2 step: 1193, loss is 0.6814852952957153\n",
      "epoch: 2 step: 1194, loss is 0.507888674736023\n",
      "epoch: 2 step: 1195, loss is 0.5838699340820312\n",
      "epoch: 2 step: 1196, loss is 0.4223592281341553\n",
      "epoch: 2 step: 1197, loss is 0.5301588177680969\n",
      "epoch: 2 step: 1198, loss is 0.4770548343658447\n",
      "epoch: 2 step: 1199, loss is 0.6204439997673035\n",
      "epoch: 2 step: 1200, loss is 0.4480850100517273\n",
      "epoch: 2 step: 1201, loss is 0.7192922830581665\n",
      "epoch: 2 step: 1202, loss is 0.779722273349762\n",
      "epoch: 2 step: 1203, loss is 0.7426513433456421\n",
      "epoch: 2 step: 1204, loss is 0.5505537986755371\n",
      "epoch: 2 step: 1205, loss is 0.6223239302635193\n",
      "epoch: 2 step: 1206, loss is 0.44921359419822693\n",
      "epoch: 2 step: 1207, loss is 0.5306487083435059\n",
      "epoch: 2 step: 1208, loss is 0.5996941924095154\n",
      "epoch: 2 step: 1209, loss is 0.4205795228481293\n",
      "epoch: 2 step: 1210, loss is 0.692743718624115\n",
      "epoch: 2 step: 1211, loss is 0.6118256449699402\n",
      "epoch: 2 step: 1212, loss is 0.39644378423690796\n",
      "epoch: 2 step: 1213, loss is 0.534024178981781\n",
      "epoch: 2 step: 1214, loss is 0.6671772599220276\n",
      "epoch: 2 step: 1215, loss is 0.8363246917724609\n",
      "epoch: 2 step: 1216, loss is 0.7566461563110352\n",
      "epoch: 2 step: 1217, loss is 0.2992771863937378\n",
      "epoch: 2 step: 1218, loss is 0.515910804271698\n",
      "epoch: 2 step: 1219, loss is 0.6415488719940186\n",
      "epoch: 2 step: 1220, loss is 0.8148562908172607\n",
      "epoch: 2 step: 1221, loss is 0.45715051889419556\n",
      "epoch: 2 step: 1222, loss is 0.8270624876022339\n",
      "epoch: 2 step: 1223, loss is 0.49346381425857544\n",
      "epoch: 2 step: 1224, loss is 0.30257466435432434\n",
      "epoch: 2 step: 1225, loss is 0.88536137342453\n",
      "epoch: 2 step: 1226, loss is 0.6989002823829651\n",
      "epoch: 2 step: 1227, loss is 0.725338339805603\n",
      "epoch: 2 step: 1228, loss is 0.5793462991714478\n",
      "epoch: 2 step: 1229, loss is 0.6080519556999207\n",
      "epoch: 2 step: 1230, loss is 0.48312556743621826\n",
      "epoch: 2 step: 1231, loss is 0.8150663375854492\n",
      "epoch: 2 step: 1232, loss is 0.5606817007064819\n",
      "epoch: 2 step: 1233, loss is 0.5100001692771912\n",
      "epoch: 2 step: 1234, loss is 0.8657832741737366\n",
      "epoch: 2 step: 1235, loss is 0.43122968077659607\n",
      "epoch: 2 step: 1236, loss is 0.4534864127635956\n",
      "epoch: 2 step: 1237, loss is 0.7790409922599792\n",
      "epoch: 2 step: 1238, loss is 0.6092972159385681\n",
      "epoch: 2 step: 1239, loss is 0.313419371843338\n",
      "epoch: 2 step: 1240, loss is 0.6239179372787476\n",
      "epoch: 2 step: 1241, loss is 0.6613451242446899\n",
      "epoch: 2 step: 1242, loss is 0.6281870007514954\n",
      "epoch: 2 step: 1243, loss is 0.6632291078567505\n",
      "epoch: 2 step: 1244, loss is 0.5273725986480713\n",
      "epoch: 2 step: 1245, loss is 0.4078514575958252\n",
      "epoch: 2 step: 1246, loss is 0.7142676711082458\n",
      "epoch: 2 step: 1247, loss is 0.7581957578659058\n",
      "epoch: 2 step: 1248, loss is 0.3629080057144165\n",
      "epoch: 2 step: 1249, loss is 0.9453870058059692\n",
      "epoch: 2 step: 1250, loss is 0.47769686579704285\n",
      "epoch: 2 step: 1251, loss is 0.8018351197242737\n",
      "epoch: 2 step: 1252, loss is 0.37623804807662964\n",
      "epoch: 2 step: 1253, loss is 0.6877933740615845\n",
      "epoch: 2 step: 1254, loss is 0.6407023072242737\n",
      "epoch: 2 step: 1255, loss is 0.7697257399559021\n",
      "epoch: 2 step: 1256, loss is 0.4397448003292084\n",
      "epoch: 2 step: 1257, loss is 0.8069395422935486\n",
      "epoch: 2 step: 1258, loss is 0.8219541311264038\n",
      "epoch: 2 step: 1259, loss is 0.47841745615005493\n",
      "epoch: 2 step: 1260, loss is 0.490459680557251\n",
      "epoch: 2 step: 1261, loss is 0.9567683935165405\n",
      "epoch: 2 step: 1262, loss is 0.3358958661556244\n",
      "epoch: 2 step: 1263, loss is 0.5795221924781799\n",
      "epoch: 2 step: 1264, loss is 0.5136983394622803\n",
      "epoch: 2 step: 1265, loss is 0.6552360653877258\n",
      "epoch: 2 step: 1266, loss is 0.5620243549346924\n",
      "epoch: 2 step: 1267, loss is 0.4586262106895447\n",
      "epoch: 2 step: 1268, loss is 0.6593875288963318\n",
      "epoch: 2 step: 1269, loss is 0.6416687965393066\n",
      "epoch: 2 step: 1270, loss is 0.7093551754951477\n",
      "epoch: 2 step: 1271, loss is 0.7283750772476196\n",
      "epoch: 2 step: 1272, loss is 0.7366218566894531\n",
      "epoch: 2 step: 1273, loss is 0.6064659953117371\n",
      "epoch: 2 step: 1274, loss is 1.0978032350540161\n",
      "epoch: 2 step: 1275, loss is 0.48532095551490784\n",
      "epoch: 2 step: 1276, loss is 0.9351027607917786\n",
      "epoch: 2 step: 1277, loss is 0.6708872318267822\n",
      "epoch: 2 step: 1278, loss is 0.7118450999259949\n",
      "epoch: 2 step: 1279, loss is 0.5658990144729614\n",
      "epoch: 2 step: 1280, loss is 0.661105751991272\n",
      "epoch: 2 step: 1281, loss is 0.5746883749961853\n",
      "epoch: 2 step: 1282, loss is 0.360551118850708\n",
      "epoch: 2 step: 1283, loss is 0.9807575345039368\n",
      "epoch: 2 step: 1284, loss is 0.6473287343978882\n",
      "epoch: 2 step: 1285, loss is 0.5623633861541748\n",
      "epoch: 2 step: 1286, loss is 0.5099952816963196\n",
      "epoch: 2 step: 1287, loss is 0.5947674512863159\n",
      "epoch: 2 step: 1288, loss is 0.7082877159118652\n",
      "epoch: 2 step: 1289, loss is 0.6507121920585632\n",
      "epoch: 2 step: 1290, loss is 0.8676286935806274\n",
      "epoch: 2 step: 1291, loss is 0.5480126142501831\n",
      "epoch: 2 step: 1292, loss is 0.7878726720809937\n",
      "epoch: 2 step: 1293, loss is 0.6758039593696594\n",
      "epoch: 2 step: 1294, loss is 0.5393925309181213\n",
      "epoch: 2 step: 1295, loss is 0.7667666673660278\n",
      "epoch: 2 step: 1296, loss is 0.5787137150764465\n",
      "epoch: 2 step: 1297, loss is 0.5147496461868286\n",
      "epoch: 2 step: 1298, loss is 0.8023975491523743\n",
      "epoch: 2 step: 1299, loss is 0.530376136302948\n",
      "epoch: 2 step: 1300, loss is 0.6081026792526245\n",
      "epoch: 2 step: 1301, loss is 0.6506754159927368\n",
      "epoch: 2 step: 1302, loss is 0.7882611155509949\n",
      "epoch: 2 step: 1303, loss is 0.6572843194007874\n",
      "epoch: 2 step: 1304, loss is 0.7478070259094238\n",
      "epoch: 2 step: 1305, loss is 0.7383723258972168\n",
      "epoch: 2 step: 1306, loss is 0.8079628348350525\n",
      "epoch: 2 step: 1307, loss is 0.5271236300468445\n",
      "epoch: 2 step: 1308, loss is 0.58531254529953\n",
      "epoch: 2 step: 1309, loss is 0.7313390970230103\n",
      "epoch: 2 step: 1310, loss is 0.6989758610725403\n",
      "epoch: 2 step: 1311, loss is 0.6728302836418152\n",
      "epoch: 2 step: 1312, loss is 0.5345640182495117\n",
      "epoch: 2 step: 1313, loss is 1.0013407468795776\n",
      "epoch: 2 step: 1314, loss is 0.616603672504425\n",
      "epoch: 2 step: 1315, loss is 0.6248904466629028\n",
      "epoch: 2 step: 1316, loss is 0.9218294024467468\n",
      "epoch: 2 step: 1317, loss is 0.5116899609565735\n",
      "epoch: 2 step: 1318, loss is 0.5369494557380676\n",
      "epoch: 2 step: 1319, loss is 0.5607262849807739\n",
      "epoch: 2 step: 1320, loss is 0.6239087581634521\n",
      "epoch: 2 step: 1321, loss is 0.6144421696662903\n",
      "epoch: 2 step: 1322, loss is 0.7326233983039856\n",
      "epoch: 2 step: 1323, loss is 0.7575210928916931\n",
      "epoch: 2 step: 1324, loss is 0.6855154633522034\n",
      "epoch: 2 step: 1325, loss is 0.6130677461624146\n",
      "epoch: 2 step: 1326, loss is 0.4201280176639557\n",
      "epoch: 2 step: 1327, loss is 0.5219227075576782\n",
      "epoch: 2 step: 1328, loss is 0.5000316500663757\n",
      "epoch: 2 step: 1329, loss is 0.6512567400932312\n",
      "epoch: 2 step: 1330, loss is 0.5109823942184448\n",
      "epoch: 2 step: 1331, loss is 0.3695385158061981\n",
      "epoch: 2 step: 1332, loss is 0.8580976128578186\n",
      "epoch: 2 step: 1333, loss is 0.5430405735969543\n",
      "epoch: 2 step: 1334, loss is 0.5584844350814819\n",
      "epoch: 2 step: 1335, loss is 0.5712580680847168\n",
      "epoch: 2 step: 1336, loss is 0.747616708278656\n",
      "epoch: 2 step: 1337, loss is 0.6455725431442261\n",
      "epoch: 2 step: 1338, loss is 0.5242412686347961\n",
      "epoch: 2 step: 1339, loss is 0.5127080082893372\n",
      "epoch: 2 step: 1340, loss is 0.45402395725250244\n",
      "epoch: 2 step: 1341, loss is 0.649970293045044\n",
      "epoch: 2 step: 1342, loss is 0.40777134895324707\n",
      "epoch: 2 step: 1343, loss is 0.5743530988693237\n",
      "epoch: 2 step: 1344, loss is 0.6404284834861755\n",
      "epoch: 2 step: 1345, loss is 0.8996460437774658\n",
      "epoch: 2 step: 1346, loss is 0.5612463355064392\n",
      "epoch: 2 step: 1347, loss is 0.5420427918434143\n",
      "epoch: 2 step: 1348, loss is 0.6412618160247803\n",
      "epoch: 2 step: 1349, loss is 0.74090576171875\n",
      "epoch: 2 step: 1350, loss is 0.6427125334739685\n",
      "epoch: 2 step: 1351, loss is 0.8170894384384155\n",
      "epoch: 2 step: 1352, loss is 0.4618719220161438\n",
      "epoch: 2 step: 1353, loss is 0.4034249186515808\n",
      "epoch: 2 step: 1354, loss is 0.4827822148799896\n",
      "epoch: 2 step: 1355, loss is 0.7808887958526611\n",
      "epoch: 2 step: 1356, loss is 0.615295946598053\n",
      "epoch: 2 step: 1357, loss is 0.6365190148353577\n",
      "epoch: 2 step: 1358, loss is 0.8954801559448242\n",
      "epoch: 2 step: 1359, loss is 0.7176943421363831\n",
      "epoch: 2 step: 1360, loss is 0.5386767387390137\n",
      "epoch: 2 step: 1361, loss is 0.502544105052948\n",
      "epoch: 2 step: 1362, loss is 0.6154564023017883\n",
      "epoch: 2 step: 1363, loss is 0.6216461062431335\n",
      "epoch: 2 step: 1364, loss is 0.46155846118927\n",
      "epoch: 2 step: 1365, loss is 0.4828440546989441\n",
      "epoch: 2 step: 1366, loss is 0.6955687999725342\n",
      "epoch: 2 step: 1367, loss is 0.552640438079834\n",
      "epoch: 2 step: 1368, loss is 0.5815478563308716\n",
      "epoch: 2 step: 1369, loss is 0.6340327858924866\n",
      "epoch: 2 step: 1370, loss is 0.530326783657074\n",
      "epoch: 2 step: 1371, loss is 0.7276303768157959\n",
      "epoch: 2 step: 1372, loss is 0.32164624333381653\n",
      "epoch: 2 step: 1373, loss is 0.673526406288147\n",
      "epoch: 2 step: 1374, loss is 0.5579220652580261\n",
      "epoch: 2 step: 1375, loss is 0.7396447062492371\n",
      "epoch: 2 step: 1376, loss is 0.8872407078742981\n",
      "epoch: 2 step: 1377, loss is 0.41737818717956543\n",
      "epoch: 2 step: 1378, loss is 0.7766295671463013\n",
      "epoch: 2 step: 1379, loss is 0.40586987137794495\n",
      "epoch: 2 step: 1380, loss is 0.6329179406166077\n",
      "epoch: 2 step: 1381, loss is 0.5124556422233582\n",
      "epoch: 2 step: 1382, loss is 0.5595765113830566\n",
      "epoch: 2 step: 1383, loss is 0.49085912108421326\n",
      "epoch: 2 step: 1384, loss is 0.8771089315414429\n",
      "epoch: 2 step: 1385, loss is 0.5876843333244324\n",
      "epoch: 2 step: 1386, loss is 0.5552678108215332\n",
      "epoch: 2 step: 1387, loss is 0.5260787010192871\n",
      "epoch: 2 step: 1388, loss is 0.34663647413253784\n",
      "epoch: 2 step: 1389, loss is 0.8377774357795715\n",
      "epoch: 2 step: 1390, loss is 0.37423187494277954\n",
      "epoch: 2 step: 1391, loss is 0.47989651560783386\n",
      "epoch: 2 step: 1392, loss is 0.46107757091522217\n",
      "epoch: 2 step: 1393, loss is 0.9612618088722229\n",
      "epoch: 2 step: 1394, loss is 0.7383285164833069\n",
      "epoch: 2 step: 1395, loss is 0.9650841355323792\n",
      "epoch: 2 step: 1396, loss is 0.5251189470291138\n",
      "epoch: 2 step: 1397, loss is 0.5897302627563477\n",
      "epoch: 2 step: 1398, loss is 0.6688836216926575\n",
      "epoch: 2 step: 1399, loss is 0.6343425512313843\n",
      "epoch: 2 step: 1400, loss is 0.6947959661483765\n",
      "epoch: 2 step: 1401, loss is 0.5990930795669556\n",
      "epoch: 2 step: 1402, loss is 0.536375105381012\n",
      "epoch: 2 step: 1403, loss is 0.6502543687820435\n",
      "epoch: 2 step: 1404, loss is 0.7900573015213013\n",
      "epoch: 2 step: 1405, loss is 0.5801659226417542\n",
      "epoch: 2 step: 1406, loss is 0.5525543689727783\n",
      "epoch: 2 step: 1407, loss is 0.7172577977180481\n",
      "epoch: 2 step: 1408, loss is 0.645922839641571\n",
      "epoch: 2 step: 1409, loss is 0.9629431366920471\n",
      "epoch: 2 step: 1410, loss is 0.6698187589645386\n",
      "epoch: 2 step: 1411, loss is 0.5639466047286987\n",
      "epoch: 2 step: 1412, loss is 0.44119617342948914\n",
      "epoch: 2 step: 1413, loss is 0.5340216159820557\n",
      "epoch: 2 step: 1414, loss is 0.6241081953048706\n",
      "epoch: 2 step: 1415, loss is 0.5544015169143677\n",
      "epoch: 2 step: 1416, loss is 0.5424500107765198\n",
      "epoch: 2 step: 1417, loss is 0.5922345519065857\n",
      "epoch: 2 step: 1418, loss is 0.9351466298103333\n",
      "epoch: 2 step: 1419, loss is 0.6961655616760254\n",
      "epoch: 2 step: 1420, loss is 0.6255227327346802\n",
      "epoch: 2 step: 1421, loss is 0.715700089931488\n",
      "epoch: 2 step: 1422, loss is 1.065768837928772\n",
      "epoch: 2 step: 1423, loss is 0.7160785794258118\n",
      "epoch: 2 step: 1424, loss is 0.5490272045135498\n",
      "epoch: 2 step: 1425, loss is 0.7276464700698853\n",
      "epoch: 2 step: 1426, loss is 0.8043078184127808\n",
      "epoch: 2 step: 1427, loss is 0.6597425937652588\n",
      "epoch: 2 step: 1428, loss is 0.6594902276992798\n",
      "epoch: 2 step: 1429, loss is 0.7412619590759277\n",
      "epoch: 2 step: 1430, loss is 0.653071939945221\n",
      "epoch: 2 step: 1431, loss is 0.7450269460678101\n",
      "epoch: 2 step: 1432, loss is 0.5430973768234253\n",
      "epoch: 2 step: 1433, loss is 0.5586189031600952\n",
      "epoch: 2 step: 1434, loss is 0.629622220993042\n",
      "epoch: 2 step: 1435, loss is 0.5956994295120239\n",
      "epoch: 2 step: 1436, loss is 0.5567681193351746\n",
      "epoch: 2 step: 1437, loss is 0.5211129188537598\n",
      "epoch: 2 step: 1438, loss is 0.6580792665481567\n",
      "epoch: 2 step: 1439, loss is 0.45274490118026733\n",
      "epoch: 2 step: 1440, loss is 0.5923662185668945\n",
      "epoch: 2 step: 1441, loss is 0.6458678245544434\n",
      "epoch: 2 step: 1442, loss is 0.8234497904777527\n",
      "epoch: 2 step: 1443, loss is 0.6621714234352112\n",
      "epoch: 2 step: 1444, loss is 0.3499545156955719\n",
      "epoch: 2 step: 1445, loss is 0.6916012167930603\n",
      "epoch: 2 step: 1446, loss is 0.47949573397636414\n",
      "epoch: 2 step: 1447, loss is 0.6678940057754517\n",
      "epoch: 2 step: 1448, loss is 0.7265291213989258\n",
      "epoch: 2 step: 1449, loss is 0.561293363571167\n",
      "epoch: 2 step: 1450, loss is 0.6829189658164978\n",
      "epoch: 2 step: 1451, loss is 0.7936245799064636\n",
      "epoch: 2 step: 1452, loss is 0.6830662488937378\n",
      "epoch: 2 step: 1453, loss is 0.5063900351524353\n",
      "epoch: 2 step: 1454, loss is 0.7195041179656982\n",
      "epoch: 2 step: 1455, loss is 0.35485270619392395\n",
      "epoch: 2 step: 1456, loss is 0.6471574306488037\n",
      "epoch: 2 step: 1457, loss is 0.6379793286323547\n",
      "epoch: 2 step: 1458, loss is 0.5687651038169861\n",
      "epoch: 2 step: 1459, loss is 0.6481163501739502\n",
      "epoch: 2 step: 1460, loss is 0.5294981598854065\n",
      "epoch: 2 step: 1461, loss is 0.5041912198066711\n",
      "epoch: 2 step: 1462, loss is 0.3798656165599823\n",
      "epoch: 2 step: 1463, loss is 0.6815176606178284\n",
      "epoch: 2 step: 1464, loss is 0.7609779238700867\n",
      "epoch: 2 step: 1465, loss is 0.5441032648086548\n",
      "epoch: 2 step: 1466, loss is 0.6978498697280884\n",
      "epoch: 2 step: 1467, loss is 0.5940807461738586\n",
      "epoch: 2 step: 1468, loss is 0.7967730164527893\n",
      "epoch: 2 step: 1469, loss is 0.5982938408851624\n",
      "epoch: 2 step: 1470, loss is 0.5964164137840271\n",
      "epoch: 2 step: 1471, loss is 0.8095030188560486\n",
      "epoch: 2 step: 1472, loss is 0.6424422860145569\n",
      "epoch: 2 step: 1473, loss is 0.7161968946456909\n",
      "epoch: 2 step: 1474, loss is 0.3941470980644226\n",
      "epoch: 2 step: 1475, loss is 0.5218874216079712\n",
      "epoch: 2 step: 1476, loss is 0.552240788936615\n",
      "epoch: 2 step: 1477, loss is 0.6666417121887207\n",
      "epoch: 2 step: 1478, loss is 0.6462504267692566\n",
      "epoch: 2 step: 1479, loss is 0.5430352091789246\n",
      "epoch: 2 step: 1480, loss is 0.7366966605186462\n",
      "epoch: 2 step: 1481, loss is 0.8149498105049133\n",
      "epoch: 2 step: 1482, loss is 0.5010238289833069\n",
      "epoch: 2 step: 1483, loss is 0.5244044661521912\n",
      "epoch: 2 step: 1484, loss is 0.7203935980796814\n",
      "epoch: 2 step: 1485, loss is 0.705419659614563\n",
      "epoch: 2 step: 1486, loss is 0.5584688186645508\n",
      "epoch: 2 step: 1487, loss is 0.6334680318832397\n",
      "epoch: 2 step: 1488, loss is 0.5278531908988953\n",
      "epoch: 2 step: 1489, loss is 0.4303200840950012\n",
      "epoch: 2 step: 1490, loss is 0.7965767979621887\n",
      "epoch: 2 step: 1491, loss is 0.7916074395179749\n",
      "epoch: 2 step: 1492, loss is 0.40501517057418823\n",
      "epoch: 2 step: 1493, loss is 0.7971010804176331\n",
      "epoch: 2 step: 1494, loss is 0.4951139986515045\n",
      "epoch: 2 step: 1495, loss is 0.5615264177322388\n",
      "epoch: 2 step: 1496, loss is 0.46028104424476624\n",
      "epoch: 2 step: 1497, loss is 0.5684677362442017\n",
      "epoch: 2 step: 1498, loss is 0.5712449550628662\n",
      "epoch: 2 step: 1499, loss is 0.7826313376426697\n",
      "epoch: 2 step: 1500, loss is 0.775155782699585\n",
      "epoch: 2 step: 1501, loss is 0.7197717428207397\n",
      "epoch: 2 step: 1502, loss is 0.7017070055007935\n",
      "epoch: 2 step: 1503, loss is 0.6241028308868408\n",
      "epoch: 2 step: 1504, loss is 0.3998735547065735\n",
      "epoch: 2 step: 1505, loss is 0.8181682229042053\n",
      "epoch: 2 step: 1506, loss is 0.9087362289428711\n",
      "epoch: 2 step: 1507, loss is 0.8559668064117432\n",
      "epoch: 2 step: 1508, loss is 0.4840913712978363\n",
      "epoch: 2 step: 1509, loss is 0.6094592809677124\n",
      "epoch: 2 step: 1510, loss is 0.6989535689353943\n",
      "epoch: 2 step: 1511, loss is 1.1641112565994263\n",
      "epoch: 2 step: 1512, loss is 0.8147178292274475\n",
      "epoch: 2 step: 1513, loss is 0.5161188840866089\n",
      "epoch: 2 step: 1514, loss is 0.562314510345459\n",
      "epoch: 2 step: 1515, loss is 0.6508802771568298\n",
      "epoch: 2 step: 1516, loss is 0.677345335483551\n",
      "epoch: 2 step: 1517, loss is 0.7457942962646484\n",
      "epoch: 2 step: 1518, loss is 0.6597381830215454\n",
      "epoch: 2 step: 1519, loss is 0.5397276282310486\n",
      "epoch: 2 step: 1520, loss is 0.524097740650177\n",
      "epoch: 2 step: 1521, loss is 0.5777528285980225\n",
      "epoch: 2 step: 1522, loss is 0.6181614398956299\n",
      "epoch: 2 step: 1523, loss is 0.4562974274158478\n",
      "epoch: 2 step: 1524, loss is 0.5735795497894287\n",
      "epoch: 2 step: 1525, loss is 0.7143855690956116\n",
      "epoch: 2 step: 1526, loss is 0.46290719509124756\n",
      "epoch: 2 step: 1527, loss is 0.6398114562034607\n",
      "epoch: 2 step: 1528, loss is 0.6191078424453735\n",
      "epoch: 2 step: 1529, loss is 0.5965583920478821\n",
      "epoch: 2 step: 1530, loss is 0.4577828347682953\n",
      "epoch: 2 step: 1531, loss is 0.7180986404418945\n",
      "epoch: 2 step: 1532, loss is 0.8230567574501038\n",
      "epoch: 2 step: 1533, loss is 0.6432551145553589\n",
      "epoch: 2 step: 1534, loss is 0.6851829290390015\n",
      "epoch: 2 step: 1535, loss is 0.5769354104995728\n",
      "epoch: 2 step: 1536, loss is 0.6872451305389404\n",
      "epoch: 2 step: 1537, loss is 0.7529876828193665\n",
      "epoch: 2 step: 1538, loss is 0.8816897869110107\n",
      "epoch: 2 step: 1539, loss is 0.474616140127182\n",
      "epoch: 2 step: 1540, loss is 0.5592284202575684\n",
      "epoch: 2 step: 1541, loss is 0.44335299730300903\n",
      "epoch: 2 step: 1542, loss is 0.7250747084617615\n",
      "epoch: 2 step: 1543, loss is 0.6981777548789978\n",
      "epoch: 2 step: 1544, loss is 0.6053024530410767\n",
      "epoch: 2 step: 1545, loss is 0.4067472219467163\n",
      "epoch: 2 step: 1546, loss is 0.5284798741340637\n",
      "epoch: 2 step: 1547, loss is 0.4384273290634155\n",
      "epoch: 2 step: 1548, loss is 0.6910107731819153\n",
      "epoch: 2 step: 1549, loss is 0.6004344820976257\n",
      "epoch: 2 step: 1550, loss is 0.4375394880771637\n",
      "epoch: 2 step: 1551, loss is 0.8235900402069092\n",
      "epoch: 2 step: 1552, loss is 0.47393491864204407\n",
      "epoch: 2 step: 1553, loss is 0.6284418106079102\n",
      "epoch: 2 step: 1554, loss is 0.7469903230667114\n",
      "epoch: 2 step: 1555, loss is 0.5907794237136841\n",
      "epoch: 2 step: 1556, loss is 0.7957007884979248\n",
      "epoch: 2 step: 1557, loss is 0.4935798943042755\n",
      "epoch: 2 step: 1558, loss is 0.4963253438472748\n",
      "epoch: 2 step: 1559, loss is 0.667503833770752\n",
      "epoch: 2 step: 1560, loss is 0.35810649394989014\n",
      "epoch: 2 step: 1561, loss is 0.5965719223022461\n",
      "epoch: 2 step: 1562, loss is 0.739234447479248\n",
      "epoch: 2 step: 1563, loss is 0.5109039545059204\n",
      "epoch: 2 step: 1564, loss is 0.5711871981620789\n",
      "epoch: 2 step: 1565, loss is 0.5496696829795837\n",
      "epoch: 2 step: 1566, loss is 0.7327436208724976\n",
      "epoch: 2 step: 1567, loss is 0.5493648648262024\n",
      "epoch: 2 step: 1568, loss is 0.4911130368709564\n",
      "epoch: 2 step: 1569, loss is 0.7304335236549377\n",
      "epoch: 2 step: 1570, loss is 0.3560248613357544\n",
      "epoch: 2 step: 1571, loss is 0.6524617671966553\n",
      "epoch: 2 step: 1572, loss is 0.5026509165763855\n",
      "epoch: 2 step: 1573, loss is 0.5296754240989685\n",
      "epoch: 2 step: 1574, loss is 0.85174560546875\n",
      "epoch: 2 step: 1575, loss is 0.6008338928222656\n",
      "epoch: 2 step: 1576, loss is 0.5598838925361633\n",
      "epoch: 2 step: 1577, loss is 0.517906904220581\n",
      "epoch: 2 step: 1578, loss is 0.7526975870132446\n",
      "epoch: 2 step: 1579, loss is 0.37518900632858276\n",
      "epoch: 2 step: 1580, loss is 0.6593875885009766\n",
      "epoch: 2 step: 1581, loss is 0.6964565515518188\n",
      "epoch: 2 step: 1582, loss is 0.7526896595954895\n",
      "epoch: 2 step: 1583, loss is 0.4704086482524872\n",
      "epoch: 2 step: 1584, loss is 0.4331153929233551\n",
      "epoch: 2 step: 1585, loss is 0.43270814418792725\n",
      "epoch: 2 step: 1586, loss is 0.6142918467521667\n",
      "epoch: 2 step: 1587, loss is 0.944395899772644\n",
      "epoch: 2 step: 1588, loss is 0.5352360606193542\n",
      "epoch: 2 step: 1589, loss is 0.6046854853630066\n",
      "epoch: 2 step: 1590, loss is 0.4898545444011688\n",
      "epoch: 2 step: 1591, loss is 0.8766213655471802\n",
      "epoch: 2 step: 1592, loss is 0.5180842876434326\n",
      "epoch: 2 step: 1593, loss is 0.5149315595626831\n",
      "epoch: 2 step: 1594, loss is 0.6366927027702332\n",
      "epoch: 2 step: 1595, loss is 0.6874574422836304\n",
      "epoch: 2 step: 1596, loss is 0.6065014004707336\n",
      "epoch: 2 step: 1597, loss is 0.5322988033294678\n",
      "epoch: 2 step: 1598, loss is 0.7678563594818115\n",
      "epoch: 2 step: 1599, loss is 0.5339426398277283\n",
      "epoch: 2 step: 1600, loss is 0.8176625967025757\n",
      "epoch: 2 step: 1601, loss is 0.674859881401062\n",
      "epoch: 2 step: 1602, loss is 0.6431112289428711\n",
      "epoch: 2 step: 1603, loss is 0.3976304531097412\n",
      "epoch: 2 step: 1604, loss is 0.5797797441482544\n",
      "epoch: 2 step: 1605, loss is 0.44566071033477783\n",
      "epoch: 2 step: 1606, loss is 0.39623475074768066\n",
      "epoch: 2 step: 1607, loss is 0.6673731803894043\n",
      "epoch: 2 step: 1608, loss is 0.4103882908821106\n",
      "epoch: 2 step: 1609, loss is 0.5972752571105957\n",
      "epoch: 2 step: 1610, loss is 0.4427635371685028\n",
      "epoch: 2 step: 1611, loss is 0.44320419430732727\n",
      "epoch: 2 step: 1612, loss is 0.5207840204238892\n",
      "epoch: 2 step: 1613, loss is 0.6121771931648254\n",
      "epoch: 2 step: 1614, loss is 0.44211021065711975\n",
      "epoch: 2 step: 1615, loss is 0.7150228023529053\n",
      "epoch: 2 step: 1616, loss is 0.5567805767059326\n",
      "epoch: 2 step: 1617, loss is 0.6633728742599487\n",
      "epoch: 2 step: 1618, loss is 0.662909746170044\n",
      "epoch: 2 step: 1619, loss is 0.6002491116523743\n",
      "epoch: 2 step: 1620, loss is 0.9270919561386108\n",
      "epoch: 2 step: 1621, loss is 0.6011554002761841\n",
      "epoch: 2 step: 1622, loss is 0.5474821329116821\n",
      "epoch: 2 step: 1623, loss is 1.011640191078186\n",
      "epoch: 2 step: 1624, loss is 0.527591347694397\n",
      "epoch: 2 step: 1625, loss is 0.6696699857711792\n",
      "epoch: 2 step: 1626, loss is 0.6375339031219482\n",
      "epoch: 2 step: 1627, loss is 0.4672526717185974\n",
      "epoch: 2 step: 1628, loss is 0.44390010833740234\n",
      "epoch: 2 step: 1629, loss is 0.8267070055007935\n",
      "epoch: 2 step: 1630, loss is 0.2990880608558655\n",
      "epoch: 2 step: 1631, loss is 0.4401204288005829\n",
      "epoch: 2 step: 1632, loss is 0.7005615234375\n",
      "epoch: 2 step: 1633, loss is 0.4189319610595703\n",
      "epoch: 2 step: 1634, loss is 0.5863357782363892\n",
      "epoch: 2 step: 1635, loss is 0.6532629728317261\n",
      "epoch: 2 step: 1636, loss is 0.8635687828063965\n",
      "epoch: 2 step: 1637, loss is 0.8616589307785034\n",
      "epoch: 2 step: 1638, loss is 0.3952067792415619\n",
      "epoch: 2 step: 1639, loss is 1.096985101699829\n",
      "epoch: 2 step: 1640, loss is 0.45990660786628723\n",
      "epoch: 2 step: 1641, loss is 0.5430618524551392\n",
      "epoch: 2 step: 1642, loss is 0.5585614442825317\n",
      "epoch: 2 step: 1643, loss is 0.788287341594696\n",
      "epoch: 2 step: 1644, loss is 0.5891695022583008\n",
      "epoch: 2 step: 1645, loss is 0.6434277892112732\n",
      "epoch: 2 step: 1646, loss is 0.38955727219581604\n",
      "epoch: 2 step: 1647, loss is 0.6073253154754639\n",
      "epoch: 2 step: 1648, loss is 0.7850781083106995\n",
      "epoch: 2 step: 1649, loss is 0.5333138108253479\n",
      "epoch: 2 step: 1650, loss is 0.5811709761619568\n",
      "epoch: 2 step: 1651, loss is 0.5741158127784729\n",
      "epoch: 2 step: 1652, loss is 0.5047378540039062\n",
      "epoch: 2 step: 1653, loss is 0.5912392735481262\n",
      "epoch: 2 step: 1654, loss is 0.4693847894668579\n",
      "epoch: 2 step: 1655, loss is 0.4731281101703644\n",
      "epoch: 2 step: 1656, loss is 0.7846390604972839\n",
      "epoch: 2 step: 1657, loss is 0.5406661629676819\n",
      "epoch: 2 step: 1658, loss is 0.7834534049034119\n",
      "epoch: 2 step: 1659, loss is 0.5206344723701477\n",
      "epoch: 2 step: 1660, loss is 0.664841890335083\n",
      "epoch: 2 step: 1661, loss is 0.5598275065422058\n",
      "epoch: 2 step: 1662, loss is 0.6379916071891785\n",
      "epoch: 2 step: 1663, loss is 0.8239086866378784\n",
      "epoch: 2 step: 1664, loss is 0.3838351368904114\n",
      "epoch: 2 step: 1665, loss is 0.4898025095462799\n",
      "epoch: 2 step: 1666, loss is 0.46703165769577026\n",
      "epoch: 2 step: 1667, loss is 0.3367656171321869\n",
      "epoch: 2 step: 1668, loss is 0.6150410175323486\n",
      "epoch: 2 step: 1669, loss is 0.707774817943573\n",
      "epoch: 2 step: 1670, loss is 0.40009936690330505\n",
      "epoch: 2 step: 1671, loss is 0.2766503393650055\n",
      "epoch: 2 step: 1672, loss is 0.7035792469978333\n",
      "epoch: 2 step: 1673, loss is 1.0554395914077759\n",
      "epoch: 2 step: 1674, loss is 0.34972086548805237\n",
      "epoch: 2 step: 1675, loss is 0.3534638583660126\n",
      "epoch: 2 step: 1676, loss is 0.4533587694168091\n",
      "epoch: 2 step: 1677, loss is 0.43106457591056824\n",
      "epoch: 2 step: 1678, loss is 0.5544559359550476\n",
      "epoch: 2 step: 1679, loss is 1.4238386154174805\n",
      "epoch: 2 step: 1680, loss is 0.42782095074653625\n",
      "epoch: 2 step: 1681, loss is 1.0621474981307983\n",
      "epoch: 2 step: 1682, loss is 1.2295900583267212\n",
      "epoch: 2 step: 1683, loss is 0.9132579565048218\n",
      "epoch: 2 step: 1684, loss is 0.41236037015914917\n",
      "epoch: 2 step: 1685, loss is 0.565898060798645\n",
      "epoch: 2 step: 1686, loss is 0.7385619878768921\n",
      "epoch: 2 step: 1687, loss is 0.7613894939422607\n",
      "epoch: 2 step: 1688, loss is 0.5887792706489563\n",
      "epoch: 2 step: 1689, loss is 0.8732660412788391\n",
      "epoch: 2 step: 1690, loss is 0.6259744763374329\n",
      "epoch: 2 step: 1691, loss is 0.84574294090271\n",
      "epoch: 2 step: 1692, loss is 0.9866663813591003\n",
      "epoch: 2 step: 1693, loss is 1.223309874534607\n",
      "epoch: 2 step: 1694, loss is 1.2701712846755981\n",
      "epoch: 2 step: 1695, loss is 1.0593552589416504\n",
      "epoch: 2 step: 1696, loss is 0.7317678928375244\n",
      "epoch: 2 step: 1697, loss is 1.1948310136795044\n",
      "epoch: 2 step: 1698, loss is 0.8876640200614929\n",
      "epoch: 2 step: 1699, loss is 0.7133355140686035\n",
      "epoch: 2 step: 1700, loss is 0.6382911205291748\n",
      "epoch: 2 step: 1701, loss is 0.6885809898376465\n",
      "epoch: 2 step: 1702, loss is 0.711607038974762\n",
      "epoch: 2 step: 1703, loss is 0.7599924206733704\n",
      "epoch: 2 step: 1704, loss is 0.5231139659881592\n",
      "epoch: 2 step: 1705, loss is 0.707345724105835\n",
      "epoch: 2 step: 1706, loss is 0.7174585461616516\n",
      "epoch: 2 step: 1707, loss is 0.857020378112793\n",
      "epoch: 2 step: 1708, loss is 0.5137268900871277\n",
      "epoch: 2 step: 1709, loss is 1.2138171195983887\n",
      "epoch: 2 step: 1710, loss is 0.7006566524505615\n",
      "epoch: 2 step: 1711, loss is 0.7691827416419983\n",
      "epoch: 2 step: 1712, loss is 0.8086423873901367\n",
      "epoch: 2 step: 1713, loss is 0.6919532418251038\n",
      "epoch: 2 step: 1714, loss is 0.84518963098526\n",
      "epoch: 2 step: 1715, loss is 0.779148280620575\n",
      "epoch: 2 step: 1716, loss is 1.1223163604736328\n",
      "epoch: 2 step: 1717, loss is 0.6320846080780029\n",
      "epoch: 2 step: 1718, loss is 0.6245444416999817\n",
      "epoch: 2 step: 1719, loss is 0.5720350742340088\n",
      "epoch: 2 step: 1720, loss is 0.5614703893661499\n",
      "epoch: 2 step: 1721, loss is 0.5361629724502563\n",
      "epoch: 2 step: 1722, loss is 0.7149897217750549\n",
      "epoch: 2 step: 1723, loss is 0.6586530804634094\n",
      "epoch: 2 step: 1724, loss is 0.5985550284385681\n",
      "epoch: 2 step: 1725, loss is 0.8087968826293945\n",
      "epoch: 2 step: 1726, loss is 0.6926493644714355\n",
      "epoch: 2 step: 1727, loss is 0.5601862072944641\n",
      "epoch: 2 step: 1728, loss is 0.6283673048019409\n",
      "epoch: 2 step: 1729, loss is 0.537817120552063\n",
      "epoch: 2 step: 1730, loss is 0.72194904088974\n",
      "epoch: 2 step: 1731, loss is 0.8565396070480347\n",
      "epoch: 2 step: 1732, loss is 0.8045530319213867\n",
      "epoch: 2 step: 1733, loss is 0.6798422932624817\n",
      "epoch: 2 step: 1734, loss is 0.5002561807632446\n",
      "epoch: 2 step: 1735, loss is 0.7814869284629822\n",
      "epoch: 2 step: 1736, loss is 0.7028768062591553\n",
      "epoch: 2 step: 1737, loss is 0.639615535736084\n",
      "epoch: 2 step: 1738, loss is 0.4488101303577423\n",
      "epoch: 2 step: 1739, loss is 0.6368989944458008\n",
      "epoch: 2 step: 1740, loss is 0.8177328705787659\n",
      "epoch: 2 step: 1741, loss is 0.5700830221176147\n",
      "epoch: 2 step: 1742, loss is 0.540105938911438\n",
      "epoch: 2 step: 1743, loss is 0.49097931385040283\n",
      "epoch: 2 step: 1744, loss is 0.5940878987312317\n",
      "epoch: 2 step: 1745, loss is 0.689232349395752\n",
      "epoch: 2 step: 1746, loss is 0.5438562035560608\n",
      "epoch: 2 step: 1747, loss is 0.4333965480327606\n",
      "epoch: 2 step: 1748, loss is 0.718265950679779\n",
      "epoch: 2 step: 1749, loss is 0.43062344193458557\n",
      "epoch: 2 step: 1750, loss is 0.8741461038589478\n",
      "epoch: 2 step: 1751, loss is 0.5516988635063171\n",
      "epoch: 2 step: 1752, loss is 0.8842918872833252\n",
      "epoch: 2 step: 1753, loss is 0.7590129971504211\n",
      "epoch: 2 step: 1754, loss is 0.6665107607841492\n",
      "epoch: 2 step: 1755, loss is 0.6867755651473999\n",
      "epoch: 2 step: 1756, loss is 0.5470261573791504\n",
      "epoch: 2 step: 1757, loss is 0.6865262985229492\n",
      "epoch: 2 step: 1758, loss is 0.9639360308647156\n",
      "epoch: 2 step: 1759, loss is 0.6549160480499268\n",
      "epoch: 2 step: 1760, loss is 0.5837724208831787\n",
      "epoch: 2 step: 1761, loss is 0.7066940665245056\n",
      "epoch: 2 step: 1762, loss is 0.5033949613571167\n",
      "epoch: 2 step: 1763, loss is 0.72749263048172\n",
      "epoch: 2 step: 1764, loss is 0.47862708568573\n",
      "epoch: 2 step: 1765, loss is 0.5348171591758728\n",
      "epoch: 2 step: 1766, loss is 0.6835010051727295\n",
      "epoch: 2 step: 1767, loss is 0.783935010433197\n",
      "epoch: 2 step: 1768, loss is 0.5412824153900146\n",
      "epoch: 2 step: 1769, loss is 0.8420256972312927\n",
      "epoch: 2 step: 1770, loss is 0.5481507182121277\n",
      "epoch: 2 step: 1771, loss is 0.5834721326828003\n",
      "epoch: 2 step: 1772, loss is 0.5996282696723938\n",
      "epoch: 2 step: 1773, loss is 0.7389141321182251\n",
      "epoch: 2 step: 1774, loss is 0.5536232590675354\n",
      "epoch: 2 step: 1775, loss is 0.8909867405891418\n",
      "epoch: 2 step: 1776, loss is 0.652768611907959\n",
      "epoch: 2 step: 1777, loss is 0.6445971727371216\n",
      "epoch: 2 step: 1778, loss is 0.5788688063621521\n",
      "epoch: 2 step: 1779, loss is 0.6286191940307617\n",
      "epoch: 2 step: 1780, loss is 0.678477942943573\n",
      "epoch: 2 step: 1781, loss is 0.6344825029373169\n",
      "epoch: 2 step: 1782, loss is 0.5791000723838806\n",
      "epoch: 2 step: 1783, loss is 0.5177982449531555\n",
      "epoch: 2 step: 1784, loss is 0.45099562406539917\n",
      "epoch: 2 step: 1785, loss is 0.6101669073104858\n",
      "epoch: 2 step: 1786, loss is 0.6529226899147034\n",
      "epoch: 2 step: 1787, loss is 0.5087626576423645\n",
      "epoch: 2 step: 1788, loss is 0.394544780254364\n",
      "epoch: 2 step: 1789, loss is 0.6114248633384705\n",
      "epoch: 2 step: 1790, loss is 0.7021676898002625\n",
      "epoch: 2 step: 1791, loss is 0.6553042531013489\n",
      "epoch: 2 step: 1792, loss is 0.7079329490661621\n",
      "epoch: 2 step: 1793, loss is 0.4803886413574219\n",
      "epoch: 2 step: 1794, loss is 0.6703015565872192\n",
      "epoch: 2 step: 1795, loss is 0.7258671522140503\n",
      "epoch: 2 step: 1796, loss is 0.2665559649467468\n",
      "epoch: 2 step: 1797, loss is 0.6730116009712219\n",
      "epoch: 2 step: 1798, loss is 0.6003481149673462\n",
      "epoch: 2 step: 1799, loss is 0.5070193409919739\n",
      "epoch: 2 step: 1800, loss is 0.8325815796852112\n",
      "epoch: 2 step: 1801, loss is 0.6909539699554443\n",
      "epoch: 2 step: 1802, loss is 0.7062261700630188\n",
      "epoch: 2 step: 1803, loss is 0.48173514008522034\n",
      "epoch: 2 step: 1804, loss is 0.4381429851055145\n",
      "epoch: 2 step: 1805, loss is 0.3069714307785034\n",
      "epoch: 2 step: 1806, loss is 0.40652161836624146\n",
      "epoch: 2 step: 1807, loss is 0.516385018825531\n",
      "epoch: 2 step: 1808, loss is 0.6097396016120911\n",
      "epoch: 2 step: 1809, loss is 0.6033995151519775\n",
      "epoch: 2 step: 1810, loss is 0.4783823490142822\n",
      "epoch: 2 step: 1811, loss is 0.4159555733203888\n",
      "epoch: 2 step: 1812, loss is 0.9365628361701965\n",
      "epoch: 2 step: 1813, loss is 0.750384509563446\n",
      "epoch: 2 step: 1814, loss is 0.6564100980758667\n",
      "epoch: 2 step: 1815, loss is 1.025720477104187\n",
      "epoch: 2 step: 1816, loss is 0.4860323965549469\n",
      "epoch: 2 step: 1817, loss is 1.027193307876587\n",
      "epoch: 2 step: 1818, loss is 0.5056135058403015\n",
      "epoch: 2 step: 1819, loss is 0.6482921838760376\n",
      "epoch: 2 step: 1820, loss is 0.6688079237937927\n",
      "epoch: 2 step: 1821, loss is 0.4984305202960968\n",
      "epoch: 2 step: 1822, loss is 0.5808240175247192\n",
      "epoch: 2 step: 1823, loss is 0.6937257647514343\n",
      "epoch: 2 step: 1824, loss is 0.5335196256637573\n",
      "epoch: 2 step: 1825, loss is 0.6585388779640198\n",
      "epoch: 2 step: 1826, loss is 0.4089175760746002\n",
      "epoch: 2 step: 1827, loss is 0.682137668132782\n",
      "epoch: 2 step: 1828, loss is 0.5911468267440796\n",
      "epoch: 2 step: 1829, loss is 0.39990708231925964\n",
      "epoch: 2 step: 1830, loss is 0.7214036583900452\n",
      "epoch: 2 step: 1831, loss is 0.44988617300987244\n",
      "epoch: 2 step: 1832, loss is 0.5670396089553833\n",
      "epoch: 2 step: 1833, loss is 0.6100771427154541\n",
      "epoch: 2 step: 1834, loss is 0.8059614896774292\n",
      "epoch: 2 step: 1835, loss is 0.5368046760559082\n",
      "epoch: 2 step: 1836, loss is 0.5752725005149841\n",
      "epoch: 2 step: 1837, loss is 0.6919839382171631\n",
      "epoch: 2 step: 1838, loss is 0.3668639659881592\n",
      "epoch: 2 step: 1839, loss is 0.4697389304637909\n",
      "epoch: 2 step: 1840, loss is 0.7225751280784607\n",
      "epoch: 2 step: 1841, loss is 0.8923928737640381\n",
      "epoch: 2 step: 1842, loss is 0.3202188014984131\n",
      "epoch: 2 step: 1843, loss is 0.7098866701126099\n",
      "epoch: 2 step: 1844, loss is 0.8435016870498657\n",
      "epoch: 2 step: 1845, loss is 0.8492765426635742\n",
      "epoch: 2 step: 1846, loss is 0.6355015635490417\n",
      "epoch: 2 step: 1847, loss is 0.609573245048523\n",
      "epoch: 2 step: 1848, loss is 0.6700918674468994\n",
      "epoch: 2 step: 1849, loss is 0.7143803834915161\n",
      "epoch: 2 step: 1850, loss is 0.6587801575660706\n",
      "epoch: 2 step: 1851, loss is 0.6354707479476929\n",
      "epoch: 2 step: 1852, loss is 0.47643721103668213\n",
      "epoch: 2 step: 1853, loss is 0.6153518557548523\n",
      "epoch: 2 step: 1854, loss is 0.49597883224487305\n",
      "epoch: 2 step: 1855, loss is 0.6171779036521912\n",
      "epoch: 2 step: 1856, loss is 0.4335578382015228\n",
      "epoch: 2 step: 1857, loss is 0.5563309192657471\n",
      "epoch: 2 step: 1858, loss is 0.5547724962234497\n",
      "epoch: 2 step: 1859, loss is 0.4638822674751282\n",
      "epoch: 2 step: 1860, loss is 0.5741131901741028\n",
      "epoch: 2 step: 1861, loss is 0.5520802140235901\n",
      "epoch: 2 step: 1862, loss is 0.8142151236534119\n",
      "epoch: 2 step: 1863, loss is 0.6147463917732239\n",
      "epoch: 2 step: 1864, loss is 0.8606668710708618\n",
      "epoch: 2 step: 1865, loss is 0.4609261155128479\n",
      "epoch: 2 step: 1866, loss is 0.845725953578949\n",
      "epoch: 2 step: 1867, loss is 0.5070450901985168\n",
      "epoch: 2 step: 1868, loss is 0.40905845165252686\n",
      "epoch: 2 step: 1869, loss is 1.005218505859375\n",
      "epoch: 2 step: 1870, loss is 0.42043834924697876\n",
      "epoch: 2 step: 1871, loss is 0.508168637752533\n",
      "epoch: 2 step: 1872, loss is 1.0327447652816772\n",
      "epoch: 2 step: 1873, loss is 0.6512782573699951\n",
      "epoch: 2 step: 1874, loss is 0.5645773410797119\n",
      "epoch: 2 step: 1875, loss is 0.4962567985057831\n",
      "epoch: 2 step: 1876, loss is 0.642323911190033\n",
      "epoch: 2 step: 1877, loss is 0.5905167460441589\n",
      "epoch: 2 step: 1878, loss is 0.5993860363960266\n",
      "epoch: 2 step: 1879, loss is 0.7049341797828674\n",
      "epoch: 2 step: 1880, loss is 0.4814067780971527\n",
      "epoch: 2 step: 1881, loss is 0.5123218297958374\n",
      "epoch: 2 step: 1882, loss is 0.6491609811782837\n",
      "epoch: 2 step: 1883, loss is 0.4294046461582184\n",
      "epoch: 2 step: 1884, loss is 0.9250399470329285\n",
      "epoch: 2 step: 1885, loss is 0.7031645774841309\n",
      "epoch: 2 step: 1886, loss is 0.5558530688285828\n",
      "epoch: 2 step: 1887, loss is 0.644058346748352\n",
      "epoch: 2 step: 1888, loss is 0.7601057887077332\n",
      "epoch: 2 step: 1889, loss is 0.6996927857398987\n",
      "epoch: 2 step: 1890, loss is 0.5939618945121765\n",
      "epoch: 2 step: 1891, loss is 0.6705994009971619\n",
      "epoch: 2 step: 1892, loss is 0.5757959485054016\n",
      "epoch: 2 step: 1893, loss is 0.66408371925354\n",
      "epoch: 2 step: 1894, loss is 0.586173415184021\n",
      "epoch: 2 step: 1895, loss is 0.6669490337371826\n",
      "epoch: 2 step: 1896, loss is 0.5083576440811157\n",
      "epoch: 2 step: 1897, loss is 0.4334503710269928\n",
      "epoch: 2 step: 1898, loss is 0.754426121711731\n",
      "epoch: 2 step: 1899, loss is 0.5119028091430664\n",
      "epoch: 2 step: 1900, loss is 0.8800130486488342\n",
      "epoch: 2 step: 1901, loss is 0.6551362872123718\n",
      "epoch: 2 step: 1902, loss is 0.48695871233940125\n",
      "epoch: 2 step: 1903, loss is 0.7381083965301514\n",
      "epoch: 2 step: 1904, loss is 0.47884824872016907\n",
      "epoch: 2 step: 1905, loss is 0.5622414350509644\n",
      "epoch: 2 step: 1906, loss is 0.7492419481277466\n",
      "epoch: 2 step: 1907, loss is 0.6924436092376709\n",
      "epoch: 2 step: 1908, loss is 0.6748263835906982\n",
      "epoch: 2 step: 1909, loss is 0.7231243848800659\n",
      "epoch: 2 step: 1910, loss is 0.35504981875419617\n",
      "epoch: 2 step: 1911, loss is 0.7065541744232178\n",
      "epoch: 2 step: 1912, loss is 0.6856601238250732\n",
      "epoch: 2 step: 1913, loss is 0.5340430736541748\n",
      "epoch: 2 step: 1914, loss is 0.6699479818344116\n",
      "epoch: 2 step: 1915, loss is 0.5387522578239441\n",
      "epoch: 2 step: 1916, loss is 0.5640684962272644\n",
      "epoch: 2 step: 1917, loss is 0.6495791673660278\n",
      "epoch: 2 step: 1918, loss is 0.6178202033042908\n",
      "epoch: 2 step: 1919, loss is 0.6973150372505188\n",
      "epoch: 2 step: 1920, loss is 0.7560397386550903\n",
      "epoch: 2 step: 1921, loss is 0.6609929203987122\n",
      "epoch: 2 step: 1922, loss is 0.3983948230743408\n",
      "epoch: 2 step: 1923, loss is 0.5621207356452942\n",
      "epoch: 2 step: 1924, loss is 0.6105732917785645\n",
      "epoch: 2 step: 1925, loss is 0.46475017070770264\n",
      "epoch: 2 step: 1926, loss is 0.6471331119537354\n",
      "epoch: 2 step: 1927, loss is 1.0372827053070068\n",
      "epoch: 2 step: 1928, loss is 0.4921896457672119\n",
      "epoch: 2 step: 1929, loss is 0.6564282178878784\n",
      "epoch: 2 step: 1930, loss is 0.5837774276733398\n",
      "epoch: 2 step: 1931, loss is 0.48939207196235657\n",
      "epoch: 2 step: 1932, loss is 0.7262911200523376\n",
      "epoch: 2 step: 1933, loss is 0.5410054326057434\n",
      "epoch: 2 step: 1934, loss is 0.6113404035568237\n",
      "epoch: 2 step: 1935, loss is 0.6718248128890991\n",
      "epoch: 2 step: 1936, loss is 0.5804452300071716\n",
      "epoch: 2 step: 1937, loss is 0.7509466409683228\n",
      "epoch: 2 step: 1938, loss is 0.5822197794914246\n",
      "epoch: 2 step: 1939, loss is 0.7410261034965515\n",
      "epoch: 2 step: 1940, loss is 0.66774982213974\n",
      "epoch: 2 step: 1941, loss is 0.5197727084159851\n",
      "epoch: 2 step: 1942, loss is 0.6600598096847534\n",
      "epoch: 2 step: 1943, loss is 0.5560179948806763\n",
      "epoch: 2 step: 1944, loss is 0.5729547739028931\n",
      "epoch: 2 step: 1945, loss is 0.5216865539550781\n",
      "epoch: 2 step: 1946, loss is 0.4206734597682953\n",
      "epoch: 2 step: 1947, loss is 0.5952713489532471\n",
      "epoch: 2 step: 1948, loss is 0.4734012186527252\n",
      "epoch: 2 step: 1949, loss is 0.5900017023086548\n",
      "epoch: 2 step: 1950, loss is 0.8793087601661682\n",
      "epoch: 2 step: 1951, loss is 0.6268599033355713\n",
      "epoch: 2 step: 1952, loss is 0.6438740491867065\n",
      "epoch: 2 step: 1953, loss is 0.7977504134178162\n",
      "epoch: 2 step: 1954, loss is 0.6610605716705322\n",
      "epoch: 2 step: 1955, loss is 0.4578734338283539\n",
      "epoch: 2 step: 1956, loss is 0.6318487524986267\n",
      "epoch: 2 step: 1957, loss is 0.5196484327316284\n",
      "epoch: 2 step: 1958, loss is 0.5825851559638977\n",
      "epoch: 2 step: 1959, loss is 0.38697323203086853\n",
      "epoch: 2 step: 1960, loss is 0.48546382784843445\n",
      "epoch: 2 step: 1961, loss is 0.5003954172134399\n",
      "epoch: 2 step: 1962, loss is 0.5666646957397461\n",
      "epoch: 2 step: 1963, loss is 0.686145007610321\n",
      "epoch: 2 step: 1964, loss is 0.5574387311935425\n",
      "epoch: 2 step: 1965, loss is 0.7370123863220215\n",
      "epoch: 2 step: 1966, loss is 0.441270649433136\n",
      "epoch: 2 step: 1967, loss is 0.5547400116920471\n",
      "epoch: 2 step: 1968, loss is 0.43570733070373535\n",
      "epoch: 2 step: 1969, loss is 0.48281803727149963\n",
      "epoch: 2 step: 1970, loss is 0.4517335891723633\n",
      "epoch: 2 step: 1971, loss is 0.4778178036212921\n",
      "epoch: 2 step: 1972, loss is 0.6101654171943665\n",
      "epoch: 2 step: 1973, loss is 0.4577731490135193\n",
      "epoch: 2 step: 1974, loss is 0.3776394724845886\n",
      "epoch: 2 step: 1975, loss is 0.5694469213485718\n",
      "epoch: 2 step: 1976, loss is 0.9547326564788818\n",
      "epoch: 2 step: 1977, loss is 1.537509560585022\n",
      "epoch: 2 step: 1978, loss is 0.42752695083618164\n",
      "epoch: 2 step: 1979, loss is 0.4522947371006012\n",
      "epoch: 2 step: 1980, loss is 0.8069350123405457\n",
      "epoch: 2 step: 1981, loss is 0.6299164295196533\n",
      "epoch: 2 step: 1982, loss is 0.3673720061779022\n",
      "epoch: 2 step: 1983, loss is 0.6218756437301636\n",
      "epoch: 2 step: 1984, loss is 0.7804752588272095\n",
      "epoch: 2 step: 1985, loss is 0.9944261908531189\n",
      "epoch: 2 step: 1986, loss is 0.7447420358657837\n",
      "epoch: 2 step: 1987, loss is 0.6211145520210266\n",
      "epoch: 2 step: 1988, loss is 0.7086052298545837\n",
      "epoch: 2 step: 1989, loss is 0.6184532046318054\n",
      "epoch: 2 step: 1990, loss is 0.5050551891326904\n",
      "epoch: 2 step: 1991, loss is 0.7477844953536987\n",
      "epoch: 2 step: 1992, loss is 0.836273193359375\n",
      "epoch: 2 step: 1993, loss is 0.5531845688819885\n",
      "epoch: 2 step: 1994, loss is 0.8409250974655151\n",
      "epoch: 2 step: 1995, loss is 0.5277875065803528\n",
      "epoch: 2 step: 1996, loss is 0.5379495024681091\n",
      "epoch: 2 step: 1997, loss is 0.5042393207550049\n",
      "epoch: 2 step: 1998, loss is 0.5269619822502136\n",
      "epoch: 2 step: 1999, loss is 0.5585346817970276\n",
      "epoch: 2 step: 2000, loss is 0.7583976984024048\n",
      "epoch: 2 step: 2001, loss is 0.6524509191513062\n",
      "epoch: 2 step: 2002, loss is 0.5016754865646362\n",
      "epoch: 2 step: 2003, loss is 0.43569138646125793\n",
      "epoch: 2 step: 2004, loss is 0.7941888570785522\n",
      "epoch: 2 step: 2005, loss is 0.5043720602989197\n",
      "epoch: 2 step: 2006, loss is 0.5726622939109802\n",
      "epoch: 2 step: 2007, loss is 0.6523467898368835\n",
      "epoch: 2 step: 2008, loss is 0.4973810911178589\n",
      "epoch: 2 step: 2009, loss is 0.5611647963523865\n",
      "epoch: 2 step: 2010, loss is 0.37870800495147705\n",
      "epoch: 2 step: 2011, loss is 0.4818763732910156\n",
      "epoch: 2 step: 2012, loss is 0.7702881693840027\n",
      "epoch: 2 step: 2013, loss is 0.5814893245697021\n",
      "epoch: 2 step: 2014, loss is 0.4834537208080292\n",
      "epoch: 2 step: 2015, loss is 0.8380452394485474\n",
      "epoch: 2 step: 2016, loss is 0.46344247460365295\n",
      "epoch: 2 step: 2017, loss is 0.5533189177513123\n",
      "epoch: 2 step: 2018, loss is 0.36372917890548706\n",
      "epoch: 2 step: 2019, loss is 0.7025987505912781\n",
      "epoch: 2 step: 2020, loss is 0.7660071849822998\n",
      "epoch: 2 step: 2021, loss is 0.3497233986854553\n",
      "epoch: 2 step: 2022, loss is 0.9885210990905762\n",
      "epoch: 2 step: 2023, loss is 0.3892883360385895\n",
      "epoch: 2 step: 2024, loss is 0.6032972931861877\n",
      "epoch: 2 step: 2025, loss is 0.5370096564292908\n",
      "epoch: 2 step: 2026, loss is 0.3561617136001587\n",
      "epoch: 2 step: 2027, loss is 0.306789755821228\n",
      "epoch: 2 step: 2028, loss is 0.4041167199611664\n",
      "epoch: 2 step: 2029, loss is 0.6114814877510071\n",
      "epoch: 2 step: 2030, loss is 0.8277337551116943\n",
      "epoch: 2 step: 2031, loss is 0.5202021598815918\n",
      "epoch: 2 step: 2032, loss is 0.5521540641784668\n",
      "epoch: 2 step: 2033, loss is 0.6035100221633911\n",
      "epoch: 2 step: 2034, loss is 0.31502005457878113\n",
      "epoch: 2 step: 2035, loss is 1.4362679719924927\n",
      "epoch: 2 step: 2036, loss is 0.9346409440040588\n",
      "epoch: 2 step: 2037, loss is 1.0909762382507324\n",
      "epoch: 2 step: 2038, loss is 0.7558553218841553\n",
      "epoch: 2 step: 2039, loss is 0.6554321646690369\n",
      "epoch: 2 step: 2040, loss is 0.5994799733161926\n",
      "epoch: 2 step: 2041, loss is 0.4761234521865845\n",
      "epoch: 2 step: 2042, loss is 0.742969274520874\n",
      "epoch: 2 step: 2043, loss is 0.4325611889362335\n",
      "epoch: 2 step: 2044, loss is 0.7958349585533142\n",
      "epoch: 2 step: 2045, loss is 1.1438579559326172\n",
      "epoch: 2 step: 2046, loss is 0.6660743355751038\n",
      "epoch: 2 step: 2047, loss is 0.8237937688827515\n",
      "epoch: 2 step: 2048, loss is 0.6671268939971924\n",
      "epoch: 2 step: 2049, loss is 0.6959202885627747\n",
      "epoch: 2 step: 2050, loss is 0.8129493594169617\n",
      "epoch: 2 step: 2051, loss is 0.694780707359314\n",
      "epoch: 2 step: 2052, loss is 0.7334697246551514\n",
      "epoch: 2 step: 2053, loss is 0.7421183586120605\n",
      "epoch: 2 step: 2054, loss is 0.6717160940170288\n",
      "epoch: 2 step: 2055, loss is 0.46269676089286804\n",
      "epoch: 2 step: 2056, loss is 0.7504377365112305\n",
      "epoch: 2 step: 2057, loss is 0.6626030206680298\n",
      "epoch: 2 step: 2058, loss is 0.5396891236305237\n",
      "epoch: 2 step: 2059, loss is 0.4774058759212494\n",
      "epoch: 2 step: 2060, loss is 0.44512733817100525\n",
      "epoch: 2 step: 2061, loss is 0.5916944146156311\n",
      "epoch: 2 step: 2062, loss is 0.5291305184364319\n",
      "epoch: 2 step: 2063, loss is 0.7313587069511414\n",
      "epoch: 2 step: 2064, loss is 0.49770376086235046\n",
      "epoch: 2 step: 2065, loss is 0.6500148773193359\n",
      "epoch: 2 step: 2066, loss is 0.4110589027404785\n",
      "epoch: 2 step: 2067, loss is 0.9470567107200623\n",
      "epoch: 2 step: 2068, loss is 0.5023187398910522\n",
      "epoch: 2 step: 2069, loss is 0.24749651551246643\n",
      "epoch: 2 step: 2070, loss is 0.4972986578941345\n",
      "epoch: 2 step: 2071, loss is 0.4535529315471649\n",
      "epoch: 2 step: 2072, loss is 0.679835319519043\n",
      "epoch: 2 step: 2073, loss is 0.8343288898468018\n",
      "epoch: 2 step: 2074, loss is 0.972403883934021\n",
      "epoch: 2 step: 2075, loss is 0.5271016955375671\n",
      "epoch: 2 step: 2076, loss is 1.0960897207260132\n",
      "epoch: 2 step: 2077, loss is 0.6699878573417664\n",
      "epoch: 2 step: 2078, loss is 0.591850757598877\n",
      "epoch: 2 step: 2079, loss is 0.6099238395690918\n",
      "epoch: 2 step: 2080, loss is 0.5280036330223083\n",
      "epoch: 2 step: 2081, loss is 0.5315092206001282\n",
      "epoch: 2 step: 2082, loss is 0.6857489347457886\n",
      "epoch: 2 step: 2083, loss is 0.8654739856719971\n",
      "epoch: 2 step: 2084, loss is 0.8395397067070007\n",
      "epoch: 2 step: 2085, loss is 0.4147332012653351\n",
      "epoch: 2 step: 2086, loss is 0.6585685014724731\n",
      "epoch: 2 step: 2087, loss is 0.5152938961982727\n",
      "epoch: 2 step: 2088, loss is 0.46894577145576477\n",
      "epoch: 2 step: 2089, loss is 0.4210541844367981\n",
      "epoch: 2 step: 2090, loss is 0.570927619934082\n",
      "epoch: 2 step: 2091, loss is 0.650671124458313\n",
      "epoch: 2 step: 2092, loss is 0.801764726638794\n",
      "epoch: 2 step: 2093, loss is 0.5198904871940613\n",
      "epoch: 2 step: 2094, loss is 0.7906956076622009\n",
      "epoch: 2 step: 2095, loss is 0.7470778822898865\n",
      "epoch: 2 step: 2096, loss is 0.6010299921035767\n",
      "epoch: 2 step: 2097, loss is 0.88673335313797\n",
      "epoch: 2 step: 2098, loss is 0.4398859739303589\n",
      "epoch: 2 step: 2099, loss is 0.9306593537330627\n",
      "epoch: 2 step: 2100, loss is 0.8500829339027405\n",
      "epoch: 2 step: 2101, loss is 0.5404208898544312\n",
      "epoch: 2 step: 2102, loss is 0.6590471863746643\n",
      "epoch: 2 step: 2103, loss is 0.7405411005020142\n",
      "epoch: 2 step: 2104, loss is 0.6689585447311401\n",
      "epoch: 2 step: 2105, loss is 0.5115030407905579\n",
      "epoch: 2 step: 2106, loss is 0.5186654329299927\n",
      "epoch: 2 step: 2107, loss is 0.6347273588180542\n",
      "epoch: 2 step: 2108, loss is 0.7341462969779968\n",
      "epoch: 2 step: 2109, loss is 0.731482744216919\n",
      "epoch: 2 step: 2110, loss is 0.5315715670585632\n",
      "epoch: 2 step: 2111, loss is 0.4968112111091614\n",
      "epoch: 2 step: 2112, loss is 0.8779983520507812\n",
      "epoch: 2 step: 2113, loss is 0.8093787431716919\n",
      "epoch: 2 step: 2114, loss is 0.7597795724868774\n",
      "epoch: 2 step: 2115, loss is 0.6935986876487732\n",
      "epoch: 2 step: 2116, loss is 0.7030503749847412\n",
      "epoch: 2 step: 2117, loss is 0.49756816029548645\n",
      "epoch: 2 step: 2118, loss is 0.4957108795642853\n",
      "epoch: 2 step: 2119, loss is 0.5014246106147766\n",
      "epoch: 2 step: 2120, loss is 0.5309768319129944\n",
      "epoch: 2 step: 2121, loss is 0.7257236838340759\n",
      "epoch: 2 step: 2122, loss is 0.61026531457901\n",
      "epoch: 2 step: 2123, loss is 0.6222226023674011\n",
      "epoch: 2 step: 2124, loss is 0.9717339277267456\n",
      "epoch: 2 step: 2125, loss is 0.5756280422210693\n",
      "epoch: 2 step: 2126, loss is 0.510805606842041\n",
      "epoch: 2 step: 2127, loss is 0.5455995798110962\n",
      "epoch: 2 step: 2128, loss is 0.5402640104293823\n",
      "epoch: 2 step: 2129, loss is 0.4506039619445801\n",
      "epoch: 2 step: 2130, loss is 0.91354900598526\n",
      "epoch: 2 step: 2131, loss is 0.699925422668457\n",
      "epoch: 2 step: 2132, loss is 0.9512748718261719\n",
      "epoch: 2 step: 2133, loss is 0.5828731060028076\n",
      "epoch: 2 step: 2134, loss is 0.5679527521133423\n",
      "epoch: 2 step: 2135, loss is 0.5541847944259644\n",
      "epoch: 2 step: 2136, loss is 0.6541677713394165\n",
      "epoch: 2 step: 2137, loss is 0.5429746508598328\n",
      "epoch: 2 step: 2138, loss is 0.9642472863197327\n",
      "epoch: 2 step: 2139, loss is 0.6203486919403076\n",
      "epoch: 2 step: 2140, loss is 0.5009399652481079\n",
      "epoch: 2 step: 2141, loss is 0.6694170236587524\n",
      "epoch: 2 step: 2142, loss is 0.7108814716339111\n",
      "epoch: 2 step: 2143, loss is 0.599973201751709\n",
      "epoch: 2 step: 2144, loss is 0.45684921741485596\n",
      "epoch: 2 step: 2145, loss is 0.44510623812675476\n",
      "epoch: 2 step: 2146, loss is 0.7857891321182251\n",
      "epoch: 2 step: 2147, loss is 0.6171898245811462\n",
      "epoch: 2 step: 2148, loss is 0.6446767449378967\n",
      "epoch: 2 step: 2149, loss is 0.42415469884872437\n",
      "epoch: 2 step: 2150, loss is 0.5387120842933655\n",
      "epoch: 2 step: 2151, loss is 0.6403899192810059\n",
      "epoch: 2 step: 2152, loss is 0.6478680372238159\n",
      "epoch: 2 step: 2153, loss is 0.613398015499115\n",
      "epoch: 2 step: 2154, loss is 0.7032379508018494\n",
      "epoch: 2 step: 2155, loss is 0.6628260612487793\n",
      "epoch: 2 step: 2156, loss is 0.6869061589241028\n",
      "epoch: 2 step: 2157, loss is 0.5356835722923279\n",
      "epoch: 2 step: 2158, loss is 0.4495197832584381\n",
      "epoch: 2 step: 2159, loss is 0.6390553116798401\n",
      "epoch: 2 step: 2160, loss is 0.40493884682655334\n",
      "epoch: 2 step: 2161, loss is 0.5851542353630066\n",
      "epoch: 2 step: 2162, loss is 0.43552055954933167\n",
      "epoch: 2 step: 2163, loss is 0.6488089561462402\n",
      "epoch: 2 step: 2164, loss is 0.9206970930099487\n",
      "epoch: 2 step: 2165, loss is 0.6433748602867126\n",
      "epoch: 2 step: 2166, loss is 0.7965163588523865\n",
      "epoch: 2 step: 2167, loss is 0.5019878149032593\n",
      "epoch: 2 step: 2168, loss is 0.7464190721511841\n",
      "epoch: 2 step: 2169, loss is 0.831575334072113\n",
      "epoch: 2 step: 2170, loss is 0.410491406917572\n",
      "epoch: 2 step: 2171, loss is 0.5010546445846558\n",
      "epoch: 2 step: 2172, loss is 0.6693896651268005\n",
      "epoch: 2 step: 2173, loss is 0.5469054579734802\n",
      "epoch: 2 step: 2174, loss is 0.7762051820755005\n",
      "epoch: 2 step: 2175, loss is 0.5472117066383362\n",
      "epoch: 2 step: 2176, loss is 0.47419485449790955\n",
      "epoch: 2 step: 2177, loss is 1.008906602859497\n",
      "epoch: 2 step: 2178, loss is 0.5704913139343262\n",
      "epoch: 2 step: 2179, loss is 0.598872184753418\n",
      "epoch: 2 step: 2180, loss is 0.8670451641082764\n",
      "epoch: 2 step: 2181, loss is 0.5864644050598145\n",
      "epoch: 2 step: 2182, loss is 0.7330121994018555\n",
      "epoch: 2 step: 2183, loss is 0.9260438084602356\n",
      "epoch: 2 step: 2184, loss is 0.5726043581962585\n",
      "epoch: 2 step: 2185, loss is 0.7533894777297974\n",
      "epoch: 2 step: 2186, loss is 0.33828720450401306\n",
      "epoch: 2 step: 2187, loss is 0.6584036350250244\n",
      "epoch: 2 step: 2188, loss is 0.91490238904953\n",
      "epoch: 2 step: 2189, loss is 0.5581973791122437\n",
      "epoch: 2 step: 2190, loss is 0.7746347784996033\n",
      "epoch: 2 step: 2191, loss is 0.5010722875595093\n",
      "epoch: 2 step: 2192, loss is 0.7353889346122742\n",
      "epoch: 2 step: 2193, loss is 0.6567144989967346\n",
      "epoch: 2 step: 2194, loss is 0.4061588943004608\n",
      "epoch: 2 step: 2195, loss is 0.7744820713996887\n",
      "epoch: 2 step: 2196, loss is 0.5986137390136719\n",
      "epoch: 2 step: 2197, loss is 0.5695257186889648\n",
      "epoch: 2 step: 2198, loss is 0.6189361214637756\n",
      "epoch: 2 step: 2199, loss is 0.5627121329307556\n",
      "epoch: 2 step: 2200, loss is 0.5396890044212341\n",
      "epoch: 2 step: 2201, loss is 0.46172279119491577\n",
      "epoch: 2 step: 2202, loss is 0.8038496375083923\n",
      "epoch: 2 step: 2203, loss is 0.5960721373558044\n",
      "epoch: 2 step: 2204, loss is 0.41648948192596436\n",
      "epoch: 2 step: 2205, loss is 0.618955671787262\n",
      "epoch: 2 step: 2206, loss is 0.6225938200950623\n",
      "epoch: 2 step: 2207, loss is 0.45534947514533997\n",
      "epoch: 2 step: 2208, loss is 0.5631703734397888\n",
      "epoch: 2 step: 2209, loss is 0.6061798930168152\n",
      "epoch: 2 step: 2210, loss is 0.48194676637649536\n",
      "epoch: 2 step: 2211, loss is 0.5649400949478149\n",
      "epoch: 2 step: 2212, loss is 0.6516342163085938\n",
      "epoch: 2 step: 2213, loss is 0.3166922926902771\n",
      "epoch: 2 step: 2214, loss is 0.7325406670570374\n",
      "epoch: 2 step: 2215, loss is 0.64037024974823\n",
      "epoch: 2 step: 2216, loss is 0.5969583988189697\n",
      "epoch: 2 step: 2217, loss is 0.6514391899108887\n",
      "epoch: 2 step: 2218, loss is 0.548533022403717\n",
      "epoch: 2 step: 2219, loss is 0.6287641525268555\n",
      "epoch: 2 step: 2220, loss is 0.6494371891021729\n",
      "epoch: 2 step: 2221, loss is 0.47248104214668274\n",
      "epoch: 2 step: 2222, loss is 0.42343661189079285\n",
      "epoch: 2 step: 2223, loss is 0.5773125886917114\n",
      "epoch: 2 step: 2224, loss is 0.7100992202758789\n",
      "epoch: 2 step: 2225, loss is 0.5977597236633301\n",
      "epoch: 2 step: 2226, loss is 0.5953816771507263\n",
      "epoch: 2 step: 2227, loss is 0.8261603116989136\n",
      "epoch: 2 step: 2228, loss is 0.6441410779953003\n",
      "epoch: 2 step: 2229, loss is 0.5547972917556763\n",
      "epoch: 2 step: 2230, loss is 0.4628123641014099\n",
      "epoch: 2 step: 2231, loss is 0.5891828536987305\n",
      "epoch: 2 step: 2232, loss is 0.9692604541778564\n",
      "epoch: 2 step: 2233, loss is 0.46584993600845337\n",
      "epoch: 2 step: 2234, loss is 0.3180710971355438\n",
      "epoch: 2 step: 2235, loss is 0.5189131498336792\n",
      "epoch: 2 step: 2236, loss is 0.3802478313446045\n",
      "epoch: 2 step: 2237, loss is 0.5920074582099915\n",
      "epoch: 2 step: 2238, loss is 0.7585235238075256\n",
      "epoch: 2 step: 2239, loss is 0.6796666979789734\n",
      "epoch: 2 step: 2240, loss is 0.41539859771728516\n",
      "epoch: 2 step: 2241, loss is 0.5924229621887207\n",
      "epoch: 2 step: 2242, loss is 0.3641798496246338\n",
      "epoch: 2 step: 2243, loss is 0.6919379830360413\n",
      "epoch: 2 step: 2244, loss is 0.6615585684776306\n",
      "epoch: 2 step: 2245, loss is 0.4498927593231201\n",
      "epoch: 2 step: 2246, loss is 0.8752047419548035\n",
      "epoch: 2 step: 2247, loss is 0.49849432706832886\n",
      "epoch: 2 step: 2248, loss is 0.5069196820259094\n",
      "epoch: 2 step: 2249, loss is 0.4004279375076294\n",
      "epoch: 2 step: 2250, loss is 0.33558547496795654\n",
      "epoch: 2 step: 2251, loss is 0.6560017466545105\n",
      "epoch: 2 step: 2252, loss is 0.7484698295593262\n",
      "epoch: 2 step: 2253, loss is 0.5586748123168945\n",
      "epoch: 2 step: 2254, loss is 0.5022555589675903\n",
      "epoch: 2 step: 2255, loss is 0.6403794884681702\n",
      "epoch: 2 step: 2256, loss is 0.7165122628211975\n",
      "epoch: 2 step: 2257, loss is 0.4151061773300171\n",
      "epoch: 2 step: 2258, loss is 0.8219460248947144\n",
      "epoch: 2 step: 2259, loss is 0.7781556248664856\n",
      "epoch: 2 step: 2260, loss is 0.6171565055847168\n",
      "epoch: 2 step: 2261, loss is 0.6421297788619995\n",
      "epoch: 2 step: 2262, loss is 0.6324070692062378\n",
      "epoch: 2 step: 2263, loss is 0.30410638451576233\n",
      "epoch: 2 step: 2264, loss is 0.6718406677246094\n",
      "epoch: 2 step: 2265, loss is 0.8283448219299316\n",
      "epoch: 2 step: 2266, loss is 0.604210376739502\n",
      "epoch: 2 step: 2267, loss is 0.47658902406692505\n",
      "epoch: 2 step: 2268, loss is 0.5817062854766846\n",
      "epoch: 2 step: 2269, loss is 0.4388609230518341\n",
      "epoch: 2 step: 2270, loss is 0.6623985171318054\n",
      "epoch: 2 step: 2271, loss is 0.6099463105201721\n",
      "epoch: 2 step: 2272, loss is 0.5127533674240112\n",
      "epoch: 2 step: 2273, loss is 0.4904662072658539\n",
      "epoch: 2 step: 2274, loss is 0.4991287291049957\n",
      "epoch: 2 step: 2275, loss is 0.6821319460868835\n",
      "epoch: 2 step: 2276, loss is 0.6601174473762512\n",
      "epoch: 2 step: 2277, loss is 0.39575374126434326\n",
      "epoch: 2 step: 2278, loss is 0.7519124150276184\n",
      "epoch: 2 step: 2279, loss is 0.7737817764282227\n",
      "epoch: 2 step: 2280, loss is 0.5032328367233276\n",
      "epoch: 2 step: 2281, loss is 0.7970414161682129\n",
      "epoch: 2 step: 2282, loss is 0.38071659207344055\n",
      "epoch: 2 step: 2283, loss is 0.5145597457885742\n",
      "epoch: 2 step: 2284, loss is 0.590442955493927\n",
      "epoch: 2 step: 2285, loss is 0.4379952549934387\n",
      "epoch: 2 step: 2286, loss is 0.7147424817085266\n",
      "epoch: 2 step: 2287, loss is 0.6617185473442078\n",
      "epoch: 2 step: 2288, loss is 0.4719696342945099\n",
      "epoch: 2 step: 2289, loss is 0.37483668327331543\n",
      "epoch: 2 step: 2290, loss is 0.6165008544921875\n",
      "epoch: 2 step: 2291, loss is 0.4374672472476959\n",
      "epoch: 2 step: 2292, loss is 0.39368414878845215\n",
      "epoch: 2 step: 2293, loss is 0.5937197208404541\n",
      "epoch: 2 step: 2294, loss is 0.7248468995094299\n",
      "epoch: 2 step: 2295, loss is 0.2576952278614044\n",
      "epoch: 2 step: 2296, loss is 0.3611094653606415\n",
      "epoch: 2 step: 2297, loss is 0.6525054574012756\n",
      "epoch: 2 step: 2298, loss is 0.9616313576698303\n",
      "epoch: 2 step: 2299, loss is 0.9819971919059753\n",
      "epoch: 2 step: 2300, loss is 0.6006625294685364\n",
      "epoch: 2 step: 2301, loss is 0.7261918187141418\n",
      "epoch: 2 step: 2302, loss is 0.4919539988040924\n",
      "epoch: 2 step: 2303, loss is 0.5524922013282776\n",
      "epoch: 2 step: 2304, loss is 0.5357934832572937\n",
      "epoch: 2 step: 2305, loss is 0.6847579479217529\n",
      "epoch: 2 step: 2306, loss is 0.5161125063896179\n",
      "epoch: 2 step: 2307, loss is 0.5685256719589233\n",
      "epoch: 2 step: 2308, loss is 0.5014587044715881\n",
      "epoch: 2 step: 2309, loss is 0.42562803626060486\n",
      "epoch: 2 step: 2310, loss is 0.5452443361282349\n",
      "epoch: 2 step: 2311, loss is 0.4915856420993805\n",
      "epoch: 2 step: 2312, loss is 0.6661837697029114\n",
      "epoch: 2 step: 2313, loss is 0.8053431510925293\n",
      "epoch: 2 step: 2314, loss is 0.48466750979423523\n",
      "epoch: 2 step: 2315, loss is 0.4655870199203491\n",
      "epoch: 2 step: 2316, loss is 0.6791197061538696\n",
      "epoch: 2 step: 2317, loss is 1.0934805870056152\n",
      "epoch: 2 step: 2318, loss is 0.5845094323158264\n",
      "epoch: 2 step: 2319, loss is 0.4858284890651703\n",
      "epoch: 2 step: 2320, loss is 0.61531001329422\n",
      "epoch: 2 step: 2321, loss is 0.6867886781692505\n",
      "epoch: 2 step: 2322, loss is 0.5669070482254028\n",
      "epoch: 2 step: 2323, loss is 0.6559068560600281\n",
      "epoch: 2 step: 2324, loss is 0.5305688977241516\n",
      "epoch: 2 step: 2325, loss is 0.5566375255584717\n",
      "epoch: 2 step: 2326, loss is 0.9839414954185486\n",
      "epoch: 2 step: 2327, loss is 0.5621499419212341\n",
      "epoch: 2 step: 2328, loss is 0.43899571895599365\n",
      "epoch: 2 step: 2329, loss is 0.5575899481773376\n",
      "epoch: 2 step: 2330, loss is 0.5117884278297424\n",
      "epoch: 2 step: 2331, loss is 0.5880047678947449\n",
      "epoch: 2 step: 2332, loss is 0.5496678352355957\n",
      "epoch: 2 step: 2333, loss is 0.882540762424469\n",
      "epoch: 2 step: 2334, loss is 0.8246476650238037\n",
      "epoch: 2 step: 2335, loss is 0.6115517616271973\n",
      "epoch: 2 step: 2336, loss is 0.8507981300354004\n",
      "epoch: 2 step: 2337, loss is 0.5788320899009705\n",
      "epoch: 2 step: 2338, loss is 0.5752949714660645\n",
      "epoch: 2 step: 2339, loss is 0.5856826901435852\n",
      "epoch: 2 step: 2340, loss is 0.5447906851768494\n",
      "epoch: 2 step: 2341, loss is 0.4816090166568756\n",
      "epoch: 2 step: 2342, loss is 0.5680604577064514\n",
      "epoch: 2 step: 2343, loss is 0.6335394382476807\n",
      "epoch: 2 step: 2344, loss is 0.491999089717865\n",
      "epoch: 2 step: 2345, loss is 0.8729625940322876\n",
      "epoch: 2 step: 2346, loss is 0.5074172019958496\n",
      "epoch: 2 step: 2347, loss is 0.8830114603042603\n",
      "epoch: 2 step: 2348, loss is 0.6138808727264404\n",
      "epoch: 2 step: 2349, loss is 0.5463219881057739\n",
      "epoch: 2 step: 2350, loss is 0.5836818814277649\n",
      "epoch: 2 step: 2351, loss is 0.7254188656806946\n",
      "epoch: 2 step: 2352, loss is 0.9440476894378662\n",
      "epoch: 2 step: 2353, loss is 0.7081417441368103\n",
      "epoch: 2 step: 2354, loss is 0.45317938923835754\n",
      "epoch: 2 step: 2355, loss is 0.5045154690742493\n",
      "epoch: 2 step: 2356, loss is 0.7126283645629883\n",
      "epoch: 2 step: 2357, loss is 0.6315858960151672\n",
      "epoch: 2 step: 2358, loss is 0.354589581489563\n",
      "epoch: 2 step: 2359, loss is 0.3266540467739105\n",
      "epoch: 2 step: 2360, loss is 0.7039577960968018\n",
      "epoch: 2 step: 2361, loss is 0.4923456311225891\n",
      "epoch: 2 step: 2362, loss is 0.7031012773513794\n",
      "epoch: 2 step: 2363, loss is 0.5301343202590942\n",
      "epoch: 2 step: 2364, loss is 0.3214908242225647\n",
      "epoch: 2 step: 2365, loss is 0.5373627543449402\n",
      "epoch: 2 step: 2366, loss is 0.7070508003234863\n",
      "epoch: 2 step: 2367, loss is 0.578433096408844\n",
      "epoch: 2 step: 2368, loss is 0.6541648507118225\n",
      "epoch: 2 step: 2369, loss is 0.6801889538764954\n",
      "epoch: 2 step: 2370, loss is 0.5483230352401733\n",
      "epoch: 2 step: 2371, loss is 0.4314174950122833\n",
      "epoch: 2 step: 2372, loss is 0.4714707136154175\n",
      "epoch: 2 step: 2373, loss is 0.6562138795852661\n",
      "epoch: 2 step: 2374, loss is 0.6978217959403992\n",
      "epoch: 2 step: 2375, loss is 0.7725127339363098\n",
      "epoch: 2 step: 2376, loss is 0.7020795345306396\n",
      "epoch: 2 step: 2377, loss is 0.7560081481933594\n",
      "epoch: 2 step: 2378, loss is 0.5161614418029785\n",
      "epoch: 2 step: 2379, loss is 0.770320475101471\n",
      "epoch: 2 step: 2380, loss is 0.5661115646362305\n",
      "epoch: 2 step: 2381, loss is 0.990820050239563\n",
      "epoch: 2 step: 2382, loss is 0.7738332748413086\n",
      "epoch: 2 step: 2383, loss is 0.5979889631271362\n",
      "epoch: 2 step: 2384, loss is 0.4421452581882477\n",
      "epoch: 2 step: 2385, loss is 0.5069632530212402\n",
      "epoch: 2 step: 2386, loss is 0.5741276741027832\n",
      "epoch: 2 step: 2387, loss is 0.6463469862937927\n",
      "epoch: 2 step: 2388, loss is 0.46428173780441284\n",
      "epoch: 2 step: 2389, loss is 0.6634724140167236\n",
      "epoch: 2 step: 2390, loss is 0.48729121685028076\n",
      "epoch: 2 step: 2391, loss is 0.5481633543968201\n",
      "epoch: 2 step: 2392, loss is 0.5794644355773926\n",
      "epoch: 2 step: 2393, loss is 0.6985062956809998\n",
      "epoch: 2 step: 2394, loss is 0.3264363408088684\n",
      "epoch: 2 step: 2395, loss is 0.6838937401771545\n",
      "epoch: 2 step: 2396, loss is 0.39696210622787476\n",
      "epoch: 2 step: 2397, loss is 0.47846266627311707\n",
      "epoch: 2 step: 2398, loss is 0.5478839874267578\n",
      "epoch: 2 step: 2399, loss is 0.617550253868103\n",
      "epoch: 2 step: 2400, loss is 0.5026288032531738\n",
      "epoch: 2 step: 2401, loss is 0.5366030335426331\n",
      "epoch: 2 step: 2402, loss is 0.7819392085075378\n",
      "epoch: 2 step: 2403, loss is 0.7606866359710693\n",
      "epoch: 2 step: 2404, loss is 0.365503191947937\n",
      "epoch: 2 step: 2405, loss is 0.46242764592170715\n",
      "epoch: 2 step: 2406, loss is 0.4187067151069641\n",
      "epoch: 2 step: 2407, loss is 0.48539966344833374\n",
      "epoch: 2 step: 2408, loss is 0.22138039767742157\n",
      "epoch: 2 step: 2409, loss is 0.4838718771934509\n",
      "epoch: 2 step: 2410, loss is 0.48670685291290283\n",
      "epoch: 2 step: 2411, loss is 0.8843125104904175\n",
      "epoch: 2 step: 2412, loss is 0.7202833294868469\n",
      "epoch: 2 step: 2413, loss is 0.5989931225776672\n",
      "epoch: 2 step: 2414, loss is 0.5372115969657898\n",
      "epoch: 2 step: 2415, loss is 0.5318378210067749\n",
      "epoch: 2 step: 2416, loss is 0.6838816404342651\n",
      "epoch: 2 step: 2417, loss is 0.6958780288696289\n",
      "epoch: 2 step: 2418, loss is 0.7860350608825684\n",
      "epoch: 2 step: 2419, loss is 0.5844306349754333\n",
      "epoch: 2 step: 2420, loss is 0.6908402442932129\n",
      "epoch: 2 step: 2421, loss is 0.5663050413131714\n",
      "epoch: 2 step: 2422, loss is 0.5137797594070435\n",
      "epoch: 2 step: 2423, loss is 0.5817590951919556\n",
      "epoch: 2 step: 2424, loss is 0.5212008357048035\n",
      "epoch: 2 step: 2425, loss is 0.5005765557289124\n",
      "epoch: 2 step: 2426, loss is 0.3123670816421509\n",
      "epoch: 2 step: 2427, loss is 0.3876730501651764\n",
      "epoch: 2 step: 2428, loss is 0.648208737373352\n",
      "epoch: 2 step: 2429, loss is 0.7748324871063232\n",
      "epoch: 2 step: 2430, loss is 0.6392030119895935\n",
      "epoch: 2 step: 2431, loss is 0.45863473415374756\n",
      "epoch: 2 step: 2432, loss is 0.5828937888145447\n",
      "epoch: 2 step: 2433, loss is 0.5417293310165405\n",
      "epoch: 2 step: 2434, loss is 0.4052206575870514\n",
      "epoch: 2 step: 2435, loss is 0.5926295518875122\n",
      "epoch: 2 step: 2436, loss is 0.5579696297645569\n",
      "epoch: 2 step: 2437, loss is 0.49075251817703247\n",
      "epoch: 2 step: 2438, loss is 0.9121057391166687\n",
      "epoch: 2 step: 2439, loss is 0.488807737827301\n",
      "epoch: 2 step: 2440, loss is 0.7080831527709961\n",
      "epoch: 2 step: 2441, loss is 0.4015842080116272\n",
      "epoch: 2 step: 2442, loss is 0.7138315439224243\n",
      "epoch: 2 step: 2443, loss is 0.80271315574646\n",
      "epoch: 2 step: 2444, loss is 0.7375237941741943\n",
      "epoch: 2 step: 2445, loss is 0.6801429986953735\n",
      "epoch: 2 step: 2446, loss is 0.6012037992477417\n",
      "epoch: 2 step: 2447, loss is 0.8278899192810059\n",
      "epoch: 2 step: 2448, loss is 0.65848708152771\n",
      "epoch: 2 step: 2449, loss is 0.7968039512634277\n",
      "epoch: 2 step: 2450, loss is 0.5507596731185913\n",
      "epoch: 2 step: 2451, loss is 0.6385700106620789\n",
      "epoch: 2 step: 2452, loss is 0.6544317603111267\n",
      "epoch: 2 step: 2453, loss is 0.49445050954818726\n",
      "epoch: 2 step: 2454, loss is 0.5403124690055847\n",
      "epoch: 2 step: 2455, loss is 0.6402846574783325\n",
      "epoch: 2 step: 2456, loss is 0.44694384932518005\n",
      "epoch: 2 step: 2457, loss is 0.6431296467781067\n",
      "epoch: 2 step: 2458, loss is 0.603725790977478\n",
      "epoch: 2 step: 2459, loss is 0.4954569339752197\n",
      "epoch: 2 step: 2460, loss is 0.3322100341320038\n",
      "epoch: 2 step: 2461, loss is 0.5820244550704956\n",
      "epoch: 2 step: 2462, loss is 0.6654531955718994\n",
      "epoch: 2 step: 2463, loss is 0.41088855266571045\n",
      "epoch: 2 step: 2464, loss is 0.5836213827133179\n",
      "epoch: 2 step: 2465, loss is 0.5699869990348816\n",
      "epoch: 2 step: 2466, loss is 0.6414660215377808\n",
      "epoch: 2 step: 2467, loss is 0.9213464260101318\n",
      "epoch: 2 step: 2468, loss is 0.5127924680709839\n",
      "epoch: 2 step: 2469, loss is 0.8684902191162109\n",
      "epoch: 2 step: 2470, loss is 0.6239776611328125\n",
      "epoch: 2 step: 2471, loss is 1.173546314239502\n",
      "epoch: 2 step: 2472, loss is 0.5537376403808594\n",
      "epoch: 2 step: 2473, loss is 0.48992598056793213\n",
      "epoch: 2 step: 2474, loss is 0.756599485874176\n",
      "epoch: 2 step: 2475, loss is 0.5335540771484375\n",
      "epoch: 2 step: 2476, loss is 0.42567601799964905\n",
      "epoch: 2 step: 2477, loss is 0.44240689277648926\n",
      "epoch: 2 step: 2478, loss is 0.5841758251190186\n",
      "epoch: 2 step: 2479, loss is 0.6181951761245728\n",
      "epoch: 2 step: 2480, loss is 0.4163028299808502\n",
      "epoch: 2 step: 2481, loss is 0.43401390314102173\n",
      "epoch: 2 step: 2482, loss is 0.5185697674751282\n",
      "epoch: 2 step: 2483, loss is 0.5550139546394348\n",
      "epoch: 2 step: 2484, loss is 0.7394742369651794\n",
      "epoch: 2 step: 2485, loss is 0.7071974277496338\n",
      "epoch: 2 step: 2486, loss is 0.615483820438385\n",
      "epoch: 2 step: 2487, loss is 0.33420467376708984\n",
      "epoch: 2 step: 2488, loss is 0.7891820073127747\n",
      "epoch: 2 step: 2489, loss is 0.4395767152309418\n",
      "epoch: 2 step: 2490, loss is 0.7620605826377869\n",
      "epoch: 2 step: 2491, loss is 0.3786472678184509\n",
      "epoch: 2 step: 2492, loss is 0.6041873693466187\n",
      "epoch: 2 step: 2493, loss is 0.8133620023727417\n",
      "epoch: 2 step: 2494, loss is 0.690611720085144\n",
      "epoch: 2 step: 2495, loss is 0.46978479623794556\n",
      "epoch: 2 step: 2496, loss is 0.5873111486434937\n",
      "epoch: 2 step: 2497, loss is 0.7853467464447021\n",
      "epoch: 2 step: 2498, loss is 0.7989487051963806\n",
      "epoch: 2 step: 2499, loss is 0.5605199337005615\n",
      "epoch: 2 step: 2500, loss is 0.6024080514907837\n",
      "epoch: 2 step: 2501, loss is 0.871513307094574\n",
      "epoch: 2 step: 2502, loss is 0.5821714401245117\n",
      "epoch: 2 step: 2503, loss is 0.5708892345428467\n",
      "epoch: 2 step: 2504, loss is 0.7512415051460266\n",
      "epoch: 2 step: 2505, loss is 0.3457790017127991\n",
      "epoch: 2 step: 2506, loss is 0.7746278643608093\n",
      "epoch: 2 step: 2507, loss is 0.7550833225250244\n",
      "epoch: 2 step: 2508, loss is 0.586700439453125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: unknown JFIF revision number 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 3 step: 1, loss is 0.564555823802948\n",
      "epoch: 3 step: 2, loss is 0.5017753839492798\n",
      "epoch: 3 step: 3, loss is 0.5629329681396484\n",
      "epoch: 3 step: 4, loss is 0.5949744582176208\n",
      "epoch: 3 step: 5, loss is 0.6332755088806152\n",
      "epoch: 3 step: 6, loss is 0.43098896741867065\n",
      "epoch: 3 step: 7, loss is 0.5895817279815674\n",
      "epoch: 3 step: 8, loss is 0.6216392517089844\n",
      "epoch: 3 step: 9, loss is 0.4797360897064209\n",
      "epoch: 3 step: 10, loss is 0.6390798091888428\n",
      "epoch: 3 step: 11, loss is 0.5568848848342896\n",
      "epoch: 3 step: 12, loss is 0.5227073431015015\n",
      "epoch: 3 step: 13, loss is 0.5993086695671082\n",
      "epoch: 3 step: 14, loss is 0.7245907783508301\n",
      "epoch: 3 step: 15, loss is 0.7104002237319946\n",
      "epoch: 3 step: 16, loss is 0.44309481978416443\n",
      "epoch: 3 step: 17, loss is 0.7737090587615967\n",
      "epoch: 3 step: 18, loss is 0.6110953092575073\n",
      "epoch: 3 step: 19, loss is 0.5854219794273376\n",
      "epoch: 3 step: 20, loss is 0.45382559299468994\n",
      "epoch: 3 step: 21, loss is 0.7689433097839355\n",
      "epoch: 3 step: 22, loss is 0.3896219730377197\n",
      "epoch: 3 step: 23, loss is 0.863724410533905\n",
      "epoch: 3 step: 24, loss is 0.8729323148727417\n",
      "epoch: 3 step: 25, loss is 0.9980387687683105\n",
      "epoch: 3 step: 26, loss is 0.6721826195716858\n",
      "epoch: 3 step: 27, loss is 0.4498400390148163\n",
      "epoch: 3 step: 28, loss is 0.700538158416748\n",
      "epoch: 3 step: 29, loss is 0.478765606880188\n",
      "epoch: 3 step: 30, loss is 0.4525095820426941\n",
      "epoch: 3 step: 31, loss is 0.7389281988143921\n",
      "epoch: 3 step: 32, loss is 0.7134412527084351\n",
      "epoch: 3 step: 33, loss is 0.4730358421802521\n",
      "epoch: 3 step: 34, loss is 0.49350133538246155\n",
      "epoch: 3 step: 35, loss is 0.5265250205993652\n",
      "epoch: 3 step: 36, loss is 0.6633819341659546\n",
      "epoch: 3 step: 37, loss is 0.8466713428497314\n",
      "epoch: 3 step: 38, loss is 0.8046140074729919\n",
      "epoch: 3 step: 39, loss is 0.5735277533531189\n",
      "epoch: 3 step: 40, loss is 0.8585265278816223\n",
      "epoch: 3 step: 41, loss is 0.7726771235466003\n",
      "epoch: 3 step: 42, loss is 0.6408293843269348\n",
      "epoch: 3 step: 43, loss is 0.617117702960968\n",
      "epoch: 3 step: 44, loss is 0.6366669535636902\n",
      "epoch: 3 step: 45, loss is 0.7183584570884705\n",
      "epoch: 3 step: 46, loss is 0.5326462388038635\n",
      "epoch: 3 step: 47, loss is 0.44299036264419556\n",
      "epoch: 3 step: 48, loss is 0.6521377563476562\n",
      "epoch: 3 step: 49, loss is 0.48064124584198\n",
      "epoch: 3 step: 50, loss is 0.6750388741493225\n",
      "epoch: 3 step: 51, loss is 0.3933984935283661\n",
      "epoch: 3 step: 52, loss is 0.769608736038208\n",
      "epoch: 3 step: 53, loss is 0.4284609258174896\n",
      "epoch: 3 step: 54, loss is 0.5792727470397949\n",
      "epoch: 3 step: 55, loss is 0.5437444448471069\n",
      "epoch: 3 step: 56, loss is 0.4562782049179077\n",
      "epoch: 3 step: 57, loss is 0.5887272357940674\n",
      "epoch: 3 step: 58, loss is 0.7352568507194519\n",
      "epoch: 3 step: 59, loss is 0.4011828601360321\n",
      "epoch: 3 step: 60, loss is 0.4171431064605713\n",
      "epoch: 3 step: 61, loss is 0.5561124682426453\n",
      "epoch: 3 step: 62, loss is 0.4072476029396057\n",
      "epoch: 3 step: 63, loss is 0.7920598387718201\n",
      "epoch: 3 step: 64, loss is 0.6555017828941345\n",
      "epoch: 3 step: 65, loss is 0.8018354177474976\n",
      "epoch: 3 step: 66, loss is 0.4273669421672821\n",
      "epoch: 3 step: 67, loss is 0.8309040069580078\n",
      "epoch: 3 step: 68, loss is 0.45078402757644653\n",
      "epoch: 3 step: 69, loss is 0.5932654142379761\n",
      "epoch: 3 step: 70, loss is 0.5835395455360413\n",
      "epoch: 3 step: 71, loss is 0.6236451864242554\n",
      "epoch: 3 step: 72, loss is 0.47096341848373413\n",
      "epoch: 3 step: 73, loss is 0.542883038520813\n",
      "epoch: 3 step: 74, loss is 0.49131909012794495\n",
      "epoch: 3 step: 75, loss is 0.6234465837478638\n",
      "epoch: 3 step: 76, loss is 0.48034682869911194\n",
      "epoch: 3 step: 77, loss is 0.4777044355869293\n",
      "epoch: 3 step: 78, loss is 0.7622091770172119\n",
      "epoch: 3 step: 79, loss is 0.7527869939804077\n",
      "epoch: 3 step: 80, loss is 0.5232591032981873\n",
      "epoch: 3 step: 81, loss is 0.36704304814338684\n",
      "epoch: 3 step: 82, loss is 0.4551439583301544\n",
      "epoch: 3 step: 83, loss is 0.6741095781326294\n",
      "epoch: 3 step: 84, loss is 0.56617671251297\n",
      "epoch: 3 step: 85, loss is 0.4144817888736725\n",
      "epoch: 3 step: 86, loss is 0.6237016916275024\n",
      "epoch: 3 step: 87, loss is 0.7854020595550537\n",
      "epoch: 3 step: 88, loss is 0.54010009765625\n",
      "epoch: 3 step: 89, loss is 0.731143593788147\n",
      "epoch: 3 step: 90, loss is 0.3674405515193939\n",
      "epoch: 3 step: 91, loss is 0.5381685495376587\n",
      "epoch: 3 step: 92, loss is 1.2058061361312866\n",
      "epoch: 3 step: 93, loss is 0.615485668182373\n",
      "epoch: 3 step: 94, loss is 0.3815772831439972\n",
      "epoch: 3 step: 95, loss is 1.0017881393432617\n",
      "epoch: 3 step: 96, loss is 0.5735711455345154\n",
      "epoch: 3 step: 97, loss is 0.6044670343399048\n",
      "epoch: 3 step: 98, loss is 0.4835361838340759\n",
      "epoch: 3 step: 99, loss is 0.5025537610054016\n",
      "epoch: 3 step: 100, loss is 0.7506778836250305\n",
      "epoch: 3 step: 101, loss is 0.6021119952201843\n",
      "epoch: 3 step: 102, loss is 0.6325724720954895\n",
      "epoch: 3 step: 103, loss is 0.7247831225395203\n",
      "epoch: 3 step: 104, loss is 0.6728329658508301\n",
      "epoch: 3 step: 105, loss is 0.6032903790473938\n",
      "epoch: 3 step: 106, loss is 0.6163665056228638\n",
      "epoch: 3 step: 107, loss is 0.49859052896499634\n",
      "epoch: 3 step: 108, loss is 0.5713777542114258\n",
      "epoch: 3 step: 109, loss is 0.7537309527397156\n",
      "epoch: 3 step: 110, loss is 0.4439981281757355\n",
      "epoch: 3 step: 111, loss is 0.4063798189163208\n",
      "epoch: 3 step: 112, loss is 0.9366512298583984\n",
      "epoch: 3 step: 113, loss is 0.4431702494621277\n",
      "epoch: 3 step: 114, loss is 0.4563423991203308\n",
      "epoch: 3 step: 115, loss is 0.4035843312740326\n",
      "epoch: 3 step: 116, loss is 0.3868417739868164\n",
      "epoch: 3 step: 117, loss is 0.5498567819595337\n",
      "epoch: 3 step: 118, loss is 0.6074509024620056\n",
      "epoch: 3 step: 119, loss is 0.4955698251724243\n",
      "epoch: 3 step: 120, loss is 0.8450872302055359\n",
      "epoch: 3 step: 121, loss is 0.8882178068161011\n",
      "epoch: 3 step: 122, loss is 0.6644992828369141\n",
      "epoch: 3 step: 123, loss is 0.5236997604370117\n",
      "epoch: 3 step: 124, loss is 0.7770511507987976\n",
      "epoch: 3 step: 125, loss is 0.6605330109596252\n",
      "epoch: 3 step: 126, loss is 0.5501715540885925\n",
      "epoch: 3 step: 127, loss is 0.6262726187705994\n",
      "epoch: 3 step: 128, loss is 0.8878843188285828\n",
      "epoch: 3 step: 129, loss is 0.7424052953720093\n",
      "epoch: 3 step: 130, loss is 0.5501195788383484\n",
      "epoch: 3 step: 131, loss is 0.7326145768165588\n",
      "epoch: 3 step: 132, loss is 0.5552207827568054\n",
      "epoch: 3 step: 133, loss is 0.6656550168991089\n",
      "epoch: 3 step: 134, loss is 0.6804379820823669\n",
      "epoch: 3 step: 135, loss is 0.5158144235610962\n",
      "epoch: 3 step: 136, loss is 0.49274206161499023\n",
      "epoch: 3 step: 137, loss is 0.5262929797172546\n",
      "epoch: 3 step: 138, loss is 0.6146789193153381\n",
      "epoch: 3 step: 139, loss is 0.5413787364959717\n",
      "epoch: 3 step: 140, loss is 0.5983968377113342\n",
      "epoch: 3 step: 141, loss is 0.611562967300415\n",
      "epoch: 3 step: 142, loss is 0.7099979519844055\n",
      "epoch: 3 step: 143, loss is 0.6180270314216614\n",
      "epoch: 3 step: 144, loss is 0.8326476812362671\n",
      "epoch: 3 step: 145, loss is 0.6111902594566345\n",
      "epoch: 3 step: 146, loss is 0.3967663645744324\n",
      "epoch: 3 step: 147, loss is 0.4872470498085022\n",
      "epoch: 3 step: 148, loss is 0.6664738655090332\n",
      "epoch: 3 step: 149, loss is 0.4511663019657135\n",
      "epoch: 3 step: 150, loss is 0.4052680730819702\n",
      "epoch: 3 step: 151, loss is 0.7140715718269348\n",
      "epoch: 3 step: 152, loss is 0.6745091676712036\n",
      "epoch: 3 step: 153, loss is 0.8070127367973328\n",
      "epoch: 3 step: 154, loss is 0.5697237849235535\n",
      "epoch: 3 step: 155, loss is 0.40324047207832336\n",
      "epoch: 3 step: 156, loss is 0.78598952293396\n",
      "epoch: 3 step: 157, loss is 0.6110709309577942\n",
      "epoch: 3 step: 158, loss is 0.8387976884841919\n",
      "epoch: 3 step: 159, loss is 0.35793253779411316\n",
      "epoch: 3 step: 160, loss is 0.5863412618637085\n",
      "epoch: 3 step: 161, loss is 0.8297480344772339\n",
      "epoch: 3 step: 162, loss is 0.49284628033638\n",
      "epoch: 3 step: 163, loss is 0.5749024152755737\n",
      "epoch: 3 step: 164, loss is 0.8091434836387634\n",
      "epoch: 3 step: 165, loss is 0.7546178698539734\n",
      "epoch: 3 step: 166, loss is 0.6359539031982422\n",
      "epoch: 3 step: 167, loss is 0.6746199727058411\n",
      "epoch: 3 step: 168, loss is 0.7081553936004639\n",
      "epoch: 3 step: 169, loss is 0.3696659803390503\n",
      "epoch: 3 step: 170, loss is 0.32682061195373535\n",
      "epoch: 3 step: 171, loss is 0.828106701374054\n",
      "epoch: 3 step: 172, loss is 0.5008627772331238\n",
      "epoch: 3 step: 173, loss is 0.704289436340332\n",
      "epoch: 3 step: 174, loss is 0.5515092611312866\n",
      "epoch: 3 step: 175, loss is 0.4917672574520111\n",
      "epoch: 3 step: 176, loss is 0.5693398118019104\n",
      "epoch: 3 step: 177, loss is 0.5835464000701904\n",
      "epoch: 3 step: 178, loss is 0.7265282273292542\n",
      "epoch: 3 step: 179, loss is 0.7654321193695068\n",
      "epoch: 3 step: 180, loss is 0.6647151708602905\n",
      "epoch: 3 step: 181, loss is 0.7214866876602173\n",
      "epoch: 3 step: 182, loss is 0.23982197046279907\n",
      "epoch: 3 step: 183, loss is 0.5507128834724426\n",
      "epoch: 3 step: 184, loss is 0.5426505208015442\n",
      "epoch: 3 step: 185, loss is 0.45292219519615173\n",
      "epoch: 3 step: 186, loss is 0.8637462854385376\n",
      "epoch: 3 step: 187, loss is 0.39880606532096863\n",
      "epoch: 3 step: 188, loss is 0.6268869042396545\n",
      "epoch: 3 step: 189, loss is 0.5701273679733276\n",
      "epoch: 3 step: 190, loss is 0.7303169369697571\n",
      "epoch: 3 step: 191, loss is 0.6166698336601257\n",
      "epoch: 3 step: 192, loss is 0.49088650941848755\n",
      "epoch: 3 step: 193, loss is 0.543283998966217\n",
      "epoch: 3 step: 194, loss is 0.46653780341148376\n",
      "epoch: 3 step: 195, loss is 0.6208518147468567\n",
      "epoch: 3 step: 196, loss is 0.5991875529289246\n",
      "epoch: 3 step: 197, loss is 0.8887579441070557\n",
      "epoch: 3 step: 198, loss is 0.4952363967895508\n",
      "epoch: 3 step: 199, loss is 0.6050930619239807\n",
      "epoch: 3 step: 200, loss is 0.771955668926239\n",
      "epoch: 3 step: 201, loss is 0.680921196937561\n",
      "epoch: 3 step: 202, loss is 0.6153163909912109\n",
      "epoch: 3 step: 203, loss is 0.985559344291687\n",
      "epoch: 3 step: 204, loss is 0.4728640913963318\n",
      "epoch: 3 step: 205, loss is 0.7124995589256287\n",
      "epoch: 3 step: 206, loss is 0.5805183053016663\n",
      "epoch: 3 step: 207, loss is 0.4610578417778015\n",
      "epoch: 3 step: 208, loss is 0.4509738087654114\n",
      "epoch: 3 step: 209, loss is 0.6691397428512573\n",
      "epoch: 3 step: 210, loss is 0.6505802869796753\n",
      "epoch: 3 step: 211, loss is 0.4629402160644531\n",
      "epoch: 3 step: 212, loss is 0.48138928413391113\n",
      "epoch: 3 step: 213, loss is 0.565642774105072\n",
      "epoch: 3 step: 214, loss is 0.8256787657737732\n",
      "epoch: 3 step: 215, loss is 0.43066373467445374\n",
      "epoch: 3 step: 216, loss is 0.385503351688385\n",
      "epoch: 3 step: 217, loss is 0.6757668256759644\n",
      "epoch: 3 step: 218, loss is 0.5087751150131226\n",
      "epoch: 3 step: 219, loss is 0.48678654432296753\n",
      "epoch: 3 step: 220, loss is 0.6065812110900879\n",
      "epoch: 3 step: 221, loss is 0.3152734339237213\n",
      "epoch: 3 step: 222, loss is 0.9654416441917419\n",
      "epoch: 3 step: 223, loss is 0.7498646974563599\n",
      "epoch: 3 step: 224, loss is 0.5780821442604065\n",
      "epoch: 3 step: 225, loss is 0.7705104947090149\n",
      "epoch: 3 step: 226, loss is 0.5248259902000427\n",
      "epoch: 3 step: 227, loss is 0.5892888307571411\n",
      "epoch: 3 step: 228, loss is 0.35939404368400574\n",
      "epoch: 3 step: 229, loss is 0.48223382234573364\n",
      "epoch: 3 step: 230, loss is 0.5411469340324402\n",
      "epoch: 3 step: 231, loss is 0.34251663088798523\n",
      "epoch: 3 step: 232, loss is 0.47557634115219116\n",
      "epoch: 3 step: 233, loss is 0.8515543937683105\n",
      "epoch: 3 step: 234, loss is 0.4143258333206177\n",
      "epoch: 3 step: 235, loss is 0.6390877962112427\n",
      "epoch: 3 step: 236, loss is 0.620613157749176\n",
      "epoch: 3 step: 237, loss is 0.6086814403533936\n",
      "epoch: 3 step: 238, loss is 0.5516448020935059\n",
      "epoch: 3 step: 239, loss is 0.44881853461265564\n",
      "epoch: 3 step: 240, loss is 0.5191442370414734\n",
      "epoch: 3 step: 241, loss is 0.8378057479858398\n",
      "epoch: 3 step: 242, loss is 0.6773945689201355\n",
      "epoch: 3 step: 243, loss is 0.4485675096511841\n",
      "epoch: 3 step: 244, loss is 0.5138975977897644\n",
      "epoch: 3 step: 245, loss is 0.660953164100647\n",
      "epoch: 3 step: 246, loss is 0.6088218092918396\n",
      "epoch: 3 step: 247, loss is 0.34255605936050415\n",
      "epoch: 3 step: 248, loss is 0.5542175769805908\n",
      "epoch: 3 step: 249, loss is 0.6133334040641785\n",
      "epoch: 3 step: 250, loss is 0.7265176177024841\n",
      "epoch: 3 step: 251, loss is 0.6595427989959717\n",
      "epoch: 3 step: 252, loss is 0.5726158618927002\n",
      "epoch: 3 step: 253, loss is 0.4367465078830719\n",
      "epoch: 3 step: 254, loss is 0.8800135254859924\n",
      "epoch: 3 step: 255, loss is 0.8643083572387695\n",
      "epoch: 3 step: 256, loss is 0.5591347217559814\n",
      "epoch: 3 step: 257, loss is 0.7005075812339783\n",
      "epoch: 3 step: 258, loss is 0.3222242295742035\n",
      "epoch: 3 step: 259, loss is 0.43236690759658813\n",
      "epoch: 3 step: 260, loss is 0.6199458837509155\n",
      "epoch: 3 step: 261, loss is 0.7845690846443176\n",
      "epoch: 3 step: 262, loss is 0.5378105640411377\n",
      "epoch: 3 step: 263, loss is 0.3882026970386505\n",
      "epoch: 3 step: 264, loss is 0.47228941321372986\n",
      "epoch: 3 step: 265, loss is 0.4818071126937866\n",
      "epoch: 3 step: 266, loss is 0.7082625031471252\n",
      "epoch: 3 step: 267, loss is 0.554118275642395\n",
      "epoch: 3 step: 268, loss is 0.4163869023323059\n",
      "epoch: 3 step: 269, loss is 0.6191578507423401\n",
      "epoch: 3 step: 270, loss is 0.5613706707954407\n",
      "epoch: 3 step: 271, loss is 0.711836576461792\n",
      "epoch: 3 step: 272, loss is 0.530726432800293\n",
      "epoch: 3 step: 273, loss is 0.47586068511009216\n",
      "epoch: 3 step: 274, loss is 0.7176944613456726\n",
      "epoch: 3 step: 275, loss is 0.4631521999835968\n",
      "epoch: 3 step: 276, loss is 0.6852719187736511\n",
      "epoch: 3 step: 277, loss is 0.4433748126029968\n",
      "epoch: 3 step: 278, loss is 0.556081235408783\n",
      "epoch: 3 step: 279, loss is 0.3684815764427185\n",
      "epoch: 3 step: 280, loss is 0.6244053244590759\n",
      "epoch: 3 step: 281, loss is 0.5038513541221619\n",
      "epoch: 3 step: 282, loss is 0.9446418881416321\n",
      "epoch: 3 step: 283, loss is 0.5626950263977051\n",
      "epoch: 3 step: 284, loss is 0.6728150248527527\n",
      "epoch: 3 step: 285, loss is 0.3652794361114502\n",
      "epoch: 3 step: 286, loss is 0.34927764534950256\n",
      "epoch: 3 step: 287, loss is 0.31867167353630066\n",
      "epoch: 3 step: 288, loss is 0.7009568214416504\n",
      "epoch: 3 step: 289, loss is 0.8978453278541565\n",
      "epoch: 3 step: 290, loss is 0.7802791595458984\n",
      "epoch: 3 step: 291, loss is 0.4495588541030884\n",
      "epoch: 3 step: 292, loss is 0.6031935214996338\n",
      "epoch: 3 step: 293, loss is 0.8209629058837891\n",
      "epoch: 3 step: 294, loss is 0.5443164110183716\n",
      "epoch: 3 step: 295, loss is 0.7094487547874451\n",
      "epoch: 3 step: 296, loss is 0.39146938920021057\n",
      "epoch: 3 step: 297, loss is 0.790566086769104\n",
      "epoch: 3 step: 298, loss is 0.5998936295509338\n",
      "epoch: 3 step: 299, loss is 0.4953974485397339\n",
      "epoch: 3 step: 300, loss is 0.6907891631126404\n",
      "epoch: 3 step: 301, loss is 0.8578991293907166\n",
      "epoch: 3 step: 302, loss is 0.53563392162323\n",
      "epoch: 3 step: 303, loss is 0.5872645378112793\n",
      "epoch: 3 step: 304, loss is 0.4537738561630249\n",
      "epoch: 3 step: 305, loss is 0.6535966396331787\n",
      "epoch: 3 step: 306, loss is 0.49019840359687805\n",
      "epoch: 3 step: 307, loss is 0.5814218521118164\n",
      "epoch: 3 step: 308, loss is 0.5831328630447388\n",
      "epoch: 3 step: 309, loss is 0.4827686846256256\n",
      "epoch: 3 step: 310, loss is 0.8493074178695679\n",
      "epoch: 3 step: 311, loss is 0.563506007194519\n",
      "epoch: 3 step: 312, loss is 0.4348311424255371\n",
      "epoch: 3 step: 313, loss is 0.7453629970550537\n",
      "epoch: 3 step: 314, loss is 0.42896854877471924\n",
      "epoch: 3 step: 315, loss is 0.6408015489578247\n",
      "epoch: 3 step: 316, loss is 0.7642177939414978\n",
      "epoch: 3 step: 317, loss is 0.4535619914531708\n",
      "epoch: 3 step: 318, loss is 0.8619827032089233\n",
      "epoch: 3 step: 319, loss is 0.5335549116134644\n",
      "epoch: 3 step: 320, loss is 1.1727224588394165\n",
      "epoch: 3 step: 321, loss is 0.41569390892982483\n",
      "epoch: 3 step: 322, loss is 0.28195974230766296\n",
      "epoch: 3 step: 323, loss is 0.5853946805000305\n",
      "epoch: 3 step: 324, loss is 0.5477619767189026\n",
      "epoch: 3 step: 325, loss is 0.39216315746307373\n",
      "epoch: 3 step: 326, loss is 0.49891072511672974\n",
      "epoch: 3 step: 327, loss is 0.7103049159049988\n",
      "epoch: 3 step: 328, loss is 0.5352331399917603\n",
      "epoch: 3 step: 329, loss is 0.41932523250579834\n",
      "epoch: 3 step: 330, loss is 0.3807109594345093\n",
      "epoch: 3 step: 331, loss is 0.5054001212120056\n",
      "epoch: 3 step: 332, loss is 0.6800438761711121\n",
      "epoch: 3 step: 333, loss is 0.7994831204414368\n",
      "epoch: 3 step: 334, loss is 0.4222700297832489\n",
      "epoch: 3 step: 335, loss is 0.656086266040802\n",
      "epoch: 3 step: 336, loss is 0.855933427810669\n",
      "epoch: 3 step: 337, loss is 0.3274047076702118\n",
      "epoch: 3 step: 338, loss is 0.29104265570640564\n",
      "epoch: 3 step: 339, loss is 0.5547780990600586\n",
      "epoch: 3 step: 340, loss is 0.5063649415969849\n",
      "epoch: 3 step: 341, loss is 0.4948955476284027\n",
      "epoch: 3 step: 342, loss is 0.7963118553161621\n",
      "epoch: 3 step: 343, loss is 0.4812147915363312\n",
      "epoch: 3 step: 344, loss is 0.3744994103908539\n",
      "epoch: 3 step: 345, loss is 0.7072516083717346\n",
      "epoch: 3 step: 346, loss is 0.4795808792114258\n",
      "epoch: 3 step: 347, loss is 0.8663386106491089\n",
      "epoch: 3 step: 348, loss is 0.4530757665634155\n",
      "epoch: 3 step: 349, loss is 1.009615182876587\n",
      "epoch: 3 step: 350, loss is 0.4274366796016693\n",
      "epoch: 3 step: 351, loss is 0.5623767375946045\n",
      "epoch: 3 step: 352, loss is 0.9899163246154785\n",
      "epoch: 3 step: 353, loss is 0.5388839840888977\n",
      "epoch: 3 step: 354, loss is 0.540504515171051\n",
      "epoch: 3 step: 355, loss is 1.1624635457992554\n",
      "epoch: 3 step: 356, loss is 0.8644850254058838\n",
      "epoch: 3 step: 357, loss is 0.8449959754943848\n",
      "epoch: 3 step: 358, loss is 0.432263046503067\n",
      "epoch: 3 step: 359, loss is 0.48413529992103577\n",
      "epoch: 3 step: 360, loss is 0.5713624358177185\n",
      "epoch: 3 step: 361, loss is 0.5498636364936829\n",
      "epoch: 3 step: 362, loss is 0.7039947509765625\n",
      "epoch: 3 step: 363, loss is 0.4536893963813782\n",
      "epoch: 3 step: 364, loss is 0.4622173011302948\n",
      "epoch: 3 step: 365, loss is 0.4886530935764313\n",
      "epoch: 3 step: 366, loss is 0.5591657757759094\n",
      "epoch: 3 step: 367, loss is 0.3693930208683014\n",
      "epoch: 3 step: 368, loss is 0.5534825921058655\n",
      "epoch: 3 step: 369, loss is 0.4172832667827606\n",
      "epoch: 3 step: 370, loss is 0.5625603199005127\n",
      "epoch: 3 step: 371, loss is 0.9405527710914612\n",
      "epoch: 3 step: 372, loss is 0.46458056569099426\n",
      "epoch: 3 step: 373, loss is 0.42274659872055054\n",
      "epoch: 3 step: 374, loss is 0.584832489490509\n",
      "epoch: 3 step: 375, loss is 0.6570708155632019\n",
      "epoch: 3 step: 376, loss is 0.7181929349899292\n",
      "epoch: 3 step: 377, loss is 0.3696111738681793\n",
      "epoch: 3 step: 378, loss is 0.32785123586654663\n",
      "epoch: 3 step: 379, loss is 0.4644935131072998\n",
      "epoch: 3 step: 380, loss is 0.5673379302024841\n",
      "epoch: 3 step: 381, loss is 0.42209410667419434\n",
      "epoch: 3 step: 382, loss is 0.46608206629753113\n",
      "epoch: 3 step: 383, loss is 0.6653372645378113\n",
      "epoch: 3 step: 384, loss is 0.739936888217926\n",
      "epoch: 3 step: 385, loss is 0.6295273303985596\n",
      "epoch: 3 step: 386, loss is 0.48532694578170776\n",
      "epoch: 3 step: 387, loss is 0.6337659358978271\n",
      "epoch: 3 step: 388, loss is 0.8982101678848267\n",
      "epoch: 3 step: 389, loss is 0.40964147448539734\n",
      "epoch: 3 step: 390, loss is 0.4827120900154114\n",
      "epoch: 3 step: 391, loss is 0.7405245900154114\n",
      "epoch: 3 step: 392, loss is 0.6524354219436646\n",
      "epoch: 3 step: 393, loss is 0.3287741243839264\n",
      "epoch: 3 step: 394, loss is 0.6107915043830872\n",
      "epoch: 3 step: 395, loss is 0.5479536652565002\n",
      "epoch: 3 step: 396, loss is 0.9413912892341614\n",
      "epoch: 3 step: 397, loss is 0.4073584973812103\n",
      "epoch: 3 step: 398, loss is 0.5724852085113525\n",
      "epoch: 3 step: 399, loss is 0.5652111768722534\n",
      "epoch: 3 step: 400, loss is 0.8678886890411377\n",
      "epoch: 3 step: 401, loss is 0.6631684899330139\n",
      "epoch: 3 step: 402, loss is 0.36259177327156067\n",
      "epoch: 3 step: 403, loss is 0.7120981216430664\n",
      "epoch: 3 step: 404, loss is 0.5551385283470154\n",
      "epoch: 3 step: 405, loss is 0.6140557527542114\n",
      "epoch: 3 step: 406, loss is 0.5482528209686279\n",
      "epoch: 3 step: 407, loss is 0.34962302446365356\n",
      "epoch: 3 step: 408, loss is 0.5253810882568359\n",
      "epoch: 3 step: 409, loss is 0.7802830934524536\n",
      "epoch: 3 step: 410, loss is 0.5637081861495972\n",
      "epoch: 3 step: 411, loss is 0.40893927216529846\n",
      "epoch: 3 step: 412, loss is 0.9055067896842957\n",
      "epoch: 3 step: 413, loss is 0.3374003469944\n",
      "epoch: 3 step: 414, loss is 0.6523280143737793\n",
      "epoch: 3 step: 415, loss is 0.687175452709198\n",
      "epoch: 3 step: 416, loss is 0.6688564419746399\n",
      "epoch: 3 step: 417, loss is 0.3965703248977661\n",
      "epoch: 3 step: 418, loss is 0.4361703097820282\n",
      "epoch: 3 step: 419, loss is 0.5642313361167908\n",
      "epoch: 3 step: 420, loss is 0.7580471634864807\n",
      "epoch: 3 step: 421, loss is 0.5295189023017883\n",
      "epoch: 3 step: 422, loss is 0.6720642447471619\n",
      "epoch: 3 step: 423, loss is 0.43596065044403076\n",
      "epoch: 3 step: 424, loss is 0.4697323143482208\n",
      "epoch: 3 step: 425, loss is 0.2809631824493408\n",
      "epoch: 3 step: 426, loss is 0.4844419062137604\n",
      "epoch: 3 step: 427, loss is 0.5116829872131348\n",
      "epoch: 3 step: 428, loss is 0.6622346639633179\n",
      "epoch: 3 step: 429, loss is 0.6124893426895142\n",
      "epoch: 3 step: 430, loss is 0.7668953537940979\n",
      "epoch: 3 step: 431, loss is 0.5164453387260437\n",
      "epoch: 3 step: 432, loss is 1.0248669385910034\n",
      "epoch: 3 step: 433, loss is 0.655708372592926\n",
      "epoch: 3 step: 434, loss is 0.5025954246520996\n",
      "epoch: 3 step: 435, loss is 0.3944340944290161\n",
      "epoch: 3 step: 436, loss is 0.5735870003700256\n",
      "epoch: 3 step: 437, loss is 0.6451302170753479\n",
      "epoch: 3 step: 438, loss is 0.5449140667915344\n",
      "epoch: 3 step: 439, loss is 0.3578149676322937\n",
      "epoch: 3 step: 440, loss is 0.42989498376846313\n",
      "epoch: 3 step: 441, loss is 0.475434809923172\n",
      "epoch: 3 step: 442, loss is 0.5605578422546387\n",
      "epoch: 3 step: 443, loss is 0.5405388474464417\n",
      "epoch: 3 step: 444, loss is 0.3022579848766327\n",
      "epoch: 3 step: 445, loss is 0.7796365022659302\n",
      "epoch: 3 step: 446, loss is 0.8184248805046082\n",
      "epoch: 3 step: 447, loss is 0.5133787393569946\n",
      "epoch: 3 step: 448, loss is 0.8580453991889954\n",
      "epoch: 3 step: 449, loss is 0.6276574730873108\n",
      "epoch: 3 step: 450, loss is 0.6296078562736511\n",
      "epoch: 3 step: 451, loss is 0.5627750158309937\n",
      "epoch: 3 step: 452, loss is 0.5475669503211975\n",
      "epoch: 3 step: 453, loss is 0.5985828042030334\n",
      "epoch: 3 step: 454, loss is 0.7433807849884033\n",
      "epoch: 3 step: 455, loss is 0.685238778591156\n",
      "epoch: 3 step: 456, loss is 0.6184852123260498\n",
      "epoch: 3 step: 457, loss is 0.810401976108551\n",
      "epoch: 3 step: 458, loss is 0.6357488036155701\n",
      "epoch: 3 step: 459, loss is 0.6628420352935791\n",
      "epoch: 3 step: 460, loss is 0.43446865677833557\n",
      "epoch: 3 step: 461, loss is 0.4874941408634186\n",
      "epoch: 3 step: 462, loss is 0.450562983751297\n",
      "epoch: 3 step: 463, loss is 0.6769675612449646\n",
      "epoch: 3 step: 464, loss is 0.5558740496635437\n",
      "epoch: 3 step: 465, loss is 0.5104994773864746\n",
      "epoch: 3 step: 466, loss is 0.48917922377586365\n",
      "epoch: 3 step: 467, loss is 0.42894744873046875\n",
      "epoch: 3 step: 468, loss is 0.44124501943588257\n",
      "epoch: 3 step: 469, loss is 0.5024300813674927\n",
      "epoch: 3 step: 470, loss is 0.617196798324585\n",
      "epoch: 3 step: 471, loss is 0.3598268926143646\n",
      "epoch: 3 step: 472, loss is 0.546759307384491\n",
      "epoch: 3 step: 473, loss is 0.7290875911712646\n",
      "epoch: 3 step: 474, loss is 0.7395057678222656\n",
      "epoch: 3 step: 475, loss is 0.28319260478019714\n",
      "epoch: 3 step: 476, loss is 0.6600245237350464\n",
      "epoch: 3 step: 477, loss is 1.0642584562301636\n",
      "epoch: 3 step: 478, loss is 1.238526701927185\n",
      "epoch: 3 step: 479, loss is 0.7521240711212158\n",
      "epoch: 3 step: 480, loss is 0.4636547565460205\n",
      "epoch: 3 step: 481, loss is 0.6807816028594971\n",
      "epoch: 3 step: 482, loss is 0.9892621636390686\n",
      "epoch: 3 step: 483, loss is 0.28744253516197205\n",
      "epoch: 3 step: 484, loss is 0.5754735469818115\n",
      "epoch: 3 step: 485, loss is 0.6195698976516724\n",
      "epoch: 3 step: 486, loss is 0.7537844777107239\n",
      "epoch: 3 step: 487, loss is 0.510013222694397\n",
      "epoch: 3 step: 488, loss is 0.7501240372657776\n",
      "epoch: 3 step: 489, loss is 0.38442471623420715\n",
      "epoch: 3 step: 490, loss is 0.6755878925323486\n",
      "epoch: 3 step: 491, loss is 0.43094247579574585\n",
      "epoch: 3 step: 492, loss is 0.48503559827804565\n",
      "epoch: 3 step: 493, loss is 0.5318871140480042\n",
      "epoch: 3 step: 494, loss is 0.49626460671424866\n",
      "epoch: 3 step: 495, loss is 0.47600358724594116\n",
      "epoch: 3 step: 496, loss is 0.4521242082118988\n",
      "epoch: 3 step: 497, loss is 0.6271612048149109\n",
      "epoch: 3 step: 498, loss is 0.5192826986312866\n",
      "epoch: 3 step: 499, loss is 0.7490010857582092\n",
      "epoch: 3 step: 500, loss is 0.3007711172103882\n",
      "epoch: 3 step: 501, loss is 0.8121954202651978\n",
      "epoch: 3 step: 502, loss is 0.6173444390296936\n",
      "epoch: 3 step: 503, loss is 0.6460050940513611\n",
      "epoch: 3 step: 504, loss is 0.5523413419723511\n",
      "epoch: 3 step: 505, loss is 0.4675435721874237\n",
      "epoch: 3 step: 506, loss is 0.4297354817390442\n",
      "epoch: 3 step: 507, loss is 0.49276429414749146\n",
      "epoch: 3 step: 508, loss is 0.46683552861213684\n",
      "epoch: 3 step: 509, loss is 0.3753325641155243\n",
      "epoch: 3 step: 510, loss is 0.6594074368476868\n",
      "epoch: 3 step: 511, loss is 0.37131571769714355\n",
      "epoch: 3 step: 512, loss is 0.9051102995872498\n",
      "epoch: 3 step: 513, loss is 0.5609369277954102\n",
      "epoch: 3 step: 514, loss is 0.6790456175804138\n",
      "epoch: 3 step: 515, loss is 0.662512481212616\n",
      "epoch: 3 step: 516, loss is 0.47224780917167664\n",
      "epoch: 3 step: 517, loss is 0.5962728261947632\n",
      "epoch: 3 step: 518, loss is 0.49685046076774597\n",
      "epoch: 3 step: 519, loss is 0.48729249835014343\n",
      "epoch: 3 step: 520, loss is 0.512509822845459\n",
      "epoch: 3 step: 521, loss is 0.6213295459747314\n",
      "epoch: 3 step: 522, loss is 0.3788456916809082\n",
      "epoch: 3 step: 523, loss is 0.7128187417984009\n",
      "epoch: 3 step: 524, loss is 0.5848610997200012\n",
      "epoch: 3 step: 525, loss is 0.5021563768386841\n",
      "epoch: 3 step: 526, loss is 0.6217679381370544\n",
      "epoch: 3 step: 527, loss is 0.6379221677780151\n",
      "epoch: 3 step: 528, loss is 0.7313944697380066\n",
      "epoch: 3 step: 529, loss is 0.6425938606262207\n",
      "epoch: 3 step: 530, loss is 0.7967139482498169\n",
      "epoch: 3 step: 531, loss is 0.7174084782600403\n",
      "epoch: 3 step: 532, loss is 0.5773770809173584\n",
      "epoch: 3 step: 533, loss is 1.0019079446792603\n",
      "epoch: 3 step: 534, loss is 0.82146817445755\n",
      "epoch: 3 step: 535, loss is 0.5823168158531189\n",
      "epoch: 3 step: 536, loss is 0.5253738164901733\n",
      "epoch: 3 step: 537, loss is 0.4990735650062561\n",
      "epoch: 3 step: 538, loss is 0.46547532081604004\n",
      "epoch: 3 step: 539, loss is 0.7257896065711975\n",
      "epoch: 3 step: 540, loss is 0.6707362532615662\n",
      "epoch: 3 step: 541, loss is 0.5547024011611938\n",
      "epoch: 3 step: 542, loss is 0.4415692090988159\n",
      "epoch: 3 step: 543, loss is 0.41214609146118164\n",
      "epoch: 3 step: 544, loss is 0.48632103204727173\n",
      "epoch: 3 step: 545, loss is 0.5138502717018127\n",
      "epoch: 3 step: 546, loss is 0.27121344208717346\n",
      "epoch: 3 step: 547, loss is 0.6163046956062317\n",
      "epoch: 3 step: 548, loss is 0.5677272081375122\n",
      "epoch: 3 step: 549, loss is 0.532975435256958\n",
      "epoch: 3 step: 550, loss is 0.9723026752471924\n",
      "epoch: 3 step: 551, loss is 0.4668327271938324\n",
      "epoch: 3 step: 552, loss is 0.4148170053958893\n",
      "epoch: 3 step: 553, loss is 0.38258087635040283\n",
      "epoch: 3 step: 554, loss is 0.4804588556289673\n",
      "epoch: 3 step: 555, loss is 0.5624998807907104\n",
      "epoch: 3 step: 556, loss is 0.7382807731628418\n",
      "epoch: 3 step: 557, loss is 0.41018640995025635\n",
      "epoch: 3 step: 558, loss is 0.7731439471244812\n",
      "epoch: 3 step: 559, loss is 0.5625379681587219\n",
      "epoch: 3 step: 560, loss is 0.5244669914245605\n",
      "epoch: 3 step: 561, loss is 0.6695409417152405\n",
      "epoch: 3 step: 562, loss is 0.6777905225753784\n",
      "epoch: 3 step: 563, loss is 0.8279907703399658\n",
      "epoch: 3 step: 564, loss is 0.6208613514900208\n",
      "epoch: 3 step: 565, loss is 0.5492628812789917\n",
      "epoch: 3 step: 566, loss is 0.5309959650039673\n",
      "epoch: 3 step: 567, loss is 0.6157490015029907\n",
      "epoch: 3 step: 568, loss is 0.7064275741577148\n",
      "epoch: 3 step: 569, loss is 0.5483405590057373\n",
      "epoch: 3 step: 570, loss is 0.9978753924369812\n",
      "epoch: 3 step: 571, loss is 0.4954017996788025\n",
      "epoch: 3 step: 572, loss is 0.46131637692451477\n",
      "epoch: 3 step: 573, loss is 0.7655512690544128\n",
      "epoch: 3 step: 574, loss is 0.49296480417251587\n",
      "epoch: 3 step: 575, loss is 0.6008405685424805\n",
      "epoch: 3 step: 576, loss is 0.4869014024734497\n",
      "epoch: 3 step: 577, loss is 0.599196195602417\n",
      "epoch: 3 step: 578, loss is 0.4214364290237427\n",
      "epoch: 3 step: 579, loss is 0.4061219394207001\n",
      "epoch: 3 step: 580, loss is 0.44942933320999146\n",
      "epoch: 3 step: 581, loss is 0.5073179602622986\n",
      "epoch: 3 step: 582, loss is 0.6309362053871155\n",
      "epoch: 3 step: 583, loss is 0.9009015560150146\n",
      "epoch: 3 step: 584, loss is 0.6439972519874573\n",
      "epoch: 3 step: 585, loss is 0.4524090588092804\n",
      "epoch: 3 step: 586, loss is 0.3027969002723694\n",
      "epoch: 3 step: 587, loss is 0.3367622196674347\n",
      "epoch: 3 step: 588, loss is 0.4451600909233093\n",
      "epoch: 3 step: 589, loss is 0.46984803676605225\n",
      "epoch: 3 step: 590, loss is 0.8389440774917603\n",
      "epoch: 3 step: 591, loss is 0.548819363117218\n",
      "epoch: 3 step: 592, loss is 0.8372562527656555\n",
      "epoch: 3 step: 593, loss is 0.5183976292610168\n",
      "epoch: 3 step: 594, loss is 0.6345690488815308\n",
      "epoch: 3 step: 595, loss is 0.2905356287956238\n",
      "epoch: 3 step: 596, loss is 0.4643946886062622\n",
      "epoch: 3 step: 597, loss is 0.9405953288078308\n",
      "epoch: 3 step: 598, loss is 0.7843584418296814\n",
      "epoch: 3 step: 599, loss is 1.4988365173339844\n",
      "epoch: 3 step: 600, loss is 0.4007393717765808\n",
      "epoch: 3 step: 601, loss is 0.6925987601280212\n",
      "epoch: 3 step: 602, loss is 0.46111294627189636\n",
      "epoch: 3 step: 603, loss is 0.5493732690811157\n",
      "epoch: 3 step: 604, loss is 0.9018809199333191\n",
      "epoch: 3 step: 605, loss is 0.5659104585647583\n",
      "epoch: 3 step: 606, loss is 0.4742484390735626\n",
      "epoch: 3 step: 607, loss is 0.692031741142273\n",
      "epoch: 3 step: 608, loss is 0.7976771593093872\n",
      "epoch: 3 step: 609, loss is 0.6097838282585144\n",
      "epoch: 3 step: 610, loss is 0.7818260192871094\n",
      "epoch: 3 step: 611, loss is 0.48065200448036194\n",
      "epoch: 3 step: 612, loss is 0.42425981163978577\n",
      "epoch: 3 step: 613, loss is 0.5574517250061035\n",
      "epoch: 3 step: 614, loss is 0.542162299156189\n",
      "epoch: 3 step: 615, loss is 0.5522226095199585\n",
      "epoch: 3 step: 616, loss is 0.3638623356819153\n",
      "epoch: 3 step: 617, loss is 0.29722562432289124\n",
      "epoch: 3 step: 618, loss is 0.9334716200828552\n",
      "epoch: 3 step: 619, loss is 0.7303724884986877\n",
      "epoch: 3 step: 620, loss is 0.5087171792984009\n",
      "epoch: 3 step: 621, loss is 0.551609218120575\n",
      "epoch: 3 step: 622, loss is 0.4173531234264374\n",
      "epoch: 3 step: 623, loss is 0.8719245791435242\n",
      "epoch: 3 step: 624, loss is 0.5648907423019409\n",
      "epoch: 3 step: 625, loss is 0.6340668797492981\n",
      "epoch: 3 step: 626, loss is 0.9609444737434387\n",
      "epoch: 3 step: 627, loss is 0.8117889761924744\n",
      "epoch: 3 step: 628, loss is 0.629444420337677\n",
      "epoch: 3 step: 629, loss is 0.5127636790275574\n",
      "epoch: 3 step: 630, loss is 0.6183422803878784\n",
      "epoch: 3 step: 631, loss is 0.6916155219078064\n",
      "epoch: 3 step: 632, loss is 0.4824243485927582\n",
      "epoch: 3 step: 633, loss is 0.7337055802345276\n",
      "epoch: 3 step: 634, loss is 0.6833914518356323\n",
      "epoch: 3 step: 635, loss is 0.6761408448219299\n",
      "epoch: 3 step: 636, loss is 0.5614156126976013\n",
      "epoch: 3 step: 637, loss is 0.6621003150939941\n",
      "epoch: 3 step: 638, loss is 0.7481400370597839\n",
      "epoch: 3 step: 639, loss is 0.46186167001724243\n",
      "epoch: 3 step: 640, loss is 0.4884779155254364\n",
      "epoch: 3 step: 641, loss is 0.5850522518157959\n",
      "epoch: 3 step: 642, loss is 0.4875755310058594\n",
      "epoch: 3 step: 643, loss is 0.4204360544681549\n",
      "epoch: 3 step: 644, loss is 0.732126772403717\n",
      "epoch: 3 step: 645, loss is 0.5011138319969177\n",
      "epoch: 3 step: 646, loss is 0.3637768626213074\n",
      "epoch: 3 step: 647, loss is 0.6225016117095947\n",
      "epoch: 3 step: 648, loss is 1.1556031703948975\n",
      "epoch: 3 step: 649, loss is 0.42949411273002625\n",
      "epoch: 3 step: 650, loss is 0.6184917688369751\n",
      "epoch: 3 step: 651, loss is 0.5220661759376526\n",
      "epoch: 3 step: 652, loss is 0.8653326630592346\n",
      "epoch: 3 step: 653, loss is 0.8612728714942932\n",
      "epoch: 3 step: 654, loss is 0.4837460517883301\n",
      "epoch: 3 step: 655, loss is 0.5400177836418152\n",
      "epoch: 3 step: 656, loss is 0.7898494005203247\n",
      "epoch: 3 step: 657, loss is 0.5075607299804688\n",
      "epoch: 3 step: 658, loss is 0.8769030570983887\n",
      "epoch: 3 step: 659, loss is 0.6047075390815735\n",
      "epoch: 3 step: 660, loss is 0.7360589504241943\n",
      "epoch: 3 step: 661, loss is 0.6448244452476501\n",
      "epoch: 3 step: 662, loss is 0.7397852540016174\n",
      "epoch: 3 step: 663, loss is 0.5149388313293457\n",
      "epoch: 3 step: 664, loss is 0.7061903476715088\n",
      "epoch: 3 step: 665, loss is 0.47375816106796265\n",
      "epoch: 3 step: 666, loss is 0.5454739332199097\n",
      "epoch: 3 step: 667, loss is 0.3253439962863922\n",
      "epoch: 3 step: 668, loss is 0.6656586527824402\n",
      "epoch: 3 step: 669, loss is 0.747829258441925\n",
      "epoch: 3 step: 670, loss is 0.6912676692008972\n",
      "epoch: 3 step: 671, loss is 0.5319197773933411\n",
      "epoch: 3 step: 672, loss is 0.5740622282028198\n",
      "epoch: 3 step: 673, loss is 0.47163328528404236\n",
      "epoch: 3 step: 674, loss is 0.3353351056575775\n",
      "epoch: 3 step: 675, loss is 0.7580732703208923\n",
      "epoch: 3 step: 676, loss is 0.49542737007141113\n",
      "epoch: 3 step: 677, loss is 0.47421765327453613\n",
      "epoch: 3 step: 678, loss is 0.5990692377090454\n",
      "epoch: 3 step: 679, loss is 0.48820358514785767\n",
      "epoch: 3 step: 680, loss is 0.5987839698791504\n",
      "epoch: 3 step: 681, loss is 0.4670971930027008\n",
      "epoch: 3 step: 682, loss is 0.9082226753234863\n",
      "epoch: 3 step: 683, loss is 0.7836074829101562\n",
      "epoch: 3 step: 684, loss is 0.4117875099182129\n",
      "epoch: 3 step: 685, loss is 0.6437027454376221\n",
      "epoch: 3 step: 686, loss is 0.4647265672683716\n",
      "epoch: 3 step: 687, loss is 0.4421299397945404\n",
      "epoch: 3 step: 688, loss is 0.6938067674636841\n",
      "epoch: 3 step: 689, loss is 0.5494731068611145\n",
      "epoch: 3 step: 690, loss is 0.6761325597763062\n",
      "epoch: 3 step: 691, loss is 0.525159478187561\n",
      "epoch: 3 step: 692, loss is 0.6542847156524658\n",
      "epoch: 3 step: 693, loss is 0.6868034601211548\n",
      "epoch: 3 step: 694, loss is 0.6302375197410583\n",
      "epoch: 3 step: 695, loss is 0.46001875400543213\n",
      "epoch: 3 step: 696, loss is 0.5876317024230957\n",
      "epoch: 3 step: 697, loss is 0.3374592065811157\n",
      "epoch: 3 step: 698, loss is 0.6627893447875977\n",
      "epoch: 3 step: 699, loss is 0.3610551655292511\n",
      "epoch: 3 step: 700, loss is 0.4762647747993469\n",
      "epoch: 3 step: 701, loss is 0.3693059980869293\n",
      "epoch: 3 step: 702, loss is 0.46478232741355896\n",
      "epoch: 3 step: 703, loss is 0.7741310000419617\n",
      "epoch: 3 step: 704, loss is 0.7415637373924255\n",
      "epoch: 3 step: 705, loss is 0.3427635431289673\n",
      "epoch: 3 step: 706, loss is 0.4190239906311035\n",
      "epoch: 3 step: 707, loss is 0.78633052110672\n",
      "epoch: 3 step: 708, loss is 0.722866952419281\n",
      "epoch: 3 step: 709, loss is 0.659503161907196\n",
      "epoch: 3 step: 710, loss is 0.5163204073905945\n",
      "epoch: 3 step: 711, loss is 0.513248860836029\n",
      "epoch: 3 step: 712, loss is 0.5309525728225708\n",
      "epoch: 3 step: 713, loss is 0.49074220657348633\n",
      "epoch: 3 step: 714, loss is 0.5406767725944519\n",
      "epoch: 3 step: 715, loss is 0.6029747724533081\n",
      "epoch: 3 step: 716, loss is 0.6547297239303589\n",
      "epoch: 3 step: 717, loss is 0.5038738250732422\n",
      "epoch: 3 step: 718, loss is 0.6130838394165039\n",
      "epoch: 3 step: 719, loss is 0.3787073791027069\n",
      "epoch: 3 step: 720, loss is 0.3478812277317047\n",
      "epoch: 3 step: 721, loss is 0.8988257050514221\n",
      "epoch: 3 step: 722, loss is 0.6194391846656799\n",
      "epoch: 3 step: 723, loss is 0.8447214365005493\n",
      "epoch: 3 step: 724, loss is 0.7664451003074646\n",
      "epoch: 3 step: 725, loss is 0.204706609249115\n",
      "epoch: 3 step: 726, loss is 0.6042356491088867\n",
      "epoch: 3 step: 727, loss is 0.634681224822998\n",
      "epoch: 3 step: 728, loss is 0.36304646730422974\n",
      "epoch: 3 step: 729, loss is 0.37562260031700134\n",
      "epoch: 3 step: 730, loss is 0.7489460110664368\n",
      "epoch: 3 step: 731, loss is 0.8608818650245667\n",
      "epoch: 3 step: 732, loss is 0.3943965435028076\n",
      "epoch: 3 step: 733, loss is 0.36944612860679626\n",
      "epoch: 3 step: 734, loss is 0.6492174863815308\n",
      "epoch: 3 step: 735, loss is 0.4200308322906494\n",
      "epoch: 3 step: 736, loss is 0.8025314211845398\n",
      "epoch: 3 step: 737, loss is 0.6592035889625549\n",
      "epoch: 3 step: 738, loss is 0.8350033760070801\n",
      "epoch: 3 step: 739, loss is 0.24767954647541046\n",
      "epoch: 3 step: 740, loss is 1.077784776687622\n",
      "epoch: 3 step: 741, loss is 0.42749735713005066\n",
      "epoch: 3 step: 742, loss is 0.6234363317489624\n",
      "epoch: 3 step: 743, loss is 0.6840664148330688\n",
      "epoch: 3 step: 744, loss is 0.7147182822227478\n",
      "epoch: 3 step: 745, loss is 0.5419596433639526\n",
      "epoch: 3 step: 746, loss is 0.4956951439380646\n",
      "epoch: 3 step: 747, loss is 0.5932698249816895\n",
      "epoch: 3 step: 748, loss is 0.46848124265670776\n",
      "epoch: 3 step: 749, loss is 0.39185407757759094\n",
      "epoch: 3 step: 750, loss is 0.5420269966125488\n",
      "epoch: 3 step: 751, loss is 0.6618921160697937\n",
      "epoch: 3 step: 752, loss is 0.8625968098640442\n",
      "epoch: 3 step: 753, loss is 0.3296108841896057\n",
      "epoch: 3 step: 754, loss is 0.6967750787734985\n",
      "epoch: 3 step: 755, loss is 0.47065919637680054\n",
      "epoch: 3 step: 756, loss is 0.6478807926177979\n",
      "epoch: 3 step: 757, loss is 0.39838534593582153\n",
      "epoch: 3 step: 758, loss is 0.44525134563446045\n",
      "epoch: 3 step: 759, loss is 0.5185731053352356\n",
      "epoch: 3 step: 760, loss is 0.3978651463985443\n",
      "epoch: 3 step: 761, loss is 0.6358389258384705\n",
      "epoch: 3 step: 762, loss is 0.27701041102409363\n",
      "epoch: 3 step: 763, loss is 0.5104948878288269\n",
      "epoch: 3 step: 764, loss is 0.6437565684318542\n",
      "epoch: 3 step: 765, loss is 0.9055292010307312\n",
      "epoch: 3 step: 766, loss is 0.5708948969841003\n",
      "epoch: 3 step: 767, loss is 0.7159653902053833\n",
      "epoch: 3 step: 768, loss is 0.4695681929588318\n",
      "epoch: 3 step: 769, loss is 0.5202420353889465\n",
      "epoch: 3 step: 770, loss is 0.4425075650215149\n",
      "epoch: 3 step: 771, loss is 0.5396571755409241\n",
      "epoch: 3 step: 772, loss is 0.5917338728904724\n",
      "epoch: 3 step: 773, loss is 0.9405456781387329\n",
      "epoch: 3 step: 774, loss is 0.9377361536026001\n",
      "epoch: 3 step: 775, loss is 0.4178329408168793\n",
      "epoch: 3 step: 776, loss is 0.4331933856010437\n",
      "epoch: 3 step: 777, loss is 0.7363516688346863\n",
      "epoch: 3 step: 778, loss is 0.7857504487037659\n",
      "epoch: 3 step: 779, loss is 0.5365709066390991\n",
      "epoch: 3 step: 780, loss is 0.6348139047622681\n",
      "epoch: 3 step: 781, loss is 0.5004823207855225\n",
      "epoch: 3 step: 782, loss is 0.493440181016922\n",
      "epoch: 3 step: 783, loss is 0.46971508860588074\n",
      "epoch: 3 step: 784, loss is 0.318968802690506\n",
      "epoch: 3 step: 785, loss is 0.6474390029907227\n",
      "epoch: 3 step: 786, loss is 0.5263676643371582\n",
      "epoch: 3 step: 787, loss is 0.6623857021331787\n",
      "epoch: 3 step: 788, loss is 0.6842710375785828\n",
      "epoch: 3 step: 789, loss is 0.7478922605514526\n",
      "epoch: 3 step: 790, loss is 0.5904383063316345\n",
      "epoch: 3 step: 791, loss is 0.4235171973705292\n",
      "epoch: 3 step: 792, loss is 0.5323696732521057\n",
      "epoch: 3 step: 793, loss is 0.5679150819778442\n",
      "epoch: 3 step: 794, loss is 0.7222586870193481\n",
      "epoch: 3 step: 795, loss is 0.633554220199585\n",
      "epoch: 3 step: 796, loss is 0.5983575582504272\n",
      "epoch: 3 step: 797, loss is 0.7076033353805542\n",
      "epoch: 3 step: 798, loss is 0.845481276512146\n",
      "epoch: 3 step: 799, loss is 0.4466744661331177\n",
      "epoch: 3 step: 800, loss is 0.5814601182937622\n",
      "epoch: 3 step: 801, loss is 0.4236021935939789\n",
      "epoch: 3 step: 802, loss is 0.6120805144309998\n",
      "epoch: 3 step: 803, loss is 0.6585082411766052\n",
      "epoch: 3 step: 804, loss is 0.5065730810165405\n",
      "epoch: 3 step: 805, loss is 0.608163595199585\n",
      "epoch: 3 step: 806, loss is 0.4439767599105835\n",
      "epoch: 3 step: 807, loss is 0.8127850890159607\n",
      "epoch: 3 step: 808, loss is 0.7622658610343933\n",
      "epoch: 3 step: 809, loss is 0.8625560402870178\n",
      "epoch: 3 step: 810, loss is 0.683131754398346\n",
      "epoch: 3 step: 811, loss is 0.6756147742271423\n",
      "epoch: 3 step: 812, loss is 0.6177372336387634\n",
      "epoch: 3 step: 813, loss is 0.900775671005249\n",
      "epoch: 3 step: 814, loss is 0.32887375354766846\n",
      "epoch: 3 step: 815, loss is 0.7040998339653015\n",
      "epoch: 3 step: 816, loss is 0.7803420424461365\n",
      "epoch: 3 step: 817, loss is 0.6380199790000916\n",
      "epoch: 3 step: 818, loss is 0.558117687702179\n",
      "epoch: 3 step: 819, loss is 0.6833731532096863\n",
      "epoch: 3 step: 820, loss is 0.3996640741825104\n",
      "epoch: 3 step: 821, loss is 0.5350300669670105\n",
      "epoch: 3 step: 822, loss is 0.5471504926681519\n",
      "epoch: 3 step: 823, loss is 0.41637086868286133\n",
      "epoch: 3 step: 824, loss is 0.40896645188331604\n",
      "epoch: 3 step: 825, loss is 0.7011873126029968\n",
      "epoch: 3 step: 826, loss is 0.5756121873855591\n",
      "epoch: 3 step: 827, loss is 0.8316517472267151\n",
      "epoch: 3 step: 828, loss is 0.605869710445404\n",
      "epoch: 3 step: 829, loss is 0.5173446536064148\n",
      "epoch: 3 step: 830, loss is 0.4358254373073578\n",
      "epoch: 3 step: 831, loss is 0.5296978950500488\n",
      "epoch: 3 step: 832, loss is 0.381125271320343\n",
      "epoch: 3 step: 833, loss is 0.4240320324897766\n",
      "epoch: 3 step: 834, loss is 0.7193845510482788\n",
      "epoch: 3 step: 835, loss is 0.6533045768737793\n",
      "epoch: 3 step: 836, loss is 0.5141832828521729\n",
      "epoch: 3 step: 837, loss is 0.5184106826782227\n",
      "epoch: 3 step: 838, loss is 0.8006110191345215\n",
      "epoch: 3 step: 839, loss is 0.4025072157382965\n",
      "epoch: 3 step: 840, loss is 0.5865073204040527\n",
      "epoch: 3 step: 841, loss is 0.5410140156745911\n",
      "epoch: 3 step: 842, loss is 0.7622442245483398\n",
      "epoch: 3 step: 843, loss is 0.48952239751815796\n",
      "epoch: 3 step: 844, loss is 1.0566741228103638\n",
      "epoch: 3 step: 845, loss is 0.5723659992218018\n",
      "epoch: 3 step: 846, loss is 0.635135293006897\n",
      "epoch: 3 step: 847, loss is 0.797287106513977\n",
      "epoch: 3 step: 848, loss is 0.37484878301620483\n",
      "epoch: 3 step: 849, loss is 0.5166661143302917\n",
      "epoch: 3 step: 850, loss is 0.3514796495437622\n",
      "epoch: 3 step: 851, loss is 0.6463913917541504\n",
      "epoch: 3 step: 852, loss is 0.4935525953769684\n",
      "epoch: 3 step: 853, loss is 0.7381355166435242\n",
      "epoch: 3 step: 854, loss is 0.7327695488929749\n",
      "epoch: 3 step: 855, loss is 0.6521353721618652\n",
      "epoch: 3 step: 856, loss is 0.574263334274292\n",
      "epoch: 3 step: 857, loss is 0.4521614611148834\n",
      "epoch: 3 step: 858, loss is 0.5518501996994019\n",
      "epoch: 3 step: 859, loss is 0.6245326399803162\n",
      "epoch: 3 step: 860, loss is 0.5903693437576294\n",
      "epoch: 3 step: 861, loss is 1.1871756315231323\n",
      "epoch: 3 step: 862, loss is 0.6616947054862976\n",
      "epoch: 3 step: 863, loss is 0.3557915687561035\n",
      "epoch: 3 step: 864, loss is 0.8516414761543274\n",
      "epoch: 3 step: 865, loss is 0.5377674102783203\n",
      "epoch: 3 step: 866, loss is 0.5975521206855774\n",
      "epoch: 3 step: 867, loss is 0.5721566677093506\n",
      "epoch: 3 step: 868, loss is 0.37559276819229126\n",
      "epoch: 3 step: 869, loss is 0.5182358026504517\n",
      "epoch: 3 step: 870, loss is 0.4743083119392395\n",
      "epoch: 3 step: 871, loss is 0.549446702003479\n",
      "epoch: 3 step: 872, loss is 0.417481929063797\n",
      "epoch: 3 step: 873, loss is 0.5275378823280334\n",
      "epoch: 3 step: 874, loss is 0.9593662023544312\n",
      "epoch: 3 step: 875, loss is 0.507224977016449\n",
      "epoch: 3 step: 876, loss is 0.5165318846702576\n",
      "epoch: 3 step: 877, loss is 0.5710365772247314\n",
      "epoch: 3 step: 878, loss is 0.5686088800430298\n",
      "epoch: 3 step: 879, loss is 0.6066429018974304\n",
      "epoch: 3 step: 880, loss is 0.5146442651748657\n",
      "epoch: 3 step: 881, loss is 0.6226813793182373\n",
      "epoch: 3 step: 882, loss is 0.5886409282684326\n",
      "epoch: 3 step: 883, loss is 0.5791890621185303\n",
      "epoch: 3 step: 884, loss is 0.2830769121646881\n",
      "epoch: 3 step: 885, loss is 0.4579606056213379\n",
      "epoch: 3 step: 886, loss is 0.609858512878418\n",
      "epoch: 3 step: 887, loss is 0.45799708366394043\n",
      "epoch: 3 step: 888, loss is 0.8497346639633179\n",
      "epoch: 3 step: 889, loss is 0.4427137076854706\n",
      "epoch: 3 step: 890, loss is 0.604369580745697\n",
      "epoch: 3 step: 891, loss is 0.6743323802947998\n",
      "epoch: 3 step: 892, loss is 0.5743122696876526\n",
      "epoch: 3 step: 893, loss is 0.6209772229194641\n",
      "epoch: 3 step: 894, loss is 0.7201142907142639\n",
      "epoch: 3 step: 895, loss is 0.29029613733291626\n",
      "epoch: 3 step: 896, loss is 0.298468679189682\n",
      "epoch: 3 step: 897, loss is 0.46527913212776184\n",
      "epoch: 3 step: 898, loss is 0.457962304353714\n",
      "epoch: 3 step: 899, loss is 0.7922635078430176\n",
      "epoch: 3 step: 900, loss is 0.34324851632118225\n",
      "epoch: 3 step: 901, loss is 1.2574970722198486\n",
      "epoch: 3 step: 902, loss is 0.4325205385684967\n",
      "epoch: 3 step: 903, loss is 0.49023425579071045\n",
      "epoch: 3 step: 904, loss is 0.500167727470398\n",
      "epoch: 3 step: 905, loss is 0.7469975352287292\n",
      "epoch: 3 step: 906, loss is 0.7600588202476501\n",
      "epoch: 3 step: 907, loss is 0.5876675844192505\n",
      "epoch: 3 step: 908, loss is 0.4666745662689209\n",
      "epoch: 3 step: 909, loss is 0.2826755940914154\n",
      "epoch: 3 step: 910, loss is 0.5522974133491516\n",
      "epoch: 3 step: 911, loss is 0.34379786252975464\n",
      "epoch: 3 step: 912, loss is 0.471820592880249\n",
      "epoch: 3 step: 913, loss is 0.7563937306404114\n",
      "epoch: 3 step: 914, loss is 0.4790121912956238\n",
      "epoch: 3 step: 915, loss is 0.8914805054664612\n",
      "epoch: 3 step: 916, loss is 0.5178132653236389\n",
      "epoch: 3 step: 917, loss is 0.40356045961380005\n",
      "epoch: 3 step: 918, loss is 0.6036733388900757\n",
      "epoch: 3 step: 919, loss is 0.6813780069351196\n",
      "epoch: 3 step: 920, loss is 0.5124258995056152\n",
      "epoch: 3 step: 921, loss is 0.5304327011108398\n",
      "epoch: 3 step: 922, loss is 0.5580840110778809\n",
      "epoch: 3 step: 923, loss is 0.5655516982078552\n",
      "epoch: 3 step: 924, loss is 0.3663601577281952\n",
      "epoch: 3 step: 925, loss is 0.7548853754997253\n",
      "epoch: 3 step: 926, loss is 0.6141849160194397\n",
      "epoch: 3 step: 927, loss is 0.588814377784729\n",
      "epoch: 3 step: 928, loss is 0.6634561419487\n",
      "epoch: 3 step: 929, loss is 0.48107630014419556\n",
      "epoch: 3 step: 930, loss is 0.4993637204170227\n",
      "epoch: 3 step: 931, loss is 0.5461716055870056\n",
      "epoch: 3 step: 932, loss is 0.48179110884666443\n",
      "epoch: 3 step: 933, loss is 0.688675582408905\n",
      "epoch: 3 step: 934, loss is 0.6480870246887207\n",
      "epoch: 3 step: 935, loss is 0.6183435916900635\n",
      "epoch: 3 step: 936, loss is 0.9351629018783569\n",
      "epoch: 3 step: 937, loss is 0.4682192802429199\n",
      "epoch: 3 step: 938, loss is 0.5179910659790039\n",
      "epoch: 3 step: 939, loss is 0.8460551500320435\n",
      "epoch: 3 step: 940, loss is 0.47368529438972473\n",
      "epoch: 3 step: 941, loss is 1.0322504043579102\n",
      "epoch: 3 step: 942, loss is 0.5851874947547913\n",
      "epoch: 3 step: 943, loss is 0.44354069232940674\n",
      "epoch: 3 step: 944, loss is 0.5994980335235596\n",
      "epoch: 3 step: 945, loss is 0.46544909477233887\n",
      "epoch: 3 step: 946, loss is 0.6305736303329468\n",
      "epoch: 3 step: 947, loss is 0.4580962061882019\n",
      "epoch: 3 step: 948, loss is 0.7792585492134094\n",
      "epoch: 3 step: 949, loss is 0.47234177589416504\n",
      "epoch: 3 step: 950, loss is 0.6363160014152527\n",
      "epoch: 3 step: 951, loss is 0.6259190440177917\n",
      "epoch: 3 step: 952, loss is 0.6442407965660095\n",
      "epoch: 3 step: 953, loss is 0.6206530332565308\n",
      "epoch: 3 step: 954, loss is 0.5475409030914307\n",
      "epoch: 3 step: 955, loss is 0.38632556796073914\n",
      "epoch: 3 step: 956, loss is 0.5224985480308533\n",
      "epoch: 3 step: 957, loss is 0.5848768353462219\n",
      "epoch: 3 step: 958, loss is 0.5021089315414429\n",
      "epoch: 3 step: 959, loss is 0.47784459590911865\n",
      "epoch: 3 step: 960, loss is 0.6505343317985535\n",
      "epoch: 3 step: 961, loss is 0.31542783975601196\n",
      "epoch: 3 step: 962, loss is 0.3533107042312622\n",
      "epoch: 3 step: 963, loss is 0.4699241816997528\n",
      "epoch: 3 step: 964, loss is 0.4005887508392334\n",
      "epoch: 3 step: 965, loss is 0.686021089553833\n",
      "epoch: 3 step: 966, loss is 0.46453145146369934\n",
      "epoch: 3 step: 967, loss is 0.405146986246109\n",
      "epoch: 3 step: 968, loss is 0.3728845417499542\n",
      "epoch: 3 step: 969, loss is 0.488515704870224\n",
      "epoch: 3 step: 970, loss is 0.43308377265930176\n",
      "epoch: 3 step: 971, loss is 0.5315537452697754\n",
      "epoch: 3 step: 972, loss is 0.3976902663707733\n",
      "epoch: 3 step: 973, loss is 0.6881535053253174\n",
      "epoch: 3 step: 974, loss is 0.6280925869941711\n",
      "epoch: 3 step: 975, loss is 0.6259482502937317\n",
      "epoch: 3 step: 976, loss is 0.33848828077316284\n",
      "epoch: 3 step: 977, loss is 0.3791588246822357\n",
      "epoch: 3 step: 978, loss is 0.640130877494812\n",
      "epoch: 3 step: 979, loss is 0.29011642932891846\n",
      "epoch: 3 step: 980, loss is 0.8711050152778625\n",
      "epoch: 3 step: 981, loss is 0.9200326204299927\n",
      "epoch: 3 step: 982, loss is 0.7235075831413269\n",
      "epoch: 3 step: 983, loss is 0.20961715281009674\n",
      "epoch: 3 step: 984, loss is 1.524627447128296\n",
      "epoch: 3 step: 985, loss is 0.8013986945152283\n",
      "epoch: 3 step: 986, loss is 0.5998407602310181\n",
      "epoch: 3 step: 987, loss is 0.4719204902648926\n",
      "epoch: 3 step: 988, loss is 0.5915049314498901\n",
      "epoch: 3 step: 989, loss is 0.4936475157737732\n",
      "epoch: 3 step: 990, loss is 0.3930833339691162\n",
      "epoch: 3 step: 991, loss is 1.0964348316192627\n",
      "epoch: 3 step: 992, loss is 0.6802822351455688\n",
      "epoch: 3 step: 993, loss is 0.9745844602584839\n",
      "epoch: 3 step: 994, loss is 0.40150579810142517\n",
      "epoch: 3 step: 995, loss is 0.47851184010505676\n",
      "epoch: 3 step: 996, loss is 0.5167015194892883\n",
      "epoch: 3 step: 997, loss is 0.48107096552848816\n",
      "epoch: 3 step: 998, loss is 0.4757893979549408\n",
      "epoch: 3 step: 999, loss is 0.3710063397884369\n",
      "epoch: 3 step: 1000, loss is 0.944257378578186\n",
      "epoch: 3 step: 1001, loss is 0.40212520956993103\n",
      "epoch: 3 step: 1002, loss is 0.469739705324173\n",
      "epoch: 3 step: 1003, loss is 0.9120930433273315\n",
      "epoch: 3 step: 1004, loss is 0.8452906608581543\n",
      "epoch: 3 step: 1005, loss is 0.4090178608894348\n",
      "epoch: 3 step: 1006, loss is 0.6727127432823181\n",
      "epoch: 3 step: 1007, loss is 0.7136584520339966\n",
      "epoch: 3 step: 1008, loss is 1.0406723022460938\n",
      "epoch: 3 step: 1009, loss is 0.7386139631271362\n",
      "epoch: 3 step: 1010, loss is 0.4866485595703125\n",
      "epoch: 3 step: 1011, loss is 0.5378658771514893\n",
      "epoch: 3 step: 1012, loss is 0.6494532227516174\n",
      "epoch: 3 step: 1013, loss is 0.8734371066093445\n",
      "epoch: 3 step: 1014, loss is 0.7742766737937927\n",
      "epoch: 3 step: 1015, loss is 0.6006062626838684\n",
      "epoch: 3 step: 1016, loss is 0.6708017587661743\n",
      "epoch: 3 step: 1017, loss is 0.6380863189697266\n",
      "epoch: 3 step: 1018, loss is 0.7429726123809814\n",
      "epoch: 3 step: 1019, loss is 0.5874214172363281\n",
      "epoch: 3 step: 1020, loss is 0.55101478099823\n",
      "epoch: 3 step: 1021, loss is 0.52461838722229\n",
      "epoch: 3 step: 1022, loss is 0.7390467524528503\n",
      "epoch: 3 step: 1023, loss is 0.8707761764526367\n",
      "epoch: 3 step: 1024, loss is 0.6637381911277771\n",
      "epoch: 3 step: 1025, loss is 0.40354713797569275\n",
      "epoch: 3 step: 1026, loss is 0.4484266936779022\n",
      "epoch: 3 step: 1027, loss is 0.6033457517623901\n",
      "epoch: 3 step: 1028, loss is 0.76994389295578\n",
      "epoch: 3 step: 1029, loss is 0.44545653462409973\n",
      "epoch: 3 step: 1030, loss is 0.4770151972770691\n",
      "epoch: 3 step: 1031, loss is 0.41118770837783813\n",
      "epoch: 3 step: 1032, loss is 0.7167182564735413\n",
      "epoch: 3 step: 1033, loss is 0.8925129771232605\n",
      "epoch: 3 step: 1034, loss is 0.49434998631477356\n",
      "epoch: 3 step: 1035, loss is 0.4782106578350067\n",
      "epoch: 3 step: 1036, loss is 0.45161160826683044\n",
      "epoch: 3 step: 1037, loss is 0.733811616897583\n",
      "epoch: 3 step: 1038, loss is 0.7125588655471802\n",
      "epoch: 3 step: 1039, loss is 0.4073842167854309\n",
      "epoch: 3 step: 1040, loss is 0.4469056725502014\n",
      "epoch: 3 step: 1041, loss is 0.42949455976486206\n",
      "epoch: 3 step: 1042, loss is 0.6790143251419067\n",
      "epoch: 3 step: 1043, loss is 0.4722852408885956\n",
      "epoch: 3 step: 1044, loss is 0.7368870377540588\n",
      "epoch: 3 step: 1045, loss is 0.5563982725143433\n",
      "epoch: 3 step: 1046, loss is 0.4487411379814148\n",
      "epoch: 3 step: 1047, loss is 0.5849631428718567\n",
      "epoch: 3 step: 1048, loss is 0.6920914053916931\n",
      "epoch: 3 step: 1049, loss is 0.33118945360183716\n",
      "epoch: 3 step: 1050, loss is 0.42694947123527527\n",
      "epoch: 3 step: 1051, loss is 0.47533246874809265\n",
      "epoch: 3 step: 1052, loss is 0.8537254333496094\n",
      "epoch: 3 step: 1053, loss is 0.40793853998184204\n",
      "epoch: 3 step: 1054, loss is 0.477013498544693\n",
      "epoch: 3 step: 1055, loss is 0.6677597761154175\n",
      "epoch: 3 step: 1056, loss is 0.5148976445198059\n",
      "epoch: 3 step: 1057, loss is 0.5499338507652283\n",
      "epoch: 3 step: 1058, loss is 0.6044121980667114\n",
      "epoch: 3 step: 1059, loss is 0.3388564884662628\n",
      "epoch: 3 step: 1060, loss is 0.5261527299880981\n",
      "epoch: 3 step: 1061, loss is 0.6315826177597046\n",
      "epoch: 3 step: 1062, loss is 0.37636056542396545\n",
      "epoch: 3 step: 1063, loss is 0.38870328664779663\n",
      "epoch: 3 step: 1064, loss is 0.7252265214920044\n",
      "epoch: 3 step: 1065, loss is 0.3414589464664459\n",
      "epoch: 3 step: 1066, loss is 0.7944093942642212\n",
      "epoch: 3 step: 1067, loss is 0.4580139219760895\n",
      "epoch: 3 step: 1068, loss is 0.6646018624305725\n",
      "epoch: 3 step: 1069, loss is 0.42855358123779297\n",
      "epoch: 3 step: 1070, loss is 0.7866579294204712\n",
      "epoch: 3 step: 1071, loss is 0.5120217204093933\n",
      "epoch: 3 step: 1072, loss is 0.5091276168823242\n",
      "epoch: 3 step: 1073, loss is 0.3402511775493622\n",
      "epoch: 3 step: 1074, loss is 0.5023988485336304\n",
      "epoch: 3 step: 1075, loss is 0.4379022419452667\n",
      "epoch: 3 step: 1076, loss is 0.5602399110794067\n",
      "epoch: 3 step: 1077, loss is 0.6030755043029785\n",
      "epoch: 3 step: 1078, loss is 0.22209404408931732\n",
      "epoch: 3 step: 1079, loss is 0.46301332116127014\n",
      "epoch: 3 step: 1080, loss is 0.8421751260757446\n",
      "epoch: 3 step: 1081, loss is 0.5804211497306824\n",
      "epoch: 3 step: 1082, loss is 1.2820993661880493\n",
      "epoch: 3 step: 1083, loss is 0.5806238055229187\n",
      "epoch: 3 step: 1084, loss is 0.577815055847168\n",
      "epoch: 3 step: 1085, loss is 0.20454908907413483\n",
      "epoch: 3 step: 1086, loss is 0.44849729537963867\n",
      "epoch: 3 step: 1087, loss is 0.4634872078895569\n",
      "epoch: 3 step: 1088, loss is 0.8957082629203796\n",
      "epoch: 3 step: 1089, loss is 0.4882640242576599\n",
      "epoch: 3 step: 1090, loss is 0.5977100729942322\n",
      "epoch: 3 step: 1091, loss is 0.29936477541923523\n",
      "epoch: 3 step: 1092, loss is 0.8513607382774353\n",
      "epoch: 3 step: 1093, loss is 0.6122393608093262\n",
      "epoch: 3 step: 1094, loss is 0.6599864363670349\n",
      "epoch: 3 step: 1095, loss is 1.0066616535186768\n",
      "epoch: 3 step: 1096, loss is 0.5046226978302002\n",
      "epoch: 3 step: 1097, loss is 0.7828981876373291\n",
      "epoch: 3 step: 1098, loss is 0.5582391023635864\n",
      "epoch: 3 step: 1099, loss is 0.22178106009960175\n",
      "epoch: 3 step: 1100, loss is 0.39961889386177063\n",
      "epoch: 3 step: 1101, loss is 0.4558734893798828\n",
      "epoch: 3 step: 1102, loss is 0.7662531733512878\n",
      "epoch: 3 step: 1103, loss is 0.5098490118980408\n",
      "epoch: 3 step: 1104, loss is 0.44824540615081787\n",
      "epoch: 3 step: 1105, loss is 0.4628630578517914\n",
      "epoch: 3 step: 1106, loss is 0.6539427042007446\n",
      "epoch: 3 step: 1107, loss is 0.6593490242958069\n",
      "epoch: 3 step: 1108, loss is 0.565339982509613\n",
      "epoch: 3 step: 1109, loss is 0.6246418952941895\n",
      "epoch: 3 step: 1110, loss is 0.7141757607460022\n",
      "epoch: 3 step: 1111, loss is 0.3947165012359619\n",
      "epoch: 3 step: 1112, loss is 0.7603345513343811\n",
      "epoch: 3 step: 1113, loss is 0.42588508129119873\n",
      "epoch: 3 step: 1114, loss is 0.9605841636657715\n",
      "epoch: 3 step: 1115, loss is 0.3396986126899719\n",
      "epoch: 3 step: 1116, loss is 0.6237257122993469\n",
      "epoch: 3 step: 1117, loss is 0.6469398736953735\n",
      "epoch: 3 step: 1118, loss is 0.656585156917572\n",
      "epoch: 3 step: 1119, loss is 0.47439664602279663\n",
      "epoch: 3 step: 1120, loss is 0.6621730923652649\n",
      "epoch: 3 step: 1121, loss is 0.7324112057685852\n",
      "epoch: 3 step: 1122, loss is 0.7333688139915466\n",
      "epoch: 3 step: 1123, loss is 0.5105313062667847\n",
      "epoch: 3 step: 1124, loss is 0.5188639760017395\n",
      "epoch: 3 step: 1125, loss is 0.5414762496948242\n",
      "epoch: 3 step: 1126, loss is 0.42789652943611145\n",
      "epoch: 3 step: 1127, loss is 0.7071328163146973\n",
      "epoch: 3 step: 1128, loss is 0.5888038873672485\n",
      "epoch: 3 step: 1129, loss is 0.5897613167762756\n",
      "epoch: 3 step: 1130, loss is 0.47329556941986084\n",
      "epoch: 3 step: 1131, loss is 0.5074142813682556\n",
      "epoch: 3 step: 1132, loss is 0.5210655927658081\n",
      "epoch: 3 step: 1133, loss is 0.4220152795314789\n",
      "epoch: 3 step: 1134, loss is 0.5414412021636963\n",
      "epoch: 3 step: 1135, loss is 0.45395413041114807\n",
      "epoch: 3 step: 1136, loss is 0.7426201701164246\n",
      "epoch: 3 step: 1137, loss is 0.47017958760261536\n",
      "epoch: 3 step: 1138, loss is 0.7836328148841858\n",
      "epoch: 3 step: 1139, loss is 0.8185933828353882\n",
      "epoch: 3 step: 1140, loss is 0.4124685525894165\n",
      "epoch: 3 step: 1141, loss is 0.65771484375\n",
      "epoch: 3 step: 1142, loss is 0.6125596761703491\n",
      "epoch: 3 step: 1143, loss is 0.4128161370754242\n",
      "epoch: 3 step: 1144, loss is 0.3876328468322754\n",
      "epoch: 3 step: 1145, loss is 0.6078994870185852\n",
      "epoch: 3 step: 1146, loss is 0.6319777369499207\n",
      "epoch: 3 step: 1147, loss is 0.5217514634132385\n",
      "epoch: 3 step: 1148, loss is 0.6447834372520447\n",
      "epoch: 3 step: 1149, loss is 0.8634348511695862\n",
      "epoch: 3 step: 1150, loss is 0.5590131878852844\n",
      "epoch: 3 step: 1151, loss is 0.525169849395752\n",
      "epoch: 3 step: 1152, loss is 0.6730507016181946\n",
      "epoch: 3 step: 1153, loss is 0.3400811553001404\n",
      "epoch: 3 step: 1154, loss is 0.3045840859413147\n",
      "epoch: 3 step: 1155, loss is 0.5753976106643677\n",
      "epoch: 3 step: 1156, loss is 0.4533079266548157\n",
      "epoch: 3 step: 1157, loss is 0.5531558394432068\n",
      "epoch: 3 step: 1158, loss is 0.6426645517349243\n",
      "epoch: 3 step: 1159, loss is 0.2953885495662689\n",
      "epoch: 3 step: 1160, loss is 0.6451001167297363\n",
      "epoch: 3 step: 1161, loss is 0.4418751001358032\n",
      "epoch: 3 step: 1162, loss is 0.45818212628364563\n",
      "epoch: 3 step: 1163, loss is 0.5383090376853943\n",
      "epoch: 3 step: 1164, loss is 0.6912754774093628\n",
      "epoch: 3 step: 1165, loss is 0.3303515315055847\n",
      "epoch: 3 step: 1166, loss is 0.8259558081626892\n",
      "epoch: 3 step: 1167, loss is 0.3899286985397339\n",
      "epoch: 3 step: 1168, loss is 0.8520680069923401\n",
      "epoch: 3 step: 1169, loss is 0.33642318844795227\n",
      "epoch: 3 step: 1170, loss is 0.53717041015625\n",
      "epoch: 3 step: 1171, loss is 0.32740551233291626\n",
      "epoch: 3 step: 1172, loss is 0.8876628875732422\n",
      "epoch: 3 step: 1173, loss is 0.6510139107704163\n",
      "epoch: 3 step: 1174, loss is 0.33483847975730896\n",
      "epoch: 3 step: 1175, loss is 0.6231629848480225\n",
      "epoch: 3 step: 1176, loss is 0.6436973214149475\n",
      "epoch: 3 step: 1177, loss is 0.49542170763015747\n",
      "epoch: 3 step: 1178, loss is 0.7285938858985901\n",
      "epoch: 3 step: 1179, loss is 0.33509761095046997\n",
      "epoch: 3 step: 1180, loss is 1.1848971843719482\n",
      "epoch: 3 step: 1181, loss is 0.6504487991333008\n",
      "epoch: 3 step: 1182, loss is 0.4730086624622345\n",
      "epoch: 3 step: 1183, loss is 0.4748019874095917\n",
      "epoch: 3 step: 1184, loss is 0.6248539686203003\n",
      "epoch: 3 step: 1185, loss is 0.6106411218643188\n",
      "epoch: 3 step: 1186, loss is 1.0318704843521118\n",
      "epoch: 3 step: 1187, loss is 0.5966158509254456\n",
      "epoch: 3 step: 1188, loss is 0.623496413230896\n",
      "epoch: 3 step: 1189, loss is 0.4375121295452118\n",
      "epoch: 3 step: 1190, loss is 0.46214956045150757\n",
      "epoch: 3 step: 1191, loss is 0.563320517539978\n",
      "epoch: 3 step: 1192, loss is 0.31724298000335693\n",
      "epoch: 3 step: 1193, loss is 1.0100609064102173\n",
      "epoch: 3 step: 1194, loss is 0.5233854651451111\n",
      "epoch: 3 step: 1195, loss is 0.5010862946510315\n",
      "epoch: 3 step: 1196, loss is 0.6209824681282043\n",
      "epoch: 3 step: 1197, loss is 0.5161670446395874\n",
      "epoch: 3 step: 1198, loss is 0.3717224895954132\n",
      "epoch: 3 step: 1199, loss is 0.559563159942627\n",
      "epoch: 3 step: 1200, loss is 0.9317365884780884\n",
      "epoch: 3 step: 1201, loss is 0.5375821590423584\n",
      "epoch: 3 step: 1202, loss is 0.6746304035186768\n",
      "epoch: 3 step: 1203, loss is 0.5396074056625366\n",
      "epoch: 3 step: 1204, loss is 0.6197174787521362\n",
      "epoch: 3 step: 1205, loss is 0.636476457118988\n",
      "epoch: 3 step: 1206, loss is 0.49129652976989746\n",
      "epoch: 3 step: 1207, loss is 0.6792775392532349\n",
      "epoch: 3 step: 1208, loss is 0.31665194034576416\n",
      "epoch: 3 step: 1209, loss is 0.4306054413318634\n",
      "epoch: 3 step: 1210, loss is 0.45753076672554016\n",
      "epoch: 3 step: 1211, loss is 0.41423460841178894\n",
      "epoch: 3 step: 1212, loss is 0.8275542855262756\n",
      "epoch: 3 step: 1213, loss is 0.5388906598091125\n",
      "epoch: 3 step: 1214, loss is 0.5377354621887207\n",
      "epoch: 3 step: 1215, loss is 0.669060230255127\n",
      "epoch: 3 step: 1216, loss is 0.7159498929977417\n",
      "epoch: 3 step: 1217, loss is 0.52527916431427\n",
      "epoch: 3 step: 1218, loss is 0.7997552156448364\n",
      "epoch: 3 step: 1219, loss is 0.7321664094924927\n",
      "epoch: 3 step: 1220, loss is 0.462763249874115\n",
      "epoch: 3 step: 1221, loss is 0.40695422887802124\n",
      "epoch: 3 step: 1222, loss is 0.598382294178009\n",
      "epoch: 3 step: 1223, loss is 0.45996373891830444\n",
      "epoch: 3 step: 1224, loss is 0.38814711570739746\n",
      "epoch: 3 step: 1225, loss is 0.6029268503189087\n",
      "epoch: 3 step: 1226, loss is 0.7522233724594116\n",
      "epoch: 3 step: 1227, loss is 0.6028026938438416\n",
      "epoch: 3 step: 1228, loss is 1.0885512828826904\n",
      "epoch: 3 step: 1229, loss is 1.020176887512207\n",
      "epoch: 3 step: 1230, loss is 0.7609076499938965\n",
      "epoch: 3 step: 1231, loss is 0.4520382583141327\n",
      "epoch: 3 step: 1232, loss is 0.3259308934211731\n",
      "epoch: 3 step: 1233, loss is 0.4823338985443115\n",
      "epoch: 3 step: 1234, loss is 0.9136719703674316\n",
      "epoch: 3 step: 1235, loss is 0.7184786796569824\n",
      "epoch: 3 step: 1236, loss is 0.5279560685157776\n",
      "epoch: 3 step: 1237, loss is 1.0489239692687988\n",
      "epoch: 3 step: 1238, loss is 0.6739742159843445\n",
      "epoch: 3 step: 1239, loss is 0.7425437569618225\n",
      "epoch: 3 step: 1240, loss is 0.6332000494003296\n",
      "epoch: 3 step: 1241, loss is 0.5482652187347412\n",
      "epoch: 3 step: 1242, loss is 0.8955324292182922\n",
      "epoch: 3 step: 1243, loss is 0.40314167737960815\n",
      "epoch: 3 step: 1244, loss is 0.4839588403701782\n",
      "epoch: 3 step: 1245, loss is 0.37031054496765137\n",
      "epoch: 3 step: 1246, loss is 0.6242319941520691\n",
      "epoch: 3 step: 1247, loss is 0.5527055263519287\n",
      "epoch: 3 step: 1248, loss is 0.7554026246070862\n",
      "epoch: 3 step: 1249, loss is 0.40723690390586853\n",
      "epoch: 3 step: 1250, loss is 0.38508135080337524\n",
      "epoch: 3 step: 1251, loss is 0.6458684802055359\n",
      "epoch: 3 step: 1252, loss is 0.5840757489204407\n",
      "epoch: 3 step: 1253, loss is 0.4425407648086548\n",
      "epoch: 3 step: 1254, loss is 0.3268602192401886\n",
      "epoch: 3 step: 1255, loss is 0.7330860495567322\n",
      "epoch: 3 step: 1256, loss is 0.39862486720085144\n",
      "epoch: 3 step: 1257, loss is 0.7039300203323364\n",
      "epoch: 3 step: 1258, loss is 0.5958178043365479\n",
      "epoch: 3 step: 1259, loss is 0.447185754776001\n",
      "epoch: 3 step: 1260, loss is 0.20775696635246277\n",
      "epoch: 3 step: 1261, loss is 0.5608238577842712\n",
      "epoch: 3 step: 1262, loss is 0.49187156558036804\n",
      "epoch: 3 step: 1263, loss is 0.590429425239563\n",
      "epoch: 3 step: 1264, loss is 0.49565741419792175\n",
      "epoch: 3 step: 1265, loss is 0.5815727710723877\n",
      "epoch: 3 step: 1266, loss is 0.44413694739341736\n",
      "epoch: 3 step: 1267, loss is 0.5793425440788269\n",
      "epoch: 3 step: 1268, loss is 0.370115727186203\n",
      "epoch: 3 step: 1269, loss is 0.31498223543167114\n",
      "epoch: 3 step: 1270, loss is 0.44991886615753174\n",
      "epoch: 3 step: 1271, loss is 0.44459569454193115\n",
      "epoch: 3 step: 1272, loss is 0.3577553331851959\n",
      "epoch: 3 step: 1273, loss is 0.6870289444923401\n",
      "epoch: 3 step: 1274, loss is 0.16259817779064178\n",
      "epoch: 3 step: 1275, loss is 0.7611648440361023\n",
      "epoch: 3 step: 1276, loss is 0.25881248712539673\n",
      "epoch: 3 step: 1277, loss is 0.4700571894645691\n",
      "epoch: 3 step: 1278, loss is 0.4618713855743408\n",
      "epoch: 3 step: 1279, loss is 0.5378921627998352\n",
      "epoch: 3 step: 1280, loss is 1.1373919248580933\n",
      "epoch: 3 step: 1281, loss is 0.617791473865509\n",
      "epoch: 3 step: 1282, loss is 0.20051276683807373\n",
      "epoch: 3 step: 1283, loss is 0.4273526668548584\n",
      "epoch: 3 step: 1284, loss is 0.9226979613304138\n",
      "epoch: 3 step: 1285, loss is 0.3591107130050659\n",
      "epoch: 3 step: 1286, loss is 0.6157181262969971\n",
      "epoch: 3 step: 1287, loss is 0.41572731733322144\n",
      "epoch: 3 step: 1288, loss is 0.2919832170009613\n",
      "epoch: 3 step: 1289, loss is 0.9327160716056824\n",
      "epoch: 3 step: 1290, loss is 0.6881405115127563\n",
      "epoch: 3 step: 1291, loss is 0.7539126873016357\n",
      "epoch: 3 step: 1292, loss is 0.8266904950141907\n",
      "epoch: 3 step: 1293, loss is 0.8296523094177246\n",
      "epoch: 3 step: 1294, loss is 0.46401435136795044\n",
      "epoch: 3 step: 1295, loss is 0.5362215042114258\n",
      "epoch: 3 step: 1296, loss is 0.4357820153236389\n",
      "epoch: 3 step: 1297, loss is 0.8239328861236572\n",
      "epoch: 3 step: 1298, loss is 0.5074440240859985\n",
      "epoch: 3 step: 1299, loss is 0.8510615229606628\n",
      "epoch: 3 step: 1300, loss is 0.5294294357299805\n",
      "epoch: 3 step: 1301, loss is 0.5732208490371704\n",
      "epoch: 3 step: 1302, loss is 0.38077232241630554\n",
      "epoch: 3 step: 1303, loss is 0.4528236985206604\n",
      "epoch: 3 step: 1304, loss is 0.5770890712738037\n",
      "epoch: 3 step: 1305, loss is 0.8414746522903442\n",
      "epoch: 3 step: 1306, loss is 0.609133243560791\n",
      "epoch: 3 step: 1307, loss is 0.27691325545310974\n",
      "epoch: 3 step: 1308, loss is 0.7084127068519592\n",
      "epoch: 3 step: 1309, loss is 0.40969082713127136\n",
      "epoch: 3 step: 1310, loss is 0.43138760328292847\n",
      "epoch: 3 step: 1311, loss is 0.6801690459251404\n",
      "epoch: 3 step: 1312, loss is 0.4062712788581848\n",
      "epoch: 3 step: 1313, loss is 0.5553411841392517\n",
      "epoch: 3 step: 1314, loss is 0.2951599359512329\n",
      "epoch: 3 step: 1315, loss is 1.038122534751892\n",
      "epoch: 3 step: 1316, loss is 0.475747287273407\n",
      "epoch: 3 step: 1317, loss is 0.4099842309951782\n",
      "epoch: 3 step: 1318, loss is 0.7214938402175903\n",
      "epoch: 3 step: 1319, loss is 0.7514193058013916\n",
      "epoch: 3 step: 1320, loss is 0.7040173411369324\n",
      "epoch: 3 step: 1321, loss is 0.6181997656822205\n",
      "epoch: 3 step: 1322, loss is 0.5742353200912476\n",
      "epoch: 3 step: 1323, loss is 0.5696122050285339\n",
      "epoch: 3 step: 1324, loss is 0.374950110912323\n",
      "epoch: 3 step: 1325, loss is 0.7194259166717529\n",
      "epoch: 3 step: 1326, loss is 0.3809850811958313\n",
      "epoch: 3 step: 1327, loss is 0.40897509455680847\n",
      "epoch: 3 step: 1328, loss is 0.6866888403892517\n",
      "epoch: 3 step: 1329, loss is 0.3217276632785797\n",
      "epoch: 3 step: 1330, loss is 0.6265175938606262\n",
      "epoch: 3 step: 1331, loss is 0.7840107679367065\n",
      "epoch: 3 step: 1332, loss is 0.8602584600448608\n",
      "epoch: 3 step: 1333, loss is 0.34326595067977905\n",
      "epoch: 3 step: 1334, loss is 0.5961441993713379\n",
      "epoch: 3 step: 1335, loss is 0.36987534165382385\n",
      "epoch: 3 step: 1336, loss is 0.36980944871902466\n",
      "epoch: 3 step: 1337, loss is 0.5781424641609192\n",
      "epoch: 3 step: 1338, loss is 0.6338562369346619\n",
      "epoch: 3 step: 1339, loss is 0.6196679472923279\n",
      "epoch: 3 step: 1340, loss is 0.5346670150756836\n",
      "epoch: 3 step: 1341, loss is 0.5773627758026123\n",
      "epoch: 3 step: 1342, loss is 0.46060410141944885\n",
      "epoch: 3 step: 1343, loss is 0.7263033986091614\n",
      "epoch: 3 step: 1344, loss is 0.4211971163749695\n",
      "epoch: 3 step: 1345, loss is 0.6422053575515747\n",
      "epoch: 3 step: 1346, loss is 0.8922511339187622\n",
      "epoch: 3 step: 1347, loss is 0.5495049357414246\n",
      "epoch: 3 step: 1348, loss is 0.7282485365867615\n",
      "epoch: 3 step: 1349, loss is 0.37542033195495605\n",
      "epoch: 3 step: 1350, loss is 0.7119303941726685\n",
      "epoch: 3 step: 1351, loss is 0.4379864037036896\n",
      "epoch: 3 step: 1352, loss is 0.7584756016731262\n",
      "epoch: 3 step: 1353, loss is 0.4070720970630646\n",
      "epoch: 3 step: 1354, loss is 0.3315451741218567\n",
      "epoch: 3 step: 1355, loss is 0.6596155166625977\n",
      "epoch: 3 step: 1356, loss is 0.6722615361213684\n",
      "epoch: 3 step: 1357, loss is 0.41940200328826904\n",
      "epoch: 3 step: 1358, loss is 0.5206444263458252\n",
      "epoch: 3 step: 1359, loss is 0.34799784421920776\n",
      "epoch: 3 step: 1360, loss is 0.3835074305534363\n",
      "epoch: 3 step: 1361, loss is 0.6696919798851013\n",
      "epoch: 3 step: 1362, loss is 0.7758677005767822\n",
      "epoch: 3 step: 1363, loss is 0.5069551467895508\n",
      "epoch: 3 step: 1364, loss is 0.7458046078681946\n",
      "epoch: 3 step: 1365, loss is 0.34661898016929626\n",
      "epoch: 3 step: 1366, loss is 0.41109541058540344\n",
      "epoch: 3 step: 1367, loss is 0.6571226119995117\n",
      "epoch: 3 step: 1368, loss is 0.6054269075393677\n",
      "epoch: 3 step: 1369, loss is 0.4609180688858032\n",
      "epoch: 3 step: 1370, loss is 0.27143681049346924\n",
      "epoch: 3 step: 1371, loss is 0.5233668684959412\n",
      "epoch: 3 step: 1372, loss is 0.25061020255088806\n",
      "epoch: 3 step: 1373, loss is 0.3731921911239624\n",
      "epoch: 3 step: 1374, loss is 0.37529733777046204\n",
      "epoch: 3 step: 1375, loss is 0.5336767435073853\n",
      "epoch: 3 step: 1376, loss is 0.49735990166664124\n",
      "epoch: 3 step: 1377, loss is 0.29696983098983765\n",
      "epoch: 3 step: 1378, loss is 0.4317922592163086\n",
      "epoch: 3 step: 1379, loss is 0.5351973176002502\n",
      "epoch: 3 step: 1380, loss is 0.9392472505569458\n",
      "epoch: 3 step: 1381, loss is 0.9643124341964722\n",
      "epoch: 3 step: 1382, loss is 0.5051852464675903\n",
      "epoch: 3 step: 1383, loss is 0.7110787630081177\n",
      "epoch: 3 step: 1384, loss is 0.4799903631210327\n",
      "epoch: 3 step: 1385, loss is 0.445156067609787\n",
      "epoch: 3 step: 1386, loss is 0.5972685217857361\n",
      "epoch: 3 step: 1387, loss is 0.7330672144889832\n",
      "epoch: 3 step: 1388, loss is 0.3706824481487274\n",
      "epoch: 3 step: 1389, loss is 0.35460165143013\n",
      "epoch: 3 step: 1390, loss is 0.8634932637214661\n",
      "epoch: 3 step: 1391, loss is 0.468666672706604\n",
      "epoch: 3 step: 1392, loss is 0.5430468320846558\n",
      "epoch: 3 step: 1393, loss is 0.4316632151603699\n",
      "epoch: 3 step: 1394, loss is 0.6566479802131653\n",
      "epoch: 3 step: 1395, loss is 0.20625771582126617\n",
      "epoch: 3 step: 1396, loss is 0.4460722804069519\n",
      "epoch: 3 step: 1397, loss is 1.2954415082931519\n",
      "epoch: 3 step: 1398, loss is 0.7898931503295898\n",
      "epoch: 3 step: 1399, loss is 0.4907906949520111\n",
      "epoch: 3 step: 1400, loss is 0.5737282633781433\n",
      "epoch: 3 step: 1401, loss is 0.3394216299057007\n",
      "epoch: 3 step: 1402, loss is 0.676628053188324\n",
      "epoch: 3 step: 1403, loss is 0.5637472867965698\n",
      "epoch: 3 step: 1404, loss is 0.38417840003967285\n",
      "epoch: 3 step: 1405, loss is 0.7291689515113831\n",
      "epoch: 3 step: 1406, loss is 0.6227453351020813\n",
      "epoch: 3 step: 1407, loss is 0.47337478399276733\n",
      "epoch: 3 step: 1408, loss is 0.5457764267921448\n",
      "epoch: 3 step: 1409, loss is 0.5772708654403687\n",
      "epoch: 3 step: 1410, loss is 0.5906529426574707\n",
      "epoch: 3 step: 1411, loss is 0.3119393587112427\n",
      "epoch: 3 step: 1412, loss is 0.7500093579292297\n",
      "epoch: 3 step: 1413, loss is 0.697567880153656\n",
      "epoch: 3 step: 1414, loss is 0.480806440114975\n",
      "epoch: 3 step: 1415, loss is 0.35408106446266174\n",
      "epoch: 3 step: 1416, loss is 0.37650176882743835\n",
      "epoch: 3 step: 1417, loss is 0.3873118460178375\n",
      "epoch: 3 step: 1418, loss is 0.5886538028717041\n",
      "epoch: 3 step: 1419, loss is 0.6867939233779907\n",
      "epoch: 3 step: 1420, loss is 0.4928196966648102\n",
      "epoch: 3 step: 1421, loss is 0.33408793807029724\n",
      "epoch: 3 step: 1422, loss is 0.7415816187858582\n",
      "epoch: 3 step: 1423, loss is 0.41764795780181885\n",
      "epoch: 3 step: 1424, loss is 0.7062761783599854\n",
      "epoch: 3 step: 1425, loss is 0.41559675335884094\n",
      "epoch: 3 step: 1426, loss is 0.49942174553871155\n",
      "epoch: 3 step: 1427, loss is 0.8882067799568176\n",
      "epoch: 3 step: 1428, loss is 0.9716722965240479\n",
      "epoch: 3 step: 1429, loss is 0.39102762937545776\n",
      "epoch: 3 step: 1430, loss is 0.810630738735199\n",
      "epoch: 3 step: 1431, loss is 0.663927435874939\n",
      "epoch: 3 step: 1432, loss is 0.5379636883735657\n",
      "epoch: 3 step: 1433, loss is 0.4215233027935028\n",
      "epoch: 3 step: 1434, loss is 0.4927719533443451\n",
      "epoch: 3 step: 1435, loss is 0.4813489317893982\n",
      "epoch: 3 step: 1436, loss is 0.3242872655391693\n",
      "epoch: 3 step: 1437, loss is 0.3124812841415405\n",
      "epoch: 3 step: 1438, loss is 0.8320450186729431\n",
      "epoch: 3 step: 1439, loss is 0.5793511271476746\n",
      "epoch: 3 step: 1440, loss is 0.6625357270240784\n",
      "epoch: 3 step: 1441, loss is 0.9388415813446045\n",
      "epoch: 3 step: 1442, loss is 0.9989680051803589\n",
      "epoch: 3 step: 1443, loss is 0.5592707395553589\n",
      "epoch: 3 step: 1444, loss is 0.725226640701294\n",
      "epoch: 3 step: 1445, loss is 0.6200926899909973\n",
      "epoch: 3 step: 1446, loss is 0.23346257209777832\n",
      "epoch: 3 step: 1447, loss is 0.5336427092552185\n",
      "epoch: 3 step: 1448, loss is 0.36637356877326965\n",
      "epoch: 3 step: 1449, loss is 0.532920241355896\n",
      "epoch: 3 step: 1450, loss is 0.8412576913833618\n",
      "epoch: 3 step: 1451, loss is 0.5949324369430542\n",
      "epoch: 3 step: 1452, loss is 0.9674534797668457\n",
      "epoch: 3 step: 1453, loss is 0.4483293890953064\n",
      "epoch: 3 step: 1454, loss is 0.6454113125801086\n",
      "epoch: 3 step: 1455, loss is 0.4234050512313843\n",
      "epoch: 3 step: 1456, loss is 0.193324476480484\n",
      "epoch: 3 step: 1457, loss is 0.4789823591709137\n",
      "epoch: 3 step: 1458, loss is 0.7374760508537292\n",
      "epoch: 3 step: 1459, loss is 0.42677757143974304\n",
      "epoch: 3 step: 1460, loss is 0.6766228675842285\n",
      "epoch: 3 step: 1461, loss is 0.5287564992904663\n",
      "epoch: 3 step: 1462, loss is 0.4330365061759949\n",
      "epoch: 3 step: 1463, loss is 0.4360586702823639\n",
      "epoch: 3 step: 1464, loss is 0.5797697305679321\n",
      "epoch: 3 step: 1465, loss is 0.29447534680366516\n",
      "epoch: 3 step: 1466, loss is 0.5811124444007874\n",
      "epoch: 3 step: 1467, loss is 0.5092964172363281\n",
      "epoch: 3 step: 1468, loss is 0.8211249709129333\n",
      "epoch: 3 step: 1469, loss is 0.551501452922821\n",
      "epoch: 3 step: 1470, loss is 0.5181624889373779\n",
      "epoch: 3 step: 1471, loss is 0.8638759255409241\n",
      "epoch: 3 step: 1472, loss is 1.0504686832427979\n",
      "epoch: 3 step: 1473, loss is 0.40565603971481323\n",
      "epoch: 3 step: 1474, loss is 0.5563727021217346\n",
      "epoch: 3 step: 1475, loss is 0.638837993144989\n",
      "epoch: 3 step: 1476, loss is 0.6978709697723389\n",
      "epoch: 3 step: 1477, loss is 0.6492063999176025\n",
      "epoch: 3 step: 1478, loss is 0.6790778636932373\n",
      "epoch: 3 step: 1479, loss is 0.44942933320999146\n",
      "epoch: 3 step: 1480, loss is 0.47601133584976196\n",
      "epoch: 3 step: 1481, loss is 0.5409356951713562\n",
      "epoch: 3 step: 1482, loss is 0.4700463116168976\n",
      "epoch: 3 step: 1483, loss is 0.5145238041877747\n",
      "epoch: 3 step: 1484, loss is 0.6614431142807007\n",
      "epoch: 3 step: 1485, loss is 0.7107144594192505\n",
      "epoch: 3 step: 1486, loss is 0.5151050090789795\n",
      "epoch: 3 step: 1487, loss is 0.25720280408859253\n",
      "epoch: 3 step: 1488, loss is 0.647029459476471\n",
      "epoch: 3 step: 1489, loss is 0.41941606998443604\n",
      "epoch: 3 step: 1490, loss is 0.6034943461418152\n",
      "epoch: 3 step: 1491, loss is 0.6163588166236877\n",
      "epoch: 3 step: 1492, loss is 0.39101505279541016\n",
      "epoch: 3 step: 1493, loss is 0.7261552810668945\n",
      "epoch: 3 step: 1494, loss is 0.5439557433128357\n",
      "epoch: 3 step: 1495, loss is 0.6704403758049011\n",
      "epoch: 3 step: 1496, loss is 0.5509893894195557\n",
      "epoch: 3 step: 1497, loss is 0.371061235666275\n",
      "epoch: 3 step: 1498, loss is 0.4347735345363617\n",
      "epoch: 3 step: 1499, loss is 0.35258179903030396\n",
      "epoch: 3 step: 1500, loss is 0.4314861297607422\n",
      "epoch: 3 step: 1501, loss is 0.3929482102394104\n",
      "epoch: 3 step: 1502, loss is 0.6375945210456848\n",
      "epoch: 3 step: 1503, loss is 0.4685854911804199\n",
      "epoch: 3 step: 1504, loss is 0.5275999903678894\n",
      "epoch: 3 step: 1505, loss is 0.3861491084098816\n",
      "epoch: 3 step: 1506, loss is 0.5638312101364136\n",
      "epoch: 3 step: 1507, loss is 0.9350901246070862\n",
      "epoch: 3 step: 1508, loss is 1.069946050643921\n",
      "epoch: 3 step: 1509, loss is 0.41180628538131714\n",
      "epoch: 3 step: 1510, loss is 0.8185694217681885\n",
      "epoch: 3 step: 1511, loss is 0.7018285393714905\n",
      "epoch: 3 step: 1512, loss is 0.9331187009811401\n",
      "epoch: 3 step: 1513, loss is 0.9506275057792664\n",
      "epoch: 3 step: 1514, loss is 0.6335713267326355\n",
      "epoch: 3 step: 1515, loss is 0.7067625522613525\n",
      "epoch: 3 step: 1516, loss is 0.44794461131095886\n",
      "epoch: 3 step: 1517, loss is 0.9503034353256226\n",
      "epoch: 3 step: 1518, loss is 0.5942766666412354\n",
      "epoch: 3 step: 1519, loss is 0.7177009582519531\n",
      "epoch: 3 step: 1520, loss is 0.630888819694519\n",
      "epoch: 3 step: 1521, loss is 0.44621241092681885\n",
      "epoch: 3 step: 1522, loss is 0.5661309957504272\n",
      "epoch: 3 step: 1523, loss is 0.6398818492889404\n",
      "epoch: 3 step: 1524, loss is 0.7100058197975159\n",
      "epoch: 3 step: 1525, loss is 0.7032731175422668\n",
      "epoch: 3 step: 1526, loss is 0.7309494614601135\n",
      "epoch: 3 step: 1527, loss is 0.49564412236213684\n",
      "epoch: 3 step: 1528, loss is 0.5496517419815063\n",
      "epoch: 3 step: 1529, loss is 0.8271539807319641\n",
      "epoch: 3 step: 1530, loss is 0.6043416261672974\n",
      "epoch: 3 step: 1531, loss is 0.578077495098114\n",
      "epoch: 3 step: 1532, loss is 0.5278772711753845\n",
      "epoch: 3 step: 1533, loss is 0.5489965677261353\n",
      "epoch: 3 step: 1534, loss is 0.5187374353408813\n",
      "epoch: 3 step: 1535, loss is 0.6423990726470947\n",
      "epoch: 3 step: 1536, loss is 0.4745725989341736\n",
      "epoch: 3 step: 1537, loss is 0.39985403418540955\n",
      "epoch: 3 step: 1538, loss is 0.436409056186676\n",
      "epoch: 3 step: 1539, loss is 0.5764044523239136\n",
      "epoch: 3 step: 1540, loss is 0.35966047644615173\n",
      "epoch: 3 step: 1541, loss is 0.4462161064147949\n",
      "epoch: 3 step: 1542, loss is 0.6194470524787903\n",
      "epoch: 3 step: 1543, loss is 0.4828570783138275\n",
      "epoch: 3 step: 1544, loss is 0.581215500831604\n",
      "epoch: 3 step: 1545, loss is 0.5667166113853455\n",
      "epoch: 3 step: 1546, loss is 0.5299680233001709\n",
      "epoch: 3 step: 1547, loss is 0.2978415787220001\n",
      "epoch: 3 step: 1548, loss is 0.6375256776809692\n",
      "epoch: 3 step: 1549, loss is 0.27681365609169006\n",
      "epoch: 3 step: 1550, loss is 0.5146270394325256\n",
      "epoch: 3 step: 1551, loss is 0.325211763381958\n",
      "epoch: 3 step: 1552, loss is 0.4395720064640045\n",
      "epoch: 3 step: 1553, loss is 0.7404935956001282\n",
      "epoch: 3 step: 1554, loss is 0.6840280294418335\n",
      "epoch: 3 step: 1555, loss is 0.40439286828041077\n",
      "epoch: 3 step: 1556, loss is 0.4915468990802765\n",
      "epoch: 3 step: 1557, loss is 0.5733415484428406\n",
      "epoch: 3 step: 1558, loss is 0.5388283133506775\n",
      "epoch: 3 step: 1559, loss is 0.31413209438323975\n",
      "epoch: 3 step: 1560, loss is 0.8113537430763245\n",
      "epoch: 3 step: 1561, loss is 0.48613885045051575\n",
      "epoch: 3 step: 1562, loss is 0.5912575721740723\n",
      "epoch: 3 step: 1563, loss is 0.3861103057861328\n",
      "epoch: 3 step: 1564, loss is 0.4590164124965668\n",
      "epoch: 3 step: 1565, loss is 0.3041951656341553\n",
      "epoch: 3 step: 1566, loss is 0.4170263111591339\n",
      "epoch: 3 step: 1567, loss is 1.07889986038208\n",
      "epoch: 3 step: 1568, loss is 0.5574416518211365\n",
      "epoch: 3 step: 1569, loss is 1.0524487495422363\n",
      "epoch: 3 step: 1570, loss is 0.9688916802406311\n",
      "epoch: 3 step: 1571, loss is 0.8579045534133911\n",
      "epoch: 3 step: 1572, loss is 0.3450264036655426\n",
      "epoch: 3 step: 1573, loss is 0.35955747961997986\n",
      "epoch: 3 step: 1574, loss is 0.8785257935523987\n",
      "epoch: 3 step: 1575, loss is 0.4923776388168335\n",
      "epoch: 3 step: 1576, loss is 0.6698702573776245\n",
      "epoch: 3 step: 1577, loss is 0.625291109085083\n",
      "epoch: 3 step: 1578, loss is 1.1966381072998047\n",
      "epoch: 3 step: 1579, loss is 0.7346258759498596\n",
      "epoch: 3 step: 1580, loss is 0.4586927890777588\n",
      "epoch: 3 step: 1581, loss is 0.6939690709114075\n",
      "epoch: 3 step: 1582, loss is 0.4985117018222809\n",
      "epoch: 3 step: 1583, loss is 0.7064810395240784\n",
      "epoch: 3 step: 1584, loss is 0.904342770576477\n",
      "epoch: 3 step: 1585, loss is 0.39811521768569946\n",
      "epoch: 3 step: 1586, loss is 0.8485881090164185\n",
      "epoch: 3 step: 1587, loss is 0.4043924808502197\n",
      "epoch: 3 step: 1588, loss is 0.6330065131187439\n",
      "epoch: 3 step: 1589, loss is 0.4980963170528412\n",
      "epoch: 3 step: 1590, loss is 0.7413155436515808\n",
      "epoch: 3 step: 1591, loss is 0.6473154425621033\n",
      "epoch: 3 step: 1592, loss is 0.5056372880935669\n",
      "epoch: 3 step: 1593, loss is 0.9033387899398804\n",
      "epoch: 3 step: 1594, loss is 0.5882006287574768\n",
      "epoch: 3 step: 1595, loss is 0.8573639392852783\n",
      "epoch: 3 step: 1596, loss is 0.6738317608833313\n",
      "epoch: 3 step: 1597, loss is 0.7205873131752014\n",
      "epoch: 3 step: 1598, loss is 0.29647910594940186\n",
      "epoch: 3 step: 1599, loss is 0.4772866666316986\n",
      "epoch: 3 step: 1600, loss is 0.6109321117401123\n",
      "epoch: 3 step: 1601, loss is 0.6122261881828308\n",
      "epoch: 3 step: 1602, loss is 0.5765904784202576\n",
      "epoch: 3 step: 1603, loss is 0.46927738189697266\n",
      "epoch: 3 step: 1604, loss is 0.622473418712616\n",
      "epoch: 3 step: 1605, loss is 0.6662748456001282\n",
      "epoch: 3 step: 1606, loss is 0.5702500939369202\n",
      "epoch: 3 step: 1607, loss is 0.4368131160736084\n",
      "epoch: 3 step: 1608, loss is 0.5109841227531433\n",
      "epoch: 3 step: 1609, loss is 0.5148187279701233\n",
      "epoch: 3 step: 1610, loss is 0.3622344732284546\n",
      "epoch: 3 step: 1611, loss is 0.5084425210952759\n",
      "epoch: 3 step: 1612, loss is 0.4694727957248688\n",
      "epoch: 3 step: 1613, loss is 0.7237867712974548\n",
      "epoch: 3 step: 1614, loss is 0.5306799411773682\n",
      "epoch: 3 step: 1615, loss is 0.6589694619178772\n",
      "epoch: 3 step: 1616, loss is 0.49700725078582764\n",
      "epoch: 3 step: 1617, loss is 0.5314561128616333\n",
      "epoch: 3 step: 1618, loss is 0.43909355998039246\n",
      "epoch: 3 step: 1619, loss is 0.5255947113037109\n",
      "epoch: 3 step: 1620, loss is 0.5932627320289612\n",
      "epoch: 3 step: 1621, loss is 0.3827058672904968\n",
      "epoch: 3 step: 1622, loss is 0.427335649728775\n",
      "epoch: 3 step: 1623, loss is 0.6973382830619812\n",
      "epoch: 3 step: 1624, loss is 0.5310893058776855\n",
      "epoch: 3 step: 1625, loss is 0.584698498249054\n",
      "epoch: 3 step: 1626, loss is 0.5379537343978882\n",
      "epoch: 3 step: 1627, loss is 0.5036026835441589\n",
      "epoch: 3 step: 1628, loss is 0.46531111001968384\n",
      "epoch: 3 step: 1629, loss is 0.3751834034919739\n",
      "epoch: 3 step: 1630, loss is 1.0969276428222656\n",
      "epoch: 3 step: 1631, loss is 0.39810389280319214\n",
      "epoch: 3 step: 1632, loss is 0.521090567111969\n",
      "epoch: 3 step: 1633, loss is 0.4506400227546692\n",
      "epoch: 3 step: 1634, loss is 0.40370509028434753\n",
      "epoch: 3 step: 1635, loss is 0.4307173192501068\n",
      "epoch: 3 step: 1636, loss is 0.3483264446258545\n",
      "epoch: 3 step: 1637, loss is 0.3702909052371979\n",
      "epoch: 3 step: 1638, loss is 0.7142412662506104\n",
      "epoch: 3 step: 1639, loss is 0.8782917857170105\n",
      "epoch: 3 step: 1640, loss is 0.6329631209373474\n",
      "epoch: 3 step: 1641, loss is 0.8118463754653931\n",
      "epoch: 3 step: 1642, loss is 0.6307539939880371\n",
      "epoch: 3 step: 1643, loss is 0.4449279308319092\n",
      "epoch: 3 step: 1644, loss is 0.5364303588867188\n",
      "epoch: 3 step: 1645, loss is 0.5404823422431946\n",
      "epoch: 3 step: 1646, loss is 0.5078169107437134\n",
      "epoch: 3 step: 1647, loss is 0.39202946424484253\n",
      "epoch: 3 step: 1648, loss is 0.4752112627029419\n",
      "epoch: 3 step: 1649, loss is 0.45524099469184875\n",
      "epoch: 3 step: 1650, loss is 0.3417554795742035\n",
      "epoch: 3 step: 1651, loss is 0.3091244399547577\n",
      "epoch: 3 step: 1652, loss is 0.6561151146888733\n",
      "epoch: 3 step: 1653, loss is 0.720665454864502\n",
      "epoch: 3 step: 1654, loss is 0.7703544497489929\n",
      "epoch: 3 step: 1655, loss is 0.7127501964569092\n",
      "epoch: 3 step: 1656, loss is 0.41562414169311523\n",
      "epoch: 3 step: 1657, loss is 0.5943999290466309\n",
      "epoch: 3 step: 1658, loss is 0.9037637710571289\n",
      "epoch: 3 step: 1659, loss is 0.46101099252700806\n",
      "epoch: 3 step: 1660, loss is 0.7026685476303101\n",
      "epoch: 3 step: 1661, loss is 0.4743361175060272\n",
      "epoch: 3 step: 1662, loss is 0.47459033131599426\n",
      "epoch: 3 step: 1663, loss is 0.3487316966056824\n",
      "epoch: 3 step: 1664, loss is 0.5134552121162415\n",
      "epoch: 3 step: 1665, loss is 0.40388160943984985\n",
      "epoch: 3 step: 1666, loss is 0.6440231204032898\n",
      "epoch: 3 step: 1667, loss is 0.6283866763114929\n",
      "epoch: 3 step: 1668, loss is 0.45050010085105896\n",
      "epoch: 3 step: 1669, loss is 0.5002759099006653\n",
      "epoch: 3 step: 1670, loss is 0.41237756609916687\n",
      "epoch: 3 step: 1671, loss is 0.47852545976638794\n",
      "epoch: 3 step: 1672, loss is 0.4840042293071747\n",
      "epoch: 3 step: 1673, loss is 0.6180065870285034\n",
      "epoch: 3 step: 1674, loss is 0.712995171546936\n",
      "epoch: 3 step: 1675, loss is 0.6721593141555786\n",
      "epoch: 3 step: 1676, loss is 0.6456394195556641\n",
      "epoch: 3 step: 1677, loss is 0.6162204742431641\n",
      "epoch: 3 step: 1678, loss is 0.3647753596305847\n",
      "epoch: 3 step: 1679, loss is 0.37958669662475586\n",
      "epoch: 3 step: 1680, loss is 0.3624870479106903\n",
      "epoch: 3 step: 1681, loss is 0.6974503397941589\n",
      "epoch: 3 step: 1682, loss is 0.38521283864974976\n",
      "epoch: 3 step: 1683, loss is 0.480482280254364\n",
      "epoch: 3 step: 1684, loss is 0.24193933606147766\n",
      "epoch: 3 step: 1685, loss is 0.6878237128257751\n",
      "epoch: 3 step: 1686, loss is 0.3672409653663635\n",
      "epoch: 3 step: 1687, loss is 0.5840887427330017\n",
      "epoch: 3 step: 1688, loss is 0.4457029402256012\n",
      "epoch: 3 step: 1689, loss is 0.311215877532959\n",
      "epoch: 3 step: 1690, loss is 0.5534011721611023\n",
      "epoch: 3 step: 1691, loss is 1.1214805841445923\n",
      "epoch: 3 step: 1692, loss is 0.5393384695053101\n",
      "epoch: 3 step: 1693, loss is 0.461694598197937\n",
      "epoch: 3 step: 1694, loss is 0.42603427171707153\n",
      "epoch: 3 step: 1695, loss is 0.5403274297714233\n",
      "epoch: 3 step: 1696, loss is 0.8470151424407959\n",
      "epoch: 3 step: 1697, loss is 0.7818126082420349\n",
      "epoch: 3 step: 1698, loss is 0.5919647216796875\n",
      "epoch: 3 step: 1699, loss is 0.5712604522705078\n",
      "epoch: 3 step: 1700, loss is 0.49276942014694214\n",
      "epoch: 3 step: 1701, loss is 0.43284499645233154\n",
      "epoch: 3 step: 1702, loss is 0.4760461747646332\n",
      "epoch: 3 step: 1703, loss is 0.5953467488288879\n",
      "epoch: 3 step: 1704, loss is 0.5456317663192749\n",
      "epoch: 3 step: 1705, loss is 0.5247622132301331\n",
      "epoch: 3 step: 1706, loss is 0.7500798106193542\n",
      "epoch: 3 step: 1707, loss is 0.9375221133232117\n",
      "epoch: 3 step: 1708, loss is 0.49511754512786865\n",
      "epoch: 3 step: 1709, loss is 0.5260663628578186\n",
      "epoch: 3 step: 1710, loss is 0.499259352684021\n",
      "epoch: 3 step: 1711, loss is 0.38743841648101807\n",
      "epoch: 3 step: 1712, loss is 0.4033627212047577\n",
      "epoch: 3 step: 1713, loss is 0.4031325578689575\n",
      "epoch: 3 step: 1714, loss is 0.24254590272903442\n",
      "epoch: 3 step: 1715, loss is 0.6361998915672302\n",
      "epoch: 3 step: 1716, loss is 0.7326294183731079\n",
      "epoch: 3 step: 1717, loss is 0.7148038148880005\n",
      "epoch: 3 step: 1718, loss is 0.38219186663627625\n",
      "epoch: 3 step: 1719, loss is 0.5917751789093018\n",
      "epoch: 3 step: 1720, loss is 0.6668064594268799\n",
      "epoch: 3 step: 1721, loss is 0.4486583173274994\n",
      "epoch: 3 step: 1722, loss is 0.3338119089603424\n",
      "epoch: 3 step: 1723, loss is 0.37383943796157837\n",
      "epoch: 3 step: 1724, loss is 0.7069836854934692\n",
      "epoch: 3 step: 1725, loss is 0.5431559085845947\n",
      "epoch: 3 step: 1726, loss is 0.5385097861289978\n",
      "epoch: 3 step: 1727, loss is 0.7063673138618469\n",
      "epoch: 3 step: 1728, loss is 0.46735531091690063\n",
      "epoch: 3 step: 1729, loss is 0.3237859606742859\n",
      "epoch: 3 step: 1730, loss is 0.5702052116394043\n",
      "epoch: 3 step: 1731, loss is 0.29764050245285034\n",
      "epoch: 3 step: 1732, loss is 0.5605464577674866\n",
      "epoch: 3 step: 1733, loss is 0.5196789503097534\n",
      "epoch: 3 step: 1734, loss is 0.9822185635566711\n",
      "epoch: 3 step: 1735, loss is 0.5268893837928772\n",
      "epoch: 3 step: 1736, loss is 0.3923656642436981\n",
      "epoch: 3 step: 1737, loss is 0.3707737326622009\n",
      "epoch: 3 step: 1738, loss is 1.0720053911209106\n",
      "epoch: 3 step: 1739, loss is 0.18997155129909515\n",
      "epoch: 3 step: 1740, loss is 0.3679167926311493\n",
      "epoch: 3 step: 1741, loss is 0.5481711030006409\n",
      "epoch: 3 step: 1742, loss is 0.8718712329864502\n",
      "epoch: 3 step: 1743, loss is 0.23368990421295166\n",
      "epoch: 3 step: 1744, loss is 0.4171333909034729\n",
      "epoch: 3 step: 1745, loss is 0.49105221033096313\n",
      "epoch: 3 step: 1746, loss is 0.5633669495582581\n",
      "epoch: 3 step: 1747, loss is 0.4858902096748352\n",
      "epoch: 3 step: 1748, loss is 0.980721116065979\n",
      "epoch: 3 step: 1749, loss is 0.4783663749694824\n",
      "epoch: 3 step: 1750, loss is 0.4784672260284424\n",
      "epoch: 3 step: 1751, loss is 0.26101770997047424\n",
      "epoch: 3 step: 1752, loss is 0.5763373374938965\n",
      "epoch: 3 step: 1753, loss is 0.5702593326568604\n",
      "epoch: 3 step: 1754, loss is 0.4792014956474304\n",
      "epoch: 3 step: 1755, loss is 0.41023945808410645\n",
      "epoch: 3 step: 1756, loss is 0.7422189116477966\n",
      "epoch: 3 step: 1757, loss is 0.5168904066085815\n",
      "epoch: 3 step: 1758, loss is 0.46822285652160645\n",
      "epoch: 3 step: 1759, loss is 0.33884111046791077\n",
      "epoch: 3 step: 1760, loss is 0.9718590974807739\n",
      "epoch: 3 step: 1761, loss is 0.4712219834327698\n",
      "epoch: 3 step: 1762, loss is 0.7819777131080627\n",
      "epoch: 3 step: 1763, loss is 0.49982136487960815\n",
      "epoch: 3 step: 1764, loss is 0.5142210125923157\n",
      "epoch: 3 step: 1765, loss is 0.3756059408187866\n",
      "epoch: 3 step: 1766, loss is 0.48724010586738586\n",
      "epoch: 3 step: 1767, loss is 0.46575069427490234\n",
      "epoch: 3 step: 1768, loss is 0.6726952791213989\n",
      "epoch: 3 step: 1769, loss is 0.7112200856208801\n",
      "epoch: 3 step: 1770, loss is 0.45678719878196716\n",
      "epoch: 3 step: 1771, loss is 0.5107322931289673\n",
      "epoch: 3 step: 1772, loss is 0.5684752464294434\n",
      "epoch: 3 step: 1773, loss is 0.40690380334854126\n",
      "epoch: 3 step: 1774, loss is 0.47319138050079346\n",
      "epoch: 3 step: 1775, loss is 0.5001193881034851\n",
      "epoch: 3 step: 1776, loss is 0.292557030916214\n",
      "epoch: 3 step: 1777, loss is 0.5587874054908752\n",
      "epoch: 3 step: 1778, loss is 0.3710207939147949\n",
      "epoch: 3 step: 1779, loss is 0.839904248714447\n",
      "epoch: 3 step: 1780, loss is 0.31539425253868103\n",
      "epoch: 3 step: 1781, loss is 0.5819355845451355\n",
      "epoch: 3 step: 1782, loss is 0.2741689383983612\n",
      "epoch: 3 step: 1783, loss is 0.3603683114051819\n",
      "epoch: 3 step: 1784, loss is 0.2934589684009552\n",
      "epoch: 3 step: 1785, loss is 0.5519043207168579\n",
      "epoch: 3 step: 1786, loss is 0.6242586970329285\n",
      "epoch: 3 step: 1787, loss is 0.39257851243019104\n",
      "epoch: 3 step: 1788, loss is 0.7253694534301758\n",
      "epoch: 3 step: 1789, loss is 0.5092832446098328\n",
      "epoch: 3 step: 1790, loss is 0.5812863111495972\n",
      "epoch: 3 step: 1791, loss is 0.46200746297836304\n",
      "epoch: 3 step: 1792, loss is 0.7884316444396973\n",
      "epoch: 3 step: 1793, loss is 0.4947527050971985\n",
      "epoch: 3 step: 1794, loss is 0.30195149779319763\n",
      "epoch: 3 step: 1795, loss is 0.573530912399292\n",
      "epoch: 3 step: 1796, loss is 0.4150565564632416\n",
      "epoch: 3 step: 1797, loss is 0.53179532289505\n",
      "epoch: 3 step: 1798, loss is 0.3059002459049225\n",
      "epoch: 3 step: 1799, loss is 0.8022781610488892\n",
      "epoch: 3 step: 1800, loss is 0.8869806528091431\n",
      "epoch: 3 step: 1801, loss is 0.2492004930973053\n",
      "epoch: 3 step: 1802, loss is 1.0176520347595215\n",
      "epoch: 3 step: 1803, loss is 0.6644023060798645\n",
      "epoch: 3 step: 1804, loss is 0.3015028238296509\n",
      "epoch: 3 step: 1805, loss is 0.4785464107990265\n",
      "epoch: 3 step: 1806, loss is 0.3636583983898163\n",
      "epoch: 3 step: 1807, loss is 0.6774274110794067\n",
      "epoch: 3 step: 1808, loss is 0.38494279980659485\n",
      "epoch: 3 step: 1809, loss is 0.6567586064338684\n",
      "epoch: 3 step: 1810, loss is 0.6948117613792419\n",
      "epoch: 3 step: 1811, loss is 0.4382114112377167\n",
      "epoch: 3 step: 1812, loss is 0.6114017963409424\n",
      "epoch: 3 step: 1813, loss is 0.41905251145362854\n",
      "epoch: 3 step: 1814, loss is 0.7170731425285339\n",
      "epoch: 3 step: 1815, loss is 0.9578870534896851\n",
      "epoch: 3 step: 1816, loss is 0.7756623029708862\n",
      "epoch: 3 step: 1817, loss is 0.4829363524913788\n",
      "epoch: 3 step: 1818, loss is 0.3258366286754608\n",
      "epoch: 3 step: 1819, loss is 0.9086487889289856\n",
      "epoch: 3 step: 1820, loss is 0.6264572739601135\n",
      "epoch: 3 step: 1821, loss is 0.6584997773170471\n",
      "epoch: 3 step: 1822, loss is 0.4287519156932831\n",
      "epoch: 3 step: 1823, loss is 0.2618084251880646\n",
      "epoch: 3 step: 1824, loss is 0.47540268301963806\n",
      "epoch: 3 step: 1825, loss is 0.34906354546546936\n",
      "epoch: 3 step: 1826, loss is 0.7052376866340637\n",
      "epoch: 3 step: 1827, loss is 0.4942602515220642\n",
      "epoch: 3 step: 1828, loss is 0.3822488784790039\n",
      "epoch: 3 step: 1829, loss is 0.2260109931230545\n",
      "epoch: 3 step: 1830, loss is 0.6336850523948669\n",
      "epoch: 3 step: 1831, loss is 0.7441288232803345\n",
      "epoch: 3 step: 1832, loss is 1.1611863374710083\n",
      "epoch: 3 step: 1833, loss is 0.41125181317329407\n",
      "epoch: 3 step: 1834, loss is 0.7657958269119263\n",
      "epoch: 3 step: 1835, loss is 0.8547083139419556\n",
      "epoch: 3 step: 1836, loss is 1.029292106628418\n",
      "epoch: 3 step: 1837, loss is 0.5395823121070862\n",
      "epoch: 3 step: 1838, loss is 0.48493561148643494\n",
      "epoch: 3 step: 1839, loss is 0.31130245327949524\n",
      "epoch: 3 step: 1840, loss is 0.8351799249649048\n",
      "epoch: 3 step: 1841, loss is 0.37182092666625977\n",
      "epoch: 3 step: 1842, loss is 0.3051106631755829\n",
      "epoch: 3 step: 1843, loss is 0.3498230278491974\n",
      "epoch: 3 step: 1844, loss is 0.38302454352378845\n",
      "epoch: 3 step: 1845, loss is 0.5105741620063782\n",
      "epoch: 3 step: 1846, loss is 0.6414848566055298\n",
      "epoch: 3 step: 1847, loss is 0.6563788056373596\n",
      "epoch: 3 step: 1848, loss is 0.4760225713253021\n",
      "epoch: 3 step: 1849, loss is 0.569915235042572\n",
      "epoch: 3 step: 1850, loss is 0.516292929649353\n",
      "epoch: 3 step: 1851, loss is 0.45597171783447266\n",
      "epoch: 3 step: 1852, loss is 0.6525007486343384\n",
      "epoch: 3 step: 1853, loss is 0.5996915698051453\n",
      "epoch: 3 step: 1854, loss is 0.44873955845832825\n",
      "epoch: 3 step: 1855, loss is 0.4587423503398895\n",
      "epoch: 3 step: 1856, loss is 0.6132631301879883\n",
      "epoch: 3 step: 1857, loss is 0.43453964591026306\n",
      "epoch: 3 step: 1858, loss is 0.38741421699523926\n",
      "epoch: 3 step: 1859, loss is 0.5556935667991638\n",
      "epoch: 3 step: 1860, loss is 0.6539427638053894\n",
      "epoch: 3 step: 1861, loss is 0.442316472530365\n",
      "epoch: 3 step: 1862, loss is 0.5661144256591797\n",
      "epoch: 3 step: 1863, loss is 0.6112656593322754\n",
      "epoch: 3 step: 1864, loss is 0.2718977928161621\n",
      "epoch: 3 step: 1865, loss is 0.4387306571006775\n",
      "epoch: 3 step: 1866, loss is 0.7812075018882751\n",
      "epoch: 3 step: 1867, loss is 0.3524288237094879\n",
      "epoch: 3 step: 1868, loss is 0.2903842031955719\n",
      "epoch: 3 step: 1869, loss is 0.5712220668792725\n",
      "epoch: 3 step: 1870, loss is 0.34226810932159424\n",
      "epoch: 3 step: 1871, loss is 0.348994642496109\n",
      "epoch: 3 step: 1872, loss is 1.302554965019226\n",
      "epoch: 3 step: 1873, loss is 0.5061125755310059\n",
      "epoch: 3 step: 1874, loss is 0.3420705199241638\n",
      "epoch: 3 step: 1875, loss is 0.7356041669845581\n",
      "epoch: 3 step: 1876, loss is 0.4479573369026184\n",
      "epoch: 3 step: 1877, loss is 0.5067940354347229\n",
      "epoch: 3 step: 1878, loss is 0.5687124729156494\n",
      "epoch: 3 step: 1879, loss is 0.42237192392349243\n",
      "epoch: 3 step: 1880, loss is 0.686705470085144\n",
      "epoch: 3 step: 1881, loss is 0.4767042398452759\n",
      "epoch: 3 step: 1882, loss is 0.6836217641830444\n",
      "epoch: 3 step: 1883, loss is 0.6969456076622009\n",
      "epoch: 3 step: 1884, loss is 0.41555169224739075\n",
      "epoch: 3 step: 1885, loss is 0.49466148018836975\n",
      "epoch: 3 step: 1886, loss is 0.49495095014572144\n",
      "epoch: 3 step: 1887, loss is 0.42736369371414185\n",
      "epoch: 3 step: 1888, loss is 0.4844179153442383\n",
      "epoch: 3 step: 1889, loss is 0.7234065532684326\n",
      "epoch: 3 step: 1890, loss is 0.5812526345252991\n",
      "epoch: 3 step: 1891, loss is 0.2807667851448059\n",
      "epoch: 3 step: 1892, loss is 0.612784743309021\n",
      "epoch: 3 step: 1893, loss is 0.7081298232078552\n",
      "epoch: 3 step: 1894, loss is 0.39201393723487854\n",
      "epoch: 3 step: 1895, loss is 0.4540698826313019\n",
      "epoch: 3 step: 1896, loss is 0.5998361110687256\n",
      "epoch: 3 step: 1897, loss is 0.2775886654853821\n",
      "epoch: 3 step: 1898, loss is 0.9063822627067566\n",
      "epoch: 3 step: 1899, loss is 0.6916786432266235\n",
      "epoch: 3 step: 1900, loss is 0.5350136160850525\n",
      "epoch: 3 step: 1901, loss is 0.4033372104167938\n",
      "epoch: 3 step: 1902, loss is 0.5965931415557861\n",
      "epoch: 3 step: 1903, loss is 0.4420683681964874\n",
      "epoch: 3 step: 1904, loss is 0.4823698103427887\n",
      "epoch: 3 step: 1905, loss is 0.394241601228714\n",
      "epoch: 3 step: 1906, loss is 0.6035405397415161\n",
      "epoch: 3 step: 1907, loss is 0.5776490569114685\n",
      "epoch: 3 step: 1908, loss is 0.39071935415267944\n",
      "epoch: 3 step: 1909, loss is 0.6866886615753174\n",
      "epoch: 3 step: 1910, loss is 1.2051142454147339\n",
      "epoch: 3 step: 1911, loss is 0.40743714570999146\n",
      "epoch: 3 step: 1912, loss is 0.9254565238952637\n",
      "epoch: 3 step: 1913, loss is 0.38760367035865784\n",
      "epoch: 3 step: 1914, loss is 0.5384871959686279\n",
      "epoch: 3 step: 1915, loss is 0.6073652505874634\n",
      "epoch: 3 step: 1916, loss is 0.34608542919158936\n",
      "epoch: 3 step: 1917, loss is 0.5501136183738708\n",
      "epoch: 3 step: 1918, loss is 0.5612950921058655\n",
      "epoch: 3 step: 1919, loss is 0.3496091961860657\n",
      "epoch: 3 step: 1920, loss is 0.4603065848350525\n",
      "epoch: 3 step: 1921, loss is 0.39808061718940735\n",
      "epoch: 3 step: 1922, loss is 0.5601620674133301\n",
      "epoch: 3 step: 1923, loss is 0.5098337531089783\n",
      "epoch: 3 step: 1924, loss is 0.5356669425964355\n",
      "epoch: 3 step: 1925, loss is 0.6217255592346191\n",
      "epoch: 3 step: 1926, loss is 0.5333020687103271\n",
      "epoch: 3 step: 1927, loss is 1.0319654941558838\n",
      "epoch: 3 step: 1928, loss is 0.8302706480026245\n",
      "epoch: 3 step: 1929, loss is 0.5213545560836792\n",
      "epoch: 3 step: 1930, loss is 0.3064957857131958\n",
      "epoch: 3 step: 1931, loss is 0.4791131317615509\n",
      "epoch: 3 step: 1932, loss is 0.2914947271347046\n",
      "epoch: 3 step: 1933, loss is 0.47491055727005005\n",
      "epoch: 3 step: 1934, loss is 0.3946307599544525\n",
      "epoch: 3 step: 1935, loss is 0.7945507764816284\n",
      "epoch: 3 step: 1936, loss is 0.6330817937850952\n",
      "epoch: 3 step: 1937, loss is 0.27254778146743774\n",
      "epoch: 3 step: 1938, loss is 1.1881861686706543\n",
      "epoch: 3 step: 1939, loss is 0.6285737752914429\n",
      "epoch: 3 step: 1940, loss is 0.5850310921669006\n",
      "epoch: 3 step: 1941, loss is 0.4075046479701996\n",
      "epoch: 3 step: 1942, loss is 0.7789091467857361\n",
      "epoch: 3 step: 1943, loss is 0.3511286675930023\n",
      "epoch: 3 step: 1944, loss is 0.697561502456665\n",
      "epoch: 3 step: 1945, loss is 1.1378885507583618\n",
      "epoch: 3 step: 1946, loss is 0.4023797810077667\n",
      "epoch: 3 step: 1947, loss is 0.586969792842865\n",
      "epoch: 3 step: 1948, loss is 0.4402273893356323\n",
      "epoch: 3 step: 1949, loss is 0.7859529852867126\n",
      "epoch: 3 step: 1950, loss is 0.8137767314910889\n",
      "epoch: 3 step: 1951, loss is 0.47802966833114624\n",
      "epoch: 3 step: 1952, loss is 0.4827713072299957\n",
      "epoch: 3 step: 1953, loss is 0.7491405010223389\n",
      "epoch: 3 step: 1954, loss is 0.421880304813385\n",
      "epoch: 3 step: 1955, loss is 0.34309473633766174\n",
      "epoch: 3 step: 1956, loss is 0.6827724575996399\n",
      "epoch: 3 step: 1957, loss is 0.3502781391143799\n",
      "epoch: 3 step: 1958, loss is 0.6314642429351807\n",
      "epoch: 3 step: 1959, loss is 0.5850160121917725\n",
      "epoch: 3 step: 1960, loss is 0.5367108583450317\n",
      "epoch: 3 step: 1961, loss is 0.3220493495464325\n",
      "epoch: 3 step: 1962, loss is 0.5653955936431885\n",
      "epoch: 3 step: 1963, loss is 0.8545607328414917\n",
      "epoch: 3 step: 1964, loss is 0.5701289772987366\n",
      "epoch: 3 step: 1965, loss is 0.5901409983634949\n",
      "epoch: 3 step: 1966, loss is 0.6318017840385437\n",
      "epoch: 3 step: 1967, loss is 0.3515096604824066\n",
      "epoch: 3 step: 1968, loss is 0.5195079445838928\n",
      "epoch: 3 step: 1969, loss is 0.49785828590393066\n",
      "epoch: 3 step: 1970, loss is 0.5087071657180786\n",
      "epoch: 3 step: 1971, loss is 0.5878348350524902\n",
      "epoch: 3 step: 1972, loss is 0.5214453339576721\n",
      "epoch: 3 step: 1973, loss is 0.5979970693588257\n",
      "epoch: 3 step: 1974, loss is 1.3045521974563599\n",
      "epoch: 3 step: 1975, loss is 0.6562432646751404\n",
      "epoch: 3 step: 1976, loss is 0.3237937092781067\n",
      "epoch: 3 step: 1977, loss is 0.28660500049591064\n",
      "epoch: 3 step: 1978, loss is 0.3553982973098755\n",
      "epoch: 3 step: 1979, loss is 0.4491507411003113\n",
      "epoch: 3 step: 1980, loss is 0.6165593862533569\n",
      "epoch: 3 step: 1981, loss is 0.3933747112751007\n",
      "epoch: 3 step: 1982, loss is 0.7118064165115356\n",
      "epoch: 3 step: 1983, loss is 0.5461792349815369\n",
      "epoch: 3 step: 1984, loss is 0.3603917360305786\n",
      "epoch: 3 step: 1985, loss is 0.5531089901924133\n",
      "epoch: 3 step: 1986, loss is 0.733449399471283\n",
      "epoch: 3 step: 1987, loss is 0.34723523259162903\n",
      "epoch: 3 step: 1988, loss is 0.3560992181301117\n",
      "epoch: 3 step: 1989, loss is 0.6933313012123108\n",
      "epoch: 3 step: 1990, loss is 0.4713197946548462\n",
      "epoch: 3 step: 1991, loss is 0.8205234408378601\n",
      "epoch: 3 step: 1992, loss is 0.41079217195510864\n",
      "epoch: 3 step: 1993, loss is 0.4383625388145447\n",
      "epoch: 3 step: 1994, loss is 0.3902609646320343\n",
      "epoch: 3 step: 1995, loss is 0.4399510622024536\n",
      "epoch: 3 step: 1996, loss is 0.7275160551071167\n",
      "epoch: 3 step: 1997, loss is 0.5985546708106995\n",
      "epoch: 3 step: 1998, loss is 0.32162898778915405\n",
      "epoch: 3 step: 1999, loss is 0.5339539051055908\n",
      "epoch: 3 step: 2000, loss is 0.460341215133667\n",
      "epoch: 3 step: 2001, loss is 0.5561903715133667\n",
      "epoch: 3 step: 2002, loss is 0.1810929775238037\n",
      "epoch: 3 step: 2003, loss is 0.3573548495769501\n",
      "epoch: 3 step: 2004, loss is 0.5287584066390991\n",
      "epoch: 3 step: 2005, loss is 0.8865112066268921\n",
      "epoch: 3 step: 2006, loss is 0.6553955078125\n",
      "epoch: 3 step: 2007, loss is 0.4505141079425812\n",
      "epoch: 3 step: 2008, loss is 0.3294283151626587\n",
      "epoch: 3 step: 2009, loss is 0.5207944512367249\n",
      "epoch: 3 step: 2010, loss is 0.4198649525642395\n",
      "epoch: 3 step: 2011, loss is 0.9351496696472168\n",
      "epoch: 3 step: 2012, loss is 0.401571661233902\n",
      "epoch: 3 step: 2013, loss is 0.4891536235809326\n",
      "epoch: 3 step: 2014, loss is 0.7179235816001892\n",
      "epoch: 3 step: 2015, loss is 0.42845597863197327\n",
      "epoch: 3 step: 2016, loss is 0.6568754315376282\n",
      "epoch: 3 step: 2017, loss is 0.47564104199409485\n",
      "epoch: 3 step: 2018, loss is 0.45947644114494324\n",
      "epoch: 3 step: 2019, loss is 0.6896936297416687\n",
      "epoch: 3 step: 2020, loss is 0.5626590251922607\n",
      "epoch: 3 step: 2021, loss is 0.3962875008583069\n",
      "epoch: 3 step: 2022, loss is 0.43891581892967224\n",
      "epoch: 3 step: 2023, loss is 0.5227483510971069\n",
      "epoch: 3 step: 2024, loss is 0.7833802700042725\n",
      "epoch: 3 step: 2025, loss is 0.5475702285766602\n",
      "epoch: 3 step: 2026, loss is 0.495993435382843\n",
      "epoch: 3 step: 2027, loss is 0.2533436119556427\n",
      "epoch: 3 step: 2028, loss is 0.21278829872608185\n",
      "epoch: 3 step: 2029, loss is 0.24723480641841888\n",
      "epoch: 3 step: 2030, loss is 0.39883533120155334\n",
      "epoch: 3 step: 2031, loss is 0.5937561392784119\n",
      "epoch: 3 step: 2032, loss is 0.6036200523376465\n",
      "epoch: 3 step: 2033, loss is 0.2193954437971115\n",
      "epoch: 3 step: 2034, loss is 0.3364759385585785\n",
      "epoch: 3 step: 2035, loss is 1.5644539594650269\n",
      "epoch: 3 step: 2036, loss is 0.723281741142273\n",
      "epoch: 3 step: 2037, loss is 0.4406624436378479\n",
      "epoch: 3 step: 2038, loss is 0.382875919342041\n",
      "epoch: 3 step: 2039, loss is 0.42683008313179016\n",
      "epoch: 3 step: 2040, loss is 0.6380969285964966\n",
      "epoch: 3 step: 2041, loss is 0.39896246790885925\n",
      "epoch: 3 step: 2042, loss is 0.6590136289596558\n",
      "epoch: 3 step: 2043, loss is 0.5206291675567627\n",
      "epoch: 3 step: 2044, loss is 0.5842199325561523\n",
      "epoch: 3 step: 2045, loss is 0.7914362549781799\n",
      "epoch: 3 step: 2046, loss is 0.4910528063774109\n",
      "epoch: 3 step: 2047, loss is 0.8004129528999329\n",
      "epoch: 3 step: 2048, loss is 0.3796997666358948\n",
      "epoch: 3 step: 2049, loss is 0.5646202564239502\n",
      "epoch: 3 step: 2050, loss is 0.20769980549812317\n",
      "epoch: 3 step: 2051, loss is 0.5032186508178711\n",
      "epoch: 3 step: 2052, loss is 0.45861300826072693\n",
      "epoch: 3 step: 2053, loss is 0.5843675136566162\n",
      "epoch: 3 step: 2054, loss is 0.23336246609687805\n",
      "epoch: 3 step: 2055, loss is 0.6963263154029846\n",
      "epoch: 3 step: 2056, loss is 0.6272521018981934\n",
      "epoch: 3 step: 2057, loss is 0.4273984134197235\n",
      "epoch: 3 step: 2058, loss is 0.7143127918243408\n",
      "epoch: 3 step: 2059, loss is 0.4453839361667633\n",
      "epoch: 3 step: 2060, loss is 0.3187159597873688\n",
      "epoch: 3 step: 2061, loss is 0.6766656637191772\n",
      "epoch: 3 step: 2062, loss is 0.8208829760551453\n",
      "epoch: 3 step: 2063, loss is 0.6875451803207397\n",
      "epoch: 3 step: 2064, loss is 0.6649094223976135\n",
      "epoch: 3 step: 2065, loss is 0.7570673823356628\n",
      "epoch: 3 step: 2066, loss is 0.4592939615249634\n",
      "epoch: 3 step: 2067, loss is 0.8451592326164246\n",
      "epoch: 3 step: 2068, loss is 0.6446635723114014\n",
      "epoch: 3 step: 2069, loss is 0.5104457139968872\n",
      "epoch: 3 step: 2070, loss is 0.2595452666282654\n",
      "epoch: 3 step: 2071, loss is 0.4380348324775696\n",
      "epoch: 3 step: 2072, loss is 1.0338696241378784\n",
      "epoch: 3 step: 2073, loss is 0.58515465259552\n",
      "epoch: 3 step: 2074, loss is 0.3658002018928528\n",
      "epoch: 3 step: 2075, loss is 0.4572407603263855\n",
      "epoch: 3 step: 2076, loss is 0.5264167189598083\n",
      "epoch: 3 step: 2077, loss is 0.5184462070465088\n",
      "epoch: 3 step: 2078, loss is 0.6412602663040161\n",
      "epoch: 3 step: 2079, loss is 0.5078732967376709\n",
      "epoch: 3 step: 2080, loss is 0.5563015937805176\n",
      "epoch: 3 step: 2081, loss is 0.30676162242889404\n",
      "epoch: 3 step: 2082, loss is 0.7401150465011597\n",
      "epoch: 3 step: 2083, loss is 0.8137543797492981\n",
      "epoch: 3 step: 2084, loss is 0.568979799747467\n",
      "epoch: 3 step: 2085, loss is 0.9180715084075928\n",
      "epoch: 3 step: 2086, loss is 0.5464503765106201\n",
      "epoch: 3 step: 2087, loss is 0.8273807764053345\n",
      "epoch: 3 step: 2088, loss is 0.5229563117027283\n",
      "epoch: 3 step: 2089, loss is 0.310881108045578\n",
      "epoch: 3 step: 2090, loss is 0.9651826620101929\n",
      "epoch: 3 step: 2091, loss is 0.41030099987983704\n",
      "epoch: 3 step: 2092, loss is 0.3094683587551117\n",
      "epoch: 3 step: 2093, loss is 0.4549095034599304\n",
      "epoch: 3 step: 2094, loss is 0.46907177567481995\n",
      "epoch: 3 step: 2095, loss is 0.6418859958648682\n",
      "epoch: 3 step: 2096, loss is 0.7268598079681396\n",
      "epoch: 3 step: 2097, loss is 0.5125975608825684\n",
      "epoch: 3 step: 2098, loss is 0.32667654752731323\n",
      "epoch: 3 step: 2099, loss is 0.46440252661705017\n",
      "epoch: 3 step: 2100, loss is 0.4504750967025757\n",
      "epoch: 3 step: 2101, loss is 0.29288241267204285\n",
      "epoch: 3 step: 2102, loss is 0.6060805916786194\n",
      "epoch: 3 step: 2103, loss is 0.5343217849731445\n",
      "epoch: 3 step: 2104, loss is 0.6457859873771667\n",
      "epoch: 3 step: 2105, loss is 0.767294704914093\n",
      "epoch: 3 step: 2106, loss is 0.7189440727233887\n",
      "epoch: 3 step: 2107, loss is 0.5351618528366089\n",
      "epoch: 3 step: 2108, loss is 0.5043565034866333\n",
      "epoch: 3 step: 2109, loss is 0.6415286660194397\n",
      "epoch: 3 step: 2110, loss is 0.5281732082366943\n",
      "epoch: 3 step: 2111, loss is 0.6472828388214111\n",
      "epoch: 3 step: 2112, loss is 0.3320774734020233\n",
      "epoch: 3 step: 2113, loss is 0.7556815147399902\n",
      "epoch: 3 step: 2114, loss is 1.0820403099060059\n",
      "epoch: 3 step: 2115, loss is 0.5577273964881897\n",
      "epoch: 3 step: 2116, loss is 0.522396981716156\n",
      "epoch: 3 step: 2117, loss is 0.3696344792842865\n",
      "epoch: 3 step: 2118, loss is 0.6051779985427856\n",
      "epoch: 3 step: 2119, loss is 1.135082721710205\n",
      "epoch: 3 step: 2120, loss is 0.5638275742530823\n",
      "epoch: 3 step: 2121, loss is 0.5565317869186401\n",
      "epoch: 3 step: 2122, loss is 0.3747327923774719\n",
      "epoch: 3 step: 2123, loss is 0.4763127565383911\n",
      "epoch: 3 step: 2124, loss is 0.548223614692688\n",
      "epoch: 3 step: 2125, loss is 0.47797319293022156\n",
      "epoch: 3 step: 2126, loss is 0.7907924652099609\n",
      "epoch: 3 step: 2127, loss is 0.5986747741699219\n",
      "epoch: 3 step: 2128, loss is 0.3230244815349579\n",
      "epoch: 3 step: 2129, loss is 0.8235247135162354\n",
      "epoch: 3 step: 2130, loss is 0.4870423674583435\n",
      "epoch: 3 step: 2131, loss is 0.3884672522544861\n",
      "epoch: 3 step: 2132, loss is 0.4570487439632416\n",
      "epoch: 3 step: 2133, loss is 0.5351784825325012\n",
      "epoch: 3 step: 2134, loss is 0.6068869829177856\n",
      "epoch: 3 step: 2135, loss is 0.3684985935688019\n",
      "epoch: 3 step: 2136, loss is 0.7021179795265198\n",
      "epoch: 3 step: 2137, loss is 0.574753999710083\n",
      "epoch: 3 step: 2138, loss is 0.611950695514679\n",
      "epoch: 3 step: 2139, loss is 0.6590093374252319\n",
      "epoch: 3 step: 2140, loss is 0.35714638233184814\n",
      "epoch: 3 step: 2141, loss is 0.6088429689407349\n",
      "epoch: 3 step: 2142, loss is 0.5818862915039062\n",
      "epoch: 3 step: 2143, loss is 0.711692750453949\n",
      "epoch: 3 step: 2144, loss is 0.6464313268661499\n",
      "epoch: 3 step: 2145, loss is 0.6182718276977539\n",
      "epoch: 3 step: 2146, loss is 0.5346072316169739\n",
      "epoch: 3 step: 2147, loss is 0.5916619300842285\n",
      "epoch: 3 step: 2148, loss is 0.39199328422546387\n",
      "epoch: 3 step: 2149, loss is 0.6511490941047668\n",
      "epoch: 3 step: 2150, loss is 0.7403386831283569\n",
      "epoch: 3 step: 2151, loss is 0.837462306022644\n",
      "epoch: 3 step: 2152, loss is 0.5020670890808105\n",
      "epoch: 3 step: 2153, loss is 0.41768139600753784\n",
      "epoch: 3 step: 2154, loss is 0.44529274106025696\n",
      "epoch: 3 step: 2155, loss is 0.39598941802978516\n",
      "epoch: 3 step: 2156, loss is 0.37093353271484375\n",
      "epoch: 3 step: 2157, loss is 0.48940417170524597\n",
      "epoch: 3 step: 2158, loss is 0.4266325533390045\n",
      "epoch: 3 step: 2159, loss is 0.4476066529750824\n",
      "epoch: 3 step: 2160, loss is 0.32926875352859497\n",
      "epoch: 3 step: 2161, loss is 0.6360905766487122\n",
      "epoch: 3 step: 2162, loss is 0.6040740609169006\n",
      "epoch: 3 step: 2163, loss is 0.6393821239471436\n",
      "epoch: 3 step: 2164, loss is 0.395197331905365\n",
      "epoch: 3 step: 2165, loss is 0.475017249584198\n",
      "epoch: 3 step: 2166, loss is 0.48067522048950195\n",
      "epoch: 3 step: 2167, loss is 0.6600636839866638\n",
      "epoch: 3 step: 2168, loss is 0.803923487663269\n",
      "epoch: 3 step: 2169, loss is 0.6251251101493835\n",
      "epoch: 3 step: 2170, loss is 0.7291818857192993\n",
      "epoch: 3 step: 2171, loss is 0.6389675736427307\n",
      "epoch: 3 step: 2172, loss is 0.36769601702690125\n",
      "epoch: 3 step: 2173, loss is 0.5445666909217834\n",
      "epoch: 3 step: 2174, loss is 0.43371620774269104\n",
      "epoch: 3 step: 2175, loss is 0.3750687539577484\n",
      "epoch: 3 step: 2176, loss is 0.6856237649917603\n",
      "epoch: 3 step: 2177, loss is 0.5208921432495117\n",
      "epoch: 3 step: 2178, loss is 0.43084990978240967\n",
      "epoch: 3 step: 2179, loss is 0.626031756401062\n",
      "epoch: 3 step: 2180, loss is 0.44694480299949646\n",
      "epoch: 3 step: 2181, loss is 0.6321213841438293\n",
      "epoch: 3 step: 2182, loss is 0.672774076461792\n",
      "epoch: 3 step: 2183, loss is 0.5699498057365417\n",
      "epoch: 3 step: 2184, loss is 0.6968887448310852\n",
      "epoch: 3 step: 2185, loss is 0.304782897233963\n",
      "epoch: 3 step: 2186, loss is 0.48677223920822144\n",
      "epoch: 3 step: 2187, loss is 0.49054980278015137\n",
      "epoch: 3 step: 2188, loss is 0.3499530553817749\n",
      "epoch: 3 step: 2189, loss is 0.6057142615318298\n",
      "epoch: 3 step: 2190, loss is 0.42993009090423584\n",
      "epoch: 3 step: 2191, loss is 0.4385063946247101\n",
      "epoch: 3 step: 2192, loss is 0.5661944150924683\n",
      "epoch: 3 step: 2193, loss is 0.5626897811889648\n",
      "epoch: 3 step: 2194, loss is 0.8795630931854248\n",
      "epoch: 3 step: 2195, loss is 0.6591852307319641\n",
      "epoch: 3 step: 2196, loss is 0.7375779747962952\n",
      "epoch: 3 step: 2197, loss is 0.5723001956939697\n",
      "epoch: 3 step: 2198, loss is 0.448384553194046\n",
      "epoch: 3 step: 2199, loss is 0.6001019477844238\n",
      "epoch: 3 step: 2200, loss is 1.0773035287857056\n",
      "epoch: 3 step: 2201, loss is 0.6302655339241028\n",
      "epoch: 3 step: 2202, loss is 0.35355275869369507\n",
      "epoch: 3 step: 2203, loss is 0.7866261005401611\n",
      "epoch: 3 step: 2204, loss is 0.4725636839866638\n",
      "epoch: 3 step: 2205, loss is 0.44923636317253113\n",
      "epoch: 3 step: 2206, loss is 0.40675291419029236\n",
      "epoch: 3 step: 2207, loss is 0.7838003635406494\n",
      "epoch: 3 step: 2208, loss is 0.24106526374816895\n",
      "epoch: 3 step: 2209, loss is 0.40751057863235474\n",
      "epoch: 3 step: 2210, loss is 0.5377725958824158\n",
      "epoch: 3 step: 2211, loss is 0.5528661012649536\n",
      "epoch: 3 step: 2212, loss is 0.3993327021598816\n",
      "epoch: 3 step: 2213, loss is 0.2839054465293884\n",
      "epoch: 3 step: 2214, loss is 0.6704061627388\n",
      "epoch: 3 step: 2215, loss is 0.534538209438324\n",
      "epoch: 3 step: 2216, loss is 0.39193448424339294\n",
      "epoch: 3 step: 2217, loss is 0.5470296740531921\n",
      "epoch: 3 step: 2218, loss is 0.5126273036003113\n",
      "epoch: 3 step: 2219, loss is 0.27178001403808594\n",
      "epoch: 3 step: 2220, loss is 0.4805440306663513\n",
      "epoch: 3 step: 2221, loss is 0.7217896580696106\n",
      "epoch: 3 step: 2222, loss is 0.48152899742126465\n",
      "epoch: 3 step: 2223, loss is 0.5679646730422974\n",
      "epoch: 3 step: 2224, loss is 0.6107175350189209\n",
      "epoch: 3 step: 2225, loss is 0.5685903429985046\n",
      "epoch: 3 step: 2226, loss is 0.5259802341461182\n",
      "epoch: 3 step: 2227, loss is 0.30676624178886414\n",
      "epoch: 3 step: 2228, loss is 0.5842636823654175\n",
      "epoch: 3 step: 2229, loss is 0.9288633465766907\n",
      "epoch: 3 step: 2230, loss is 0.8411298990249634\n",
      "epoch: 3 step: 2231, loss is 0.35584545135498047\n",
      "epoch: 3 step: 2232, loss is 0.5311986804008484\n",
      "epoch: 3 step: 2233, loss is 0.35913434624671936\n",
      "epoch: 3 step: 2234, loss is 0.4806136190891266\n",
      "epoch: 3 step: 2235, loss is 0.501014232635498\n",
      "epoch: 3 step: 2236, loss is 0.6443063020706177\n",
      "epoch: 3 step: 2237, loss is 0.4106042981147766\n",
      "epoch: 3 step: 2238, loss is 0.433840811252594\n",
      "epoch: 3 step: 2239, loss is 0.6643946766853333\n",
      "epoch: 3 step: 2240, loss is 0.5564916133880615\n",
      "epoch: 3 step: 2241, loss is 0.32422715425491333\n",
      "epoch: 3 step: 2242, loss is 0.6823055148124695\n",
      "epoch: 3 step: 2243, loss is 0.1690729409456253\n",
      "epoch: 3 step: 2244, loss is 0.45519348978996277\n",
      "epoch: 3 step: 2245, loss is 0.7446811199188232\n",
      "epoch: 3 step: 2246, loss is 0.8283891081809998\n",
      "epoch: 3 step: 2247, loss is 0.6632513999938965\n",
      "epoch: 3 step: 2248, loss is 0.4531134366989136\n",
      "epoch: 3 step: 2249, loss is 0.49033069610595703\n",
      "epoch: 3 step: 2250, loss is 0.4772607088088989\n",
      "epoch: 3 step: 2251, loss is 0.6970847845077515\n",
      "epoch: 3 step: 2252, loss is 0.6867839694023132\n",
      "epoch: 3 step: 2253, loss is 0.7373750805854797\n",
      "epoch: 3 step: 2254, loss is 0.6960009932518005\n",
      "epoch: 3 step: 2255, loss is 0.6245096325874329\n",
      "epoch: 3 step: 2256, loss is 0.8424875140190125\n",
      "epoch: 3 step: 2257, loss is 0.3277398943901062\n",
      "epoch: 3 step: 2258, loss is 0.6472585201263428\n",
      "epoch: 3 step: 2259, loss is 0.5311177372932434\n",
      "epoch: 3 step: 2260, loss is 0.4699708819389343\n",
      "epoch: 3 step: 2261, loss is 0.3431873917579651\n",
      "epoch: 3 step: 2262, loss is 0.608815610408783\n",
      "epoch: 3 step: 2263, loss is 0.31925684213638306\n",
      "epoch: 3 step: 2264, loss is 0.5782809257507324\n",
      "epoch: 3 step: 2265, loss is 0.5438190698623657\n",
      "epoch: 3 step: 2266, loss is 0.5473802089691162\n",
      "epoch: 3 step: 2267, loss is 1.0385500192642212\n",
      "epoch: 3 step: 2268, loss is 0.7978452444076538\n",
      "epoch: 3 step: 2269, loss is 0.8746759295463562\n",
      "epoch: 3 step: 2270, loss is 0.7799686193466187\n",
      "epoch: 3 step: 2271, loss is 0.665681779384613\n",
      "epoch: 3 step: 2272, loss is 0.30240216851234436\n",
      "epoch: 3 step: 2273, loss is 0.33874809741973877\n",
      "epoch: 3 step: 2274, loss is 0.6275831460952759\n",
      "epoch: 3 step: 2275, loss is 0.39989638328552246\n",
      "epoch: 3 step: 2276, loss is 0.42544522881507874\n",
      "epoch: 3 step: 2277, loss is 0.2625293731689453\n",
      "epoch: 3 step: 2278, loss is 0.576185941696167\n",
      "epoch: 3 step: 2279, loss is 0.2994881570339203\n",
      "epoch: 3 step: 2280, loss is 0.3873752951622009\n",
      "epoch: 3 step: 2281, loss is 0.5125694274902344\n",
      "epoch: 3 step: 2282, loss is 0.5687745809555054\n",
      "epoch: 3 step: 2283, loss is 0.4560885429382324\n",
      "epoch: 3 step: 2284, loss is 0.9286491274833679\n",
      "epoch: 3 step: 2285, loss is 0.41219818592071533\n",
      "epoch: 3 step: 2286, loss is 0.4050148129463196\n",
      "epoch: 3 step: 2287, loss is 0.4235115647315979\n",
      "epoch: 3 step: 2288, loss is 0.4688950777053833\n",
      "epoch: 3 step: 2289, loss is 0.6746941208839417\n",
      "epoch: 3 step: 2290, loss is 0.31664952635765076\n",
      "epoch: 3 step: 2291, loss is 0.5416489839553833\n",
      "epoch: 3 step: 2292, loss is 0.38443195819854736\n",
      "epoch: 3 step: 2293, loss is 0.4328237473964691\n",
      "epoch: 3 step: 2294, loss is 0.387147456407547\n",
      "epoch: 3 step: 2295, loss is 0.5489560961723328\n",
      "epoch: 3 step: 2296, loss is 0.5546250343322754\n",
      "epoch: 3 step: 2297, loss is 0.7644170522689819\n",
      "epoch: 3 step: 2298, loss is 0.46572667360305786\n",
      "epoch: 3 step: 2299, loss is 0.7151504755020142\n",
      "epoch: 3 step: 2300, loss is 0.3905569314956665\n",
      "epoch: 3 step: 2301, loss is 0.5828602313995361\n",
      "epoch: 3 step: 2302, loss is 0.3977205157279968\n",
      "epoch: 3 step: 2303, loss is 0.28315144777297974\n",
      "epoch: 3 step: 2304, loss is 0.4466695189476013\n",
      "epoch: 3 step: 2305, loss is 0.38562649488449097\n",
      "epoch: 3 step: 2306, loss is 0.47191354632377625\n",
      "epoch: 3 step: 2307, loss is 0.6064004302024841\n",
      "epoch: 3 step: 2308, loss is 0.18843315541744232\n",
      "epoch: 3 step: 2309, loss is 0.34201979637145996\n",
      "epoch: 3 step: 2310, loss is 0.5617287755012512\n",
      "epoch: 3 step: 2311, loss is 0.3423056900501251\n",
      "epoch: 3 step: 2312, loss is 0.3782613277435303\n",
      "epoch: 3 step: 2313, loss is 0.7914941906929016\n",
      "epoch: 3 step: 2314, loss is 0.6756966710090637\n",
      "epoch: 3 step: 2315, loss is 0.4741728901863098\n",
      "epoch: 3 step: 2316, loss is 0.5943523645401001\n",
      "epoch: 3 step: 2317, loss is 0.3699661195278168\n",
      "epoch: 3 step: 2318, loss is 0.6735406517982483\n",
      "epoch: 3 step: 2319, loss is 0.7998567223548889\n",
      "epoch: 3 step: 2320, loss is 0.6112841367721558\n",
      "epoch: 3 step: 2321, loss is 0.41596129536628723\n",
      "epoch: 3 step: 2322, loss is 0.623261034488678\n",
      "epoch: 3 step: 2323, loss is 0.5763095021247864\n",
      "epoch: 3 step: 2324, loss is 0.5196994543075562\n",
      "epoch: 3 step: 2325, loss is 0.6707862019538879\n",
      "epoch: 3 step: 2326, loss is 0.4385938346385956\n",
      "epoch: 3 step: 2327, loss is 0.3448796570301056\n",
      "epoch: 3 step: 2328, loss is 0.39648211002349854\n",
      "epoch: 3 step: 2329, loss is 0.3631736636161804\n",
      "epoch: 3 step: 2330, loss is 0.2788580358028412\n",
      "epoch: 3 step: 2331, loss is 0.6947002410888672\n",
      "epoch: 3 step: 2332, loss is 0.6397472023963928\n",
      "epoch: 3 step: 2333, loss is 0.3046688139438629\n",
      "epoch: 3 step: 2334, loss is 1.1805732250213623\n",
      "epoch: 3 step: 2335, loss is 0.4024873673915863\n",
      "epoch: 3 step: 2336, loss is 0.5552898645401001\n",
      "epoch: 3 step: 2337, loss is 0.33756303787231445\n",
      "epoch: 3 step: 2338, loss is 0.6202262043952942\n",
      "epoch: 3 step: 2339, loss is 0.3497868776321411\n",
      "epoch: 3 step: 2340, loss is 0.6024200320243835\n",
      "epoch: 3 step: 2341, loss is 0.4487120807170868\n",
      "epoch: 3 step: 2342, loss is 0.7719107270240784\n",
      "epoch: 3 step: 2343, loss is 0.6175333261489868\n",
      "epoch: 3 step: 2344, loss is 0.604242205619812\n",
      "epoch: 3 step: 2345, loss is 0.5566925406455994\n",
      "epoch: 3 step: 2346, loss is 0.5998662114143372\n",
      "epoch: 3 step: 2347, loss is 0.7043750882148743\n",
      "epoch: 3 step: 2348, loss is 0.46111124753952026\n",
      "epoch: 3 step: 2349, loss is 0.828779399394989\n",
      "epoch: 3 step: 2350, loss is 0.5910136699676514\n",
      "epoch: 3 step: 2351, loss is 0.5524314045906067\n",
      "epoch: 3 step: 2352, loss is 0.5792364478111267\n",
      "epoch: 3 step: 2353, loss is 0.753194272518158\n",
      "epoch: 3 step: 2354, loss is 0.8100730776786804\n",
      "epoch: 3 step: 2355, loss is 0.5139539837837219\n",
      "epoch: 3 step: 2356, loss is 0.33122366666793823\n",
      "epoch: 3 step: 2357, loss is 0.4449084401130676\n",
      "epoch: 3 step: 2358, loss is 0.3504236936569214\n",
      "epoch: 3 step: 2359, loss is 0.5024679899215698\n",
      "epoch: 3 step: 2360, loss is 1.0225026607513428\n",
      "epoch: 3 step: 2361, loss is 0.4304559826850891\n",
      "epoch: 3 step: 2362, loss is 0.688666820526123\n",
      "epoch: 3 step: 2363, loss is 0.5168358683586121\n",
      "epoch: 3 step: 2364, loss is 0.4017230272293091\n",
      "epoch: 3 step: 2365, loss is 0.48261186480522156\n",
      "epoch: 3 step: 2366, loss is 0.2843150496482849\n",
      "epoch: 3 step: 2367, loss is 0.2924884855747223\n",
      "epoch: 3 step: 2368, loss is 0.7912625074386597\n",
      "epoch: 3 step: 2369, loss is 0.5999417901039124\n",
      "epoch: 3 step: 2370, loss is 0.3920738399028778\n",
      "epoch: 3 step: 2371, loss is 0.7666030526161194\n",
      "epoch: 3 step: 2372, loss is 0.44754326343536377\n",
      "epoch: 3 step: 2373, loss is 0.2481139600276947\n",
      "epoch: 3 step: 2374, loss is 0.6543031334877014\n",
      "epoch: 3 step: 2375, loss is 0.3395856022834778\n",
      "epoch: 3 step: 2376, loss is 0.38508039712905884\n",
      "epoch: 3 step: 2377, loss is 0.5679564476013184\n",
      "epoch: 3 step: 2378, loss is 0.5205795168876648\n",
      "epoch: 3 step: 2379, loss is 0.29610347747802734\n",
      "epoch: 3 step: 2380, loss is 0.8685438632965088\n",
      "epoch: 3 step: 2381, loss is 0.47406020760536194\n",
      "epoch: 3 step: 2382, loss is 0.5029041767120361\n",
      "epoch: 3 step: 2383, loss is 0.7505949139595032\n",
      "epoch: 3 step: 2384, loss is 0.42677173018455505\n",
      "epoch: 3 step: 2385, loss is 0.526658296585083\n",
      "epoch: 3 step: 2386, loss is 0.4586331844329834\n",
      "epoch: 3 step: 2387, loss is 1.2651896476745605\n",
      "epoch: 3 step: 2388, loss is 0.7333217859268188\n",
      "epoch: 3 step: 2389, loss is 0.438446581363678\n",
      "epoch: 3 step: 2390, loss is 0.331665575504303\n",
      "epoch: 3 step: 2391, loss is 0.8095635175704956\n",
      "epoch: 3 step: 2392, loss is 0.3987710475921631\n",
      "epoch: 3 step: 2393, loss is 0.7074471116065979\n",
      "epoch: 3 step: 2394, loss is 0.3442540764808655\n",
      "epoch: 3 step: 2395, loss is 0.7088199257850647\n",
      "epoch: 3 step: 2396, loss is 0.769105851650238\n",
      "epoch: 3 step: 2397, loss is 0.8042305707931519\n",
      "epoch: 3 step: 2398, loss is 0.31532102823257446\n",
      "epoch: 3 step: 2399, loss is 0.7096062302589417\n",
      "epoch: 3 step: 2400, loss is 0.3394220471382141\n",
      "epoch: 3 step: 2401, loss is 0.387709379196167\n",
      "epoch: 3 step: 2402, loss is 0.600490152835846\n",
      "epoch: 3 step: 2403, loss is 0.5096197128295898\n",
      "epoch: 3 step: 2404, loss is 0.5267296433448792\n",
      "epoch: 3 step: 2405, loss is 0.2994585633277893\n",
      "epoch: 3 step: 2406, loss is 0.6293884515762329\n",
      "epoch: 3 step: 2407, loss is 0.6752728819847107\n",
      "epoch: 3 step: 2408, loss is 0.6492718458175659\n",
      "epoch: 3 step: 2409, loss is 0.5770856738090515\n",
      "epoch: 3 step: 2410, loss is 0.4913240671157837\n",
      "epoch: 3 step: 2411, loss is 0.4794831871986389\n",
      "epoch: 3 step: 2412, loss is 0.5448082089424133\n",
      "epoch: 3 step: 2413, loss is 0.44944387674331665\n",
      "epoch: 3 step: 2414, loss is 0.9020994901657104\n",
      "epoch: 3 step: 2415, loss is 0.4923044741153717\n",
      "epoch: 3 step: 2416, loss is 0.6260091066360474\n",
      "epoch: 3 step: 2417, loss is 0.6411519050598145\n",
      "epoch: 3 step: 2418, loss is 1.0892064571380615\n",
      "epoch: 3 step: 2419, loss is 0.6513026356697083\n",
      "epoch: 3 step: 2420, loss is 0.5358883142471313\n",
      "epoch: 3 step: 2421, loss is 0.35913729667663574\n",
      "epoch: 3 step: 2422, loss is 0.3632555305957794\n",
      "epoch: 3 step: 2423, loss is 0.47914963960647583\n",
      "epoch: 3 step: 2424, loss is 0.5012230277061462\n",
      "epoch: 3 step: 2425, loss is 0.4460882842540741\n",
      "epoch: 3 step: 2426, loss is 0.5378811955451965\n",
      "epoch: 3 step: 2427, loss is 0.4171188771724701\n",
      "epoch: 3 step: 2428, loss is 0.6730799078941345\n",
      "epoch: 3 step: 2429, loss is 0.5589467287063599\n",
      "epoch: 3 step: 2430, loss is 0.6055363416671753\n",
      "epoch: 3 step: 2431, loss is 0.5544221997261047\n",
      "epoch: 3 step: 2432, loss is 0.45898765325546265\n",
      "epoch: 3 step: 2433, loss is 0.37376856803894043\n",
      "epoch: 3 step: 2434, loss is 0.7823289632797241\n",
      "epoch: 3 step: 2435, loss is 0.4321921765804291\n",
      "epoch: 3 step: 2436, loss is 0.6235500574111938\n",
      "epoch: 3 step: 2437, loss is 0.5878969430923462\n",
      "epoch: 3 step: 2438, loss is 1.0574952363967896\n",
      "epoch: 3 step: 2439, loss is 0.6102447509765625\n",
      "epoch: 3 step: 2440, loss is 0.7222970724105835\n",
      "epoch: 3 step: 2441, loss is 0.4181482195854187\n",
      "epoch: 3 step: 2442, loss is 0.7919736504554749\n",
      "epoch: 3 step: 2443, loss is 0.7208385467529297\n",
      "epoch: 3 step: 2444, loss is 0.8234289288520813\n",
      "epoch: 3 step: 2445, loss is 0.7899811863899231\n",
      "epoch: 3 step: 2446, loss is 0.37327080965042114\n",
      "epoch: 3 step: 2447, loss is 0.3977080285549164\n",
      "epoch: 3 step: 2448, loss is 0.5892140865325928\n",
      "epoch: 3 step: 2449, loss is 0.5180663466453552\n",
      "epoch: 3 step: 2450, loss is 0.5837483406066895\n",
      "epoch: 3 step: 2451, loss is 0.6856234669685364\n",
      "epoch: 3 step: 2452, loss is 0.6355401277542114\n",
      "epoch: 3 step: 2453, loss is 0.3982754051685333\n",
      "epoch: 3 step: 2454, loss is 0.6647465825080872\n",
      "epoch: 3 step: 2455, loss is 0.6579404473304749\n",
      "epoch: 3 step: 2456, loss is 0.564643919467926\n",
      "epoch: 3 step: 2457, loss is 0.4494616389274597\n",
      "epoch: 3 step: 2458, loss is 0.6816449165344238\n",
      "epoch: 3 step: 2459, loss is 0.32532745599746704\n",
      "epoch: 3 step: 2460, loss is 0.5077390074729919\n",
      "epoch: 3 step: 2461, loss is 0.7577807903289795\n",
      "epoch: 3 step: 2462, loss is 0.36030834913253784\n",
      "epoch: 3 step: 2463, loss is 0.5809144377708435\n",
      "epoch: 3 step: 2464, loss is 0.41513291001319885\n",
      "epoch: 3 step: 2465, loss is 0.7321723699569702\n",
      "epoch: 3 step: 2466, loss is 0.3645939826965332\n",
      "epoch: 3 step: 2467, loss is 0.5547596216201782\n",
      "epoch: 3 step: 2468, loss is 0.4205080270767212\n",
      "epoch: 3 step: 2469, loss is 0.5229341983795166\n",
      "epoch: 3 step: 2470, loss is 0.6113587617874146\n",
      "epoch: 3 step: 2471, loss is 0.20403838157653809\n",
      "epoch: 3 step: 2472, loss is 0.40045228600502014\n",
      "epoch: 3 step: 2473, loss is 0.5393496751785278\n",
      "epoch: 3 step: 2474, loss is 0.8404419422149658\n",
      "epoch: 3 step: 2475, loss is 0.6172011494636536\n",
      "epoch: 3 step: 2476, loss is 0.6059396266937256\n",
      "epoch: 3 step: 2477, loss is 0.590673565864563\n",
      "epoch: 3 step: 2478, loss is 0.30476489663124084\n",
      "epoch: 3 step: 2479, loss is 0.2099011391401291\n",
      "epoch: 3 step: 2480, loss is 0.7415971755981445\n",
      "epoch: 3 step: 2481, loss is 0.3474908769130707\n",
      "epoch: 3 step: 2482, loss is 0.34236252307891846\n",
      "epoch: 3 step: 2483, loss is 0.7446506023406982\n",
      "epoch: 3 step: 2484, loss is 1.1112982034683228\n",
      "epoch: 3 step: 2485, loss is 0.31862810254096985\n",
      "epoch: 3 step: 2486, loss is 0.3904189467430115\n",
      "epoch: 3 step: 2487, loss is 0.6495524644851685\n",
      "epoch: 3 step: 2488, loss is 0.2904713451862335\n",
      "epoch: 3 step: 2489, loss is 0.8060077428817749\n",
      "epoch: 3 step: 2490, loss is 0.5480477809906006\n",
      "epoch: 3 step: 2491, loss is 0.576170802116394\n",
      "epoch: 3 step: 2492, loss is 0.7099308371543884\n",
      "epoch: 3 step: 2493, loss is 0.995832622051239\n",
      "epoch: 3 step: 2494, loss is 0.5493991374969482\n",
      "epoch: 3 step: 2495, loss is 0.3945104777812958\n",
      "epoch: 3 step: 2496, loss is 0.6721556186676025\n",
      "epoch: 3 step: 2497, loss is 0.498981237411499\n",
      "epoch: 3 step: 2498, loss is 0.37910547852516174\n",
      "epoch: 3 step: 2499, loss is 0.5287628769874573\n",
      "epoch: 3 step: 2500, loss is 0.43295401334762573\n",
      "epoch: 3 step: 2501, loss is 0.47509485483169556\n",
      "epoch: 3 step: 2502, loss is 0.7230702042579651\n",
      "epoch: 3 step: 2503, loss is 0.377086877822876\n",
      "epoch: 3 step: 2504, loss is 0.47062453627586365\n",
      "epoch: 3 step: 2505, loss is 0.7765193581581116\n",
      "epoch: 3 step: 2506, loss is 0.41021496057510376\n",
      "epoch: 3 step: 2507, loss is 0.3267308473587036\n",
      "epoch: 3 step: 2508, loss is 0.8802694082260132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: unknown JFIF revision number 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 1, loss is 0.47063493728637695\n",
      "epoch: 4 step: 2, loss is 0.586329996585846\n",
      "epoch: 4 step: 3, loss is 0.5602282881736755\n",
      "epoch: 4 step: 4, loss is 0.47704917192459106\n",
      "epoch: 4 step: 5, loss is 0.634671151638031\n",
      "epoch: 4 step: 6, loss is 0.4510873258113861\n",
      "epoch: 4 step: 7, loss is 0.6007012128829956\n",
      "epoch: 4 step: 8, loss is 0.24606826901435852\n",
      "epoch: 4 step: 9, loss is 0.4211300313472748\n",
      "epoch: 4 step: 10, loss is 0.6304262280464172\n",
      "epoch: 4 step: 11, loss is 0.8232211470603943\n",
      "epoch: 4 step: 12, loss is 0.45671093463897705\n",
      "epoch: 4 step: 13, loss is 0.5684467554092407\n",
      "epoch: 4 step: 14, loss is 0.4865541458129883\n",
      "epoch: 4 step: 15, loss is 0.5513104796409607\n",
      "epoch: 4 step: 16, loss is 0.5090092420578003\n",
      "epoch: 4 step: 17, loss is 0.5836883187294006\n",
      "epoch: 4 step: 18, loss is 0.5037916898727417\n",
      "epoch: 4 step: 19, loss is 0.2364293932914734\n",
      "epoch: 4 step: 20, loss is 0.45762065052986145\n",
      "epoch: 4 step: 21, loss is 0.38240018486976624\n",
      "epoch: 4 step: 22, loss is 0.7304754257202148\n",
      "epoch: 4 step: 23, loss is 0.8149653077125549\n",
      "epoch: 4 step: 24, loss is 0.42478683590888977\n",
      "epoch: 4 step: 25, loss is 0.28130072355270386\n",
      "epoch: 4 step: 26, loss is 0.9830409288406372\n",
      "epoch: 4 step: 27, loss is 0.7125062942504883\n",
      "epoch: 4 step: 28, loss is 0.33046555519104004\n",
      "epoch: 4 step: 29, loss is 0.422067791223526\n",
      "epoch: 4 step: 30, loss is 0.6177076697349548\n",
      "epoch: 4 step: 31, loss is 0.4290132522583008\n",
      "epoch: 4 step: 32, loss is 0.44407734274864197\n",
      "epoch: 4 step: 33, loss is 0.6369706988334656\n",
      "epoch: 4 step: 34, loss is 0.542255163192749\n",
      "epoch: 4 step: 35, loss is 0.7629895806312561\n",
      "epoch: 4 step: 36, loss is 0.745644748210907\n",
      "epoch: 4 step: 37, loss is 0.769898533821106\n",
      "epoch: 4 step: 38, loss is 0.5354893207550049\n",
      "epoch: 4 step: 39, loss is 0.7328584790229797\n",
      "epoch: 4 step: 40, loss is 0.3892506957054138\n",
      "epoch: 4 step: 41, loss is 0.4704423248767853\n",
      "epoch: 4 step: 42, loss is 0.5429728627204895\n",
      "epoch: 4 step: 43, loss is 0.35934630036354065\n",
      "epoch: 4 step: 44, loss is 0.7306066751480103\n",
      "epoch: 4 step: 45, loss is 0.44673964381217957\n",
      "epoch: 4 step: 46, loss is 0.31481030583381653\n",
      "epoch: 4 step: 47, loss is 0.8960313200950623\n",
      "epoch: 4 step: 48, loss is 0.42681747674942017\n",
      "epoch: 4 step: 49, loss is 0.3617264926433563\n",
      "epoch: 4 step: 50, loss is 0.6445066928863525\n",
      "epoch: 4 step: 51, loss is 0.4450172185897827\n",
      "epoch: 4 step: 52, loss is 0.40680089592933655\n",
      "epoch: 4 step: 53, loss is 0.35834968090057373\n",
      "epoch: 4 step: 54, loss is 0.4482046961784363\n",
      "epoch: 4 step: 55, loss is 0.5181604623794556\n",
      "epoch: 4 step: 56, loss is 0.9147703647613525\n",
      "epoch: 4 step: 57, loss is 0.39099156856536865\n",
      "epoch: 4 step: 58, loss is 0.5811529755592346\n",
      "epoch: 4 step: 59, loss is 0.7164014577865601\n",
      "epoch: 4 step: 60, loss is 0.7640097141265869\n",
      "epoch: 4 step: 61, loss is 0.31543877720832825\n",
      "epoch: 4 step: 62, loss is 0.4139288365840912\n",
      "epoch: 4 step: 63, loss is 0.3375999331474304\n",
      "epoch: 4 step: 64, loss is 0.465383917093277\n",
      "epoch: 4 step: 65, loss is 0.6580440998077393\n",
      "epoch: 4 step: 66, loss is 0.5142519474029541\n",
      "epoch: 4 step: 67, loss is 0.5777621269226074\n",
      "epoch: 4 step: 68, loss is 0.4101928472518921\n",
      "epoch: 4 step: 69, loss is 0.4933033287525177\n",
      "epoch: 4 step: 70, loss is 0.4738917648792267\n",
      "epoch: 4 step: 71, loss is 0.28429242968559265\n",
      "epoch: 4 step: 72, loss is 0.6753104329109192\n",
      "epoch: 4 step: 73, loss is 0.5549208521842957\n",
      "epoch: 4 step: 74, loss is 0.3808441758155823\n",
      "epoch: 4 step: 75, loss is 0.45857861638069153\n",
      "epoch: 4 step: 76, loss is 0.6585512161254883\n",
      "epoch: 4 step: 77, loss is 0.874740481376648\n",
      "epoch: 4 step: 78, loss is 0.3729499280452728\n",
      "epoch: 4 step: 79, loss is 0.31530171632766724\n",
      "epoch: 4 step: 80, loss is 0.728019118309021\n",
      "epoch: 4 step: 81, loss is 0.3733573853969574\n",
      "epoch: 4 step: 82, loss is 0.48431500792503357\n",
      "epoch: 4 step: 83, loss is 0.5077307224273682\n",
      "epoch: 4 step: 84, loss is 0.6373372673988342\n",
      "epoch: 4 step: 85, loss is 0.3847924470901489\n",
      "epoch: 4 step: 86, loss is 0.6660209894180298\n",
      "epoch: 4 step: 87, loss is 0.4444543123245239\n",
      "epoch: 4 step: 88, loss is 0.7349475026130676\n",
      "epoch: 4 step: 89, loss is 0.640663206577301\n",
      "epoch: 4 step: 90, loss is 0.3536975681781769\n",
      "epoch: 4 step: 91, loss is 0.41329944133758545\n",
      "epoch: 4 step: 92, loss is 0.8989132046699524\n",
      "epoch: 4 step: 93, loss is 0.5337980389595032\n",
      "epoch: 4 step: 94, loss is 0.4547122120857239\n",
      "epoch: 4 step: 95, loss is 0.27762097120285034\n",
      "epoch: 4 step: 96, loss is 0.7019311785697937\n",
      "epoch: 4 step: 97, loss is 0.5323519706726074\n",
      "epoch: 4 step: 98, loss is 0.5723868608474731\n",
      "epoch: 4 step: 99, loss is 0.7048978805541992\n",
      "epoch: 4 step: 100, loss is 0.5203211307525635\n",
      "epoch: 4 step: 101, loss is 0.5903999209403992\n",
      "epoch: 4 step: 102, loss is 0.8134142756462097\n",
      "epoch: 4 step: 103, loss is 0.6567729711532593\n",
      "epoch: 4 step: 104, loss is 0.38998767733573914\n",
      "epoch: 4 step: 105, loss is 0.36306232213974\n",
      "epoch: 4 step: 106, loss is 0.21673855185508728\n",
      "epoch: 4 step: 107, loss is 0.3773174583911896\n",
      "epoch: 4 step: 108, loss is 0.5998795032501221\n",
      "epoch: 4 step: 109, loss is 0.47483792901039124\n",
      "epoch: 4 step: 110, loss is 0.5807011127471924\n",
      "epoch: 4 step: 111, loss is 0.48409026861190796\n",
      "epoch: 4 step: 112, loss is 0.36547720432281494\n",
      "epoch: 4 step: 113, loss is 0.4974985420703888\n",
      "epoch: 4 step: 114, loss is 0.808167576789856\n",
      "epoch: 4 step: 115, loss is 0.49938783049583435\n",
      "epoch: 4 step: 116, loss is 0.34370508790016174\n",
      "epoch: 4 step: 117, loss is 0.45614731311798096\n",
      "epoch: 4 step: 118, loss is 0.567162275314331\n",
      "epoch: 4 step: 119, loss is 0.41959694027900696\n",
      "epoch: 4 step: 120, loss is 0.47324690222740173\n",
      "epoch: 4 step: 121, loss is 0.6661868691444397\n",
      "epoch: 4 step: 122, loss is 0.43790510296821594\n",
      "epoch: 4 step: 123, loss is 0.2940394878387451\n",
      "epoch: 4 step: 124, loss is 0.4622569680213928\n",
      "epoch: 4 step: 125, loss is 0.3383208215236664\n",
      "epoch: 4 step: 126, loss is 0.5813906788825989\n",
      "epoch: 4 step: 127, loss is 0.36346420645713806\n",
      "epoch: 4 step: 128, loss is 0.7666761875152588\n",
      "epoch: 4 step: 129, loss is 0.6719377636909485\n",
      "epoch: 4 step: 130, loss is 0.5829347968101501\n",
      "epoch: 4 step: 131, loss is 0.5287244319915771\n",
      "epoch: 4 step: 132, loss is 0.5334671139717102\n",
      "epoch: 4 step: 133, loss is 0.5860389471054077\n",
      "epoch: 4 step: 134, loss is 0.4416339695453644\n",
      "epoch: 4 step: 135, loss is 0.42081841826438904\n",
      "epoch: 4 step: 136, loss is 0.6319595575332642\n",
      "epoch: 4 step: 137, loss is 0.555181086063385\n",
      "epoch: 4 step: 138, loss is 0.41695886850357056\n",
      "epoch: 4 step: 139, loss is 0.9522027969360352\n",
      "epoch: 4 step: 140, loss is 0.5695781707763672\n",
      "epoch: 4 step: 141, loss is 0.5173420310020447\n",
      "epoch: 4 step: 142, loss is 0.23104789853096008\n",
      "epoch: 4 step: 143, loss is 0.520064115524292\n",
      "epoch: 4 step: 144, loss is 1.0923742055892944\n",
      "epoch: 4 step: 145, loss is 0.49132266640663147\n",
      "epoch: 4 step: 146, loss is 0.49542686343193054\n",
      "epoch: 4 step: 147, loss is 0.3130701184272766\n",
      "epoch: 4 step: 148, loss is 0.54082852602005\n",
      "epoch: 4 step: 149, loss is 0.44726425409317017\n",
      "epoch: 4 step: 150, loss is 0.5924412608146667\n",
      "epoch: 4 step: 151, loss is 0.4770112931728363\n",
      "epoch: 4 step: 152, loss is 0.48574528098106384\n",
      "epoch: 4 step: 153, loss is 0.6361874341964722\n",
      "epoch: 4 step: 154, loss is 0.22516748309135437\n",
      "epoch: 4 step: 155, loss is 0.4302631616592407\n",
      "epoch: 4 step: 156, loss is 0.3393186330795288\n",
      "epoch: 4 step: 157, loss is 0.7585248947143555\n",
      "epoch: 4 step: 158, loss is 0.8805384635925293\n",
      "epoch: 4 step: 159, loss is 0.2587376832962036\n",
      "epoch: 4 step: 160, loss is 0.5213722586631775\n",
      "epoch: 4 step: 161, loss is 0.5224723815917969\n",
      "epoch: 4 step: 162, loss is 0.61521977186203\n",
      "epoch: 4 step: 163, loss is 0.5339712500572205\n",
      "epoch: 4 step: 164, loss is 0.40523847937583923\n",
      "epoch: 4 step: 165, loss is 0.18079090118408203\n",
      "epoch: 4 step: 166, loss is 0.5907058119773865\n",
      "epoch: 4 step: 167, loss is 0.3144970238208771\n",
      "epoch: 4 step: 168, loss is 0.9482128024101257\n",
      "epoch: 4 step: 169, loss is 0.534543514251709\n",
      "epoch: 4 step: 170, loss is 0.40943828225135803\n",
      "epoch: 4 step: 171, loss is 0.47846055030822754\n",
      "epoch: 4 step: 172, loss is 0.5784698724746704\n",
      "epoch: 4 step: 173, loss is 0.3981625437736511\n",
      "epoch: 4 step: 174, loss is 0.551379382610321\n",
      "epoch: 4 step: 175, loss is 0.30496346950531006\n",
      "epoch: 4 step: 176, loss is 0.831285834312439\n",
      "epoch: 4 step: 177, loss is 0.2691884934902191\n",
      "epoch: 4 step: 178, loss is 0.6023937463760376\n",
      "epoch: 4 step: 179, loss is 0.4304918050765991\n",
      "epoch: 4 step: 180, loss is 0.5099965929985046\n",
      "epoch: 4 step: 181, loss is 0.7328993678092957\n",
      "epoch: 4 step: 182, loss is 0.20976486802101135\n",
      "epoch: 4 step: 183, loss is 0.9035364985466003\n",
      "epoch: 4 step: 184, loss is 0.7962300181388855\n",
      "epoch: 4 step: 185, loss is 0.40757107734680176\n",
      "epoch: 4 step: 186, loss is 0.4290466606616974\n",
      "epoch: 4 step: 187, loss is 0.6470420360565186\n",
      "epoch: 4 step: 188, loss is 0.7712609171867371\n",
      "epoch: 4 step: 189, loss is 0.30509212613105774\n",
      "epoch: 4 step: 190, loss is 0.6566291451454163\n",
      "epoch: 4 step: 191, loss is 0.5885331034660339\n",
      "epoch: 4 step: 192, loss is 0.47243085503578186\n",
      "epoch: 4 step: 193, loss is 0.383136123418808\n",
      "epoch: 4 step: 194, loss is 0.4936710000038147\n",
      "epoch: 4 step: 195, loss is 0.6733909845352173\n",
      "epoch: 4 step: 196, loss is 0.42641618847846985\n",
      "epoch: 4 step: 197, loss is 1.0023707151412964\n",
      "epoch: 4 step: 198, loss is 0.8779788017272949\n",
      "epoch: 4 step: 199, loss is 0.6283059120178223\n",
      "epoch: 4 step: 200, loss is 0.6950348019599915\n",
      "epoch: 4 step: 201, loss is 0.4772171974182129\n",
      "epoch: 4 step: 202, loss is 0.47269678115844727\n",
      "epoch: 4 step: 203, loss is 0.5233224034309387\n",
      "epoch: 4 step: 204, loss is 0.3444669842720032\n",
      "epoch: 4 step: 205, loss is 0.5350462794303894\n",
      "epoch: 4 step: 206, loss is 0.400240421295166\n",
      "epoch: 4 step: 207, loss is 0.49394524097442627\n",
      "epoch: 4 step: 208, loss is 0.5420909523963928\n",
      "epoch: 4 step: 209, loss is 0.5371527671813965\n",
      "epoch: 4 step: 210, loss is 0.7026546597480774\n",
      "epoch: 4 step: 211, loss is 0.6501866579055786\n",
      "epoch: 4 step: 212, loss is 0.6119253039360046\n",
      "epoch: 4 step: 213, loss is 0.3480578660964966\n",
      "epoch: 4 step: 214, loss is 0.3863023519515991\n",
      "epoch: 4 step: 215, loss is 0.5243073105812073\n",
      "epoch: 4 step: 216, loss is 0.719735860824585\n",
      "epoch: 4 step: 217, loss is 0.5415207743644714\n",
      "epoch: 4 step: 218, loss is 0.6999555826187134\n",
      "epoch: 4 step: 219, loss is 0.6397889256477356\n",
      "epoch: 4 step: 220, loss is 0.5477522015571594\n",
      "epoch: 4 step: 221, loss is 0.5121470093727112\n",
      "epoch: 4 step: 222, loss is 0.4658154547214508\n",
      "epoch: 4 step: 223, loss is 0.4187755286693573\n",
      "epoch: 4 step: 224, loss is 0.3980872929096222\n",
      "epoch: 4 step: 225, loss is 0.45107558369636536\n",
      "epoch: 4 step: 226, loss is 0.3701002299785614\n",
      "epoch: 4 step: 227, loss is 0.32768920063972473\n",
      "epoch: 4 step: 228, loss is 0.9005088210105896\n",
      "epoch: 4 step: 229, loss is 0.34939679503440857\n",
      "epoch: 4 step: 230, loss is 0.5826619267463684\n",
      "epoch: 4 step: 231, loss is 0.6348726153373718\n",
      "epoch: 4 step: 232, loss is 0.616118848323822\n",
      "epoch: 4 step: 233, loss is 0.686559796333313\n",
      "epoch: 4 step: 234, loss is 0.23928096890449524\n",
      "epoch: 4 step: 235, loss is 0.5451952219009399\n",
      "epoch: 4 step: 236, loss is 0.7052175402641296\n",
      "epoch: 4 step: 237, loss is 0.5522440075874329\n",
      "epoch: 4 step: 238, loss is 0.1791432499885559\n",
      "epoch: 4 step: 239, loss is 0.5265095829963684\n",
      "epoch: 4 step: 240, loss is 0.45353999733924866\n",
      "epoch: 4 step: 241, loss is 0.7521791458129883\n",
      "epoch: 4 step: 242, loss is 0.49118128418922424\n",
      "epoch: 4 step: 243, loss is 0.49400851130485535\n",
      "epoch: 4 step: 244, loss is 0.7210299968719482\n",
      "epoch: 4 step: 245, loss is 0.7994980216026306\n",
      "epoch: 4 step: 246, loss is 0.5470036864280701\n",
      "epoch: 4 step: 247, loss is 0.7067407369613647\n",
      "epoch: 4 step: 248, loss is 0.5672546625137329\n",
      "epoch: 4 step: 249, loss is 0.4069597125053406\n",
      "epoch: 4 step: 250, loss is 0.41823214292526245\n",
      "epoch: 4 step: 251, loss is 0.5791977047920227\n",
      "epoch: 4 step: 252, loss is 0.4343075156211853\n",
      "epoch: 4 step: 253, loss is 0.48650115728378296\n",
      "epoch: 4 step: 254, loss is 0.541225016117096\n",
      "epoch: 4 step: 255, loss is 0.3500819802284241\n",
      "epoch: 4 step: 256, loss is 0.14488543570041656\n",
      "epoch: 4 step: 257, loss is 0.774097740650177\n",
      "epoch: 4 step: 258, loss is 0.43788784742355347\n",
      "epoch: 4 step: 259, loss is 0.6740282773971558\n",
      "epoch: 4 step: 260, loss is 0.5831636190414429\n",
      "epoch: 4 step: 261, loss is 1.0267314910888672\n",
      "epoch: 4 step: 262, loss is 0.38359472155570984\n",
      "epoch: 4 step: 263, loss is 0.2901124954223633\n",
      "epoch: 4 step: 264, loss is 0.34111738204956055\n",
      "epoch: 4 step: 265, loss is 0.3352351486682892\n",
      "epoch: 4 step: 266, loss is 0.5499038100242615\n",
      "epoch: 4 step: 267, loss is 0.7028020024299622\n",
      "epoch: 4 step: 268, loss is 0.5813814997673035\n",
      "epoch: 4 step: 269, loss is 0.6586637496948242\n",
      "epoch: 4 step: 270, loss is 0.5431640148162842\n",
      "epoch: 4 step: 271, loss is 0.8565876483917236\n",
      "epoch: 4 step: 272, loss is 0.4423712193965912\n",
      "epoch: 4 step: 273, loss is 0.7173891663551331\n",
      "epoch: 4 step: 274, loss is 0.4528108537197113\n",
      "epoch: 4 step: 275, loss is 0.5699753165245056\n",
      "epoch: 4 step: 276, loss is 0.7184575200080872\n",
      "epoch: 4 step: 277, loss is 0.5726088881492615\n",
      "epoch: 4 step: 278, loss is 0.50882488489151\n",
      "epoch: 4 step: 279, loss is 0.4342072010040283\n",
      "epoch: 4 step: 280, loss is 0.5007297992706299\n",
      "epoch: 4 step: 281, loss is 0.56962651014328\n",
      "epoch: 4 step: 282, loss is 0.3102235794067383\n",
      "epoch: 4 step: 283, loss is 0.87624591588974\n",
      "epoch: 4 step: 284, loss is 0.4291976988315582\n",
      "epoch: 4 step: 285, loss is 0.25751614570617676\n",
      "epoch: 4 step: 286, loss is 0.806918740272522\n",
      "epoch: 4 step: 287, loss is 0.4823116660118103\n",
      "epoch: 4 step: 288, loss is 0.7895188927650452\n",
      "epoch: 4 step: 289, loss is 0.6327154636383057\n",
      "epoch: 4 step: 290, loss is 0.6324105858802795\n",
      "epoch: 4 step: 291, loss is 0.7360200881958008\n",
      "epoch: 4 step: 292, loss is 0.3814677298069\n",
      "epoch: 4 step: 293, loss is 0.3582361936569214\n",
      "epoch: 4 step: 294, loss is 0.6943390369415283\n",
      "epoch: 4 step: 295, loss is 0.41170844435691833\n",
      "epoch: 4 step: 296, loss is 0.4383357763290405\n",
      "epoch: 4 step: 297, loss is 0.29722756147384644\n",
      "epoch: 4 step: 298, loss is 1.0739991664886475\n",
      "epoch: 4 step: 299, loss is 0.4837057888507843\n",
      "epoch: 4 step: 300, loss is 0.780858039855957\n",
      "epoch: 4 step: 301, loss is 0.7276265621185303\n",
      "epoch: 4 step: 302, loss is 0.5965406894683838\n",
      "epoch: 4 step: 303, loss is 0.40296775102615356\n",
      "epoch: 4 step: 304, loss is 0.3283872902393341\n",
      "epoch: 4 step: 305, loss is 0.38133659958839417\n",
      "epoch: 4 step: 306, loss is 0.41171789169311523\n",
      "epoch: 4 step: 307, loss is 0.6052684783935547\n",
      "epoch: 4 step: 308, loss is 1.0402257442474365\n",
      "epoch: 4 step: 309, loss is 0.44874516129493713\n",
      "epoch: 4 step: 310, loss is 0.4185144603252411\n",
      "epoch: 4 step: 311, loss is 0.48857149481773376\n",
      "epoch: 4 step: 312, loss is 0.34691545367240906\n",
      "epoch: 4 step: 313, loss is 0.3690587282180786\n",
      "epoch: 4 step: 314, loss is 0.33674174547195435\n",
      "epoch: 4 step: 315, loss is 0.4622744023799896\n",
      "epoch: 4 step: 316, loss is 0.5503842830657959\n",
      "epoch: 4 step: 317, loss is 0.35754093527793884\n",
      "epoch: 4 step: 318, loss is 0.9906719326972961\n",
      "epoch: 4 step: 319, loss is 0.609310507774353\n",
      "epoch: 4 step: 320, loss is 0.526960015296936\n",
      "epoch: 4 step: 321, loss is 0.4996417164802551\n",
      "epoch: 4 step: 322, loss is 0.4125199317932129\n",
      "epoch: 4 step: 323, loss is 0.6756530404090881\n",
      "epoch: 4 step: 324, loss is 0.6653704643249512\n",
      "epoch: 4 step: 325, loss is 0.8514646291732788\n",
      "epoch: 4 step: 326, loss is 0.8089942932128906\n",
      "epoch: 4 step: 327, loss is 0.2573765516281128\n",
      "epoch: 4 step: 328, loss is 0.31258273124694824\n",
      "epoch: 4 step: 329, loss is 0.27153539657592773\n",
      "epoch: 4 step: 330, loss is 0.2931949496269226\n",
      "epoch: 4 step: 331, loss is 0.834739625453949\n",
      "epoch: 4 step: 332, loss is 0.5775638222694397\n",
      "epoch: 4 step: 333, loss is 0.7345520257949829\n",
      "epoch: 4 step: 334, loss is 0.6733890771865845\n",
      "epoch: 4 step: 335, loss is 0.514151930809021\n",
      "epoch: 4 step: 336, loss is 0.7662152647972107\n",
      "epoch: 4 step: 337, loss is 0.2684442698955536\n",
      "epoch: 4 step: 338, loss is 0.4020444452762604\n",
      "epoch: 4 step: 339, loss is 0.6645806431770325\n",
      "epoch: 4 step: 340, loss is 0.2593259811401367\n",
      "epoch: 4 step: 341, loss is 0.31071725487709045\n",
      "epoch: 4 step: 342, loss is 0.34758761525154114\n",
      "epoch: 4 step: 343, loss is 0.6921160221099854\n",
      "epoch: 4 step: 344, loss is 0.621910572052002\n",
      "epoch: 4 step: 345, loss is 0.6282104253768921\n",
      "epoch: 4 step: 346, loss is 0.46840086579322815\n",
      "epoch: 4 step: 347, loss is 0.6408905386924744\n",
      "epoch: 4 step: 348, loss is 0.32203298807144165\n",
      "epoch: 4 step: 349, loss is 0.6093820929527283\n",
      "epoch: 4 step: 350, loss is 0.48712053894996643\n",
      "epoch: 4 step: 351, loss is 0.703001856803894\n",
      "epoch: 4 step: 352, loss is 0.6666585803031921\n",
      "epoch: 4 step: 353, loss is 0.3962779641151428\n",
      "epoch: 4 step: 354, loss is 0.40445154905319214\n",
      "epoch: 4 step: 355, loss is 0.261588990688324\n",
      "epoch: 4 step: 356, loss is 0.8674411177635193\n",
      "epoch: 4 step: 357, loss is 0.4122219681739807\n",
      "epoch: 4 step: 358, loss is 0.5376685857772827\n",
      "epoch: 4 step: 359, loss is 0.6175572872161865\n",
      "epoch: 4 step: 360, loss is 0.3490957021713257\n",
      "epoch: 4 step: 361, loss is 0.7951705455780029\n",
      "epoch: 4 step: 362, loss is 0.4801737368106842\n",
      "epoch: 4 step: 363, loss is 0.2893582582473755\n",
      "epoch: 4 step: 364, loss is 0.37739297747612\n",
      "epoch: 4 step: 365, loss is 0.3703716993331909\n",
      "epoch: 4 step: 366, loss is 0.38805097341537476\n",
      "epoch: 4 step: 367, loss is 0.6227799654006958\n",
      "epoch: 4 step: 368, loss is 0.4274027943611145\n",
      "epoch: 4 step: 369, loss is 0.5134307742118835\n",
      "epoch: 4 step: 370, loss is 0.21392840147018433\n",
      "epoch: 4 step: 371, loss is 0.2811385691165924\n",
      "epoch: 4 step: 372, loss is 0.3356291651725769\n",
      "epoch: 4 step: 373, loss is 0.921708345413208\n",
      "epoch: 4 step: 374, loss is 0.4043377637863159\n",
      "epoch: 4 step: 375, loss is 0.6155725121498108\n",
      "epoch: 4 step: 376, loss is 0.45604121685028076\n",
      "epoch: 4 step: 377, loss is 0.5445832014083862\n",
      "epoch: 4 step: 378, loss is 0.3507373631000519\n",
      "epoch: 4 step: 379, loss is 0.7016348242759705\n",
      "epoch: 4 step: 380, loss is 0.4187343418598175\n",
      "epoch: 4 step: 381, loss is 0.6846960186958313\n",
      "epoch: 4 step: 382, loss is 0.5434343814849854\n",
      "epoch: 4 step: 383, loss is 0.5723025798797607\n",
      "epoch: 4 step: 384, loss is 0.4811706244945526\n",
      "epoch: 4 step: 385, loss is 0.49098002910614014\n",
      "epoch: 4 step: 386, loss is 0.6100888252258301\n",
      "epoch: 4 step: 387, loss is 0.6028781533241272\n",
      "epoch: 4 step: 388, loss is 0.4999294877052307\n",
      "epoch: 4 step: 389, loss is 0.4805796444416046\n",
      "epoch: 4 step: 390, loss is 0.7691061496734619\n",
      "epoch: 4 step: 391, loss is 0.5018787384033203\n",
      "epoch: 4 step: 392, loss is 0.625609278678894\n",
      "epoch: 4 step: 393, loss is 0.48919811844825745\n",
      "epoch: 4 step: 394, loss is 0.3622061312198639\n",
      "epoch: 4 step: 395, loss is 0.44941821694374084\n",
      "epoch: 4 step: 396, loss is 0.4425024390220642\n",
      "epoch: 4 step: 397, loss is 0.2985767424106598\n",
      "epoch: 4 step: 398, loss is 0.34158629179000854\n",
      "epoch: 4 step: 399, loss is 0.4982040524482727\n",
      "epoch: 4 step: 400, loss is 0.3679191768169403\n",
      "epoch: 4 step: 401, loss is 0.5399782657623291\n",
      "epoch: 4 step: 402, loss is 0.3029431998729706\n",
      "epoch: 4 step: 403, loss is 1.2241785526275635\n",
      "epoch: 4 step: 404, loss is 0.685016393661499\n",
      "epoch: 4 step: 405, loss is 0.8266165256500244\n",
      "epoch: 4 step: 406, loss is 0.6754000782966614\n",
      "epoch: 4 step: 407, loss is 0.7310541868209839\n",
      "epoch: 4 step: 408, loss is 0.8894222378730774\n",
      "epoch: 4 step: 409, loss is 0.5211102962493896\n",
      "epoch: 4 step: 410, loss is 0.6888635754585266\n",
      "epoch: 4 step: 411, loss is 0.6750065088272095\n",
      "epoch: 4 step: 412, loss is 0.8251789212226868\n",
      "epoch: 4 step: 413, loss is 0.6745325922966003\n",
      "epoch: 4 step: 414, loss is 0.8131852746009827\n",
      "epoch: 4 step: 415, loss is 0.9322676062583923\n",
      "epoch: 4 step: 416, loss is 0.844939649105072\n",
      "epoch: 4 step: 417, loss is 0.4198983609676361\n",
      "epoch: 4 step: 418, loss is 0.4330458641052246\n",
      "epoch: 4 step: 419, loss is 0.6198598742485046\n",
      "epoch: 4 step: 420, loss is 0.4607796370983124\n",
      "epoch: 4 step: 421, loss is 0.6936237812042236\n",
      "epoch: 4 step: 422, loss is 0.5738994479179382\n",
      "epoch: 4 step: 423, loss is 0.37921079993247986\n",
      "epoch: 4 step: 424, loss is 0.5971966981887817\n",
      "epoch: 4 step: 425, loss is 0.5281181931495667\n",
      "epoch: 4 step: 426, loss is 0.4913359582424164\n",
      "epoch: 4 step: 427, loss is 0.6601575613021851\n",
      "epoch: 4 step: 428, loss is 0.41428542137145996\n",
      "epoch: 4 step: 429, loss is 0.5655626654624939\n",
      "epoch: 4 step: 430, loss is 0.3711550533771515\n",
      "epoch: 4 step: 431, loss is 0.38258734345436096\n",
      "epoch: 4 step: 432, loss is 0.5051818490028381\n",
      "epoch: 4 step: 433, loss is 0.7718519568443298\n",
      "epoch: 4 step: 434, loss is 0.5180830359458923\n",
      "epoch: 4 step: 435, loss is 0.33194100856781006\n",
      "epoch: 4 step: 436, loss is 0.7928541302680969\n",
      "epoch: 4 step: 437, loss is 0.5554627180099487\n",
      "epoch: 4 step: 438, loss is 0.6272516250610352\n",
      "epoch: 4 step: 439, loss is 0.5344005227088928\n",
      "epoch: 4 step: 440, loss is 0.3185669183731079\n",
      "epoch: 4 step: 441, loss is 0.6199219822883606\n",
      "epoch: 4 step: 442, loss is 0.5912314653396606\n",
      "epoch: 4 step: 443, loss is 0.15543152391910553\n",
      "epoch: 4 step: 444, loss is 0.5682343244552612\n",
      "epoch: 4 step: 445, loss is 0.4194006323814392\n",
      "epoch: 4 step: 446, loss is 0.3387182652950287\n",
      "epoch: 4 step: 447, loss is 0.6590132713317871\n",
      "epoch: 4 step: 448, loss is 0.4820553958415985\n",
      "epoch: 4 step: 449, loss is 0.5861194729804993\n",
      "epoch: 4 step: 450, loss is 0.6581329107284546\n",
      "epoch: 4 step: 451, loss is 0.8050237894058228\n",
      "epoch: 4 step: 452, loss is 0.6161087155342102\n",
      "epoch: 4 step: 453, loss is 0.19994650781154633\n",
      "epoch: 4 step: 454, loss is 0.8260250687599182\n",
      "epoch: 4 step: 455, loss is 0.503373920917511\n",
      "epoch: 4 step: 456, loss is 0.6945163607597351\n",
      "epoch: 4 step: 457, loss is 0.2736159563064575\n",
      "epoch: 4 step: 458, loss is 0.5604013800621033\n",
      "epoch: 4 step: 459, loss is 0.6416597366333008\n",
      "epoch: 4 step: 460, loss is 0.5197882652282715\n",
      "epoch: 4 step: 461, loss is 0.8641679286956787\n",
      "epoch: 4 step: 462, loss is 0.4345557689666748\n",
      "epoch: 4 step: 463, loss is 0.6773726940155029\n",
      "epoch: 4 step: 464, loss is 0.3055315315723419\n",
      "epoch: 4 step: 465, loss is 0.5124759078025818\n",
      "epoch: 4 step: 466, loss is 0.35714811086654663\n",
      "epoch: 4 step: 467, loss is 0.5283294916152954\n",
      "epoch: 4 step: 468, loss is 0.5753911733627319\n",
      "epoch: 4 step: 469, loss is 0.28707465529441833\n",
      "epoch: 4 step: 470, loss is 0.3390423655509949\n",
      "epoch: 4 step: 471, loss is 0.2808476686477661\n",
      "epoch: 4 step: 472, loss is 0.5694816708564758\n",
      "epoch: 4 step: 473, loss is 0.5474271774291992\n",
      "epoch: 4 step: 474, loss is 0.4985102713108063\n",
      "epoch: 4 step: 475, loss is 0.32998186349868774\n",
      "epoch: 4 step: 476, loss is 0.626147449016571\n",
      "epoch: 4 step: 477, loss is 0.2846381962299347\n",
      "epoch: 4 step: 478, loss is 0.4565325379371643\n",
      "epoch: 4 step: 479, loss is 0.6103160381317139\n",
      "epoch: 4 step: 480, loss is 0.44465795159339905\n",
      "epoch: 4 step: 481, loss is 0.26728519797325134\n",
      "epoch: 4 step: 482, loss is 0.621251106262207\n",
      "epoch: 4 step: 483, loss is 0.6435509920120239\n",
      "epoch: 4 step: 484, loss is 0.5066395401954651\n",
      "epoch: 4 step: 485, loss is 0.5145944952964783\n",
      "epoch: 4 step: 486, loss is 0.242129385471344\n",
      "epoch: 4 step: 487, loss is 0.4148852825164795\n",
      "epoch: 4 step: 488, loss is 0.13943564891815186\n",
      "epoch: 4 step: 489, loss is 0.4276827275753021\n",
      "epoch: 4 step: 490, loss is 0.6651899218559265\n",
      "epoch: 4 step: 491, loss is 0.6239397525787354\n",
      "epoch: 4 step: 492, loss is 0.27563050389289856\n",
      "epoch: 4 step: 493, loss is 0.13834665715694427\n",
      "epoch: 4 step: 494, loss is 0.6231695413589478\n",
      "epoch: 4 step: 495, loss is 0.6225707530975342\n",
      "epoch: 4 step: 496, loss is 0.40872713923454285\n",
      "epoch: 4 step: 497, loss is 0.3085794746875763\n",
      "epoch: 4 step: 498, loss is 0.5523528456687927\n",
      "epoch: 4 step: 499, loss is 0.5978975892066956\n",
      "epoch: 4 step: 500, loss is 0.45709994435310364\n",
      "epoch: 4 step: 501, loss is 0.7367739081382751\n",
      "epoch: 4 step: 502, loss is 0.5582255721092224\n",
      "epoch: 4 step: 503, loss is 0.48393645882606506\n",
      "epoch: 4 step: 504, loss is 0.3264382779598236\n",
      "epoch: 4 step: 505, loss is 0.3343813419342041\n",
      "epoch: 4 step: 506, loss is 0.8026787042617798\n",
      "epoch: 4 step: 507, loss is 0.8011043071746826\n",
      "epoch: 4 step: 508, loss is 0.17582431435585022\n",
      "epoch: 4 step: 509, loss is 0.6264151334762573\n",
      "epoch: 4 step: 510, loss is 0.5833693742752075\n",
      "epoch: 4 step: 511, loss is 0.9016773700714111\n",
      "epoch: 4 step: 512, loss is 0.3626856803894043\n",
      "epoch: 4 step: 513, loss is 0.5157591104507446\n",
      "epoch: 4 step: 514, loss is 0.548101544380188\n",
      "epoch: 4 step: 515, loss is 0.39546483755111694\n",
      "epoch: 4 step: 516, loss is 0.4832563102245331\n",
      "epoch: 4 step: 517, loss is 0.2630670368671417\n",
      "epoch: 4 step: 518, loss is 0.52613365650177\n",
      "epoch: 4 step: 519, loss is 0.28622251749038696\n",
      "epoch: 4 step: 520, loss is 0.8021759986877441\n",
      "epoch: 4 step: 521, loss is 0.8300234079360962\n",
      "epoch: 4 step: 522, loss is 0.15407520532608032\n",
      "epoch: 4 step: 523, loss is 0.5906515717506409\n",
      "epoch: 4 step: 524, loss is 0.27553170919418335\n",
      "epoch: 4 step: 525, loss is 0.4906874895095825\n",
      "epoch: 4 step: 526, loss is 0.4463665783405304\n",
      "epoch: 4 step: 527, loss is 0.44331586360931396\n",
      "epoch: 4 step: 528, loss is 0.8675214052200317\n",
      "epoch: 4 step: 529, loss is 0.830861508846283\n",
      "epoch: 4 step: 530, loss is 0.39297398924827576\n",
      "epoch: 4 step: 531, loss is 0.7694165110588074\n",
      "epoch: 4 step: 532, loss is 0.48379185795783997\n",
      "epoch: 4 step: 533, loss is 0.8749166131019592\n",
      "epoch: 4 step: 534, loss is 0.7135738730430603\n",
      "epoch: 4 step: 535, loss is 0.6185801029205322\n",
      "epoch: 4 step: 536, loss is 0.4464462101459503\n",
      "epoch: 4 step: 537, loss is 0.6991890072822571\n",
      "epoch: 4 step: 538, loss is 0.4561944007873535\n",
      "epoch: 4 step: 539, loss is 0.5615288019180298\n",
      "epoch: 4 step: 540, loss is 0.5338807106018066\n",
      "epoch: 4 step: 541, loss is 0.6996009349822998\n",
      "epoch: 4 step: 542, loss is 0.5240786075592041\n",
      "epoch: 4 step: 543, loss is 0.7515169382095337\n",
      "epoch: 4 step: 544, loss is 0.4927901029586792\n",
      "epoch: 4 step: 545, loss is 0.3399962782859802\n",
      "epoch: 4 step: 546, loss is 0.407166987657547\n",
      "epoch: 4 step: 547, loss is 0.6566981673240662\n",
      "epoch: 4 step: 548, loss is 0.27327612042427063\n",
      "epoch: 4 step: 549, loss is 0.4797602891921997\n",
      "epoch: 4 step: 550, loss is 0.6900628209114075\n",
      "epoch: 4 step: 551, loss is 0.4134424924850464\n",
      "epoch: 4 step: 552, loss is 0.7744348645210266\n",
      "epoch: 4 step: 553, loss is 0.5068341493606567\n",
      "epoch: 4 step: 554, loss is 0.6220706105232239\n",
      "epoch: 4 step: 555, loss is 0.3017927408218384\n",
      "epoch: 4 step: 556, loss is 0.7618082761764526\n",
      "epoch: 4 step: 557, loss is 0.4895414710044861\n",
      "epoch: 4 step: 558, loss is 0.49657002091407776\n",
      "epoch: 4 step: 559, loss is 0.47816336154937744\n",
      "epoch: 4 step: 560, loss is 0.2428283989429474\n",
      "epoch: 4 step: 561, loss is 0.5915847420692444\n",
      "epoch: 4 step: 562, loss is 0.5718532800674438\n",
      "epoch: 4 step: 563, loss is 0.4232524037361145\n",
      "epoch: 4 step: 564, loss is 0.15579338371753693\n",
      "epoch: 4 step: 565, loss is 0.46824243664741516\n",
      "epoch: 4 step: 566, loss is 0.23253634572029114\n",
      "epoch: 4 step: 567, loss is 0.363690048456192\n",
      "epoch: 4 step: 568, loss is 0.5295838713645935\n",
      "epoch: 4 step: 569, loss is 0.20913000404834747\n",
      "epoch: 4 step: 570, loss is 0.8002630472183228\n",
      "epoch: 4 step: 571, loss is 0.3850880563259125\n",
      "epoch: 4 step: 572, loss is 0.457984983921051\n",
      "epoch: 4 step: 573, loss is 0.8808939456939697\n",
      "epoch: 4 step: 574, loss is 0.6984800696372986\n",
      "epoch: 4 step: 575, loss is 0.5761677622795105\n",
      "epoch: 4 step: 576, loss is 0.65474534034729\n",
      "epoch: 4 step: 577, loss is 0.5482354164123535\n",
      "epoch: 4 step: 578, loss is 0.4335649013519287\n",
      "epoch: 4 step: 579, loss is 0.47786957025527954\n",
      "epoch: 4 step: 580, loss is 0.38269269466400146\n",
      "epoch: 4 step: 581, loss is 0.47013747692108154\n",
      "epoch: 4 step: 582, loss is 0.4559415578842163\n",
      "epoch: 4 step: 583, loss is 0.614917516708374\n",
      "epoch: 4 step: 584, loss is 0.21441271901130676\n",
      "epoch: 4 step: 585, loss is 0.35599926114082336\n",
      "epoch: 4 step: 586, loss is 0.1721736192703247\n",
      "epoch: 4 step: 587, loss is 0.5349521636962891\n",
      "epoch: 4 step: 588, loss is 0.9661785364151001\n",
      "epoch: 4 step: 589, loss is 0.7305437326431274\n",
      "epoch: 4 step: 590, loss is 0.4410759210586548\n",
      "epoch: 4 step: 591, loss is 0.4214133024215698\n",
      "epoch: 4 step: 592, loss is 0.7082497477531433\n",
      "epoch: 4 step: 593, loss is 0.47258034348487854\n",
      "epoch: 4 step: 594, loss is 0.5190173983573914\n",
      "epoch: 4 step: 595, loss is 0.6149068474769592\n",
      "epoch: 4 step: 596, loss is 0.3810178339481354\n",
      "epoch: 4 step: 597, loss is 0.3551125228404999\n",
      "epoch: 4 step: 598, loss is 0.2643841803073883\n",
      "epoch: 4 step: 599, loss is 0.5504195690155029\n",
      "epoch: 4 step: 600, loss is 0.3491448760032654\n",
      "epoch: 4 step: 601, loss is 0.6066694259643555\n",
      "epoch: 4 step: 602, loss is 0.7809150218963623\n",
      "epoch: 4 step: 603, loss is 0.374103844165802\n",
      "epoch: 4 step: 604, loss is 0.2744882106781006\n",
      "epoch: 4 step: 605, loss is 0.5914823412895203\n",
      "epoch: 4 step: 606, loss is 0.3343074321746826\n",
      "epoch: 4 step: 607, loss is 0.6003915071487427\n",
      "epoch: 4 step: 608, loss is 0.8123559951782227\n",
      "epoch: 4 step: 609, loss is 0.7066144347190857\n",
      "epoch: 4 step: 610, loss is 0.5021851658821106\n",
      "epoch: 4 step: 611, loss is 0.1389663964509964\n",
      "epoch: 4 step: 612, loss is 0.4380091726779938\n",
      "epoch: 4 step: 613, loss is 0.4763159453868866\n",
      "epoch: 4 step: 614, loss is 0.6261205077171326\n",
      "epoch: 4 step: 615, loss is 0.3876916468143463\n",
      "epoch: 4 step: 616, loss is 0.5067148208618164\n",
      "epoch: 4 step: 617, loss is 0.3383105993270874\n",
      "epoch: 4 step: 618, loss is 0.6398941874504089\n",
      "epoch: 4 step: 619, loss is 1.3137338161468506\n",
      "epoch: 4 step: 620, loss is 0.5076358318328857\n",
      "epoch: 4 step: 621, loss is 0.37022003531455994\n",
      "epoch: 4 step: 622, loss is 0.41998526453971863\n",
      "epoch: 4 step: 623, loss is 0.5559523105621338\n",
      "epoch: 4 step: 624, loss is 0.27888721227645874\n",
      "epoch: 4 step: 625, loss is 0.689885139465332\n",
      "epoch: 4 step: 626, loss is 0.6404990553855896\n",
      "epoch: 4 step: 627, loss is 0.6573781371116638\n",
      "epoch: 4 step: 628, loss is 0.6372048258781433\n",
      "epoch: 4 step: 629, loss is 0.4671752154827118\n",
      "epoch: 4 step: 630, loss is 0.443177193403244\n",
      "epoch: 4 step: 631, loss is 0.5086082220077515\n",
      "epoch: 4 step: 632, loss is 0.7115439176559448\n",
      "epoch: 4 step: 633, loss is 0.4181010127067566\n",
      "epoch: 4 step: 634, loss is 0.4655439853668213\n",
      "epoch: 4 step: 635, loss is 0.46869200468063354\n",
      "epoch: 4 step: 636, loss is 0.591895341873169\n",
      "epoch: 4 step: 637, loss is 0.5766363739967346\n",
      "epoch: 4 step: 638, loss is 0.525788426399231\n",
      "epoch: 4 step: 639, loss is 0.4947601854801178\n",
      "epoch: 4 step: 640, loss is 0.8948522210121155\n",
      "epoch: 4 step: 641, loss is 0.30001842975616455\n",
      "epoch: 4 step: 642, loss is 0.6720471978187561\n",
      "epoch: 4 step: 643, loss is 0.7751258015632629\n",
      "epoch: 4 step: 644, loss is 0.44473645091056824\n",
      "epoch: 4 step: 645, loss is 0.5786314606666565\n",
      "epoch: 4 step: 646, loss is 0.5095128417015076\n",
      "epoch: 4 step: 647, loss is 0.5147047638893127\n",
      "epoch: 4 step: 648, loss is 0.2567813992500305\n",
      "epoch: 4 step: 649, loss is 0.46161577105522156\n",
      "epoch: 4 step: 650, loss is 0.7424055337905884\n",
      "epoch: 4 step: 651, loss is 0.564125657081604\n",
      "epoch: 4 step: 652, loss is 0.4957066774368286\n",
      "epoch: 4 step: 653, loss is 0.45900967717170715\n",
      "epoch: 4 step: 654, loss is 0.3358101546764374\n",
      "epoch: 4 step: 655, loss is 0.2713833153247833\n",
      "epoch: 4 step: 656, loss is 0.3913356065750122\n",
      "epoch: 4 step: 657, loss is 0.5840501189231873\n",
      "epoch: 4 step: 658, loss is 0.6438896059989929\n",
      "epoch: 4 step: 659, loss is 0.5231621265411377\n",
      "epoch: 4 step: 660, loss is 0.17718873918056488\n",
      "epoch: 4 step: 661, loss is 0.3981548249721527\n",
      "epoch: 4 step: 662, loss is 0.5488102436065674\n",
      "epoch: 4 step: 663, loss is 0.288193017244339\n",
      "epoch: 4 step: 664, loss is 0.7385651469230652\n",
      "epoch: 4 step: 665, loss is 0.2966058552265167\n",
      "epoch: 4 step: 666, loss is 0.5247098207473755\n",
      "epoch: 4 step: 667, loss is 0.5578429698944092\n",
      "epoch: 4 step: 668, loss is 0.25872641801834106\n",
      "epoch: 4 step: 669, loss is 0.43089279532432556\n",
      "epoch: 4 step: 670, loss is 0.28948166966438293\n",
      "epoch: 4 step: 671, loss is 0.4793066382408142\n",
      "epoch: 4 step: 672, loss is 0.5199581384658813\n",
      "epoch: 4 step: 673, loss is 0.34041494131088257\n",
      "epoch: 4 step: 674, loss is 0.9190353155136108\n",
      "epoch: 4 step: 675, loss is 0.5073542594909668\n",
      "epoch: 4 step: 676, loss is 0.42986395955085754\n",
      "epoch: 4 step: 677, loss is 0.7351104617118835\n",
      "epoch: 4 step: 678, loss is 0.6795579195022583\n",
      "epoch: 4 step: 679, loss is 0.29999053478240967\n",
      "epoch: 4 step: 680, loss is 0.6548708081245422\n",
      "epoch: 4 step: 681, loss is 0.2477554976940155\n",
      "epoch: 4 step: 682, loss is 0.45889678597450256\n",
      "epoch: 4 step: 683, loss is 0.21906118094921112\n",
      "epoch: 4 step: 684, loss is 0.3564518094062805\n",
      "epoch: 4 step: 685, loss is 0.520926833152771\n",
      "epoch: 4 step: 686, loss is 0.5216819643974304\n",
      "epoch: 4 step: 687, loss is 0.31345677375793457\n",
      "epoch: 4 step: 688, loss is 0.6389577984809875\n",
      "epoch: 4 step: 689, loss is 1.3326897621154785\n",
      "epoch: 4 step: 690, loss is 0.4670405685901642\n",
      "epoch: 4 step: 691, loss is 0.5916721224784851\n",
      "epoch: 4 step: 692, loss is 0.5748333930969238\n",
      "epoch: 4 step: 693, loss is 0.4770337641239166\n",
      "epoch: 4 step: 694, loss is 0.4234619438648224\n",
      "epoch: 4 step: 695, loss is 0.33659014105796814\n",
      "epoch: 4 step: 696, loss is 0.4479835033416748\n",
      "epoch: 4 step: 697, loss is 0.3269554078578949\n",
      "epoch: 4 step: 698, loss is 0.4216877222061157\n",
      "epoch: 4 step: 699, loss is 0.7204817533493042\n",
      "epoch: 4 step: 700, loss is 0.4360470473766327\n",
      "epoch: 4 step: 701, loss is 0.6696726679801941\n",
      "epoch: 4 step: 702, loss is 0.5224708318710327\n",
      "epoch: 4 step: 703, loss is 0.5863142013549805\n",
      "epoch: 4 step: 704, loss is 0.4473549425601959\n",
      "epoch: 4 step: 705, loss is 0.6217430830001831\n",
      "epoch: 4 step: 706, loss is 0.35225409269332886\n",
      "epoch: 4 step: 707, loss is 0.4218745529651642\n",
      "epoch: 4 step: 708, loss is 0.504070520401001\n",
      "epoch: 4 step: 709, loss is 0.6120191216468811\n",
      "epoch: 4 step: 710, loss is 0.35257017612457275\n",
      "epoch: 4 step: 711, loss is 0.5664385557174683\n",
      "epoch: 4 step: 712, loss is 0.46785399317741394\n",
      "epoch: 4 step: 713, loss is 0.5014760494232178\n",
      "epoch: 4 step: 714, loss is 0.5829564332962036\n",
      "epoch: 4 step: 715, loss is 0.3712339997291565\n",
      "epoch: 4 step: 716, loss is 0.6568000912666321\n",
      "epoch: 4 step: 717, loss is 0.3741856813430786\n",
      "epoch: 4 step: 718, loss is 0.5322889089584351\n",
      "epoch: 4 step: 719, loss is 0.6746975183486938\n",
      "epoch: 4 step: 720, loss is 0.38435256481170654\n",
      "epoch: 4 step: 721, loss is 0.7348958849906921\n",
      "epoch: 4 step: 722, loss is 0.49142277240753174\n",
      "epoch: 4 step: 723, loss is 0.7313508987426758\n",
      "epoch: 4 step: 724, loss is 0.24314887821674347\n",
      "epoch: 4 step: 725, loss is 0.7702080607414246\n",
      "epoch: 4 step: 726, loss is 0.587806761264801\n",
      "epoch: 4 step: 727, loss is 0.6943901777267456\n",
      "epoch: 4 step: 728, loss is 0.7577556371688843\n",
      "epoch: 4 step: 729, loss is 0.5491451025009155\n",
      "epoch: 4 step: 730, loss is 0.609933614730835\n",
      "epoch: 4 step: 731, loss is 0.16465875506401062\n",
      "epoch: 4 step: 732, loss is 0.6473457217216492\n",
      "epoch: 4 step: 733, loss is 0.504102885723114\n",
      "epoch: 4 step: 734, loss is 0.6215962171554565\n",
      "epoch: 4 step: 735, loss is 0.37405717372894287\n",
      "epoch: 4 step: 736, loss is 0.4743453562259674\n",
      "epoch: 4 step: 737, loss is 0.19050173461437225\n",
      "epoch: 4 step: 738, loss is 0.43236127495765686\n",
      "epoch: 4 step: 739, loss is 0.3397340178489685\n",
      "epoch: 4 step: 740, loss is 0.47626522183418274\n",
      "epoch: 4 step: 741, loss is 0.3487529754638672\n",
      "epoch: 4 step: 742, loss is 0.3518473207950592\n",
      "epoch: 4 step: 743, loss is 0.7781221270561218\n",
      "epoch: 4 step: 744, loss is 0.34975558519363403\n",
      "epoch: 4 step: 745, loss is 0.3064063787460327\n",
      "epoch: 4 step: 746, loss is 0.6802126169204712\n",
      "epoch: 4 step: 747, loss is 0.3680651783943176\n",
      "epoch: 4 step: 748, loss is 0.45373067259788513\n",
      "epoch: 4 step: 749, loss is 0.28066423535346985\n",
      "epoch: 4 step: 750, loss is 0.4418736696243286\n",
      "epoch: 4 step: 751, loss is 0.8497761487960815\n",
      "epoch: 4 step: 752, loss is 0.38820427656173706\n",
      "epoch: 4 step: 753, loss is 0.2962638735771179\n",
      "epoch: 4 step: 754, loss is 0.3108803927898407\n",
      "epoch: 4 step: 755, loss is 0.483551025390625\n",
      "epoch: 4 step: 756, loss is 0.35568302869796753\n",
      "epoch: 4 step: 757, loss is 0.730478048324585\n",
      "epoch: 4 step: 758, loss is 0.6538563370704651\n",
      "epoch: 4 step: 759, loss is 0.7526654601097107\n",
      "epoch: 4 step: 760, loss is 0.7610453963279724\n",
      "epoch: 4 step: 761, loss is 0.5058152079582214\n",
      "epoch: 4 step: 762, loss is 0.49141761660575867\n",
      "epoch: 4 step: 763, loss is 0.6246407628059387\n",
      "epoch: 4 step: 764, loss is 0.33218565583229065\n",
      "epoch: 4 step: 765, loss is 0.3256172239780426\n",
      "epoch: 4 step: 766, loss is 0.2955681085586548\n",
      "epoch: 4 step: 767, loss is 0.31329864263534546\n",
      "epoch: 4 step: 768, loss is 0.7884055376052856\n",
      "epoch: 4 step: 769, loss is 0.5337483286857605\n",
      "epoch: 4 step: 770, loss is 0.21529121696949005\n",
      "epoch: 4 step: 771, loss is 0.7689257264137268\n",
      "epoch: 4 step: 772, loss is 0.353687047958374\n",
      "epoch: 4 step: 773, loss is 0.2729910612106323\n",
      "epoch: 4 step: 774, loss is 0.3602040708065033\n",
      "epoch: 4 step: 775, loss is 0.37770187854766846\n",
      "epoch: 4 step: 776, loss is 0.3949633836746216\n",
      "epoch: 4 step: 777, loss is 0.30568966269493103\n",
      "epoch: 4 step: 778, loss is 0.7352824211120605\n",
      "epoch: 4 step: 779, loss is 0.29889145493507385\n",
      "epoch: 4 step: 780, loss is 0.3264128267765045\n",
      "epoch: 4 step: 781, loss is 0.7837526202201843\n",
      "epoch: 4 step: 782, loss is 0.254174143075943\n",
      "epoch: 4 step: 783, loss is 0.5070480108261108\n",
      "epoch: 4 step: 784, loss is 0.6276503801345825\n",
      "epoch: 4 step: 785, loss is 0.8687636256217957\n",
      "epoch: 4 step: 786, loss is 0.3430519998073578\n",
      "epoch: 4 step: 787, loss is 0.7785452604293823\n",
      "epoch: 4 step: 788, loss is 0.5537307858467102\n",
      "epoch: 4 step: 789, loss is 0.6842451095581055\n",
      "epoch: 4 step: 790, loss is 0.36594632267951965\n",
      "epoch: 4 step: 791, loss is 0.27919119596481323\n",
      "epoch: 4 step: 792, loss is 0.41146329045295715\n",
      "epoch: 4 step: 793, loss is 0.3849773705005646\n",
      "epoch: 4 step: 794, loss is 0.33808135986328125\n",
      "epoch: 4 step: 795, loss is 0.5751209855079651\n",
      "epoch: 4 step: 796, loss is 0.311412513256073\n",
      "epoch: 4 step: 797, loss is 0.34865447878837585\n",
      "epoch: 4 step: 798, loss is 0.6057570576667786\n",
      "epoch: 4 step: 799, loss is 0.6806804537773132\n",
      "epoch: 4 step: 800, loss is 0.5044011473655701\n",
      "epoch: 4 step: 801, loss is 0.432041198015213\n",
      "epoch: 4 step: 802, loss is 0.6110891103744507\n",
      "epoch: 4 step: 803, loss is 0.5227594971656799\n",
      "epoch: 4 step: 804, loss is 0.21376454830169678\n",
      "epoch: 4 step: 805, loss is 0.38388296961784363\n",
      "epoch: 4 step: 806, loss is 0.11633820831775665\n",
      "epoch: 4 step: 807, loss is 0.31509822607040405\n",
      "epoch: 4 step: 808, loss is 0.4015057682991028\n",
      "epoch: 4 step: 809, loss is 1.123978614807129\n",
      "epoch: 4 step: 810, loss is 0.2624928951263428\n",
      "epoch: 4 step: 811, loss is 0.4157874286174774\n",
      "epoch: 4 step: 812, loss is 1.0672868490219116\n",
      "epoch: 4 step: 813, loss is 0.6957175731658936\n",
      "epoch: 4 step: 814, loss is 0.4423287510871887\n",
      "epoch: 4 step: 815, loss is 0.5196580290794373\n",
      "epoch: 4 step: 816, loss is 0.4061409533023834\n",
      "epoch: 4 step: 817, loss is 0.4704717695713043\n",
      "epoch: 4 step: 818, loss is 0.29692327976226807\n",
      "epoch: 4 step: 819, loss is 0.37077927589416504\n",
      "epoch: 4 step: 820, loss is 0.7041688561439514\n",
      "epoch: 4 step: 821, loss is 0.726210355758667\n",
      "epoch: 4 step: 822, loss is 0.4513687193393707\n",
      "epoch: 4 step: 823, loss is 0.774236798286438\n",
      "epoch: 4 step: 824, loss is 0.6901333928108215\n",
      "epoch: 4 step: 825, loss is 0.5188207030296326\n",
      "epoch: 4 step: 826, loss is 0.5479265451431274\n",
      "epoch: 4 step: 827, loss is 0.8363603949546814\n",
      "epoch: 4 step: 828, loss is 0.13136397302150726\n",
      "epoch: 4 step: 829, loss is 0.6655632257461548\n",
      "epoch: 4 step: 830, loss is 0.5728542804718018\n",
      "epoch: 4 step: 831, loss is 0.9219615459442139\n",
      "epoch: 4 step: 832, loss is 0.440416544675827\n",
      "epoch: 4 step: 833, loss is 0.48802125453948975\n",
      "epoch: 4 step: 834, loss is 0.4772739112377167\n",
      "epoch: 4 step: 835, loss is 0.720611572265625\n",
      "epoch: 4 step: 836, loss is 0.5505396127700806\n",
      "epoch: 4 step: 837, loss is 0.7636244297027588\n",
      "epoch: 4 step: 838, loss is 0.41034066677093506\n",
      "epoch: 4 step: 839, loss is 0.5281951427459717\n",
      "epoch: 4 step: 840, loss is 0.6253598928451538\n",
      "epoch: 4 step: 841, loss is 0.4992918372154236\n",
      "epoch: 4 step: 842, loss is 0.46380358934402466\n",
      "epoch: 4 step: 843, loss is 0.5734184980392456\n",
      "epoch: 4 step: 844, loss is 0.802374541759491\n",
      "epoch: 4 step: 845, loss is 0.47534722089767456\n",
      "epoch: 4 step: 846, loss is 0.2982045114040375\n",
      "epoch: 4 step: 847, loss is 0.35630011558532715\n",
      "epoch: 4 step: 848, loss is 0.5982587933540344\n",
      "epoch: 4 step: 849, loss is 0.4761465787887573\n",
      "epoch: 4 step: 850, loss is 0.5207317471504211\n",
      "epoch: 4 step: 851, loss is 0.963847279548645\n",
      "epoch: 4 step: 852, loss is 0.43614462018013\n",
      "epoch: 4 step: 853, loss is 0.7101300358772278\n",
      "epoch: 4 step: 854, loss is 0.5659006237983704\n",
      "epoch: 4 step: 855, loss is 0.5026830434799194\n",
      "epoch: 4 step: 856, loss is 0.7025413513183594\n",
      "epoch: 4 step: 857, loss is 0.29862743616104126\n",
      "epoch: 4 step: 858, loss is 0.4950270652770996\n",
      "epoch: 4 step: 859, loss is 0.4683699309825897\n",
      "epoch: 4 step: 860, loss is 0.5064346790313721\n",
      "epoch: 4 step: 861, loss is 0.5838855504989624\n",
      "epoch: 4 step: 862, loss is 0.36959943175315857\n",
      "epoch: 4 step: 863, loss is 0.33908140659332275\n",
      "epoch: 4 step: 864, loss is 0.5722509622573853\n",
      "epoch: 4 step: 865, loss is 0.39331933856010437\n",
      "epoch: 4 step: 866, loss is 0.4315151274204254\n",
      "epoch: 4 step: 867, loss is 0.328080952167511\n",
      "epoch: 4 step: 868, loss is 0.32184329628944397\n",
      "epoch: 4 step: 869, loss is 0.4585130512714386\n",
      "epoch: 4 step: 870, loss is 0.6032624840736389\n",
      "epoch: 4 step: 871, loss is 0.6615126132965088\n",
      "epoch: 4 step: 872, loss is 0.5174182057380676\n",
      "epoch: 4 step: 873, loss is 0.6710283756256104\n",
      "epoch: 4 step: 874, loss is 0.5506803393363953\n",
      "epoch: 4 step: 875, loss is 0.28280648589134216\n",
      "epoch: 4 step: 876, loss is 0.5571292042732239\n",
      "epoch: 4 step: 877, loss is 0.37452542781829834\n",
      "epoch: 4 step: 878, loss is 0.45250681042671204\n",
      "epoch: 4 step: 879, loss is 0.47893598675727844\n",
      "epoch: 4 step: 880, loss is 0.4961637258529663\n",
      "epoch: 4 step: 881, loss is 0.46873739361763\n",
      "epoch: 4 step: 882, loss is 0.5054420232772827\n",
      "epoch: 4 step: 883, loss is 0.2017277032136917\n",
      "epoch: 4 step: 884, loss is 0.23829840123653412\n",
      "epoch: 4 step: 885, loss is 0.7429665327072144\n",
      "epoch: 4 step: 886, loss is 0.5943364500999451\n",
      "epoch: 4 step: 887, loss is 0.46223336458206177\n",
      "epoch: 4 step: 888, loss is 0.18391762673854828\n",
      "epoch: 4 step: 889, loss is 0.9693419933319092\n",
      "epoch: 4 step: 890, loss is 0.8755930662155151\n",
      "epoch: 4 step: 891, loss is 0.5956749320030212\n",
      "epoch: 4 step: 892, loss is 0.6394362449645996\n",
      "epoch: 4 step: 893, loss is 0.5731494426727295\n",
      "epoch: 4 step: 894, loss is 0.5963115096092224\n",
      "epoch: 4 step: 895, loss is 0.5387104153633118\n",
      "epoch: 4 step: 896, loss is 0.8054201006889343\n",
      "epoch: 4 step: 897, loss is 0.4316326975822449\n",
      "epoch: 4 step: 898, loss is 0.6837044358253479\n",
      "epoch: 4 step: 899, loss is 0.3198685646057129\n",
      "epoch: 4 step: 900, loss is 0.3083329200744629\n",
      "epoch: 4 step: 901, loss is 0.33508798480033875\n",
      "epoch: 4 step: 902, loss is 0.34526675939559937\n",
      "epoch: 4 step: 903, loss is 0.611743688583374\n",
      "epoch: 4 step: 904, loss is 0.7338168025016785\n",
      "epoch: 4 step: 905, loss is 0.3625336289405823\n",
      "epoch: 4 step: 906, loss is 0.8108083605766296\n",
      "epoch: 4 step: 907, loss is 0.48460444808006287\n",
      "epoch: 4 step: 908, loss is 0.5421656370162964\n",
      "epoch: 4 step: 909, loss is 0.5625766515731812\n",
      "epoch: 4 step: 910, loss is 0.2590113878250122\n",
      "epoch: 4 step: 911, loss is 0.3050817549228668\n",
      "epoch: 4 step: 912, loss is 0.918023407459259\n",
      "epoch: 4 step: 913, loss is 0.38393697142601013\n",
      "epoch: 4 step: 914, loss is 0.6646254658699036\n",
      "epoch: 4 step: 915, loss is 0.6657444834709167\n",
      "epoch: 4 step: 916, loss is 0.7626702189445496\n",
      "epoch: 4 step: 917, loss is 0.6241582632064819\n",
      "epoch: 4 step: 918, loss is 0.39480850100517273\n",
      "epoch: 4 step: 919, loss is 0.37047648429870605\n",
      "epoch: 4 step: 920, loss is 0.5901260375976562\n",
      "epoch: 4 step: 921, loss is 0.47622591257095337\n",
      "epoch: 4 step: 922, loss is 0.5689423084259033\n",
      "epoch: 4 step: 923, loss is 0.3837781846523285\n",
      "epoch: 4 step: 924, loss is 0.7672591209411621\n",
      "epoch: 4 step: 925, loss is 0.4691799581050873\n",
      "epoch: 4 step: 926, loss is 0.5071984529495239\n",
      "epoch: 4 step: 927, loss is 0.8103474378585815\n",
      "epoch: 4 step: 928, loss is 0.35820379853248596\n",
      "epoch: 4 step: 929, loss is 0.6058787107467651\n",
      "epoch: 4 step: 930, loss is 0.3418034613132477\n",
      "epoch: 4 step: 931, loss is 0.4791020154953003\n",
      "epoch: 4 step: 932, loss is 0.3574983775615692\n",
      "epoch: 4 step: 933, loss is 0.644510805606842\n",
      "epoch: 4 step: 934, loss is 0.34476378560066223\n",
      "epoch: 4 step: 935, loss is 0.41426414251327515\n",
      "epoch: 4 step: 936, loss is 0.6275463104248047\n",
      "epoch: 4 step: 937, loss is 0.35543540120124817\n",
      "epoch: 4 step: 938, loss is 0.9488039016723633\n",
      "epoch: 4 step: 939, loss is 0.42869019508361816\n",
      "epoch: 4 step: 940, loss is 0.293124794960022\n",
      "epoch: 4 step: 941, loss is 0.8016957640647888\n",
      "epoch: 4 step: 942, loss is 0.5199432373046875\n",
      "epoch: 4 step: 943, loss is 0.6982715129852295\n",
      "epoch: 4 step: 944, loss is 0.4590706527233124\n",
      "epoch: 4 step: 945, loss is 0.28708940744400024\n",
      "epoch: 4 step: 946, loss is 0.31898048520088196\n",
      "epoch: 4 step: 947, loss is 0.8554036617279053\n",
      "epoch: 4 step: 948, loss is 0.4452729821205139\n",
      "epoch: 4 step: 949, loss is 0.6591915488243103\n",
      "epoch: 4 step: 950, loss is 0.7027948498725891\n",
      "epoch: 4 step: 951, loss is 0.7147710919380188\n",
      "epoch: 4 step: 952, loss is 0.4052841365337372\n",
      "epoch: 4 step: 953, loss is 0.40405702590942383\n",
      "epoch: 4 step: 954, loss is 0.3659447729587555\n",
      "epoch: 4 step: 955, loss is 1.0496946573257446\n",
      "epoch: 4 step: 956, loss is 0.4877195954322815\n",
      "epoch: 4 step: 957, loss is 0.6872187256813049\n",
      "epoch: 4 step: 958, loss is 0.5350252389907837\n",
      "epoch: 4 step: 959, loss is 0.3205283284187317\n",
      "epoch: 4 step: 960, loss is 0.20870190858840942\n",
      "epoch: 4 step: 961, loss is 0.5647302865982056\n",
      "epoch: 4 step: 962, loss is 0.2278626710176468\n",
      "epoch: 4 step: 963, loss is 0.32123351097106934\n",
      "epoch: 4 step: 964, loss is 0.8143918514251709\n",
      "epoch: 4 step: 965, loss is 0.41851258277893066\n",
      "epoch: 4 step: 966, loss is 0.5225223898887634\n",
      "epoch: 4 step: 967, loss is 0.3389397859573364\n",
      "epoch: 4 step: 968, loss is 0.4094865620136261\n",
      "epoch: 4 step: 969, loss is 0.3472774624824524\n",
      "epoch: 4 step: 970, loss is 0.5343242287635803\n",
      "epoch: 4 step: 971, loss is 1.0228279829025269\n",
      "epoch: 4 step: 972, loss is 0.5539209246635437\n",
      "epoch: 4 step: 973, loss is 0.5987246036529541\n",
      "epoch: 4 step: 974, loss is 0.6131765842437744\n",
      "epoch: 4 step: 975, loss is 0.4711332321166992\n",
      "epoch: 4 step: 976, loss is 0.5348084568977356\n",
      "epoch: 4 step: 977, loss is 0.40821951627731323\n",
      "epoch: 4 step: 978, loss is 0.3954280614852905\n",
      "epoch: 4 step: 979, loss is 0.5632009506225586\n",
      "epoch: 4 step: 980, loss is 0.576117753982544\n",
      "epoch: 4 step: 981, loss is 0.24848520755767822\n",
      "epoch: 4 step: 982, loss is 0.5436823964118958\n",
      "epoch: 4 step: 983, loss is 0.5460242033004761\n",
      "epoch: 4 step: 984, loss is 0.8567858338356018\n",
      "epoch: 4 step: 985, loss is 0.4513128101825714\n",
      "epoch: 4 step: 986, loss is 0.21385106444358826\n",
      "epoch: 4 step: 987, loss is 0.4908272624015808\n",
      "epoch: 4 step: 988, loss is 0.7096812129020691\n",
      "epoch: 4 step: 989, loss is 0.6689073443412781\n",
      "epoch: 4 step: 990, loss is 0.24424487352371216\n",
      "epoch: 4 step: 991, loss is 0.19071941077709198\n",
      "epoch: 4 step: 992, loss is 0.8488726615905762\n",
      "epoch: 4 step: 993, loss is 0.7970783114433289\n",
      "epoch: 4 step: 994, loss is 0.2010132372379303\n",
      "epoch: 4 step: 995, loss is 0.5638514161109924\n",
      "epoch: 4 step: 996, loss is 0.3106794059276581\n",
      "epoch: 4 step: 997, loss is 0.5598993897438049\n",
      "epoch: 4 step: 998, loss is 0.30872273445129395\n",
      "epoch: 4 step: 999, loss is 0.392038494348526\n",
      "epoch: 4 step: 1000, loss is 0.39597901701927185\n",
      "epoch: 4 step: 1001, loss is 0.6612489223480225\n",
      "epoch: 4 step: 1002, loss is 0.6785165071487427\n",
      "epoch: 4 step: 1003, loss is 0.6761581897735596\n",
      "epoch: 4 step: 1004, loss is 0.8264141082763672\n",
      "epoch: 4 step: 1005, loss is 0.4822155237197876\n",
      "epoch: 4 step: 1006, loss is 0.4266733229160309\n",
      "epoch: 4 step: 1007, loss is 0.4730287492275238\n",
      "epoch: 4 step: 1008, loss is 0.3877156674861908\n",
      "epoch: 4 step: 1009, loss is 0.40069228410720825\n",
      "epoch: 4 step: 1010, loss is 0.39685294032096863\n",
      "epoch: 4 step: 1011, loss is 0.5405381917953491\n",
      "epoch: 4 step: 1012, loss is 0.6822484731674194\n",
      "epoch: 4 step: 1013, loss is 0.3040066659450531\n",
      "epoch: 4 step: 1014, loss is 0.3420490622520447\n",
      "epoch: 4 step: 1015, loss is 0.5176361203193665\n",
      "epoch: 4 step: 1016, loss is 0.4553903639316559\n",
      "epoch: 4 step: 1017, loss is 0.4043814241886139\n",
      "epoch: 4 step: 1018, loss is 0.4734727740287781\n",
      "epoch: 4 step: 1019, loss is 0.519356906414032\n",
      "epoch: 4 step: 1020, loss is 0.9324394464492798\n",
      "epoch: 4 step: 1021, loss is 0.4817943274974823\n",
      "epoch: 4 step: 1022, loss is 0.43497931957244873\n",
      "epoch: 4 step: 1023, loss is 0.4494072496891022\n",
      "epoch: 4 step: 1024, loss is 0.3650064766407013\n",
      "epoch: 4 step: 1025, loss is 0.6056962609291077\n",
      "epoch: 4 step: 1026, loss is 0.34980642795562744\n",
      "epoch: 4 step: 1027, loss is 0.4084959626197815\n",
      "epoch: 4 step: 1028, loss is 0.8405136466026306\n",
      "epoch: 4 step: 1029, loss is 0.48852384090423584\n",
      "epoch: 4 step: 1030, loss is 0.7136567831039429\n",
      "epoch: 4 step: 1031, loss is 0.46762752532958984\n",
      "epoch: 4 step: 1032, loss is 0.6751134395599365\n",
      "epoch: 4 step: 1033, loss is 0.48282390832901\n",
      "epoch: 4 step: 1034, loss is 0.6157701015472412\n",
      "epoch: 4 step: 1035, loss is 0.7290350198745728\n",
      "epoch: 4 step: 1036, loss is 0.35821110010147095\n",
      "epoch: 4 step: 1037, loss is 0.32762160897254944\n",
      "epoch: 4 step: 1038, loss is 0.6602265238761902\n",
      "epoch: 4 step: 1039, loss is 0.7506818771362305\n",
      "epoch: 4 step: 1040, loss is 0.5522487163543701\n",
      "epoch: 4 step: 1041, loss is 0.5851497054100037\n",
      "epoch: 4 step: 1042, loss is 0.8171806931495667\n",
      "epoch: 4 step: 1043, loss is 0.35280758142471313\n",
      "epoch: 4 step: 1044, loss is 0.5291212797164917\n",
      "epoch: 4 step: 1045, loss is 0.5693310499191284\n",
      "epoch: 4 step: 1046, loss is 0.6216275095939636\n",
      "epoch: 4 step: 1047, loss is 0.5196858048439026\n",
      "epoch: 4 step: 1048, loss is 0.27065128087997437\n",
      "epoch: 4 step: 1049, loss is 0.4401201903820038\n",
      "epoch: 4 step: 1050, loss is 0.5948131680488586\n",
      "epoch: 4 step: 1051, loss is 0.3559485375881195\n",
      "epoch: 4 step: 1052, loss is 0.4168790578842163\n",
      "epoch: 4 step: 1053, loss is 0.3557735085487366\n",
      "epoch: 4 step: 1054, loss is 0.7910668253898621\n",
      "epoch: 4 step: 1055, loss is 0.5701054334640503\n",
      "epoch: 4 step: 1056, loss is 0.47216758131980896\n",
      "epoch: 4 step: 1057, loss is 0.29413557052612305\n",
      "epoch: 4 step: 1058, loss is 0.5980232954025269\n",
      "epoch: 4 step: 1059, loss is 0.4983116388320923\n",
      "epoch: 4 step: 1060, loss is 0.551582932472229\n",
      "epoch: 4 step: 1061, loss is 0.42734819650650024\n",
      "epoch: 4 step: 1062, loss is 0.5437192916870117\n",
      "epoch: 4 step: 1063, loss is 0.2929101884365082\n",
      "epoch: 4 step: 1064, loss is 0.3689335584640503\n",
      "epoch: 4 step: 1065, loss is 0.4864998161792755\n",
      "epoch: 4 step: 1066, loss is 0.6093587875366211\n",
      "epoch: 4 step: 1067, loss is 0.3378598988056183\n",
      "epoch: 4 step: 1068, loss is 0.38604000210762024\n",
      "epoch: 4 step: 1069, loss is 0.17501573264598846\n",
      "epoch: 4 step: 1070, loss is 0.5409494638442993\n",
      "epoch: 4 step: 1071, loss is 0.45295852422714233\n",
      "epoch: 4 step: 1072, loss is 0.5070539712905884\n",
      "epoch: 4 step: 1073, loss is 0.33272647857666016\n",
      "epoch: 4 step: 1074, loss is 0.8346108794212341\n",
      "epoch: 4 step: 1075, loss is 0.4752280116081238\n",
      "epoch: 4 step: 1076, loss is 0.27786412835121155\n",
      "epoch: 4 step: 1077, loss is 0.48405811190605164\n",
      "epoch: 4 step: 1078, loss is 0.23883968591690063\n",
      "epoch: 4 step: 1079, loss is 0.4954482913017273\n",
      "epoch: 4 step: 1080, loss is 0.3660697042942047\n",
      "epoch: 4 step: 1081, loss is 0.39051324129104614\n",
      "epoch: 4 step: 1082, loss is 0.954929769039154\n",
      "epoch: 4 step: 1083, loss is 0.26683008670806885\n",
      "epoch: 4 step: 1084, loss is 0.5191322565078735\n",
      "epoch: 4 step: 1085, loss is 0.4747357964515686\n",
      "epoch: 4 step: 1086, loss is 0.3701756000518799\n",
      "epoch: 4 step: 1087, loss is 0.36789998412132263\n",
      "epoch: 4 step: 1088, loss is 0.3350282311439514\n",
      "epoch: 4 step: 1089, loss is 0.5675092935562134\n",
      "epoch: 4 step: 1090, loss is 0.6982588171958923\n",
      "epoch: 4 step: 1091, loss is 0.6951771378517151\n",
      "epoch: 4 step: 1092, loss is 0.6663756966590881\n",
      "epoch: 4 step: 1093, loss is 0.2179863154888153\n",
      "epoch: 4 step: 1094, loss is 0.4729880690574646\n",
      "epoch: 4 step: 1095, loss is 0.7773701548576355\n",
      "epoch: 4 step: 1096, loss is 0.9648094773292542\n",
      "epoch: 4 step: 1097, loss is 0.3987034559249878\n",
      "epoch: 4 step: 1098, loss is 0.3240049481391907\n",
      "epoch: 4 step: 1099, loss is 0.48376893997192383\n",
      "epoch: 4 step: 1100, loss is 0.4834710359573364\n",
      "epoch: 4 step: 1101, loss is 0.29238900542259216\n",
      "epoch: 4 step: 1102, loss is 0.7570103406906128\n",
      "epoch: 4 step: 1103, loss is 0.531745433807373\n",
      "epoch: 4 step: 1104, loss is 0.4501250684261322\n",
      "epoch: 4 step: 1105, loss is 0.40282461047172546\n",
      "epoch: 4 step: 1106, loss is 0.332852840423584\n",
      "epoch: 4 step: 1107, loss is 0.6445497274398804\n",
      "epoch: 4 step: 1108, loss is 0.20858728885650635\n",
      "epoch: 4 step: 1109, loss is 0.5183278918266296\n",
      "epoch: 4 step: 1110, loss is 0.45829206705093384\n",
      "epoch: 4 step: 1111, loss is 0.8203093409538269\n",
      "epoch: 4 step: 1112, loss is 0.6889039874076843\n",
      "epoch: 4 step: 1113, loss is 0.5099983811378479\n",
      "epoch: 4 step: 1114, loss is 0.320266991853714\n",
      "epoch: 4 step: 1115, loss is 0.37372922897338867\n",
      "epoch: 4 step: 1116, loss is 0.8792089819908142\n",
      "epoch: 4 step: 1117, loss is 0.7885290384292603\n",
      "epoch: 4 step: 1118, loss is 0.4477998614311218\n",
      "epoch: 4 step: 1119, loss is 0.4351547360420227\n",
      "epoch: 4 step: 1120, loss is 0.5447717308998108\n",
      "epoch: 4 step: 1121, loss is 0.6561421155929565\n",
      "epoch: 4 step: 1122, loss is 0.2876453697681427\n",
      "epoch: 4 step: 1123, loss is 0.3542982041835785\n",
      "epoch: 4 step: 1124, loss is 0.44657886028289795\n",
      "epoch: 4 step: 1125, loss is 0.33781903982162476\n",
      "epoch: 4 step: 1126, loss is 0.6423280835151672\n",
      "epoch: 4 step: 1127, loss is 0.2630362808704376\n",
      "epoch: 4 step: 1128, loss is 0.5015318393707275\n",
      "epoch: 4 step: 1129, loss is 0.7586848139762878\n",
      "epoch: 4 step: 1130, loss is 0.3361007273197174\n",
      "epoch: 4 step: 1131, loss is 0.298434317111969\n",
      "epoch: 4 step: 1132, loss is 0.36464712023735046\n",
      "epoch: 4 step: 1133, loss is 0.5921976566314697\n",
      "epoch: 4 step: 1134, loss is 0.7588628530502319\n",
      "epoch: 4 step: 1135, loss is 0.39794647693634033\n",
      "epoch: 4 step: 1136, loss is 0.4282718598842621\n",
      "epoch: 4 step: 1137, loss is 0.2960638701915741\n",
      "epoch: 4 step: 1138, loss is 0.45140329003334045\n",
      "epoch: 4 step: 1139, loss is 0.5651867389678955\n",
      "epoch: 4 step: 1140, loss is 0.27324017882347107\n",
      "epoch: 4 step: 1141, loss is 0.7368524670600891\n",
      "epoch: 4 step: 1142, loss is 0.6326107382774353\n",
      "epoch: 4 step: 1143, loss is 0.6173176169395447\n",
      "epoch: 4 step: 1144, loss is 0.2732834815979004\n",
      "epoch: 4 step: 1145, loss is 0.49360063672065735\n",
      "epoch: 4 step: 1146, loss is 0.42262694239616394\n",
      "epoch: 4 step: 1147, loss is 0.49666154384613037\n",
      "epoch: 4 step: 1148, loss is 0.4626554846763611\n",
      "epoch: 4 step: 1149, loss is 0.4963431656360626\n",
      "epoch: 4 step: 1150, loss is 0.26220202445983887\n",
      "epoch: 4 step: 1151, loss is 0.35385391116142273\n",
      "epoch: 4 step: 1152, loss is 0.23062251508235931\n",
      "epoch: 4 step: 1153, loss is 0.5697464346885681\n",
      "epoch: 4 step: 1154, loss is 0.17266340553760529\n",
      "epoch: 4 step: 1155, loss is 0.7022333145141602\n",
      "epoch: 4 step: 1156, loss is 0.43666666746139526\n",
      "epoch: 4 step: 1157, loss is 0.6031962633132935\n",
      "epoch: 4 step: 1158, loss is 0.606208860874176\n",
      "epoch: 4 step: 1159, loss is 0.3300705552101135\n",
      "epoch: 4 step: 1160, loss is 0.4710700213909149\n",
      "epoch: 4 step: 1161, loss is 0.8980452418327332\n",
      "epoch: 4 step: 1162, loss is 0.6560002565383911\n",
      "epoch: 4 step: 1163, loss is 0.4835064113140106\n",
      "epoch: 4 step: 1164, loss is 0.7423848509788513\n",
      "epoch: 4 step: 1165, loss is 0.4608687162399292\n",
      "epoch: 4 step: 1166, loss is 0.5224859118461609\n",
      "epoch: 4 step: 1167, loss is 0.22183063626289368\n",
      "epoch: 4 step: 1168, loss is 0.6193351149559021\n",
      "epoch: 4 step: 1169, loss is 0.635707437992096\n",
      "epoch: 4 step: 1170, loss is 0.7523885369300842\n",
      "epoch: 4 step: 1171, loss is 1.0384292602539062\n",
      "epoch: 4 step: 1172, loss is 0.6987413763999939\n",
      "epoch: 4 step: 1173, loss is 0.4814421534538269\n",
      "epoch: 4 step: 1174, loss is 0.5919380784034729\n",
      "epoch: 4 step: 1175, loss is 0.7531523704528809\n",
      "epoch: 4 step: 1176, loss is 0.4742002487182617\n",
      "epoch: 4 step: 1177, loss is 0.2756231725215912\n",
      "epoch: 4 step: 1178, loss is 0.678003191947937\n",
      "epoch: 4 step: 1179, loss is 0.4608270525932312\n",
      "epoch: 4 step: 1180, loss is 0.9591922760009766\n",
      "epoch: 4 step: 1181, loss is 0.63401198387146\n",
      "epoch: 4 step: 1182, loss is 0.5806768536567688\n",
      "epoch: 4 step: 1183, loss is 0.41666850447654724\n",
      "epoch: 4 step: 1184, loss is 0.4762691259384155\n",
      "epoch: 4 step: 1185, loss is 0.4501071870326996\n",
      "epoch: 4 step: 1186, loss is 0.35327333211898804\n",
      "epoch: 4 step: 1187, loss is 0.8123587965965271\n",
      "epoch: 4 step: 1188, loss is 0.45048093795776367\n",
      "epoch: 4 step: 1189, loss is 0.23741097748279572\n",
      "epoch: 4 step: 1190, loss is 0.4521837532520294\n",
      "epoch: 4 step: 1191, loss is 0.5500774383544922\n",
      "epoch: 4 step: 1192, loss is 0.8110876679420471\n",
      "epoch: 4 step: 1193, loss is 0.6294376254081726\n",
      "epoch: 4 step: 1194, loss is 0.15191099047660828\n",
      "epoch: 4 step: 1195, loss is 0.5222665071487427\n",
      "epoch: 4 step: 1196, loss is 0.39573970437049866\n",
      "epoch: 4 step: 1197, loss is 0.2895180284976959\n",
      "epoch: 4 step: 1198, loss is 0.5616515278816223\n",
      "epoch: 4 step: 1199, loss is 0.6647601127624512\n",
      "epoch: 4 step: 1200, loss is 0.5698490738868713\n",
      "epoch: 4 step: 1201, loss is 0.5512626767158508\n",
      "epoch: 4 step: 1202, loss is 0.4396325647830963\n",
      "epoch: 4 step: 1203, loss is 0.44734054803848267\n",
      "epoch: 4 step: 1204, loss is 0.5345128178596497\n",
      "epoch: 4 step: 1205, loss is 0.4068353474140167\n",
      "epoch: 4 step: 1206, loss is 0.42047715187072754\n",
      "epoch: 4 step: 1207, loss is 0.39459916949272156\n",
      "epoch: 4 step: 1208, loss is 0.43129870295524597\n",
      "epoch: 4 step: 1209, loss is 0.731325626373291\n",
      "epoch: 4 step: 1210, loss is 0.6384128928184509\n",
      "epoch: 4 step: 1211, loss is 0.35588523745536804\n",
      "epoch: 4 step: 1212, loss is 1.1986660957336426\n",
      "epoch: 4 step: 1213, loss is 0.30108094215393066\n",
      "epoch: 4 step: 1214, loss is 0.39206624031066895\n",
      "epoch: 4 step: 1215, loss is 0.3659987151622772\n",
      "epoch: 4 step: 1216, loss is 1.0159028768539429\n",
      "epoch: 4 step: 1217, loss is 0.44681641459465027\n",
      "epoch: 4 step: 1218, loss is 0.492115318775177\n",
      "epoch: 4 step: 1219, loss is 0.4695621430873871\n",
      "epoch: 4 step: 1220, loss is 0.39619728922843933\n",
      "epoch: 4 step: 1221, loss is 0.506603479385376\n",
      "epoch: 4 step: 1222, loss is 0.5348871350288391\n",
      "epoch: 4 step: 1223, loss is 0.3503718972206116\n",
      "epoch: 4 step: 1224, loss is 0.43759849667549133\n",
      "epoch: 4 step: 1225, loss is 0.4721371829509735\n",
      "epoch: 4 step: 1226, loss is 0.5003098845481873\n",
      "epoch: 4 step: 1227, loss is 0.401491641998291\n",
      "epoch: 4 step: 1228, loss is 0.7032972574234009\n",
      "epoch: 4 step: 1229, loss is 0.8948753476142883\n",
      "epoch: 4 step: 1230, loss is 0.4202069938182831\n",
      "epoch: 4 step: 1231, loss is 0.3483329117298126\n",
      "epoch: 4 step: 1232, loss is 0.3535374104976654\n",
      "epoch: 4 step: 1233, loss is 0.3950899839401245\n",
      "epoch: 4 step: 1234, loss is 0.885256826877594\n",
      "epoch: 4 step: 1235, loss is 0.5035176873207092\n",
      "epoch: 4 step: 1236, loss is 0.7145504951477051\n",
      "epoch: 4 step: 1237, loss is 0.2562061548233032\n",
      "epoch: 4 step: 1238, loss is 0.5205692052841187\n",
      "epoch: 4 step: 1239, loss is 0.7834943532943726\n",
      "epoch: 4 step: 1240, loss is 0.16571640968322754\n",
      "epoch: 4 step: 1241, loss is 0.8473110198974609\n",
      "epoch: 4 step: 1242, loss is 0.38877955079078674\n",
      "epoch: 4 step: 1243, loss is 0.993678629398346\n",
      "epoch: 4 step: 1244, loss is 0.31896841526031494\n",
      "epoch: 4 step: 1245, loss is 0.31078019738197327\n",
      "epoch: 4 step: 1246, loss is 0.268419474363327\n",
      "epoch: 4 step: 1247, loss is 0.17796562612056732\n",
      "epoch: 4 step: 1248, loss is 0.5294592976570129\n",
      "epoch: 4 step: 1249, loss is 1.1724578142166138\n",
      "epoch: 4 step: 1250, loss is 0.4156557619571686\n",
      "epoch: 4 step: 1251, loss is 0.46338558197021484\n",
      "epoch: 4 step: 1252, loss is 0.7889074087142944\n",
      "epoch: 4 step: 1253, loss is 0.9403060674667358\n",
      "epoch: 4 step: 1254, loss is 0.476572185754776\n",
      "epoch: 4 step: 1255, loss is 0.3770603537559509\n",
      "epoch: 4 step: 1256, loss is 0.421693354845047\n",
      "epoch: 4 step: 1257, loss is 0.8028773665428162\n",
      "epoch: 4 step: 1258, loss is 0.3875367045402527\n",
      "epoch: 4 step: 1259, loss is 0.5323657989501953\n",
      "epoch: 4 step: 1260, loss is 0.4715372920036316\n",
      "epoch: 4 step: 1261, loss is 0.2625562250614166\n",
      "epoch: 4 step: 1262, loss is 0.8200187683105469\n",
      "epoch: 4 step: 1263, loss is 0.4045722782611847\n",
      "epoch: 4 step: 1264, loss is 0.4630926251411438\n",
      "epoch: 4 step: 1265, loss is 0.40520066022872925\n",
      "epoch: 4 step: 1266, loss is 0.32697510719299316\n",
      "epoch: 4 step: 1267, loss is 0.7291175723075867\n",
      "epoch: 4 step: 1268, loss is 0.6418769955635071\n",
      "epoch: 4 step: 1269, loss is 0.4850161373615265\n",
      "epoch: 4 step: 1270, loss is 0.5029881000518799\n",
      "epoch: 4 step: 1271, loss is 0.46486443281173706\n",
      "epoch: 4 step: 1272, loss is 0.38839101791381836\n",
      "epoch: 4 step: 1273, loss is 0.8177282810211182\n",
      "epoch: 4 step: 1274, loss is 0.5775688290596008\n",
      "epoch: 4 step: 1275, loss is 0.30310919880867004\n",
      "epoch: 4 step: 1276, loss is 0.4255005717277527\n",
      "epoch: 4 step: 1277, loss is 0.4970639646053314\n",
      "epoch: 4 step: 1278, loss is 0.48083269596099854\n",
      "epoch: 4 step: 1279, loss is 0.9528689980506897\n",
      "epoch: 4 step: 1280, loss is 0.2736562490463257\n",
      "epoch: 4 step: 1281, loss is 0.8033928871154785\n",
      "epoch: 4 step: 1282, loss is 0.48854440450668335\n",
      "epoch: 4 step: 1283, loss is 0.794633686542511\n",
      "epoch: 4 step: 1284, loss is 0.5426546931266785\n",
      "epoch: 4 step: 1285, loss is 0.5566200017929077\n",
      "epoch: 4 step: 1286, loss is 0.23997485637664795\n",
      "epoch: 4 step: 1287, loss is 0.6029329895973206\n",
      "epoch: 4 step: 1288, loss is 0.5097971558570862\n",
      "epoch: 4 step: 1289, loss is 0.24170295894145966\n",
      "epoch: 4 step: 1290, loss is 0.5861747860908508\n",
      "epoch: 4 step: 1291, loss is 0.35216835141181946\n",
      "epoch: 4 step: 1292, loss is 0.46771448850631714\n",
      "epoch: 4 step: 1293, loss is 0.38410651683807373\n",
      "epoch: 4 step: 1294, loss is 0.5414251685142517\n",
      "epoch: 4 step: 1295, loss is 0.33444350957870483\n",
      "epoch: 4 step: 1296, loss is 0.4087570905685425\n",
      "epoch: 4 step: 1297, loss is 0.3654358983039856\n",
      "epoch: 4 step: 1298, loss is 0.7187972664833069\n",
      "epoch: 4 step: 1299, loss is 0.32359063625335693\n",
      "epoch: 4 step: 1300, loss is 0.4598819315433502\n",
      "epoch: 4 step: 1301, loss is 0.3357200026512146\n",
      "epoch: 4 step: 1302, loss is 0.4977484941482544\n",
      "epoch: 4 step: 1303, loss is 0.8597478866577148\n",
      "epoch: 4 step: 1304, loss is 0.4363638460636139\n",
      "epoch: 4 step: 1305, loss is 0.24412816762924194\n",
      "epoch: 4 step: 1306, loss is 0.6414970755577087\n",
      "epoch: 4 step: 1307, loss is 0.33471786975860596\n",
      "epoch: 4 step: 1308, loss is 0.38551414012908936\n",
      "epoch: 4 step: 1309, loss is 0.5226752758026123\n",
      "epoch: 4 step: 1310, loss is 0.592289924621582\n",
      "epoch: 4 step: 1311, loss is 0.41759783029556274\n",
      "epoch: 4 step: 1312, loss is 0.38089632987976074\n",
      "epoch: 4 step: 1313, loss is 0.4813652038574219\n",
      "epoch: 4 step: 1314, loss is 0.40016239881515503\n",
      "epoch: 4 step: 1315, loss is 0.4768257737159729\n",
      "epoch: 4 step: 1316, loss is 0.24206534028053284\n",
      "epoch: 4 step: 1317, loss is 0.6204198002815247\n",
      "epoch: 4 step: 1318, loss is 0.48773396015167236\n",
      "epoch: 4 step: 1319, loss is 0.46415919065475464\n",
      "epoch: 4 step: 1320, loss is 0.8704695105552673\n",
      "epoch: 4 step: 1321, loss is 0.2313879430294037\n",
      "epoch: 4 step: 1322, loss is 0.5717686414718628\n",
      "epoch: 4 step: 1323, loss is 0.22834543883800507\n",
      "epoch: 4 step: 1324, loss is 0.2679145038127899\n",
      "epoch: 4 step: 1325, loss is 0.7200883626937866\n",
      "epoch: 4 step: 1326, loss is 0.42662397027015686\n",
      "epoch: 4 step: 1327, loss is 0.208751380443573\n",
      "epoch: 4 step: 1328, loss is 0.31584280729293823\n",
      "epoch: 4 step: 1329, loss is 0.4147484600543976\n",
      "epoch: 4 step: 1330, loss is 0.4731762111186981\n",
      "epoch: 4 step: 1331, loss is 0.4133087396621704\n",
      "epoch: 4 step: 1332, loss is 0.9601082801818848\n",
      "epoch: 4 step: 1333, loss is 0.35113635659217834\n",
      "epoch: 4 step: 1334, loss is 0.3872569501399994\n",
      "epoch: 4 step: 1335, loss is 0.22535984218120575\n",
      "epoch: 4 step: 1336, loss is 0.5663052201271057\n",
      "epoch: 4 step: 1337, loss is 0.6403281092643738\n",
      "epoch: 4 step: 1338, loss is 0.26286396384239197\n",
      "epoch: 4 step: 1339, loss is 0.3595898747444153\n",
      "epoch: 4 step: 1340, loss is 0.18800616264343262\n",
      "epoch: 4 step: 1341, loss is 0.36230793595314026\n",
      "epoch: 4 step: 1342, loss is 0.12318964302539825\n",
      "epoch: 4 step: 1343, loss is 0.9935632944107056\n",
      "epoch: 4 step: 1344, loss is 0.7071012258529663\n",
      "epoch: 4 step: 1345, loss is 0.3765152394771576\n",
      "epoch: 4 step: 1346, loss is 0.4149795472621918\n",
      "epoch: 4 step: 1347, loss is 0.5511481761932373\n",
      "epoch: 4 step: 1348, loss is 0.6430429816246033\n",
      "epoch: 4 step: 1349, loss is 0.5477472543716431\n",
      "epoch: 4 step: 1350, loss is 0.36248651146888733\n",
      "epoch: 4 step: 1351, loss is 0.3894382119178772\n",
      "epoch: 4 step: 1352, loss is 0.516756534576416\n",
      "epoch: 4 step: 1353, loss is 0.38450944423675537\n",
      "epoch: 4 step: 1354, loss is 0.6344067454338074\n",
      "epoch: 4 step: 1355, loss is 0.6595060229301453\n",
      "epoch: 4 step: 1356, loss is 0.549706220626831\n",
      "epoch: 4 step: 1357, loss is 0.6457303762435913\n",
      "epoch: 4 step: 1358, loss is 0.3327290117740631\n",
      "epoch: 4 step: 1359, loss is 0.2750527560710907\n",
      "epoch: 4 step: 1360, loss is 0.3214752674102783\n",
      "epoch: 4 step: 1361, loss is 0.29213112592697144\n",
      "epoch: 4 step: 1362, loss is 0.584446907043457\n",
      "epoch: 4 step: 1363, loss is 0.2845574915409088\n",
      "epoch: 4 step: 1364, loss is 0.2353680580854416\n",
      "epoch: 4 step: 1365, loss is 0.8392817378044128\n",
      "epoch: 4 step: 1366, loss is 0.39601975679397583\n",
      "epoch: 4 step: 1367, loss is 0.39421600103378296\n",
      "epoch: 4 step: 1368, loss is 0.38628900051116943\n",
      "epoch: 4 step: 1369, loss is 0.322693407535553\n",
      "epoch: 4 step: 1370, loss is 0.4480212330818176\n",
      "epoch: 4 step: 1371, loss is 0.7203738689422607\n",
      "epoch: 4 step: 1372, loss is 0.25614655017852783\n",
      "epoch: 4 step: 1373, loss is 0.4206310212612152\n",
      "epoch: 4 step: 1374, loss is 0.9216196537017822\n",
      "epoch: 4 step: 1375, loss is 0.4844326674938202\n",
      "epoch: 4 step: 1376, loss is 0.6955761313438416\n",
      "epoch: 4 step: 1377, loss is 0.4814000725746155\n",
      "epoch: 4 step: 1378, loss is 0.6573627591133118\n",
      "epoch: 4 step: 1379, loss is 0.45021289587020874\n",
      "epoch: 4 step: 1380, loss is 0.6080988049507141\n",
      "epoch: 4 step: 1381, loss is 0.4631635546684265\n",
      "epoch: 4 step: 1382, loss is 0.37076443433761597\n",
      "epoch: 4 step: 1383, loss is 0.280382364988327\n",
      "epoch: 4 step: 1384, loss is 0.762511134147644\n",
      "epoch: 4 step: 1385, loss is 0.5734778642654419\n",
      "epoch: 4 step: 1386, loss is 0.515285313129425\n",
      "epoch: 4 step: 1387, loss is 0.5966711044311523\n",
      "epoch: 4 step: 1388, loss is 0.38025933504104614\n",
      "epoch: 4 step: 1389, loss is 0.5513303875923157\n",
      "epoch: 4 step: 1390, loss is 0.5590806603431702\n",
      "epoch: 4 step: 1391, loss is 0.3215124309062958\n",
      "epoch: 4 step: 1392, loss is 0.4178488254547119\n",
      "epoch: 4 step: 1393, loss is 0.246778205037117\n",
      "epoch: 4 step: 1394, loss is 0.48723867535591125\n",
      "epoch: 4 step: 1395, loss is 0.23730723559856415\n",
      "epoch: 4 step: 1396, loss is 0.49404504895210266\n",
      "epoch: 4 step: 1397, loss is 0.398989200592041\n",
      "epoch: 4 step: 1398, loss is 0.638729453086853\n",
      "epoch: 4 step: 1399, loss is 0.3724406957626343\n",
      "epoch: 4 step: 1400, loss is 0.5968475937843323\n",
      "epoch: 4 step: 1401, loss is 0.8481629490852356\n",
      "epoch: 4 step: 1402, loss is 0.8611852526664734\n",
      "epoch: 4 step: 1403, loss is 0.4358166754245758\n",
      "epoch: 4 step: 1404, loss is 0.29581156373023987\n",
      "epoch: 4 step: 1405, loss is 0.5893290042877197\n",
      "epoch: 4 step: 1406, loss is 0.40706294775009155\n",
      "epoch: 4 step: 1407, loss is 0.548494279384613\n",
      "epoch: 4 step: 1408, loss is 0.37511348724365234\n",
      "epoch: 4 step: 1409, loss is 0.2270381599664688\n",
      "epoch: 4 step: 1410, loss is 0.5385268330574036\n",
      "epoch: 4 step: 1411, loss is 0.5443809628486633\n",
      "epoch: 4 step: 1412, loss is 1.126405119895935\n",
      "epoch: 4 step: 1413, loss is 0.42818862199783325\n",
      "epoch: 4 step: 1414, loss is 0.538066565990448\n",
      "epoch: 4 step: 1415, loss is 0.4525075852870941\n",
      "epoch: 4 step: 1416, loss is 0.18647240102291107\n",
      "epoch: 4 step: 1417, loss is 0.4781721234321594\n",
      "epoch: 4 step: 1418, loss is 0.4507356286048889\n",
      "epoch: 4 step: 1419, loss is 0.6704535484313965\n",
      "epoch: 4 step: 1420, loss is 0.7087808847427368\n",
      "epoch: 4 step: 1421, loss is 0.5014409422874451\n",
      "epoch: 4 step: 1422, loss is 0.21428926289081573\n",
      "epoch: 4 step: 1423, loss is 0.5116557478904724\n",
      "epoch: 4 step: 1424, loss is 0.49754685163497925\n",
      "epoch: 4 step: 1425, loss is 0.9802099466323853\n",
      "epoch: 4 step: 1426, loss is 0.5851024389266968\n",
      "epoch: 4 step: 1427, loss is 0.4537229537963867\n",
      "epoch: 4 step: 1428, loss is 0.41007333993911743\n",
      "epoch: 4 step: 1429, loss is 0.6722595691680908\n",
      "epoch: 4 step: 1430, loss is 0.7987102270126343\n",
      "epoch: 4 step: 1431, loss is 0.6689496040344238\n",
      "epoch: 4 step: 1432, loss is 0.7984808683395386\n",
      "epoch: 4 step: 1433, loss is 0.3752366602420807\n",
      "epoch: 4 step: 1434, loss is 0.28359246253967285\n",
      "epoch: 4 step: 1435, loss is 0.38961440324783325\n",
      "epoch: 4 step: 1436, loss is 0.3507797122001648\n",
      "epoch: 4 step: 1437, loss is 0.24957048892974854\n",
      "epoch: 4 step: 1438, loss is 1.065043330192566\n",
      "epoch: 4 step: 1439, loss is 0.2528861463069916\n",
      "epoch: 4 step: 1440, loss is 0.8853112459182739\n",
      "epoch: 4 step: 1441, loss is 0.7740655541419983\n",
      "epoch: 4 step: 1442, loss is 0.8607556223869324\n",
      "epoch: 4 step: 1443, loss is 0.31634145975112915\n",
      "epoch: 4 step: 1444, loss is 0.5488642454147339\n",
      "epoch: 4 step: 1445, loss is 0.4275958836078644\n",
      "epoch: 4 step: 1446, loss is 0.39651256799697876\n",
      "epoch: 4 step: 1447, loss is 0.6960170865058899\n",
      "epoch: 4 step: 1448, loss is 0.5732828378677368\n",
      "epoch: 4 step: 1449, loss is 0.49745747447013855\n",
      "epoch: 4 step: 1450, loss is 0.49837586283683777\n",
      "epoch: 4 step: 1451, loss is 0.6126216053962708\n",
      "epoch: 4 step: 1452, loss is 0.3169582486152649\n",
      "epoch: 4 step: 1453, loss is 0.6327287554740906\n",
      "epoch: 4 step: 1454, loss is 0.5425522923469543\n",
      "epoch: 4 step: 1455, loss is 0.47154510021209717\n",
      "epoch: 4 step: 1456, loss is 0.5349013209342957\n",
      "epoch: 4 step: 1457, loss is 0.5106995701789856\n",
      "epoch: 4 step: 1458, loss is 0.5542382597923279\n",
      "epoch: 4 step: 1459, loss is 0.6596571207046509\n",
      "epoch: 4 step: 1460, loss is 0.5631177425384521\n",
      "epoch: 4 step: 1461, loss is 0.37571924924850464\n",
      "epoch: 4 step: 1462, loss is 0.5971441268920898\n",
      "epoch: 4 step: 1463, loss is 0.6135121583938599\n",
      "epoch: 4 step: 1464, loss is 0.49596932530403137\n",
      "epoch: 4 step: 1465, loss is 0.786920964717865\n",
      "epoch: 4 step: 1466, loss is 0.5582824349403381\n",
      "epoch: 4 step: 1467, loss is 0.519269585609436\n",
      "epoch: 4 step: 1468, loss is 0.5839872360229492\n",
      "epoch: 4 step: 1469, loss is 0.9768785238265991\n",
      "epoch: 4 step: 1470, loss is 0.513093888759613\n",
      "epoch: 4 step: 1471, loss is 0.6217623949050903\n",
      "epoch: 4 step: 1472, loss is 0.3980956971645355\n",
      "epoch: 4 step: 1473, loss is 0.8177289962768555\n",
      "epoch: 4 step: 1474, loss is 0.21106481552124023\n",
      "epoch: 4 step: 1475, loss is 0.5670921206474304\n",
      "epoch: 4 step: 1476, loss is 0.4530884027481079\n",
      "epoch: 4 step: 1477, loss is 0.6004921197891235\n",
      "epoch: 4 step: 1478, loss is 0.7001308798789978\n",
      "epoch: 4 step: 1479, loss is 0.4799158275127411\n",
      "epoch: 4 step: 1480, loss is 0.28654831647872925\n",
      "epoch: 4 step: 1481, loss is 0.513578474521637\n",
      "epoch: 4 step: 1482, loss is 0.4248991906642914\n",
      "epoch: 4 step: 1483, loss is 0.40271663665771484\n",
      "epoch: 4 step: 1484, loss is 0.2798129618167877\n",
      "epoch: 4 step: 1485, loss is 0.295841246843338\n",
      "epoch: 4 step: 1486, loss is 0.537700355052948\n",
      "epoch: 4 step: 1487, loss is 0.49622607231140137\n",
      "epoch: 4 step: 1488, loss is 0.23429325222969055\n",
      "epoch: 4 step: 1489, loss is 0.2069339156150818\n",
      "epoch: 4 step: 1490, loss is 0.6674299836158752\n",
      "epoch: 4 step: 1491, loss is 0.456919401884079\n",
      "epoch: 4 step: 1492, loss is 0.28872770071029663\n",
      "epoch: 4 step: 1493, loss is 0.3126314580440521\n",
      "epoch: 4 step: 1494, loss is 0.6651102304458618\n",
      "epoch: 4 step: 1495, loss is 0.43206486105918884\n",
      "epoch: 4 step: 1496, loss is 0.34524762630462646\n",
      "epoch: 4 step: 1497, loss is 0.43854820728302\n",
      "epoch: 4 step: 1498, loss is 0.5519731640815735\n",
      "epoch: 4 step: 1499, loss is 0.8399524688720703\n",
      "epoch: 4 step: 1500, loss is 0.3301992416381836\n",
      "epoch: 4 step: 1501, loss is 0.4044798016548157\n",
      "epoch: 4 step: 1502, loss is 0.4287410080432892\n",
      "epoch: 4 step: 1503, loss is 0.3370193541049957\n",
      "epoch: 4 step: 1504, loss is 0.3364512026309967\n",
      "epoch: 4 step: 1505, loss is 0.6168757081031799\n",
      "epoch: 4 step: 1506, loss is 0.7402933239936829\n",
      "epoch: 4 step: 1507, loss is 0.4243202209472656\n",
      "epoch: 4 step: 1508, loss is 0.67829829454422\n",
      "epoch: 4 step: 1509, loss is 0.38023844361305237\n",
      "epoch: 4 step: 1510, loss is 0.3417752683162689\n",
      "epoch: 4 step: 1511, loss is 0.4968777596950531\n",
      "epoch: 4 step: 1512, loss is 0.3801048696041107\n",
      "epoch: 4 step: 1513, loss is 0.29796507954597473\n",
      "epoch: 4 step: 1514, loss is 0.44917353987693787\n",
      "epoch: 4 step: 1515, loss is 0.5256110429763794\n",
      "epoch: 4 step: 1516, loss is 1.2526671886444092\n",
      "epoch: 4 step: 1517, loss is 0.6699495315551758\n",
      "epoch: 4 step: 1518, loss is 0.6641072630882263\n",
      "epoch: 4 step: 1519, loss is 0.5157062411308289\n",
      "epoch: 4 step: 1520, loss is 0.37571245431900024\n",
      "epoch: 4 step: 1521, loss is 0.28308242559432983\n",
      "epoch: 4 step: 1522, loss is 0.5598835349082947\n",
      "epoch: 4 step: 1523, loss is 0.6189493536949158\n",
      "epoch: 4 step: 1524, loss is 0.6290045976638794\n",
      "epoch: 4 step: 1525, loss is 0.14515310525894165\n",
      "epoch: 4 step: 1526, loss is 0.6081449389457703\n",
      "epoch: 4 step: 1527, loss is 0.4346621334552765\n",
      "epoch: 4 step: 1528, loss is 0.3946129083633423\n",
      "epoch: 4 step: 1529, loss is 0.409980833530426\n",
      "epoch: 4 step: 1530, loss is 0.8223423957824707\n",
      "epoch: 4 step: 1531, loss is 0.6667177081108093\n",
      "epoch: 4 step: 1532, loss is 0.3249785006046295\n",
      "epoch: 4 step: 1533, loss is 0.35672008991241455\n",
      "epoch: 4 step: 1534, loss is 0.4711344242095947\n",
      "epoch: 4 step: 1535, loss is 0.3589221239089966\n",
      "epoch: 4 step: 1536, loss is 0.3637961149215698\n",
      "epoch: 4 step: 1537, loss is 0.6381027102470398\n",
      "epoch: 4 step: 1538, loss is 0.7039689421653748\n",
      "epoch: 4 step: 1539, loss is 0.539820671081543\n",
      "epoch: 4 step: 1540, loss is 0.3497523367404938\n",
      "epoch: 4 step: 1541, loss is 0.8225524425506592\n",
      "epoch: 4 step: 1542, loss is 0.40524864196777344\n",
      "epoch: 4 step: 1543, loss is 0.38396474719047546\n",
      "epoch: 4 step: 1544, loss is 0.70379239320755\n",
      "epoch: 4 step: 1545, loss is 0.28184762597084045\n",
      "epoch: 4 step: 1546, loss is 0.5078667998313904\n",
      "epoch: 4 step: 1547, loss is 0.27099037170410156\n",
      "epoch: 4 step: 1548, loss is 0.3668263554573059\n",
      "epoch: 4 step: 1549, loss is 0.6109046936035156\n",
      "epoch: 4 step: 1550, loss is 0.3429734408855438\n",
      "epoch: 4 step: 1551, loss is 0.7978653311729431\n",
      "epoch: 4 step: 1552, loss is 0.5714029669761658\n",
      "epoch: 4 step: 1553, loss is 0.48941129446029663\n",
      "epoch: 4 step: 1554, loss is 0.7770974040031433\n",
      "epoch: 4 step: 1555, loss is 0.5932815670967102\n",
      "epoch: 4 step: 1556, loss is 0.7944855690002441\n",
      "epoch: 4 step: 1557, loss is 0.7623937726020813\n",
      "epoch: 4 step: 1558, loss is 0.5071878433227539\n",
      "epoch: 4 step: 1559, loss is 0.4549860656261444\n",
      "epoch: 4 step: 1560, loss is 0.5486275553703308\n",
      "epoch: 4 step: 1561, loss is 0.40573519468307495\n",
      "epoch: 4 step: 1562, loss is 0.5043138265609741\n",
      "epoch: 4 step: 1563, loss is 0.5874563455581665\n",
      "epoch: 4 step: 1564, loss is 0.31583714485168457\n",
      "epoch: 4 step: 1565, loss is 0.43633100390434265\n",
      "epoch: 4 step: 1566, loss is 0.5600088238716125\n",
      "epoch: 4 step: 1567, loss is 0.36285412311553955\n",
      "epoch: 4 step: 1568, loss is 0.5593556761741638\n",
      "epoch: 4 step: 1569, loss is 0.6016851663589478\n",
      "epoch: 4 step: 1570, loss is 0.32429325580596924\n",
      "epoch: 4 step: 1571, loss is 0.3439687490463257\n",
      "epoch: 4 step: 1572, loss is 0.4887532889842987\n",
      "epoch: 4 step: 1573, loss is 0.5050051212310791\n",
      "epoch: 4 step: 1574, loss is 0.33076903223991394\n",
      "epoch: 4 step: 1575, loss is 0.30708467960357666\n",
      "epoch: 4 step: 1576, loss is 0.5788101553916931\n",
      "epoch: 4 step: 1577, loss is 0.4056372046470642\n",
      "epoch: 4 step: 1578, loss is 0.7890220284461975\n",
      "epoch: 4 step: 1579, loss is 0.6033733487129211\n",
      "epoch: 4 step: 1580, loss is 0.676018476486206\n",
      "epoch: 4 step: 1581, loss is 0.363567590713501\n",
      "epoch: 4 step: 1582, loss is 0.7180400490760803\n",
      "epoch: 4 step: 1583, loss is 0.3048608601093292\n",
      "epoch: 4 step: 1584, loss is 0.3176692724227905\n",
      "epoch: 4 step: 1585, loss is 0.44544705748558044\n",
      "epoch: 4 step: 1586, loss is 0.5114096403121948\n",
      "epoch: 4 step: 1587, loss is 0.43063345551490784\n",
      "epoch: 4 step: 1588, loss is 0.9229856133460999\n",
      "epoch: 4 step: 1589, loss is 0.48900729417800903\n",
      "epoch: 4 step: 1590, loss is 1.0611042976379395\n",
      "epoch: 4 step: 1591, loss is 0.3460356295108795\n",
      "epoch: 4 step: 1592, loss is 0.5130500197410583\n",
      "epoch: 4 step: 1593, loss is 0.644202470779419\n",
      "epoch: 4 step: 1594, loss is 0.3601822257041931\n",
      "epoch: 4 step: 1595, loss is 0.373410701751709\n",
      "epoch: 4 step: 1596, loss is 0.533731997013092\n",
      "epoch: 4 step: 1597, loss is 0.44041064381599426\n",
      "epoch: 4 step: 1598, loss is 0.5665735602378845\n",
      "epoch: 4 step: 1599, loss is 0.5466381907463074\n",
      "epoch: 4 step: 1600, loss is 0.46040117740631104\n",
      "epoch: 4 step: 1601, loss is 0.23657546937465668\n",
      "epoch: 4 step: 1602, loss is 0.4973032474517822\n",
      "epoch: 4 step: 1603, loss is 0.5856214761734009\n",
      "epoch: 4 step: 1604, loss is 0.40414831042289734\n",
      "epoch: 4 step: 1605, loss is 0.5520850419998169\n",
      "epoch: 4 step: 1606, loss is 0.46597325801849365\n",
      "epoch: 4 step: 1607, loss is 0.3517645299434662\n",
      "epoch: 4 step: 1608, loss is 0.5725721120834351\n",
      "epoch: 4 step: 1609, loss is 0.46721380949020386\n",
      "epoch: 4 step: 1610, loss is 0.46431076526641846\n",
      "epoch: 4 step: 1611, loss is 0.4602617919445038\n",
      "epoch: 4 step: 1612, loss is 0.42608529329299927\n",
      "epoch: 4 step: 1613, loss is 0.32622337341308594\n",
      "epoch: 4 step: 1614, loss is 0.3847019076347351\n",
      "epoch: 4 step: 1615, loss is 0.33832889795303345\n",
      "epoch: 4 step: 1616, loss is 0.16948941349983215\n",
      "epoch: 4 step: 1617, loss is 0.49412986636161804\n",
      "epoch: 4 step: 1618, loss is 0.21384495496749878\n",
      "epoch: 4 step: 1619, loss is 0.3842979967594147\n",
      "epoch: 4 step: 1620, loss is 0.9771857857704163\n",
      "epoch: 4 step: 1621, loss is 0.5331906080245972\n",
      "epoch: 4 step: 1622, loss is 0.5129354596138\n",
      "epoch: 4 step: 1623, loss is 0.11121590435504913\n",
      "epoch: 4 step: 1624, loss is 0.2597395181655884\n",
      "epoch: 4 step: 1625, loss is 0.5666826963424683\n",
      "epoch: 4 step: 1626, loss is 0.4376968443393707\n",
      "epoch: 4 step: 1627, loss is 0.18294888734817505\n",
      "epoch: 4 step: 1628, loss is 0.46998608112335205\n",
      "epoch: 4 step: 1629, loss is 0.2544441819190979\n",
      "epoch: 4 step: 1630, loss is 0.756085216999054\n",
      "epoch: 4 step: 1631, loss is 0.6142306923866272\n",
      "epoch: 4 step: 1632, loss is 0.1664617657661438\n",
      "epoch: 4 step: 1633, loss is 0.43770554661750793\n",
      "epoch: 4 step: 1634, loss is 0.6582911610603333\n",
      "epoch: 4 step: 1635, loss is 0.5737581849098206\n",
      "epoch: 4 step: 1636, loss is 0.37383580207824707\n",
      "epoch: 4 step: 1637, loss is 0.47328677773475647\n",
      "epoch: 4 step: 1638, loss is 0.2549719512462616\n",
      "epoch: 4 step: 1639, loss is 0.38111501932144165\n",
      "epoch: 4 step: 1640, loss is 0.8611565828323364\n",
      "epoch: 4 step: 1641, loss is 0.5613181591033936\n",
      "epoch: 4 step: 1642, loss is 0.6139046549797058\n",
      "epoch: 4 step: 1643, loss is 0.3277786374092102\n",
      "epoch: 4 step: 1644, loss is 0.31866025924682617\n",
      "epoch: 4 step: 1645, loss is 0.3164735436439514\n",
      "epoch: 4 step: 1646, loss is 0.9321656227111816\n",
      "epoch: 4 step: 1647, loss is 0.46179574728012085\n",
      "epoch: 4 step: 1648, loss is 0.9630839824676514\n",
      "epoch: 4 step: 1649, loss is 0.34821733832359314\n",
      "epoch: 4 step: 1650, loss is 0.5619902610778809\n",
      "epoch: 4 step: 1651, loss is 0.34672319889068604\n",
      "epoch: 4 step: 1652, loss is 0.7510164976119995\n",
      "epoch: 4 step: 1653, loss is 0.3849409818649292\n",
      "epoch: 4 step: 1654, loss is 0.6776925921440125\n",
      "epoch: 4 step: 1655, loss is 0.6222796440124512\n",
      "epoch: 4 step: 1656, loss is 0.5613433718681335\n",
      "epoch: 4 step: 1657, loss is 0.35562336444854736\n",
      "epoch: 4 step: 1658, loss is 0.3661079704761505\n",
      "epoch: 4 step: 1659, loss is 0.5796045064926147\n",
      "epoch: 4 step: 1660, loss is 0.5243052840232849\n",
      "epoch: 4 step: 1661, loss is 0.32473698258399963\n",
      "epoch: 4 step: 1662, loss is 0.4160565733909607\n",
      "epoch: 4 step: 1663, loss is 0.4459341764450073\n",
      "epoch: 4 step: 1664, loss is 0.47132188081741333\n",
      "epoch: 4 step: 1665, loss is 0.5607815980911255\n",
      "epoch: 4 step: 1666, loss is 0.4739331603050232\n",
      "epoch: 4 step: 1667, loss is 0.3784262239933014\n",
      "epoch: 4 step: 1668, loss is 0.5925643444061279\n",
      "epoch: 4 step: 1669, loss is 0.27592355012893677\n",
      "epoch: 4 step: 1670, loss is 0.31079334020614624\n",
      "epoch: 4 step: 1671, loss is 0.3505270183086395\n",
      "epoch: 4 step: 1672, loss is 0.5590763092041016\n",
      "epoch: 4 step: 1673, loss is 0.5038129687309265\n",
      "epoch: 4 step: 1674, loss is 0.5706321001052856\n",
      "epoch: 4 step: 1675, loss is 0.4497377276420593\n",
      "epoch: 4 step: 1676, loss is 0.36879080533981323\n",
      "epoch: 4 step: 1677, loss is 0.4133991301059723\n",
      "epoch: 4 step: 1678, loss is 0.4794268012046814\n",
      "epoch: 4 step: 1679, loss is 0.4351900815963745\n",
      "epoch: 4 step: 1680, loss is 0.2997956871986389\n",
      "epoch: 4 step: 1681, loss is 0.31057173013687134\n",
      "epoch: 4 step: 1682, loss is 0.3905365467071533\n",
      "epoch: 4 step: 1683, loss is 0.5634262561798096\n",
      "epoch: 4 step: 1684, loss is 0.7478755712509155\n",
      "epoch: 4 step: 1685, loss is 0.1793326884508133\n",
      "epoch: 4 step: 1686, loss is 0.3833833932876587\n",
      "epoch: 4 step: 1687, loss is 0.21566763520240784\n",
      "epoch: 4 step: 1688, loss is 0.6532220244407654\n",
      "epoch: 4 step: 1689, loss is 0.2612040340900421\n",
      "epoch: 4 step: 1690, loss is 0.17543980479240417\n",
      "epoch: 4 step: 1691, loss is 0.3018677830696106\n",
      "epoch: 4 step: 1692, loss is 0.467660129070282\n",
      "epoch: 4 step: 1693, loss is 0.23084919154644012\n",
      "epoch: 4 step: 1694, loss is 0.48751112818717957\n",
      "epoch: 4 step: 1695, loss is 0.5275437235832214\n",
      "epoch: 4 step: 1696, loss is 0.32398927211761475\n",
      "epoch: 4 step: 1697, loss is 0.21022705733776093\n",
      "epoch: 4 step: 1698, loss is 1.0370192527770996\n",
      "epoch: 4 step: 1699, loss is 0.5308733582496643\n",
      "epoch: 4 step: 1700, loss is 0.4115120768547058\n",
      "epoch: 4 step: 1701, loss is 0.5334833264350891\n",
      "epoch: 4 step: 1702, loss is 0.23610751330852509\n",
      "epoch: 4 step: 1703, loss is 0.4867042303085327\n",
      "epoch: 4 step: 1704, loss is 0.4864439070224762\n",
      "epoch: 4 step: 1705, loss is 0.522344172000885\n",
      "epoch: 4 step: 1706, loss is 0.44007542729377747\n",
      "epoch: 4 step: 1707, loss is 0.3366925120353699\n",
      "epoch: 4 step: 1708, loss is 0.7499555349349976\n",
      "epoch: 4 step: 1709, loss is 0.5706990361213684\n",
      "epoch: 4 step: 1710, loss is 0.289186030626297\n",
      "epoch: 4 step: 1711, loss is 0.1303991973400116\n",
      "epoch: 4 step: 1712, loss is 0.6070595979690552\n",
      "epoch: 4 step: 1713, loss is 0.6683619618415833\n",
      "epoch: 4 step: 1714, loss is 0.8287028074264526\n",
      "epoch: 4 step: 1715, loss is 0.3620603382587433\n",
      "epoch: 4 step: 1716, loss is 0.2142321616411209\n",
      "epoch: 4 step: 1717, loss is 0.5745667815208435\n",
      "epoch: 4 step: 1718, loss is 0.4656108617782593\n",
      "epoch: 4 step: 1719, loss is 0.6496016979217529\n",
      "epoch: 4 step: 1720, loss is 1.04828679561615\n",
      "epoch: 4 step: 1721, loss is 0.5160038471221924\n",
      "epoch: 4 step: 1722, loss is 0.4786844551563263\n",
      "epoch: 4 step: 1723, loss is 0.5847612619400024\n",
      "epoch: 4 step: 1724, loss is 1.1478153467178345\n",
      "epoch: 4 step: 1725, loss is 0.6273636221885681\n",
      "epoch: 4 step: 1726, loss is 0.6783391237258911\n",
      "epoch: 4 step: 1727, loss is 0.6873135566711426\n",
      "epoch: 4 step: 1728, loss is 0.6020123958587646\n",
      "epoch: 4 step: 1729, loss is 0.3112555146217346\n",
      "epoch: 4 step: 1730, loss is 0.5341589450836182\n",
      "epoch: 4 step: 1731, loss is 0.44261041283607483\n",
      "epoch: 4 step: 1732, loss is 0.4842680096626282\n",
      "epoch: 4 step: 1733, loss is 0.36872395873069763\n",
      "epoch: 4 step: 1734, loss is 0.3361305892467499\n",
      "epoch: 4 step: 1735, loss is 0.5598872303962708\n",
      "epoch: 4 step: 1736, loss is 0.3384700119495392\n",
      "epoch: 4 step: 1737, loss is 0.3994417190551758\n",
      "epoch: 4 step: 1738, loss is 0.3940097689628601\n",
      "epoch: 4 step: 1739, loss is 0.45943936705589294\n",
      "epoch: 4 step: 1740, loss is 0.3408288359642029\n",
      "epoch: 4 step: 1741, loss is 0.8136053681373596\n",
      "epoch: 4 step: 1742, loss is 0.28637298941612244\n",
      "epoch: 4 step: 1743, loss is 0.23569649457931519\n",
      "epoch: 4 step: 1744, loss is 0.6970332264900208\n",
      "epoch: 4 step: 1745, loss is 0.8799927234649658\n",
      "epoch: 4 step: 1746, loss is 0.5095968246459961\n",
      "epoch: 4 step: 1747, loss is 1.1905606985092163\n",
      "epoch: 4 step: 1748, loss is 0.5799725651741028\n",
      "epoch: 4 step: 1749, loss is 0.3203911781311035\n",
      "epoch: 4 step: 1750, loss is 0.5625901222229004\n",
      "epoch: 4 step: 1751, loss is 0.49375709891319275\n",
      "epoch: 4 step: 1752, loss is 0.4446639120578766\n",
      "epoch: 4 step: 1753, loss is 0.429803729057312\n",
      "epoch: 4 step: 1754, loss is 0.5732773542404175\n",
      "epoch: 4 step: 1755, loss is 0.8104411959648132\n",
      "epoch: 4 step: 1756, loss is 0.3214152157306671\n",
      "epoch: 4 step: 1757, loss is 0.4472448527812958\n",
      "epoch: 4 step: 1758, loss is 0.531080424785614\n",
      "epoch: 4 step: 1759, loss is 0.7268239855766296\n",
      "epoch: 4 step: 1760, loss is 0.3203621506690979\n",
      "epoch: 4 step: 1761, loss is 0.3533760607242584\n",
      "epoch: 4 step: 1762, loss is 0.4497736692428589\n",
      "epoch: 4 step: 1763, loss is 0.48946669697761536\n",
      "epoch: 4 step: 1764, loss is 0.4460042119026184\n",
      "epoch: 4 step: 1765, loss is 0.38009411096572876\n",
      "epoch: 4 step: 1766, loss is 0.4775983691215515\n",
      "epoch: 4 step: 1767, loss is 0.38662901520729065\n",
      "epoch: 4 step: 1768, loss is 0.4559400677680969\n",
      "epoch: 4 step: 1769, loss is 0.4658307433128357\n",
      "epoch: 4 step: 1770, loss is 0.48519042134284973\n",
      "epoch: 4 step: 1771, loss is 0.9195570349693298\n",
      "epoch: 4 step: 1772, loss is 0.5895102024078369\n",
      "epoch: 4 step: 1773, loss is 0.8006359934806824\n",
      "epoch: 4 step: 1774, loss is 0.5114713311195374\n",
      "epoch: 4 step: 1775, loss is 0.2735871970653534\n",
      "epoch: 4 step: 1776, loss is 0.44890275597572327\n",
      "epoch: 4 step: 1777, loss is 0.42979884147644043\n",
      "epoch: 4 step: 1778, loss is 0.43567636609077454\n",
      "epoch: 4 step: 1779, loss is 0.3469546139240265\n",
      "epoch: 4 step: 1780, loss is 0.4232718050479889\n",
      "epoch: 4 step: 1781, loss is 0.5971429347991943\n",
      "epoch: 4 step: 1782, loss is 0.719962477684021\n",
      "epoch: 4 step: 1783, loss is 0.4610596001148224\n",
      "epoch: 4 step: 1784, loss is 0.3841092884540558\n",
      "epoch: 4 step: 1785, loss is 0.5480191707611084\n",
      "epoch: 4 step: 1786, loss is 0.4158552587032318\n",
      "epoch: 4 step: 1787, loss is 0.6006420850753784\n",
      "epoch: 4 step: 1788, loss is 0.29045578837394714\n",
      "epoch: 4 step: 1789, loss is 0.521189272403717\n",
      "epoch: 4 step: 1790, loss is 0.5142290592193604\n",
      "epoch: 4 step: 1791, loss is 0.27515262365341187\n",
      "epoch: 4 step: 1792, loss is 0.5911340117454529\n",
      "epoch: 4 step: 1793, loss is 0.2732602059841156\n",
      "epoch: 4 step: 1794, loss is 0.7862229347229004\n",
      "epoch: 4 step: 1795, loss is 0.45976635813713074\n",
      "epoch: 4 step: 1796, loss is 0.17782068252563477\n",
      "epoch: 4 step: 1797, loss is 0.29916468262672424\n",
      "epoch: 4 step: 1798, loss is 0.4086025059223175\n",
      "epoch: 4 step: 1799, loss is 0.2242554873228073\n",
      "epoch: 4 step: 1800, loss is 0.3048992156982422\n",
      "epoch: 4 step: 1801, loss is 0.39902520179748535\n",
      "epoch: 4 step: 1802, loss is 0.2936190068721771\n",
      "epoch: 4 step: 1803, loss is 0.6525409817695618\n",
      "epoch: 4 step: 1804, loss is 0.48819515109062195\n",
      "epoch: 4 step: 1805, loss is 0.42967453598976135\n",
      "epoch: 4 step: 1806, loss is 0.2524692714214325\n",
      "epoch: 4 step: 1807, loss is 0.39071422815322876\n",
      "epoch: 4 step: 1808, loss is 0.3026801347732544\n",
      "epoch: 4 step: 1809, loss is 0.557188868522644\n",
      "epoch: 4 step: 1810, loss is 0.30519217252731323\n",
      "epoch: 4 step: 1811, loss is 0.2699260711669922\n",
      "epoch: 4 step: 1812, loss is 0.6134482622146606\n",
      "epoch: 4 step: 1813, loss is 0.375961035490036\n",
      "epoch: 4 step: 1814, loss is 0.7081526517868042\n",
      "epoch: 4 step: 1815, loss is 0.3278760313987732\n",
      "epoch: 4 step: 1816, loss is 0.4874780476093292\n",
      "epoch: 4 step: 1817, loss is 0.20676277577877045\n",
      "epoch: 4 step: 1818, loss is 0.3503091037273407\n",
      "epoch: 4 step: 1819, loss is 0.2664952576160431\n",
      "epoch: 4 step: 1820, loss is 0.34218499064445496\n",
      "epoch: 4 step: 1821, loss is 0.7322437763214111\n",
      "epoch: 4 step: 1822, loss is 0.4401171803474426\n",
      "epoch: 4 step: 1823, loss is 0.7033872604370117\n",
      "epoch: 4 step: 1824, loss is 0.3796851634979248\n",
      "epoch: 4 step: 1825, loss is 0.3431141674518585\n",
      "epoch: 4 step: 1826, loss is 0.8233458399772644\n",
      "epoch: 4 step: 1827, loss is 0.6215863823890686\n",
      "epoch: 4 step: 1828, loss is 0.3330087959766388\n",
      "epoch: 4 step: 1829, loss is 0.5303006172180176\n",
      "epoch: 4 step: 1830, loss is 0.8902764916419983\n",
      "epoch: 4 step: 1831, loss is 0.6340020298957825\n",
      "epoch: 4 step: 1832, loss is 0.497802734375\n",
      "epoch: 4 step: 1833, loss is 0.5798019170761108\n",
      "epoch: 4 step: 1834, loss is 0.461982399225235\n",
      "epoch: 4 step: 1835, loss is 0.639350414276123\n",
      "epoch: 4 step: 1836, loss is 0.8203592300415039\n",
      "epoch: 4 step: 1837, loss is 0.3912734389305115\n",
      "epoch: 4 step: 1838, loss is 0.5362824201583862\n",
      "epoch: 4 step: 1839, loss is 0.5070056915283203\n",
      "epoch: 4 step: 1840, loss is 0.5172364115715027\n",
      "epoch: 4 step: 1841, loss is 0.29133859276771545\n",
      "epoch: 4 step: 1842, loss is 0.7908459305763245\n",
      "epoch: 4 step: 1843, loss is 0.3162473440170288\n",
      "epoch: 4 step: 1844, loss is 0.4959985613822937\n",
      "epoch: 4 step: 1845, loss is 0.36876338720321655\n",
      "epoch: 4 step: 1846, loss is 0.3302770256996155\n",
      "epoch: 4 step: 1847, loss is 0.6424327492713928\n",
      "epoch: 4 step: 1848, loss is 0.5026698112487793\n",
      "epoch: 4 step: 1849, loss is 0.5371671319007874\n",
      "epoch: 4 step: 1850, loss is 0.7181659936904907\n",
      "epoch: 4 step: 1851, loss is 0.24717295169830322\n",
      "epoch: 4 step: 1852, loss is 0.5713780522346497\n",
      "epoch: 4 step: 1853, loss is 0.35180747509002686\n",
      "epoch: 4 step: 1854, loss is 0.26872995495796204\n",
      "epoch: 4 step: 1855, loss is 0.8016467690467834\n",
      "epoch: 4 step: 1856, loss is 0.6050061583518982\n",
      "epoch: 4 step: 1857, loss is 0.3154512941837311\n",
      "epoch: 4 step: 1858, loss is 0.3445874750614166\n",
      "epoch: 4 step: 1859, loss is 0.4869951605796814\n",
      "epoch: 4 step: 1860, loss is 0.5173420906066895\n",
      "epoch: 4 step: 1861, loss is 0.4671330749988556\n",
      "epoch: 4 step: 1862, loss is 0.3268621563911438\n",
      "epoch: 4 step: 1863, loss is 0.3789513409137726\n",
      "epoch: 4 step: 1864, loss is 0.3432314693927765\n",
      "epoch: 4 step: 1865, loss is 0.44821619987487793\n",
      "epoch: 4 step: 1866, loss is 0.44665464758872986\n",
      "epoch: 4 step: 1867, loss is 0.3361445665359497\n",
      "epoch: 4 step: 1868, loss is 0.19696727395057678\n",
      "epoch: 4 step: 1869, loss is 0.39092639088630676\n",
      "epoch: 4 step: 1870, loss is 0.6130115389823914\n",
      "epoch: 4 step: 1871, loss is 0.48883599042892456\n",
      "epoch: 4 step: 1872, loss is 0.5508636832237244\n",
      "epoch: 4 step: 1873, loss is 0.14497682452201843\n",
      "epoch: 4 step: 1874, loss is 0.3563471734523773\n",
      "epoch: 4 step: 1875, loss is 0.46867236495018005\n",
      "epoch: 4 step: 1876, loss is 0.3594714105129242\n",
      "epoch: 4 step: 1877, loss is 0.5496891140937805\n",
      "epoch: 4 step: 1878, loss is 0.8702126741409302\n",
      "epoch: 4 step: 1879, loss is 0.5266525745391846\n",
      "epoch: 4 step: 1880, loss is 0.3968212306499481\n",
      "epoch: 4 step: 1881, loss is 0.20703735947608948\n",
      "epoch: 4 step: 1882, loss is 0.6585320830345154\n",
      "epoch: 4 step: 1883, loss is 1.203643798828125\n",
      "epoch: 4 step: 1884, loss is 0.30529940128326416\n",
      "epoch: 4 step: 1885, loss is 0.3030146360397339\n",
      "epoch: 4 step: 1886, loss is 0.5131512880325317\n",
      "epoch: 4 step: 1887, loss is 0.4042816460132599\n",
      "epoch: 4 step: 1888, loss is 0.47329580783843994\n",
      "epoch: 4 step: 1889, loss is 0.4476832151412964\n",
      "epoch: 4 step: 1890, loss is 0.2230549454689026\n",
      "epoch: 4 step: 1891, loss is 0.4770083725452423\n",
      "epoch: 4 step: 1892, loss is 0.8537188768386841\n",
      "epoch: 4 step: 1893, loss is 0.5610775947570801\n",
      "epoch: 4 step: 1894, loss is 0.6284599304199219\n",
      "epoch: 4 step: 1895, loss is 0.4462805986404419\n",
      "epoch: 4 step: 1896, loss is 0.28803327679634094\n",
      "epoch: 4 step: 1897, loss is 0.5408692359924316\n",
      "epoch: 4 step: 1898, loss is 0.34345847368240356\n",
      "epoch: 4 step: 1899, loss is 0.4842345118522644\n",
      "epoch: 4 step: 1900, loss is 0.6081154346466064\n",
      "epoch: 4 step: 1901, loss is 0.5050992369651794\n",
      "epoch: 4 step: 1902, loss is 0.6803403496742249\n",
      "epoch: 4 step: 1903, loss is 0.41300636529922485\n",
      "epoch: 4 step: 1904, loss is 0.45859599113464355\n",
      "epoch: 4 step: 1905, loss is 0.3541387617588043\n",
      "epoch: 4 step: 1906, loss is 0.28831949830055237\n",
      "epoch: 4 step: 1907, loss is 0.7128773927688599\n",
      "epoch: 4 step: 1908, loss is 0.43016934394836426\n",
      "epoch: 4 step: 1909, loss is 0.5827828645706177\n",
      "epoch: 4 step: 1910, loss is 0.3447306156158447\n",
      "epoch: 4 step: 1911, loss is 0.6005406379699707\n",
      "epoch: 4 step: 1912, loss is 0.5336302518844604\n",
      "epoch: 4 step: 1913, loss is 0.5792566537857056\n",
      "epoch: 4 step: 1914, loss is 0.5806408524513245\n",
      "epoch: 4 step: 1915, loss is 0.37072062492370605\n",
      "epoch: 4 step: 1916, loss is 0.38814404606819153\n",
      "epoch: 4 step: 1917, loss is 0.30773553252220154\n",
      "epoch: 4 step: 1918, loss is 0.4451005756855011\n",
      "epoch: 4 step: 1919, loss is 0.5018247961997986\n",
      "epoch: 4 step: 1920, loss is 0.4683308005332947\n",
      "epoch: 4 step: 1921, loss is 0.4106777310371399\n",
      "epoch: 4 step: 1922, loss is 0.4144340753555298\n",
      "epoch: 4 step: 1923, loss is 0.1289079785346985\n",
      "epoch: 4 step: 1924, loss is 0.5640587210655212\n",
      "epoch: 4 step: 1925, loss is 0.25227516889572144\n",
      "epoch: 4 step: 1926, loss is 0.8201695084571838\n",
      "epoch: 4 step: 1927, loss is 0.4496666193008423\n",
      "epoch: 4 step: 1928, loss is 0.3893730938434601\n",
      "epoch: 4 step: 1929, loss is 0.26088830828666687\n",
      "epoch: 4 step: 1930, loss is 0.21867325901985168\n",
      "epoch: 4 step: 1931, loss is 0.3343879282474518\n",
      "epoch: 4 step: 1932, loss is 0.6558971405029297\n",
      "epoch: 4 step: 1933, loss is 0.4541822671890259\n",
      "epoch: 4 step: 1934, loss is 0.774978756904602\n",
      "epoch: 4 step: 1935, loss is 0.727065920829773\n",
      "epoch: 4 step: 1936, loss is 0.4868515133857727\n",
      "epoch: 4 step: 1937, loss is 0.6131765842437744\n",
      "epoch: 4 step: 1938, loss is 0.2076168954372406\n",
      "epoch: 4 step: 1939, loss is 0.5619772672653198\n",
      "epoch: 4 step: 1940, loss is 0.3939937949180603\n",
      "epoch: 4 step: 1941, loss is 0.21025437116622925\n",
      "epoch: 4 step: 1942, loss is 0.255813330411911\n",
      "epoch: 4 step: 1943, loss is 0.3545653820037842\n",
      "epoch: 4 step: 1944, loss is 0.3687942624092102\n",
      "epoch: 4 step: 1945, loss is 0.18284159898757935\n",
      "epoch: 4 step: 1946, loss is 0.3038082420825958\n",
      "epoch: 4 step: 1947, loss is 0.2263721078634262\n",
      "epoch: 4 step: 1948, loss is 0.3111375868320465\n",
      "epoch: 4 step: 1949, loss is 0.28514420986175537\n",
      "epoch: 4 step: 1950, loss is 0.26231756806373596\n",
      "epoch: 4 step: 1951, loss is 0.4231428802013397\n",
      "epoch: 4 step: 1952, loss is 0.20003408193588257\n",
      "epoch: 4 step: 1953, loss is 0.5631358027458191\n",
      "epoch: 4 step: 1954, loss is 0.5184082388877869\n",
      "epoch: 4 step: 1955, loss is 0.42411044239997864\n",
      "epoch: 4 step: 1956, loss is 0.3867439925670624\n",
      "epoch: 4 step: 1957, loss is 0.4691055715084076\n",
      "epoch: 4 step: 1958, loss is 0.6455360054969788\n",
      "epoch: 4 step: 1959, loss is 0.5991280674934387\n",
      "epoch: 4 step: 1960, loss is 0.2167988121509552\n",
      "epoch: 4 step: 1961, loss is 0.4942241609096527\n",
      "epoch: 4 step: 1962, loss is 0.8479165434837341\n",
      "epoch: 4 step: 1963, loss is 0.659063994884491\n",
      "epoch: 4 step: 1964, loss is 0.38202765583992004\n",
      "epoch: 4 step: 1965, loss is 0.5164071321487427\n",
      "epoch: 4 step: 1966, loss is 0.8662124276161194\n",
      "epoch: 4 step: 1967, loss is 1.3191099166870117\n",
      "epoch: 4 step: 1968, loss is 0.37254393100738525\n",
      "epoch: 4 step: 1969, loss is 0.45465362071990967\n",
      "epoch: 4 step: 1970, loss is 0.311845988035202\n",
      "epoch: 4 step: 1971, loss is 0.5051670074462891\n",
      "epoch: 4 step: 1972, loss is 0.4116069972515106\n",
      "epoch: 4 step: 1973, loss is 0.5083844065666199\n",
      "epoch: 4 step: 1974, loss is 0.7036932706832886\n",
      "epoch: 4 step: 1975, loss is 0.4239688217639923\n",
      "epoch: 4 step: 1976, loss is 0.6129433512687683\n",
      "epoch: 4 step: 1977, loss is 0.4000830054283142\n",
      "epoch: 4 step: 1978, loss is 0.4496162235736847\n",
      "epoch: 4 step: 1979, loss is 0.28784650564193726\n",
      "epoch: 4 step: 1980, loss is 0.394682914018631\n",
      "epoch: 4 step: 1981, loss is 0.3872428238391876\n",
      "epoch: 4 step: 1982, loss is 0.410569965839386\n",
      "epoch: 4 step: 1983, loss is 0.7997080683708191\n",
      "epoch: 4 step: 1984, loss is 0.45240581035614014\n",
      "epoch: 4 step: 1985, loss is 0.5141105055809021\n",
      "epoch: 4 step: 1986, loss is 0.5650802254676819\n",
      "epoch: 4 step: 1987, loss is 1.0642013549804688\n",
      "epoch: 4 step: 1988, loss is 0.3847523331642151\n",
      "epoch: 4 step: 1989, loss is 0.4337194263935089\n",
      "epoch: 4 step: 1990, loss is 0.6169129610061646\n",
      "epoch: 4 step: 1991, loss is 0.46700435876846313\n",
      "epoch: 4 step: 1992, loss is 0.4692692160606384\n",
      "epoch: 4 step: 1993, loss is 0.4107298254966736\n",
      "epoch: 4 step: 1994, loss is 0.38811925053596497\n",
      "epoch: 4 step: 1995, loss is 0.44982829689979553\n",
      "epoch: 4 step: 1996, loss is 0.6609819531440735\n",
      "epoch: 4 step: 1997, loss is 0.3357396125793457\n",
      "epoch: 4 step: 1998, loss is 0.7478146553039551\n",
      "epoch: 4 step: 1999, loss is 0.32525330781936646\n",
      "epoch: 4 step: 2000, loss is 0.6173998117446899\n",
      "epoch: 4 step: 2001, loss is 0.4676521122455597\n",
      "epoch: 4 step: 2002, loss is 0.5490466356277466\n",
      "epoch: 4 step: 2003, loss is 0.48940056562423706\n",
      "epoch: 4 step: 2004, loss is 0.43540307879447937\n",
      "epoch: 4 step: 2005, loss is 0.5489096641540527\n",
      "epoch: 4 step: 2006, loss is 0.14801658689975739\n",
      "epoch: 4 step: 2007, loss is 0.2520548701286316\n",
      "epoch: 4 step: 2008, loss is 0.4629621207714081\n",
      "epoch: 4 step: 2009, loss is 0.7625183463096619\n",
      "epoch: 4 step: 2010, loss is 0.47705793380737305\n",
      "epoch: 4 step: 2011, loss is 0.8787360787391663\n",
      "epoch: 4 step: 2012, loss is 0.7298370599746704\n",
      "epoch: 4 step: 2013, loss is 0.48056820034980774\n",
      "epoch: 4 step: 2014, loss is 0.6869227886199951\n",
      "epoch: 4 step: 2015, loss is 0.28344404697418213\n",
      "epoch: 4 step: 2016, loss is 0.4665066599845886\n",
      "epoch: 4 step: 2017, loss is 0.7049088478088379\n",
      "epoch: 4 step: 2018, loss is 0.414163738489151\n",
      "epoch: 4 step: 2019, loss is 0.4795708954334259\n",
      "epoch: 4 step: 2020, loss is 0.4004414677619934\n",
      "epoch: 4 step: 2021, loss is 0.31022214889526367\n",
      "epoch: 4 step: 2022, loss is 0.5236547589302063\n",
      "epoch: 4 step: 2023, loss is 0.3025665879249573\n",
      "epoch: 4 step: 2024, loss is 0.3004169464111328\n",
      "epoch: 4 step: 2025, loss is 0.1298973262310028\n",
      "epoch: 4 step: 2026, loss is 0.4327302575111389\n",
      "epoch: 4 step: 2027, loss is 0.4383876621723175\n",
      "epoch: 4 step: 2028, loss is 0.28039273619651794\n",
      "epoch: 4 step: 2029, loss is 0.3450246751308441\n",
      "epoch: 4 step: 2030, loss is 0.6470844149589539\n",
      "epoch: 4 step: 2031, loss is 0.3003450632095337\n",
      "epoch: 4 step: 2032, loss is 0.2699713408946991\n",
      "epoch: 4 step: 2033, loss is 0.762798011302948\n",
      "epoch: 4 step: 2034, loss is 0.8228375911712646\n",
      "epoch: 4 step: 2035, loss is 0.2945482134819031\n",
      "epoch: 4 step: 2036, loss is 0.6790012121200562\n",
      "epoch: 4 step: 2037, loss is 0.3668278455734253\n",
      "epoch: 4 step: 2038, loss is 0.6914287209510803\n",
      "epoch: 4 step: 2039, loss is 0.626177966594696\n",
      "epoch: 4 step: 2040, loss is 0.1859344094991684\n",
      "epoch: 4 step: 2041, loss is 0.365360289812088\n",
      "epoch: 4 step: 2042, loss is 0.26144975423812866\n",
      "epoch: 4 step: 2043, loss is 0.551238477230072\n",
      "epoch: 4 step: 2044, loss is 0.2749169170856476\n",
      "epoch: 4 step: 2045, loss is 0.233739972114563\n",
      "epoch: 4 step: 2046, loss is 0.5724262595176697\n",
      "epoch: 4 step: 2047, loss is 0.30906012654304504\n",
      "epoch: 4 step: 2048, loss is 0.6325027942657471\n",
      "epoch: 4 step: 2049, loss is 0.5101240277290344\n",
      "epoch: 4 step: 2050, loss is 0.8133430480957031\n",
      "epoch: 4 step: 2051, loss is 0.6323021650314331\n",
      "epoch: 4 step: 2052, loss is 0.07676515728235245\n",
      "epoch: 4 step: 2053, loss is 1.1402838230133057\n",
      "epoch: 4 step: 2054, loss is 0.5690940618515015\n",
      "epoch: 4 step: 2055, loss is 0.23158320784568787\n",
      "epoch: 4 step: 2056, loss is 0.3768712878227234\n",
      "epoch: 4 step: 2057, loss is 0.4360666275024414\n",
      "epoch: 4 step: 2058, loss is 0.49348312616348267\n",
      "epoch: 4 step: 2059, loss is 0.220469132065773\n",
      "epoch: 4 step: 2060, loss is 0.5473375916481018\n",
      "epoch: 4 step: 2061, loss is 0.1490001529455185\n",
      "epoch: 4 step: 2062, loss is 0.6836337447166443\n",
      "epoch: 4 step: 2063, loss is 0.4844842851161957\n",
      "epoch: 4 step: 2064, loss is 0.5304179787635803\n",
      "epoch: 4 step: 2065, loss is 0.6173633337020874\n",
      "epoch: 4 step: 2066, loss is 0.9456170201301575\n",
      "epoch: 4 step: 2067, loss is 0.4517388939857483\n",
      "epoch: 4 step: 2068, loss is 0.46633392572402954\n",
      "epoch: 4 step: 2069, loss is 0.15976737439632416\n",
      "epoch: 4 step: 2070, loss is 0.233900785446167\n",
      "epoch: 4 step: 2071, loss is 0.4998442232608795\n",
      "epoch: 4 step: 2072, loss is 0.15021923184394836\n",
      "epoch: 4 step: 2073, loss is 0.368878036737442\n",
      "epoch: 4 step: 2074, loss is 0.2756419777870178\n",
      "epoch: 4 step: 2075, loss is 0.4307191073894501\n",
      "epoch: 4 step: 2076, loss is 0.2889772057533264\n",
      "epoch: 4 step: 2077, loss is 0.562253475189209\n",
      "epoch: 4 step: 2078, loss is 0.4881872534751892\n",
      "epoch: 4 step: 2079, loss is 0.44263026118278503\n",
      "epoch: 4 step: 2080, loss is 0.35347700119018555\n",
      "epoch: 4 step: 2081, loss is 0.4362489879131317\n",
      "epoch: 4 step: 2082, loss is 0.5179541707038879\n",
      "epoch: 4 step: 2083, loss is 0.3522992730140686\n",
      "epoch: 4 step: 2084, loss is 0.4807747006416321\n",
      "epoch: 4 step: 2085, loss is 0.7423784732818604\n",
      "epoch: 4 step: 2086, loss is 0.5367261171340942\n",
      "epoch: 4 step: 2087, loss is 0.21955570578575134\n",
      "epoch: 4 step: 2088, loss is 0.5608615279197693\n",
      "epoch: 4 step: 2089, loss is 0.2795011103153229\n",
      "epoch: 4 step: 2090, loss is 0.5306661128997803\n",
      "epoch: 4 step: 2091, loss is 0.4858993589878082\n",
      "epoch: 4 step: 2092, loss is 0.23331385850906372\n",
      "epoch: 4 step: 2093, loss is 0.26320478320121765\n",
      "epoch: 4 step: 2094, loss is 0.2415001541376114\n",
      "epoch: 4 step: 2095, loss is 0.48586305975914\n",
      "epoch: 4 step: 2096, loss is 0.4383394718170166\n",
      "epoch: 4 step: 2097, loss is 0.5586692094802856\n",
      "epoch: 4 step: 2098, loss is 0.7567832469940186\n",
      "epoch: 4 step: 2099, loss is 0.45661404728889465\n",
      "epoch: 4 step: 2100, loss is 0.2867538034915924\n",
      "epoch: 4 step: 2101, loss is 0.3515242338180542\n",
      "epoch: 4 step: 2102, loss is 0.514625608921051\n",
      "epoch: 4 step: 2103, loss is 0.55841064453125\n",
      "epoch: 4 step: 2104, loss is 0.48781245946884155\n",
      "epoch: 4 step: 2105, loss is 0.30756232142448425\n",
      "epoch: 4 step: 2106, loss is 0.32341456413269043\n",
      "epoch: 4 step: 2107, loss is 0.31824663281440735\n",
      "epoch: 4 step: 2108, loss is 0.425284743309021\n",
      "epoch: 4 step: 2109, loss is 0.4274868071079254\n",
      "epoch: 4 step: 2110, loss is 0.878554105758667\n",
      "epoch: 4 step: 2111, loss is 0.8818277716636658\n",
      "epoch: 4 step: 2112, loss is 0.555343508720398\n",
      "epoch: 4 step: 2113, loss is 0.4995002746582031\n",
      "epoch: 4 step: 2114, loss is 0.3131539821624756\n",
      "epoch: 4 step: 2115, loss is 0.40188148617744446\n",
      "epoch: 4 step: 2116, loss is 0.5372431874275208\n",
      "epoch: 4 step: 2117, loss is 0.47311854362487793\n",
      "epoch: 4 step: 2118, loss is 0.4298962354660034\n",
      "epoch: 4 step: 2119, loss is 0.9054993987083435\n",
      "epoch: 4 step: 2120, loss is 0.5369123816490173\n",
      "epoch: 4 step: 2121, loss is 0.2504533529281616\n",
      "epoch: 4 step: 2122, loss is 0.27973806858062744\n",
      "epoch: 4 step: 2123, loss is 0.8273041844367981\n",
      "epoch: 4 step: 2124, loss is 0.4964195489883423\n",
      "epoch: 4 step: 2125, loss is 0.3720044493675232\n",
      "epoch: 4 step: 2126, loss is 0.4070613980293274\n",
      "epoch: 4 step: 2127, loss is 0.5152702331542969\n",
      "epoch: 4 step: 2128, loss is 0.588179886341095\n",
      "epoch: 4 step: 2129, loss is 0.4999796450138092\n",
      "epoch: 4 step: 2130, loss is 0.6099059581756592\n",
      "epoch: 4 step: 2131, loss is 0.6629018783569336\n",
      "epoch: 4 step: 2132, loss is 0.3369380831718445\n",
      "epoch: 4 step: 2133, loss is 0.2486373633146286\n",
      "epoch: 4 step: 2134, loss is 0.3904381990432739\n",
      "epoch: 4 step: 2135, loss is 0.5883789658546448\n",
      "epoch: 4 step: 2136, loss is 0.23675045371055603\n",
      "epoch: 4 step: 2137, loss is 0.5105802416801453\n",
      "epoch: 4 step: 2138, loss is 0.37453633546829224\n",
      "epoch: 4 step: 2139, loss is 0.37931230664253235\n",
      "epoch: 4 step: 2140, loss is 0.311087965965271\n",
      "epoch: 4 step: 2141, loss is 0.21902480721473694\n",
      "epoch: 4 step: 2142, loss is 0.6880700588226318\n",
      "epoch: 4 step: 2143, loss is 0.943034827709198\n",
      "epoch: 4 step: 2144, loss is 0.2927092909812927\n",
      "epoch: 4 step: 2145, loss is 1.0274946689605713\n",
      "epoch: 4 step: 2146, loss is 0.5537739396095276\n",
      "epoch: 4 step: 2147, loss is 0.43358075618743896\n",
      "epoch: 4 step: 2148, loss is 0.3651712238788605\n",
      "epoch: 4 step: 2149, loss is 0.29699042439460754\n",
      "epoch: 4 step: 2150, loss is 0.6296653151512146\n",
      "epoch: 4 step: 2151, loss is 0.3126812279224396\n",
      "epoch: 4 step: 2152, loss is 0.06363646686077118\n",
      "epoch: 4 step: 2153, loss is 0.3064906895160675\n",
      "epoch: 4 step: 2154, loss is 0.4037955105304718\n",
      "epoch: 4 step: 2155, loss is 1.2128806114196777\n",
      "epoch: 4 step: 2156, loss is 0.5608422756195068\n",
      "epoch: 4 step: 2157, loss is 0.5592086911201477\n",
      "epoch: 4 step: 2158, loss is 0.2996605336666107\n",
      "epoch: 4 step: 2159, loss is 0.6293309330940247\n",
      "epoch: 4 step: 2160, loss is 0.4192727208137512\n",
      "epoch: 4 step: 2161, loss is 0.5110175609588623\n",
      "epoch: 4 step: 2162, loss is 0.39754176139831543\n",
      "epoch: 4 step: 2163, loss is 0.27799704670906067\n",
      "epoch: 4 step: 2164, loss is 0.5875970125198364\n",
      "epoch: 4 step: 2165, loss is 0.35934722423553467\n",
      "epoch: 4 step: 2166, loss is 0.43902820348739624\n",
      "epoch: 4 step: 2167, loss is 0.3786066174507141\n",
      "epoch: 4 step: 2168, loss is 0.46170327067375183\n",
      "epoch: 4 step: 2169, loss is 0.3823557496070862\n",
      "epoch: 4 step: 2170, loss is 0.4093373715877533\n",
      "epoch: 4 step: 2171, loss is 0.6872867941856384\n",
      "epoch: 4 step: 2172, loss is 0.49359118938446045\n",
      "epoch: 4 step: 2173, loss is 0.7801775336265564\n",
      "epoch: 4 step: 2174, loss is 0.7849637269973755\n",
      "epoch: 4 step: 2175, loss is 0.5236079096794128\n",
      "epoch: 4 step: 2176, loss is 0.310465544462204\n",
      "epoch: 4 step: 2177, loss is 0.3590807318687439\n",
      "epoch: 4 step: 2178, loss is 0.4841807186603546\n",
      "epoch: 4 step: 2179, loss is 0.2019060254096985\n",
      "epoch: 4 step: 2180, loss is 0.2748590111732483\n",
      "epoch: 4 step: 2181, loss is 0.5076640248298645\n",
      "epoch: 4 step: 2182, loss is 0.5959869623184204\n",
      "epoch: 4 step: 2183, loss is 0.5188387632369995\n",
      "epoch: 4 step: 2184, loss is 0.8830963373184204\n",
      "epoch: 4 step: 2185, loss is 0.5046271085739136\n",
      "epoch: 4 step: 2186, loss is 0.723539412021637\n",
      "epoch: 4 step: 2187, loss is 0.5658155679702759\n",
      "epoch: 4 step: 2188, loss is 0.48608142137527466\n",
      "epoch: 4 step: 2189, loss is 0.332040935754776\n",
      "epoch: 4 step: 2190, loss is 0.18903715908527374\n",
      "epoch: 4 step: 2191, loss is 0.34068936109542847\n",
      "epoch: 4 step: 2192, loss is 0.5338246822357178\n",
      "epoch: 4 step: 2193, loss is 0.2804452180862427\n",
      "epoch: 4 step: 2194, loss is 0.5234568119049072\n",
      "epoch: 4 step: 2195, loss is 0.6775104403495789\n",
      "epoch: 4 step: 2196, loss is 0.6619402766227722\n",
      "epoch: 4 step: 2197, loss is 0.7695488929748535\n",
      "epoch: 4 step: 2198, loss is 0.6322323083877563\n",
      "epoch: 4 step: 2199, loss is 0.2952278256416321\n",
      "epoch: 4 step: 2200, loss is 0.2627212405204773\n",
      "epoch: 4 step: 2201, loss is 0.21665988862514496\n",
      "epoch: 4 step: 2202, loss is 0.5440588593482971\n",
      "epoch: 4 step: 2203, loss is 0.37434133887290955\n",
      "epoch: 4 step: 2204, loss is 0.5880491733551025\n",
      "epoch: 4 step: 2205, loss is 0.4024113118648529\n",
      "epoch: 4 step: 2206, loss is 0.23585918545722961\n",
      "epoch: 4 step: 2207, loss is 0.645512580871582\n",
      "epoch: 4 step: 2208, loss is 0.4951692521572113\n",
      "epoch: 4 step: 2209, loss is 0.8606167435646057\n",
      "epoch: 4 step: 2210, loss is 0.3837302327156067\n",
      "epoch: 4 step: 2211, loss is 0.33757391571998596\n",
      "epoch: 4 step: 2212, loss is 0.5066095590591431\n",
      "epoch: 4 step: 2213, loss is 0.5216269493103027\n",
      "epoch: 4 step: 2214, loss is 1.0506064891815186\n",
      "epoch: 4 step: 2215, loss is 0.7929674386978149\n",
      "epoch: 4 step: 2216, loss is 0.3584475815296173\n",
      "epoch: 4 step: 2217, loss is 0.41807714104652405\n",
      "epoch: 4 step: 2218, loss is 0.24302808940410614\n",
      "epoch: 4 step: 2219, loss is 0.21979548037052155\n",
      "epoch: 4 step: 2220, loss is 0.4310589134693146\n",
      "epoch: 4 step: 2221, loss is 0.45914092659950256\n",
      "epoch: 4 step: 2222, loss is 1.3229389190673828\n",
      "epoch: 4 step: 2223, loss is 0.25462809205055237\n",
      "epoch: 4 step: 2224, loss is 0.24364060163497925\n",
      "epoch: 4 step: 2225, loss is 0.3009974956512451\n",
      "epoch: 4 step: 2226, loss is 0.25499144196510315\n",
      "epoch: 4 step: 2227, loss is 0.5498383045196533\n",
      "epoch: 4 step: 2228, loss is 0.3766699433326721\n",
      "epoch: 4 step: 2229, loss is 0.3186386525630951\n",
      "epoch: 4 step: 2230, loss is 0.2129233032464981\n",
      "epoch: 4 step: 2231, loss is 0.35193344950675964\n",
      "epoch: 4 step: 2232, loss is 0.6802766919136047\n",
      "epoch: 4 step: 2233, loss is 0.42293795943260193\n",
      "epoch: 4 step: 2234, loss is 0.3716408610343933\n",
      "epoch: 4 step: 2235, loss is 0.4562114477157593\n",
      "epoch: 4 step: 2236, loss is 0.35134434700012207\n",
      "epoch: 4 step: 2237, loss is 0.2514529526233673\n",
      "epoch: 4 step: 2238, loss is 0.3273579776287079\n",
      "epoch: 4 step: 2239, loss is 0.864774227142334\n",
      "epoch: 4 step: 2240, loss is 0.3240032494068146\n",
      "epoch: 4 step: 2241, loss is 0.6045076847076416\n",
      "epoch: 4 step: 2242, loss is 0.43370991945266724\n",
      "epoch: 4 step: 2243, loss is 0.5688035488128662\n",
      "epoch: 4 step: 2244, loss is 0.5245179533958435\n",
      "epoch: 4 step: 2245, loss is 0.4786246716976166\n",
      "epoch: 4 step: 2246, loss is 0.13338685035705566\n",
      "epoch: 4 step: 2247, loss is 0.6946043968200684\n",
      "epoch: 4 step: 2248, loss is 0.477914422750473\n",
      "epoch: 4 step: 2249, loss is 0.7725315093994141\n",
      "epoch: 4 step: 2250, loss is 0.992133617401123\n",
      "epoch: 4 step: 2251, loss is 0.07329615205526352\n",
      "epoch: 4 step: 2252, loss is 0.2967171370983124\n",
      "epoch: 4 step: 2253, loss is 0.19061261415481567\n",
      "epoch: 4 step: 2254, loss is 0.6619458198547363\n",
      "epoch: 4 step: 2255, loss is 0.8561569452285767\n",
      "epoch: 4 step: 2256, loss is 0.2295762449502945\n",
      "epoch: 4 step: 2257, loss is 0.4598813056945801\n",
      "epoch: 4 step: 2258, loss is 0.6987171173095703\n",
      "epoch: 4 step: 2259, loss is 0.4460127651691437\n",
      "epoch: 4 step: 2260, loss is 0.554234504699707\n",
      "epoch: 4 step: 2261, loss is 0.3980531096458435\n",
      "epoch: 4 step: 2262, loss is 0.578139066696167\n",
      "epoch: 4 step: 2263, loss is 0.40148890018463135\n",
      "epoch: 4 step: 2264, loss is 0.3383476436138153\n",
      "epoch: 4 step: 2265, loss is 0.3895868957042694\n",
      "epoch: 4 step: 2266, loss is 0.2839781641960144\n",
      "epoch: 4 step: 2267, loss is 0.5236775279045105\n",
      "epoch: 4 step: 2268, loss is 0.4405694901943207\n",
      "epoch: 4 step: 2269, loss is 0.34134629368782043\n",
      "epoch: 4 step: 2270, loss is 0.4830969572067261\n",
      "epoch: 4 step: 2271, loss is 0.7009738683700562\n",
      "epoch: 4 step: 2272, loss is 0.317443311214447\n",
      "epoch: 4 step: 2273, loss is 0.5972670912742615\n",
      "epoch: 4 step: 2274, loss is 0.5648029446601868\n",
      "epoch: 4 step: 2275, loss is 0.584651529788971\n",
      "epoch: 4 step: 2276, loss is 0.32793107628822327\n",
      "epoch: 4 step: 2277, loss is 0.5066672563552856\n",
      "epoch: 4 step: 2278, loss is 0.4525716006755829\n",
      "epoch: 4 step: 2279, loss is 0.7154525518417358\n",
      "epoch: 4 step: 2280, loss is 0.3737563490867615\n",
      "epoch: 4 step: 2281, loss is 0.4218757748603821\n",
      "epoch: 4 step: 2282, loss is 0.38494497537612915\n",
      "epoch: 4 step: 2283, loss is 0.17462535202503204\n",
      "epoch: 4 step: 2284, loss is 0.6569347381591797\n",
      "epoch: 4 step: 2285, loss is 0.42957043647766113\n",
      "epoch: 4 step: 2286, loss is 0.5685374140739441\n",
      "epoch: 4 step: 2287, loss is 0.27324339747428894\n",
      "epoch: 4 step: 2288, loss is 0.3607487678527832\n",
      "epoch: 4 step: 2289, loss is 0.5095460414886475\n",
      "epoch: 4 step: 2290, loss is 0.9504233598709106\n",
      "epoch: 4 step: 2291, loss is 0.6539254784584045\n",
      "epoch: 4 step: 2292, loss is 0.35788020491600037\n",
      "epoch: 4 step: 2293, loss is 0.19403783977031708\n",
      "epoch: 4 step: 2294, loss is 0.40598395466804504\n",
      "epoch: 4 step: 2295, loss is 0.4800260365009308\n",
      "epoch: 4 step: 2296, loss is 0.9111484289169312\n",
      "epoch: 4 step: 2297, loss is 0.4689618647098541\n",
      "epoch: 4 step: 2298, loss is 0.3293949365615845\n",
      "epoch: 4 step: 2299, loss is 0.7465953826904297\n",
      "epoch: 4 step: 2300, loss is 0.6005586385726929\n",
      "epoch: 4 step: 2301, loss is 0.4171578586101532\n",
      "epoch: 4 step: 2302, loss is 0.7061833739280701\n",
      "epoch: 4 step: 2303, loss is 0.5202440023422241\n",
      "epoch: 4 step: 2304, loss is 0.38207629323005676\n",
      "epoch: 4 step: 2305, loss is 0.2978835105895996\n",
      "epoch: 4 step: 2306, loss is 0.27538514137268066\n",
      "epoch: 4 step: 2307, loss is 0.4581107497215271\n",
      "epoch: 4 step: 2308, loss is 0.5876433849334717\n",
      "epoch: 4 step: 2309, loss is 0.22651435434818268\n",
      "epoch: 4 step: 2310, loss is 0.6850768327713013\n",
      "epoch: 4 step: 2311, loss is 0.41370508074760437\n",
      "epoch: 4 step: 2312, loss is 0.3960441052913666\n",
      "epoch: 4 step: 2313, loss is 0.4961143136024475\n",
      "epoch: 4 step: 2314, loss is 0.5183359980583191\n",
      "epoch: 4 step: 2315, loss is 0.3966265320777893\n",
      "epoch: 4 step: 2316, loss is 0.5449079871177673\n",
      "epoch: 4 step: 2317, loss is 0.2384980022907257\n",
      "epoch: 4 step: 2318, loss is 0.47166186571121216\n",
      "epoch: 4 step: 2319, loss is 0.28683558106422424\n",
      "epoch: 4 step: 2320, loss is 0.43221214413642883\n",
      "epoch: 4 step: 2321, loss is 0.7990993857383728\n",
      "epoch: 4 step: 2322, loss is 0.5671882629394531\n",
      "epoch: 4 step: 2323, loss is 0.9127245545387268\n",
      "epoch: 4 step: 2324, loss is 0.2529085576534271\n",
      "epoch: 4 step: 2325, loss is 0.419808566570282\n",
      "epoch: 4 step: 2326, loss is 0.3472693860530853\n",
      "epoch: 4 step: 2327, loss is 0.5689404606819153\n",
      "epoch: 4 step: 2328, loss is 0.4593975841999054\n",
      "epoch: 4 step: 2329, loss is 0.31999117136001587\n",
      "epoch: 4 step: 2330, loss is 0.3029952943325043\n",
      "epoch: 4 step: 2331, loss is 0.5402023792266846\n",
      "epoch: 4 step: 2332, loss is 0.31705453991889954\n",
      "epoch: 4 step: 2333, loss is 0.6863088607788086\n",
      "epoch: 4 step: 2334, loss is 0.8070275187492371\n",
      "epoch: 4 step: 2335, loss is 0.4954983592033386\n",
      "epoch: 4 step: 2336, loss is 0.17812998592853546\n",
      "epoch: 4 step: 2337, loss is 0.3827102482318878\n",
      "epoch: 4 step: 2338, loss is 0.43665045499801636\n",
      "epoch: 4 step: 2339, loss is 0.5716795921325684\n",
      "epoch: 4 step: 2340, loss is 0.6037469506263733\n",
      "epoch: 4 step: 2341, loss is 0.6465755701065063\n",
      "epoch: 4 step: 2342, loss is 0.618911623954773\n",
      "epoch: 4 step: 2343, loss is 0.48337844014167786\n",
      "epoch: 4 step: 2344, loss is 0.2791854441165924\n",
      "epoch: 4 step: 2345, loss is 0.41788867115974426\n",
      "epoch: 4 step: 2346, loss is 0.14437025785446167\n",
      "epoch: 4 step: 2347, loss is 0.3296132981777191\n",
      "epoch: 4 step: 2348, loss is 1.1019232273101807\n",
      "epoch: 4 step: 2349, loss is 0.28073570132255554\n",
      "epoch: 4 step: 2350, loss is 0.49396395683288574\n",
      "epoch: 4 step: 2351, loss is 0.5005145072937012\n",
      "epoch: 4 step: 2352, loss is 0.23067991435527802\n",
      "epoch: 4 step: 2353, loss is 0.8039573431015015\n",
      "epoch: 4 step: 2354, loss is 0.49080416560173035\n",
      "epoch: 4 step: 2355, loss is 0.6085780262947083\n",
      "epoch: 4 step: 2356, loss is 0.4794129729270935\n",
      "epoch: 4 step: 2357, loss is 0.4523729085922241\n",
      "epoch: 4 step: 2358, loss is 0.2871045470237732\n",
      "epoch: 4 step: 2359, loss is 0.5475343465805054\n",
      "epoch: 4 step: 2360, loss is 0.3087620735168457\n",
      "epoch: 4 step: 2361, loss is 0.6419945955276489\n",
      "epoch: 4 step: 2362, loss is 0.3284493088722229\n",
      "epoch: 4 step: 2363, loss is 0.23086529970169067\n",
      "epoch: 4 step: 2364, loss is 0.2086964100599289\n",
      "epoch: 4 step: 2365, loss is 0.22689852118492126\n",
      "epoch: 4 step: 2366, loss is 0.5864894986152649\n",
      "epoch: 4 step: 2367, loss is 0.40069037675857544\n",
      "epoch: 4 step: 2368, loss is 0.6807704567909241\n",
      "epoch: 4 step: 2369, loss is 0.42562422156333923\n",
      "epoch: 4 step: 2370, loss is 0.497667521238327\n",
      "epoch: 4 step: 2371, loss is 0.5021218657493591\n",
      "epoch: 4 step: 2372, loss is 0.6500617861747742\n",
      "epoch: 4 step: 2373, loss is 0.4307670295238495\n",
      "epoch: 4 step: 2374, loss is 0.5094265341758728\n",
      "epoch: 4 step: 2375, loss is 0.4241390824317932\n",
      "epoch: 4 step: 2376, loss is 0.5550218820571899\n",
      "epoch: 4 step: 2377, loss is 0.4656575620174408\n",
      "epoch: 4 step: 2378, loss is 0.1338648647069931\n",
      "epoch: 4 step: 2379, loss is 0.456572562456131\n",
      "epoch: 4 step: 2380, loss is 0.9135206937789917\n",
      "epoch: 4 step: 2381, loss is 0.6412221789360046\n",
      "epoch: 4 step: 2382, loss is 0.929330050945282\n",
      "epoch: 4 step: 2383, loss is 0.14432691037654877\n",
      "epoch: 4 step: 2384, loss is 0.28919658064842224\n",
      "epoch: 4 step: 2385, loss is 0.6371178030967712\n",
      "epoch: 4 step: 2386, loss is 0.3830724358558655\n",
      "epoch: 4 step: 2387, loss is 0.4369792938232422\n",
      "epoch: 4 step: 2388, loss is 0.6217882633209229\n",
      "epoch: 4 step: 2389, loss is 0.38326773047447205\n",
      "epoch: 4 step: 2390, loss is 0.7640184760093689\n",
      "epoch: 4 step: 2391, loss is 0.5057826638221741\n",
      "epoch: 4 step: 2392, loss is 0.8245770335197449\n",
      "epoch: 4 step: 2393, loss is 0.4902401268482208\n",
      "epoch: 4 step: 2394, loss is 0.5286297798156738\n",
      "epoch: 4 step: 2395, loss is 0.3534493148326874\n",
      "epoch: 4 step: 2396, loss is 0.2994496822357178\n",
      "epoch: 4 step: 2397, loss is 0.6961230635643005\n",
      "epoch: 4 step: 2398, loss is 0.4276171326637268\n",
      "epoch: 4 step: 2399, loss is 0.2171800434589386\n",
      "epoch: 4 step: 2400, loss is 0.3372837007045746\n",
      "epoch: 4 step: 2401, loss is 0.5961871147155762\n",
      "epoch: 4 step: 2402, loss is 0.18302389979362488\n",
      "epoch: 4 step: 2403, loss is 0.34358569979667664\n",
      "epoch: 4 step: 2404, loss is 0.3461388051509857\n",
      "epoch: 4 step: 2405, loss is 0.6153569221496582\n",
      "epoch: 4 step: 2406, loss is 0.2876782715320587\n",
      "epoch: 4 step: 2407, loss is 0.6996626853942871\n",
      "epoch: 4 step: 2408, loss is 0.2828485071659088\n",
      "epoch: 4 step: 2409, loss is 0.7250350713729858\n",
      "epoch: 4 step: 2410, loss is 0.20632804930210114\n",
      "epoch: 4 step: 2411, loss is 0.41726478934288025\n",
      "epoch: 4 step: 2412, loss is 0.41752269864082336\n",
      "epoch: 4 step: 2413, loss is 0.6214507818222046\n",
      "epoch: 4 step: 2414, loss is 0.7226753830909729\n",
      "epoch: 4 step: 2415, loss is 0.4854787290096283\n",
      "epoch: 4 step: 2416, loss is 0.4691120684146881\n",
      "epoch: 4 step: 2417, loss is 0.49016493558883667\n",
      "epoch: 4 step: 2418, loss is 0.39448249340057373\n",
      "epoch: 4 step: 2419, loss is 0.5064153671264648\n",
      "epoch: 4 step: 2420, loss is 0.3023736774921417\n",
      "epoch: 4 step: 2421, loss is 0.5210661888122559\n",
      "epoch: 4 step: 2422, loss is 0.5516679286956787\n",
      "epoch: 4 step: 2423, loss is 0.5618704557418823\n",
      "epoch: 4 step: 2424, loss is 0.5877867341041565\n",
      "epoch: 4 step: 2425, loss is 0.6249890327453613\n",
      "epoch: 4 step: 2426, loss is 0.7204391360282898\n",
      "epoch: 4 step: 2427, loss is 0.35957545042037964\n",
      "epoch: 4 step: 2428, loss is 0.2584974467754364\n",
      "epoch: 4 step: 2429, loss is 0.3982286751270294\n",
      "epoch: 4 step: 2430, loss is 0.6131435632705688\n",
      "epoch: 4 step: 2431, loss is 0.5206965208053589\n",
      "epoch: 4 step: 2432, loss is 0.3615480661392212\n",
      "epoch: 4 step: 2433, loss is 0.34206125140190125\n",
      "epoch: 4 step: 2434, loss is 0.19681309163570404\n",
      "epoch: 4 step: 2435, loss is 0.6708440184593201\n",
      "epoch: 4 step: 2436, loss is 0.6635360717773438\n",
      "epoch: 4 step: 2437, loss is 0.44366636872291565\n",
      "epoch: 4 step: 2438, loss is 0.3537745475769043\n",
      "epoch: 4 step: 2439, loss is 0.615666925907135\n",
      "epoch: 4 step: 2440, loss is 0.5749115943908691\n",
      "epoch: 4 step: 2441, loss is 0.31538137793540955\n",
      "epoch: 4 step: 2442, loss is 0.36269327998161316\n",
      "epoch: 4 step: 2443, loss is 0.19641445577144623\n",
      "epoch: 4 step: 2444, loss is 0.4725705683231354\n",
      "epoch: 4 step: 2445, loss is 0.6404902338981628\n",
      "epoch: 4 step: 2446, loss is 0.6856287121772766\n",
      "epoch: 4 step: 2447, loss is 0.3106592297554016\n",
      "epoch: 4 step: 2448, loss is 0.5199284553527832\n",
      "epoch: 4 step: 2449, loss is 0.4899088740348816\n",
      "epoch: 4 step: 2450, loss is 0.3012823462486267\n",
      "epoch: 4 step: 2451, loss is 0.6727065443992615\n",
      "epoch: 4 step: 2452, loss is 0.6087212562561035\n",
      "epoch: 4 step: 2453, loss is 0.33179715275764465\n",
      "epoch: 4 step: 2454, loss is 0.3650628328323364\n",
      "epoch: 4 step: 2455, loss is 0.6009576916694641\n",
      "epoch: 4 step: 2456, loss is 0.3311663568019867\n",
      "epoch: 4 step: 2457, loss is 0.43425703048706055\n",
      "epoch: 4 step: 2458, loss is 0.7208423018455505\n",
      "epoch: 4 step: 2459, loss is 0.4701753258705139\n",
      "epoch: 4 step: 2460, loss is 0.24979938566684723\n",
      "epoch: 4 step: 2461, loss is 0.32395467162132263\n",
      "epoch: 4 step: 2462, loss is 0.24400518834590912\n",
      "epoch: 4 step: 2463, loss is 0.33507364988327026\n",
      "epoch: 4 step: 2464, loss is 0.8130542635917664\n",
      "epoch: 4 step: 2465, loss is 0.2169342041015625\n",
      "epoch: 4 step: 2466, loss is 1.0988469123840332\n",
      "epoch: 4 step: 2467, loss is 0.6787211298942566\n",
      "epoch: 4 step: 2468, loss is 0.19098173081874847\n",
      "epoch: 4 step: 2469, loss is 0.7325024604797363\n",
      "epoch: 4 step: 2470, loss is 0.237983837723732\n",
      "epoch: 4 step: 2471, loss is 0.14982415735721588\n",
      "epoch: 4 step: 2472, loss is 0.3670786917209625\n",
      "epoch: 4 step: 2473, loss is 0.4909602403640747\n",
      "epoch: 4 step: 2474, loss is 0.6818087100982666\n",
      "epoch: 4 step: 2475, loss is 0.21375365555286407\n",
      "epoch: 4 step: 2476, loss is 0.35075733065605164\n",
      "epoch: 4 step: 2477, loss is 0.5425058603286743\n",
      "epoch: 4 step: 2478, loss is 0.8203021883964539\n",
      "epoch: 4 step: 2479, loss is 0.48021847009658813\n",
      "epoch: 4 step: 2480, loss is 0.3860011696815491\n",
      "epoch: 4 step: 2481, loss is 0.45064544677734375\n",
      "epoch: 4 step: 2482, loss is 1.0824029445648193\n",
      "epoch: 4 step: 2483, loss is 0.3415846526622772\n",
      "epoch: 4 step: 2484, loss is 0.6750781536102295\n",
      "epoch: 4 step: 2485, loss is 0.1852935403585434\n",
      "epoch: 4 step: 2486, loss is 0.2946385443210602\n",
      "epoch: 4 step: 2487, loss is 0.3067959249019623\n",
      "epoch: 4 step: 2488, loss is 0.4315279424190521\n",
      "epoch: 4 step: 2489, loss is 0.28456631302833557\n",
      "epoch: 4 step: 2490, loss is 0.5889666080474854\n",
      "epoch: 4 step: 2491, loss is 0.6123526096343994\n",
      "epoch: 4 step: 2492, loss is 0.34453433752059937\n",
      "epoch: 4 step: 2493, loss is 0.5568943619728088\n",
      "epoch: 4 step: 2494, loss is 0.2656874656677246\n",
      "epoch: 4 step: 2495, loss is 0.2935364246368408\n",
      "epoch: 4 step: 2496, loss is 0.235689178109169\n",
      "epoch: 4 step: 2497, loss is 0.48701488971710205\n",
      "epoch: 4 step: 2498, loss is 0.48376771807670593\n",
      "epoch: 4 step: 2499, loss is 0.3989437222480774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: unknown JFIF revision number 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 2500, loss is 0.2798921465873718\n",
      "epoch: 4 step: 2501, loss is 0.4671650230884552\n",
      "epoch: 4 step: 2502, loss is 0.8840817809104919\n",
      "epoch: 4 step: 2503, loss is 0.3174862563610077\n",
      "epoch: 4 step: 2504, loss is 0.42528170347213745\n",
      "epoch: 4 step: 2505, loss is 0.5682119727134705\n",
      "epoch: 4 step: 2506, loss is 0.3455196022987366\n",
      "epoch: 4 step: 2507, loss is 0.16962619125843048\n",
      "epoch: 4 step: 2508, loss is 0.32621893286705017\n",
      "epoch: 5 step: 1, loss is 0.4449954628944397\n",
      "epoch: 5 step: 2, loss is 0.33611810207366943\n",
      "epoch: 5 step: 3, loss is 0.11877581477165222\n",
      "epoch: 5 step: 4, loss is 0.18464501202106476\n",
      "epoch: 5 step: 5, loss is 0.43471643328666687\n",
      "epoch: 5 step: 6, loss is 0.2369561493396759\n",
      "epoch: 5 step: 7, loss is 0.09789130836725235\n",
      "epoch: 5 step: 8, loss is 0.8517168164253235\n",
      "epoch: 5 step: 9, loss is 0.7678530812263489\n",
      "epoch: 5 step: 10, loss is 0.24687789380550385\n",
      "epoch: 5 step: 11, loss is 0.5042166113853455\n",
      "epoch: 5 step: 12, loss is 0.4754415452480316\n",
      "epoch: 5 step: 13, loss is 0.425491064786911\n",
      "epoch: 5 step: 14, loss is 0.21126297116279602\n",
      "epoch: 5 step: 15, loss is 0.46941038966178894\n",
      "epoch: 5 step: 16, loss is 0.2075585573911667\n",
      "epoch: 5 step: 17, loss is 0.537170946598053\n",
      "epoch: 5 step: 18, loss is 0.30163154006004333\n",
      "epoch: 5 step: 19, loss is 0.2479061633348465\n",
      "epoch: 5 step: 20, loss is 0.5296230316162109\n",
      "epoch: 5 step: 21, loss is 0.639418363571167\n",
      "epoch: 5 step: 22, loss is 0.2738904356956482\n",
      "epoch: 5 step: 23, loss is 0.36844098567962646\n",
      "epoch: 5 step: 24, loss is 0.44498634338378906\n",
      "epoch: 5 step: 25, loss is 0.5020745992660522\n",
      "epoch: 5 step: 26, loss is 0.6170592308044434\n",
      "epoch: 5 step: 27, loss is 0.19596891105175018\n",
      "epoch: 5 step: 28, loss is 0.3161918520927429\n",
      "epoch: 5 step: 29, loss is 0.27872490882873535\n",
      "epoch: 5 step: 30, loss is 0.18853339552879333\n",
      "epoch: 5 step: 31, loss is 1.0728305578231812\n",
      "epoch: 5 step: 32, loss is 0.2437770813703537\n",
      "epoch: 5 step: 33, loss is 0.34727558493614197\n",
      "epoch: 5 step: 34, loss is 0.24137496948242188\n",
      "epoch: 5 step: 35, loss is 0.36763501167297363\n",
      "epoch: 5 step: 36, loss is 0.6347641944885254\n",
      "epoch: 5 step: 37, loss is 0.3838743269443512\n",
      "epoch: 5 step: 38, loss is 0.9645124673843384\n",
      "epoch: 5 step: 39, loss is 0.6167877316474915\n",
      "epoch: 5 step: 40, loss is 0.5311090350151062\n",
      "epoch: 5 step: 41, loss is 0.6367380619049072\n",
      "epoch: 5 step: 42, loss is 0.7438438534736633\n",
      "epoch: 5 step: 43, loss is 0.2786139249801636\n",
      "epoch: 5 step: 44, loss is 0.4785122275352478\n",
      "epoch: 5 step: 45, loss is 0.4230238199234009\n",
      "epoch: 5 step: 46, loss is 0.5234564542770386\n",
      "epoch: 5 step: 47, loss is 0.33628398180007935\n",
      "epoch: 5 step: 48, loss is 0.6852853894233704\n",
      "epoch: 5 step: 49, loss is 0.5420626401901245\n",
      "epoch: 5 step: 50, loss is 0.30665820837020874\n",
      "epoch: 5 step: 51, loss is 0.31970471143722534\n",
      "epoch: 5 step: 52, loss is 0.4980802536010742\n",
      "epoch: 5 step: 53, loss is 0.5095712542533875\n",
      "epoch: 5 step: 54, loss is 0.3463040292263031\n",
      "epoch: 5 step: 55, loss is 0.5636569857597351\n",
      "epoch: 5 step: 56, loss is 0.4041299521923065\n",
      "epoch: 5 step: 57, loss is 0.27435675263404846\n",
      "epoch: 5 step: 58, loss is 0.2632428705692291\n",
      "epoch: 5 step: 59, loss is 0.3998178541660309\n",
      "epoch: 5 step: 60, loss is 0.28483161330223083\n",
      "epoch: 5 step: 61, loss is 0.23602333664894104\n",
      "epoch: 5 step: 62, loss is 0.5829755067825317\n",
      "epoch: 5 step: 63, loss is 0.46265164017677307\n",
      "epoch: 5 step: 64, loss is 0.4272855520248413\n",
      "epoch: 5 step: 65, loss is 0.32050493359565735\n",
      "epoch: 5 step: 66, loss is 0.5373712182044983\n",
      "epoch: 5 step: 67, loss is 0.28863900899887085\n",
      "epoch: 5 step: 68, loss is 0.8587204813957214\n",
      "epoch: 5 step: 69, loss is 0.2085702270269394\n",
      "epoch: 5 step: 70, loss is 0.3643859624862671\n",
      "epoch: 5 step: 71, loss is 0.2543882727622986\n",
      "epoch: 5 step: 72, loss is 0.392328143119812\n",
      "epoch: 5 step: 73, loss is 0.3500138521194458\n",
      "epoch: 5 step: 74, loss is 0.2991950511932373\n",
      "epoch: 5 step: 75, loss is 0.5683965086936951\n",
      "epoch: 5 step: 76, loss is 0.4030354917049408\n",
      "epoch: 5 step: 77, loss is 0.5030304193496704\n",
      "epoch: 5 step: 78, loss is 0.6322065591812134\n",
      "epoch: 5 step: 79, loss is 0.3521413207054138\n",
      "epoch: 5 step: 80, loss is 0.25701913237571716\n",
      "epoch: 5 step: 81, loss is 0.14634427428245544\n",
      "epoch: 5 step: 82, loss is 0.5214786529541016\n",
      "epoch: 5 step: 83, loss is 0.2355632185935974\n",
      "epoch: 5 step: 84, loss is 0.20589160919189453\n",
      "epoch: 5 step: 85, loss is 0.13628102838993073\n",
      "epoch: 5 step: 86, loss is 0.4899619221687317\n",
      "epoch: 5 step: 87, loss is 0.6208445429801941\n",
      "epoch: 5 step: 88, loss is 0.37378427386283875\n",
      "epoch: 5 step: 89, loss is 0.44241955876350403\n",
      "epoch: 5 step: 90, loss is 0.5629531145095825\n",
      "epoch: 5 step: 91, loss is 0.7675167918205261\n",
      "epoch: 5 step: 92, loss is 0.1357869654893875\n",
      "epoch: 5 step: 93, loss is 0.49315887689590454\n",
      "epoch: 5 step: 94, loss is 0.35091379284858704\n",
      "epoch: 5 step: 95, loss is 0.5111362338066101\n",
      "epoch: 5 step: 96, loss is 0.15214085578918457\n",
      "epoch: 5 step: 97, loss is 0.2534359395503998\n",
      "epoch: 5 step: 98, loss is 0.3411910831928253\n",
      "epoch: 5 step: 99, loss is 0.32273247838020325\n",
      "epoch: 5 step: 100, loss is 0.29528769850730896\n",
      "epoch: 5 step: 101, loss is 0.3654457926750183\n",
      "epoch: 5 step: 102, loss is 0.3657331168651581\n",
      "epoch: 5 step: 103, loss is 0.7011291980743408\n",
      "epoch: 5 step: 104, loss is 0.6947431564331055\n",
      "epoch: 5 step: 105, loss is 0.3092727065086365\n",
      "epoch: 5 step: 106, loss is 0.39589717984199524\n",
      "epoch: 5 step: 107, loss is 0.5183756351470947\n",
      "epoch: 5 step: 108, loss is 1.1933051347732544\n",
      "epoch: 5 step: 109, loss is 0.21008813381195068\n",
      "epoch: 5 step: 110, loss is 0.33265581727027893\n",
      "epoch: 5 step: 111, loss is 0.3506932258605957\n",
      "epoch: 5 step: 112, loss is 0.8150595426559448\n",
      "epoch: 5 step: 113, loss is 0.3158465027809143\n",
      "epoch: 5 step: 114, loss is 0.313305526971817\n",
      "epoch: 5 step: 115, loss is 0.24109521508216858\n",
      "epoch: 5 step: 116, loss is 0.3443106412887573\n",
      "epoch: 5 step: 117, loss is 0.5942071080207825\n",
      "epoch: 5 step: 118, loss is 0.5753865242004395\n",
      "epoch: 5 step: 119, loss is 0.2748793661594391\n",
      "epoch: 5 step: 120, loss is 0.40490758419036865\n",
      "epoch: 5 step: 121, loss is 0.40733736753463745\n",
      "epoch: 5 step: 122, loss is 0.6223089694976807\n",
      "epoch: 5 step: 123, loss is 0.19061237573623657\n",
      "epoch: 5 step: 124, loss is 0.3744282126426697\n",
      "epoch: 5 step: 125, loss is 0.5339987874031067\n",
      "epoch: 5 step: 126, loss is 0.2542901337146759\n",
      "epoch: 5 step: 127, loss is 0.572664201259613\n",
      "epoch: 5 step: 128, loss is 0.7014685869216919\n",
      "epoch: 5 step: 129, loss is 0.407615065574646\n",
      "epoch: 5 step: 130, loss is 0.12622106075286865\n",
      "epoch: 5 step: 131, loss is 0.2580197751522064\n",
      "epoch: 5 step: 132, loss is 0.4609892666339874\n",
      "epoch: 5 step: 133, loss is 0.26578909158706665\n",
      "epoch: 5 step: 134, loss is 0.3714812695980072\n",
      "epoch: 5 step: 135, loss is 0.284630686044693\n",
      "epoch: 5 step: 136, loss is 0.39009028673171997\n",
      "epoch: 5 step: 137, loss is 0.41594770550727844\n",
      "epoch: 5 step: 138, loss is 0.4899464547634125\n",
      "epoch: 5 step: 139, loss is 0.05726321041584015\n",
      "epoch: 5 step: 140, loss is 0.3285813331604004\n",
      "epoch: 5 step: 141, loss is 0.26501908898353577\n",
      "epoch: 5 step: 142, loss is 0.3486558496952057\n",
      "epoch: 5 step: 143, loss is 0.17753025889396667\n",
      "epoch: 5 step: 144, loss is 0.25941240787506104\n",
      "epoch: 5 step: 145, loss is 0.09867248684167862\n",
      "epoch: 5 step: 146, loss is 0.2840498089790344\n",
      "epoch: 5 step: 147, loss is 0.6348206400871277\n",
      "epoch: 5 step: 148, loss is 0.814896821975708\n",
      "epoch: 5 step: 149, loss is 0.3272019624710083\n",
      "epoch: 5 step: 150, loss is 0.20505519211292267\n",
      "epoch: 5 step: 151, loss is 0.6408181190490723\n",
      "epoch: 5 step: 152, loss is 0.19050613045692444\n",
      "epoch: 5 step: 153, loss is 0.6945351958274841\n",
      "epoch: 5 step: 154, loss is 0.7998087406158447\n",
      "epoch: 5 step: 155, loss is 0.1436937302350998\n",
      "epoch: 5 step: 156, loss is 0.39001333713531494\n",
      "epoch: 5 step: 157, loss is 0.3785608410835266\n",
      "epoch: 5 step: 158, loss is 0.844004213809967\n",
      "epoch: 5 step: 159, loss is 0.6308915615081787\n",
      "epoch: 5 step: 160, loss is 0.5082505345344543\n",
      "epoch: 5 step: 161, loss is 0.4250483810901642\n",
      "epoch: 5 step: 162, loss is 0.5028539896011353\n",
      "epoch: 5 step: 163, loss is 0.09629141539335251\n",
      "epoch: 5 step: 164, loss is 0.559658944606781\n",
      "epoch: 5 step: 165, loss is 0.7733571529388428\n",
      "epoch: 5 step: 166, loss is 0.22129026055335999\n",
      "epoch: 5 step: 167, loss is 0.2576606273651123\n",
      "epoch: 5 step: 168, loss is 0.3252451419830322\n",
      "epoch: 5 step: 169, loss is 0.42280709743499756\n",
      "epoch: 5 step: 170, loss is 0.20504388213157654\n",
      "epoch: 5 step: 171, loss is 0.44049933552742004\n",
      "epoch: 5 step: 172, loss is 0.32337620854377747\n",
      "epoch: 5 step: 173, loss is 0.4339328408241272\n",
      "epoch: 5 step: 174, loss is 0.6138037443161011\n",
      "epoch: 5 step: 175, loss is 0.40011581778526306\n",
      "epoch: 5 step: 176, loss is 0.5336989164352417\n",
      "epoch: 5 step: 177, loss is 0.4607565701007843\n",
      "epoch: 5 step: 178, loss is 0.4992590844631195\n",
      "epoch: 5 step: 179, loss is 0.09150366485118866\n",
      "epoch: 5 step: 180, loss is 0.47842416167259216\n",
      "epoch: 5 step: 181, loss is 0.3630244731903076\n",
      "epoch: 5 step: 182, loss is 0.4968167841434479\n",
      "epoch: 5 step: 183, loss is 0.45174670219421387\n",
      "epoch: 5 step: 184, loss is 0.26413607597351074\n",
      "epoch: 5 step: 185, loss is 0.6281813383102417\n",
      "epoch: 5 step: 186, loss is 0.5520600080490112\n",
      "epoch: 5 step: 187, loss is 0.16219349205493927\n",
      "epoch: 5 step: 188, loss is 0.1917930692434311\n",
      "epoch: 5 step: 189, loss is 0.8920447826385498\n",
      "epoch: 5 step: 190, loss is 0.22759872674942017\n",
      "epoch: 5 step: 191, loss is 0.5719618201255798\n",
      "epoch: 5 step: 192, loss is 0.254604697227478\n",
      "epoch: 5 step: 193, loss is 0.5057864785194397\n",
      "epoch: 5 step: 194, loss is 0.9706987738609314\n",
      "epoch: 5 step: 195, loss is 0.26157307624816895\n",
      "epoch: 5 step: 196, loss is 0.450621634721756\n",
      "epoch: 5 step: 197, loss is 0.2319546490907669\n",
      "epoch: 5 step: 198, loss is 0.3947008550167084\n",
      "epoch: 5 step: 199, loss is 0.23154960572719574\n",
      "epoch: 5 step: 200, loss is 0.1488572657108307\n",
      "epoch: 5 step: 201, loss is 0.18689408898353577\n",
      "epoch: 5 step: 202, loss is 0.1673620343208313\n",
      "epoch: 5 step: 203, loss is 0.6684455275535583\n",
      "epoch: 5 step: 204, loss is 0.45370739698410034\n",
      "epoch: 5 step: 205, loss is 0.19909965991973877\n",
      "epoch: 5 step: 206, loss is 0.2169002741575241\n",
      "epoch: 5 step: 207, loss is 0.2659129798412323\n",
      "epoch: 5 step: 208, loss is 0.36834654211997986\n",
      "epoch: 5 step: 209, loss is 0.36102476716041565\n",
      "epoch: 5 step: 210, loss is 0.16726231575012207\n",
      "epoch: 5 step: 211, loss is 0.40330639481544495\n",
      "epoch: 5 step: 212, loss is 0.29202720522880554\n",
      "epoch: 5 step: 213, loss is 0.7303033471107483\n",
      "epoch: 5 step: 214, loss is 0.17775723338127136\n",
      "epoch: 5 step: 215, loss is 0.3671843409538269\n",
      "epoch: 5 step: 216, loss is 0.39242109656333923\n",
      "epoch: 5 step: 217, loss is 0.6752479076385498\n",
      "epoch: 5 step: 218, loss is 0.6818949580192566\n",
      "epoch: 5 step: 219, loss is 0.6177811026573181\n",
      "epoch: 5 step: 220, loss is 0.3315202295780182\n",
      "epoch: 5 step: 221, loss is 0.29977214336395264\n",
      "epoch: 5 step: 222, loss is 0.3818463981151581\n",
      "epoch: 5 step: 223, loss is 1.0993478298187256\n",
      "epoch: 5 step: 224, loss is 0.8485459089279175\n",
      "epoch: 5 step: 225, loss is 0.6848056316375732\n",
      "epoch: 5 step: 226, loss is 0.42210569977760315\n",
      "epoch: 5 step: 227, loss is 0.2975165545940399\n",
      "epoch: 5 step: 228, loss is 0.5391009449958801\n",
      "epoch: 5 step: 229, loss is 0.5405189990997314\n",
      "epoch: 5 step: 230, loss is 0.3793046474456787\n",
      "epoch: 5 step: 231, loss is 0.35424861311912537\n",
      "epoch: 5 step: 232, loss is 0.7605469822883606\n",
      "epoch: 5 step: 233, loss is 0.3349132537841797\n",
      "epoch: 5 step: 234, loss is 0.4921839237213135\n",
      "epoch: 5 step: 235, loss is 0.38171520829200745\n",
      "epoch: 5 step: 236, loss is 0.40786728262901306\n",
      "epoch: 5 step: 237, loss is 0.46664100885391235\n",
      "epoch: 5 step: 238, loss is 0.3905903697013855\n",
      "epoch: 5 step: 239, loss is 0.5121041536331177\n",
      "epoch: 5 step: 240, loss is 0.723690390586853\n",
      "epoch: 5 step: 241, loss is 0.22755587100982666\n",
      "epoch: 5 step: 242, loss is 0.19667953252792358\n",
      "epoch: 5 step: 243, loss is 0.5034235715866089\n",
      "epoch: 5 step: 244, loss is 0.19283832609653473\n",
      "epoch: 5 step: 245, loss is 0.39679157733917236\n",
      "epoch: 5 step: 246, loss is 0.42618241906166077\n",
      "epoch: 5 step: 247, loss is 0.234320729970932\n",
      "epoch: 5 step: 248, loss is 0.6415022611618042\n",
      "epoch: 5 step: 249, loss is 0.4630601406097412\n",
      "epoch: 5 step: 250, loss is 0.34028178453445435\n",
      "epoch: 5 step: 251, loss is 0.20037902891635895\n",
      "epoch: 5 step: 252, loss is 0.5344986915588379\n",
      "epoch: 5 step: 253, loss is 0.4615614414215088\n",
      "epoch: 5 step: 254, loss is 0.59413743019104\n",
      "epoch: 5 step: 255, loss is 0.6736155152320862\n",
      "epoch: 5 step: 256, loss is 1.007646918296814\n",
      "epoch: 5 step: 257, loss is 0.30875399708747864\n",
      "epoch: 5 step: 258, loss is 0.12870001792907715\n",
      "epoch: 5 step: 259, loss is 0.11900933086872101\n",
      "epoch: 5 step: 260, loss is 0.39403191208839417\n",
      "epoch: 5 step: 261, loss is 0.5581348538398743\n",
      "epoch: 5 step: 262, loss is 0.6190726161003113\n",
      "epoch: 5 step: 263, loss is 0.4125797748565674\n",
      "epoch: 5 step: 264, loss is 0.8686032295227051\n",
      "epoch: 5 step: 265, loss is 0.5860718488693237\n",
      "epoch: 5 step: 266, loss is 1.0082744359970093\n",
      "epoch: 5 step: 267, loss is 0.7664148807525635\n",
      "epoch: 5 step: 268, loss is 0.396785169839859\n",
      "epoch: 5 step: 269, loss is 0.23053480684757233\n",
      "epoch: 5 step: 270, loss is 0.30556386709213257\n",
      "epoch: 5 step: 271, loss is 0.4977937340736389\n",
      "epoch: 5 step: 272, loss is 0.48215451836586\n",
      "epoch: 5 step: 273, loss is 0.6065341830253601\n",
      "epoch: 5 step: 274, loss is 0.6855456829071045\n",
      "epoch: 5 step: 275, loss is 0.44426143169403076\n",
      "epoch: 5 step: 276, loss is 0.7061251401901245\n",
      "epoch: 5 step: 277, loss is 0.4729292392730713\n",
      "epoch: 5 step: 278, loss is 0.2600008547306061\n",
      "epoch: 5 step: 279, loss is 0.42098432779312134\n",
      "epoch: 5 step: 280, loss is 0.33602437376976013\n",
      "epoch: 5 step: 281, loss is 0.36767899990081787\n",
      "epoch: 5 step: 282, loss is 0.4277345538139343\n",
      "epoch: 5 step: 283, loss is 0.36128106713294983\n",
      "epoch: 5 step: 284, loss is 0.5251764059066772\n",
      "epoch: 5 step: 285, loss is 0.475528359413147\n",
      "epoch: 5 step: 286, loss is 0.8202188014984131\n",
      "epoch: 5 step: 287, loss is 0.8851650357246399\n",
      "epoch: 5 step: 288, loss is 0.6013376712799072\n",
      "epoch: 5 step: 289, loss is 0.491362601518631\n",
      "epoch: 5 step: 290, loss is 0.4867669641971588\n",
      "epoch: 5 step: 291, loss is 0.2609524130821228\n",
      "epoch: 5 step: 292, loss is 0.35674962401390076\n",
      "epoch: 5 step: 293, loss is 0.22224614024162292\n",
      "epoch: 5 step: 294, loss is 0.18524865806102753\n",
      "epoch: 5 step: 295, loss is 1.0513863563537598\n",
      "epoch: 5 step: 296, loss is 0.5056241154670715\n",
      "epoch: 5 step: 297, loss is 0.5865563154220581\n",
      "epoch: 5 step: 298, loss is 0.5463675260543823\n",
      "epoch: 5 step: 299, loss is 0.26092588901519775\n",
      "epoch: 5 step: 300, loss is 0.8526114821434021\n",
      "epoch: 5 step: 301, loss is 0.42504554986953735\n",
      "epoch: 5 step: 302, loss is 0.5827202796936035\n",
      "epoch: 5 step: 303, loss is 0.21756663918495178\n",
      "epoch: 5 step: 304, loss is 0.2606099545955658\n",
      "epoch: 5 step: 305, loss is 0.21662834286689758\n",
      "epoch: 5 step: 306, loss is 0.25201839208602905\n",
      "epoch: 5 step: 307, loss is 0.3774418234825134\n",
      "epoch: 5 step: 308, loss is 0.4874630570411682\n",
      "epoch: 5 step: 309, loss is 0.3376559913158417\n",
      "epoch: 5 step: 310, loss is 0.5573631525039673\n",
      "epoch: 5 step: 311, loss is 0.18509094417095184\n",
      "epoch: 5 step: 312, loss is 0.38317230343818665\n",
      "epoch: 5 step: 313, loss is 0.2916561961174011\n",
      "epoch: 5 step: 314, loss is 0.7408461570739746\n",
      "epoch: 5 step: 315, loss is 0.3469233810901642\n",
      "epoch: 5 step: 316, loss is 0.32875290513038635\n",
      "epoch: 5 step: 317, loss is 0.34085404872894287\n",
      "epoch: 5 step: 318, loss is 0.4206700325012207\n",
      "epoch: 5 step: 319, loss is 0.46194779872894287\n",
      "epoch: 5 step: 320, loss is 1.1565953493118286\n",
      "epoch: 5 step: 321, loss is 0.27113449573516846\n",
      "epoch: 5 step: 322, loss is 0.13836351037025452\n",
      "epoch: 5 step: 323, loss is 0.33463042974472046\n",
      "epoch: 5 step: 324, loss is 0.3750583827495575\n",
      "epoch: 5 step: 325, loss is 0.31622594594955444\n",
      "epoch: 5 step: 326, loss is 0.16422316431999207\n",
      "epoch: 5 step: 327, loss is 0.9542424082756042\n",
      "epoch: 5 step: 328, loss is 0.2908902168273926\n",
      "epoch: 5 step: 329, loss is 0.2289353907108307\n",
      "epoch: 5 step: 330, loss is 0.27218005061149597\n",
      "epoch: 5 step: 331, loss is 0.422629714012146\n",
      "epoch: 5 step: 332, loss is 0.3476554751396179\n",
      "epoch: 5 step: 333, loss is 0.510884702205658\n",
      "epoch: 5 step: 334, loss is 0.7602474093437195\n",
      "epoch: 5 step: 335, loss is 0.4305741786956787\n",
      "epoch: 5 step: 336, loss is 1.153290033340454\n",
      "epoch: 5 step: 337, loss is 1.0221328735351562\n",
      "epoch: 5 step: 338, loss is 0.36546632647514343\n",
      "epoch: 5 step: 339, loss is 0.37817153334617615\n",
      "epoch: 5 step: 340, loss is 0.30797186493873596\n",
      "epoch: 5 step: 341, loss is 0.31812357902526855\n",
      "epoch: 5 step: 342, loss is 0.6586074829101562\n",
      "epoch: 5 step: 343, loss is 0.3411433696746826\n",
      "epoch: 5 step: 344, loss is 0.4389062225818634\n",
      "epoch: 5 step: 345, loss is 0.4000125825405121\n",
      "epoch: 5 step: 346, loss is 0.43558552861213684\n",
      "epoch: 5 step: 347, loss is 0.6156885623931885\n",
      "epoch: 5 step: 348, loss is 0.46391966938972473\n",
      "epoch: 5 step: 349, loss is 0.5433149337768555\n",
      "epoch: 5 step: 350, loss is 0.29332220554351807\n",
      "epoch: 5 step: 351, loss is 0.7527039647102356\n",
      "epoch: 5 step: 352, loss is 0.5201938152313232\n",
      "epoch: 5 step: 353, loss is 0.5203936100006104\n",
      "epoch: 5 step: 354, loss is 0.25094929337501526\n",
      "epoch: 5 step: 355, loss is 0.5460882186889648\n",
      "epoch: 5 step: 356, loss is 0.3396565318107605\n",
      "epoch: 5 step: 357, loss is 0.2645018398761749\n",
      "epoch: 5 step: 358, loss is 0.8852164149284363\n",
      "epoch: 5 step: 359, loss is 0.31458956003189087\n",
      "epoch: 5 step: 360, loss is 0.4222290515899658\n",
      "epoch: 5 step: 361, loss is 0.6603030562400818\n",
      "epoch: 5 step: 362, loss is 0.40348556637763977\n",
      "epoch: 5 step: 363, loss is 0.6398667693138123\n",
      "epoch: 5 step: 364, loss is 0.6864373087882996\n",
      "epoch: 5 step: 365, loss is 0.6100680232048035\n",
      "epoch: 5 step: 366, loss is 0.43336325883865356\n",
      "epoch: 5 step: 367, loss is 0.3382835388183594\n",
      "epoch: 5 step: 368, loss is 0.9077476859092712\n",
      "epoch: 5 step: 369, loss is 0.2975809872150421\n",
      "epoch: 5 step: 370, loss is 0.5266330242156982\n",
      "epoch: 5 step: 371, loss is 0.6294085383415222\n",
      "epoch: 5 step: 372, loss is 0.6033627390861511\n",
      "epoch: 5 step: 373, loss is 0.6064865589141846\n",
      "epoch: 5 step: 374, loss is 0.23961420357227325\n",
      "epoch: 5 step: 375, loss is 0.5054514408111572\n",
      "epoch: 5 step: 376, loss is 0.3605973720550537\n",
      "epoch: 5 step: 377, loss is 0.606301486492157\n",
      "epoch: 5 step: 378, loss is 0.18210789561271667\n",
      "epoch: 5 step: 379, loss is 0.26975739002227783\n",
      "epoch: 5 step: 380, loss is 0.2740184962749481\n",
      "epoch: 5 step: 381, loss is 0.7244564890861511\n",
      "epoch: 5 step: 382, loss is 0.5305846929550171\n",
      "epoch: 5 step: 383, loss is 0.36170536279678345\n",
      "epoch: 5 step: 384, loss is 0.32741379737854004\n",
      "epoch: 5 step: 385, loss is 0.7444658279418945\n",
      "epoch: 5 step: 386, loss is 0.45495137572288513\n",
      "epoch: 5 step: 387, loss is 0.343563973903656\n",
      "epoch: 5 step: 388, loss is 0.8653767704963684\n",
      "epoch: 5 step: 389, loss is 0.4941023886203766\n",
      "epoch: 5 step: 390, loss is 0.6117926239967346\n",
      "epoch: 5 step: 391, loss is 0.3284722566604614\n",
      "epoch: 5 step: 392, loss is 0.758679986000061\n",
      "epoch: 5 step: 393, loss is 0.3517420291900635\n",
      "epoch: 5 step: 394, loss is 0.30084115266799927\n",
      "epoch: 5 step: 395, loss is 0.29500141739845276\n",
      "epoch: 5 step: 396, loss is 0.3997866213321686\n",
      "epoch: 5 step: 397, loss is 0.2391296774148941\n",
      "epoch: 5 step: 398, loss is 0.6398541331291199\n",
      "epoch: 5 step: 399, loss is 0.10299531370401382\n",
      "epoch: 5 step: 400, loss is 0.5169686079025269\n",
      "epoch: 5 step: 401, loss is 0.4975757300853729\n",
      "epoch: 5 step: 402, loss is 0.5394049882888794\n",
      "epoch: 5 step: 403, loss is 0.5376145243644714\n",
      "epoch: 5 step: 404, loss is 0.39805373549461365\n",
      "epoch: 5 step: 405, loss is 0.2699834704399109\n",
      "epoch: 5 step: 406, loss is 0.29571351408958435\n",
      "epoch: 5 step: 407, loss is 0.6397327184677124\n",
      "epoch: 5 step: 408, loss is 0.5677925944328308\n",
      "epoch: 5 step: 409, loss is 0.3391755223274231\n",
      "epoch: 5 step: 410, loss is 0.30108726024627686\n",
      "epoch: 5 step: 411, loss is 0.42378342151641846\n",
      "epoch: 5 step: 412, loss is 0.36495453119277954\n",
      "epoch: 5 step: 413, loss is 0.3031819462776184\n",
      "epoch: 5 step: 414, loss is 0.4564819633960724\n",
      "epoch: 5 step: 415, loss is 0.2709687650203705\n",
      "epoch: 5 step: 416, loss is 0.5433124303817749\n",
      "epoch: 5 step: 417, loss is 0.501092255115509\n",
      "epoch: 5 step: 418, loss is 0.23195578157901764\n",
      "epoch: 5 step: 419, loss is 0.7051718235015869\n",
      "epoch: 5 step: 420, loss is 0.48014989495277405\n",
      "epoch: 5 step: 421, loss is 0.2074308544397354\n",
      "epoch: 5 step: 422, loss is 0.4390776455402374\n",
      "epoch: 5 step: 423, loss is 0.28294873237609863\n",
      "epoch: 5 step: 424, loss is 0.3919311463832855\n",
      "epoch: 5 step: 425, loss is 0.8437495231628418\n",
      "epoch: 5 step: 426, loss is 0.12319239974021912\n",
      "epoch: 5 step: 427, loss is 1.097304344177246\n",
      "epoch: 5 step: 428, loss is 0.2899688184261322\n",
      "epoch: 5 step: 429, loss is 0.8751963376998901\n",
      "epoch: 5 step: 430, loss is 0.764050304889679\n",
      "epoch: 5 step: 431, loss is 0.4993698298931122\n",
      "epoch: 5 step: 432, loss is 0.2601691484451294\n",
      "epoch: 5 step: 433, loss is 0.32064324617385864\n",
      "epoch: 5 step: 434, loss is 0.1999363899230957\n",
      "epoch: 5 step: 435, loss is 0.549511194229126\n",
      "epoch: 5 step: 436, loss is 0.4221116602420807\n",
      "epoch: 5 step: 437, loss is 0.8484178185462952\n",
      "epoch: 5 step: 438, loss is 0.28830552101135254\n",
      "epoch: 5 step: 439, loss is 0.5350421667098999\n",
      "epoch: 5 step: 440, loss is 0.559170126914978\n",
      "epoch: 5 step: 441, loss is 0.15241116285324097\n",
      "epoch: 5 step: 442, loss is 0.40040478110313416\n",
      "epoch: 5 step: 443, loss is 0.3955812156200409\n",
      "epoch: 5 step: 444, loss is 0.37893325090408325\n",
      "epoch: 5 step: 445, loss is 0.4457338750362396\n",
      "epoch: 5 step: 446, loss is 0.8499352335929871\n",
      "epoch: 5 step: 447, loss is 0.36932772397994995\n",
      "epoch: 5 step: 448, loss is 0.3871157169342041\n",
      "epoch: 5 step: 449, loss is 0.3489988148212433\n",
      "epoch: 5 step: 450, loss is 0.4514111876487732\n",
      "epoch: 5 step: 451, loss is 0.39394059777259827\n",
      "epoch: 5 step: 452, loss is 0.5654711723327637\n",
      "epoch: 5 step: 453, loss is 0.48447105288505554\n",
      "epoch: 5 step: 454, loss is 0.12267160415649414\n",
      "epoch: 5 step: 455, loss is 0.4007469713687897\n",
      "epoch: 5 step: 456, loss is 0.21384960412979126\n",
      "epoch: 5 step: 457, loss is 0.1830911934375763\n",
      "epoch: 5 step: 458, loss is 0.37989410758018494\n",
      "epoch: 5 step: 459, loss is 0.23201292753219604\n",
      "epoch: 5 step: 460, loss is 0.4919337034225464\n",
      "epoch: 5 step: 461, loss is 0.3323504328727722\n",
      "epoch: 5 step: 462, loss is 0.29223254323005676\n",
      "epoch: 5 step: 463, loss is 0.5997941493988037\n",
      "epoch: 5 step: 464, loss is 0.4797865152359009\n",
      "epoch: 5 step: 465, loss is 0.37924930453300476\n",
      "epoch: 5 step: 466, loss is 0.3843262195587158\n",
      "epoch: 5 step: 467, loss is 0.46342962980270386\n",
      "epoch: 5 step: 468, loss is 0.47046154737472534\n",
      "epoch: 5 step: 469, loss is 0.2493419498205185\n",
      "epoch: 5 step: 470, loss is 0.42058539390563965\n",
      "epoch: 5 step: 471, loss is 0.33510565757751465\n",
      "epoch: 5 step: 472, loss is 0.8061642646789551\n",
      "epoch: 5 step: 473, loss is 0.4169854521751404\n",
      "epoch: 5 step: 474, loss is 0.27408990263938904\n",
      "epoch: 5 step: 475, loss is 0.19935598969459534\n",
      "epoch: 5 step: 476, loss is 0.38010451197624207\n",
      "epoch: 5 step: 477, loss is 0.1621408313512802\n",
      "epoch: 5 step: 478, loss is 0.3494524657726288\n",
      "epoch: 5 step: 479, loss is 0.33463767170906067\n",
      "epoch: 5 step: 480, loss is 0.2148171067237854\n",
      "epoch: 5 step: 481, loss is 0.3532256484031677\n",
      "epoch: 5 step: 482, loss is 0.610349714756012\n",
      "epoch: 5 step: 483, loss is 0.5524075627326965\n",
      "epoch: 5 step: 484, loss is 0.43639129400253296\n",
      "epoch: 5 step: 485, loss is 0.3033845126628876\n",
      "epoch: 5 step: 486, loss is 0.16370658576488495\n",
      "epoch: 5 step: 487, loss is 0.8998600840568542\n",
      "epoch: 5 step: 488, loss is 0.44745105504989624\n",
      "epoch: 5 step: 489, loss is 0.563967227935791\n",
      "epoch: 5 step: 490, loss is 0.7655502557754517\n",
      "epoch: 5 step: 491, loss is 0.3057905435562134\n",
      "epoch: 5 step: 492, loss is 0.2627941071987152\n",
      "epoch: 5 step: 493, loss is 0.4643336236476898\n",
      "epoch: 5 step: 494, loss is 0.5820459723472595\n",
      "epoch: 5 step: 495, loss is 0.15847882628440857\n",
      "epoch: 5 step: 496, loss is 0.4844808578491211\n",
      "epoch: 5 step: 497, loss is 0.3982911705970764\n",
      "epoch: 5 step: 498, loss is 0.21551631391048431\n",
      "epoch: 5 step: 499, loss is 0.16459602117538452\n",
      "epoch: 5 step: 500, loss is 0.4658688008785248\n",
      "epoch: 5 step: 501, loss is 0.4620344340801239\n",
      "epoch: 5 step: 502, loss is 0.43592604994773865\n",
      "epoch: 5 step: 503, loss is 0.8649030923843384\n",
      "epoch: 5 step: 504, loss is 0.3663250803947449\n",
      "epoch: 5 step: 505, loss is 0.6219841241836548\n",
      "epoch: 5 step: 506, loss is 0.8036286234855652\n",
      "epoch: 5 step: 507, loss is 0.48489105701446533\n",
      "epoch: 5 step: 508, loss is 0.4515446424484253\n",
      "epoch: 5 step: 509, loss is 0.5705904960632324\n",
      "epoch: 5 step: 510, loss is 0.3054467737674713\n",
      "epoch: 5 step: 511, loss is 0.3876262307167053\n",
      "epoch: 5 step: 512, loss is 0.20775020122528076\n",
      "epoch: 5 step: 513, loss is 0.4037410616874695\n",
      "epoch: 5 step: 514, loss is 0.5785164833068848\n",
      "epoch: 5 step: 515, loss is 0.4396902918815613\n",
      "epoch: 5 step: 516, loss is 0.4248344302177429\n",
      "epoch: 5 step: 517, loss is 0.2432224303483963\n",
      "epoch: 5 step: 518, loss is 0.7099120616912842\n",
      "epoch: 5 step: 519, loss is 0.5433614253997803\n",
      "epoch: 5 step: 520, loss is 0.460329532623291\n",
      "epoch: 5 step: 521, loss is 0.304447740316391\n",
      "epoch: 5 step: 522, loss is 0.5270403027534485\n",
      "epoch: 5 step: 523, loss is 0.4447714686393738\n",
      "epoch: 5 step: 524, loss is 0.24551957845687866\n",
      "epoch: 5 step: 525, loss is 0.7056549787521362\n",
      "epoch: 5 step: 526, loss is 0.41295406222343445\n",
      "epoch: 5 step: 527, loss is 0.5390123128890991\n",
      "epoch: 5 step: 528, loss is 0.43372106552124023\n",
      "epoch: 5 step: 529, loss is 0.657661497592926\n",
      "epoch: 5 step: 530, loss is 0.5085674524307251\n",
      "epoch: 5 step: 531, loss is 0.4690879285335541\n",
      "epoch: 5 step: 532, loss is 0.46822381019592285\n",
      "epoch: 5 step: 533, loss is 0.11776874214410782\n",
      "epoch: 5 step: 534, loss is 0.6217595934867859\n",
      "epoch: 5 step: 535, loss is 0.6583723425865173\n",
      "epoch: 5 step: 536, loss is 0.2664984166622162\n",
      "epoch: 5 step: 537, loss is 0.33677950501441956\n",
      "epoch: 5 step: 538, loss is 0.3554660975933075\n",
      "epoch: 5 step: 539, loss is 0.36328747868537903\n",
      "epoch: 5 step: 540, loss is 0.38154321908950806\n",
      "epoch: 5 step: 541, loss is 0.36995163559913635\n",
      "epoch: 5 step: 542, loss is 0.2144651859998703\n",
      "epoch: 5 step: 543, loss is 0.34543952345848083\n",
      "epoch: 5 step: 544, loss is 0.7429132461547852\n",
      "epoch: 5 step: 545, loss is 0.3211009204387665\n",
      "epoch: 5 step: 546, loss is 0.4480195939540863\n",
      "epoch: 5 step: 547, loss is 0.31031888723373413\n",
      "epoch: 5 step: 548, loss is 0.2009264975786209\n",
      "epoch: 5 step: 549, loss is 0.43237388134002686\n",
      "epoch: 5 step: 550, loss is 0.3139590322971344\n",
      "epoch: 5 step: 551, loss is 0.2531607151031494\n",
      "epoch: 5 step: 552, loss is 0.6272822022438049\n",
      "epoch: 5 step: 553, loss is 0.45529982447624207\n",
      "epoch: 5 step: 554, loss is 0.23540422320365906\n",
      "epoch: 5 step: 555, loss is 0.4013725519180298\n",
      "epoch: 5 step: 556, loss is 0.49093058705329895\n",
      "epoch: 5 step: 557, loss is 0.6549622416496277\n",
      "epoch: 5 step: 558, loss is 0.46893662214279175\n",
      "epoch: 5 step: 559, loss is 0.6202460527420044\n",
      "epoch: 5 step: 560, loss is 0.3669775724411011\n",
      "epoch: 5 step: 561, loss is 0.47919365763664246\n",
      "epoch: 5 step: 562, loss is 0.39563488960266113\n",
      "epoch: 5 step: 563, loss is 0.6938012838363647\n",
      "epoch: 5 step: 564, loss is 0.6482470035552979\n",
      "epoch: 5 step: 565, loss is 0.2950383722782135\n",
      "epoch: 5 step: 566, loss is 0.42847445607185364\n",
      "epoch: 5 step: 567, loss is 0.39133012294769287\n",
      "epoch: 5 step: 568, loss is 0.16730523109436035\n",
      "epoch: 5 step: 569, loss is 0.6871707439422607\n",
      "epoch: 5 step: 570, loss is 0.7048330903053284\n",
      "epoch: 5 step: 571, loss is 0.26596665382385254\n",
      "epoch: 5 step: 572, loss is 0.47558173537254333\n",
      "epoch: 5 step: 573, loss is 0.828626811504364\n",
      "epoch: 5 step: 574, loss is 0.4657023847103119\n",
      "epoch: 5 step: 575, loss is 0.4787209630012512\n",
      "epoch: 5 step: 576, loss is 0.3592894375324249\n",
      "epoch: 5 step: 577, loss is 0.3730427324771881\n",
      "epoch: 5 step: 578, loss is 0.5000443458557129\n",
      "epoch: 5 step: 579, loss is 0.46141618490219116\n",
      "epoch: 5 step: 580, loss is 0.3886324167251587\n",
      "epoch: 5 step: 581, loss is 0.28464019298553467\n",
      "epoch: 5 step: 582, loss is 0.2740596532821655\n",
      "epoch: 5 step: 583, loss is 0.6587820649147034\n",
      "epoch: 5 step: 584, loss is 0.6427949070930481\n",
      "epoch: 5 step: 585, loss is 0.4052210748195648\n",
      "epoch: 5 step: 586, loss is 0.31223657727241516\n",
      "epoch: 5 step: 587, loss is 0.41776126623153687\n",
      "epoch: 5 step: 588, loss is 0.44528907537460327\n",
      "epoch: 5 step: 589, loss is 0.2665368318557739\n",
      "epoch: 5 step: 590, loss is 0.36854061484336853\n",
      "epoch: 5 step: 591, loss is 0.8358914852142334\n",
      "epoch: 5 step: 592, loss is 0.6703661680221558\n",
      "epoch: 5 step: 593, loss is 0.5359128713607788\n",
      "epoch: 5 step: 594, loss is 0.17974871397018433\n",
      "epoch: 5 step: 595, loss is 0.6064805388450623\n",
      "epoch: 5 step: 596, loss is 0.2257789522409439\n",
      "epoch: 5 step: 597, loss is 0.21075975894927979\n",
      "epoch: 5 step: 598, loss is 0.43140077590942383\n",
      "epoch: 5 step: 599, loss is 0.6719332933425903\n",
      "epoch: 5 step: 600, loss is 0.22076213359832764\n",
      "epoch: 5 step: 601, loss is 0.3759057819843292\n",
      "epoch: 5 step: 602, loss is 0.1313847452402115\n",
      "epoch: 5 step: 603, loss is 0.5104029774665833\n",
      "epoch: 5 step: 604, loss is 0.918701171875\n",
      "epoch: 5 step: 605, loss is 0.6704851388931274\n",
      "epoch: 5 step: 606, loss is 0.5491162538528442\n",
      "epoch: 5 step: 607, loss is 0.6037085652351379\n",
      "epoch: 5 step: 608, loss is 0.21797999739646912\n",
      "epoch: 5 step: 609, loss is 0.7188266515731812\n",
      "epoch: 5 step: 610, loss is 0.549247682094574\n",
      "epoch: 5 step: 611, loss is 0.7169535160064697\n",
      "epoch: 5 step: 612, loss is 0.35396432876586914\n",
      "epoch: 5 step: 613, loss is 0.4563515782356262\n",
      "epoch: 5 step: 614, loss is 0.40320685505867004\n",
      "epoch: 5 step: 615, loss is 0.3799531161785126\n",
      "epoch: 5 step: 616, loss is 0.7086272835731506\n",
      "epoch: 5 step: 617, loss is 0.1875627636909485\n",
      "epoch: 5 step: 618, loss is 0.6738376617431641\n",
      "epoch: 5 step: 619, loss is 0.7985314130783081\n",
      "epoch: 5 step: 620, loss is 0.16092391312122345\n",
      "epoch: 5 step: 621, loss is 0.2504691481590271\n",
      "epoch: 5 step: 622, loss is 0.5483426451683044\n",
      "epoch: 5 step: 623, loss is 0.3029579222202301\n",
      "epoch: 5 step: 624, loss is 0.5377302169799805\n",
      "epoch: 5 step: 625, loss is 0.6793921589851379\n",
      "epoch: 5 step: 626, loss is 0.5012000203132629\n",
      "epoch: 5 step: 627, loss is 0.16572009027004242\n",
      "epoch: 5 step: 628, loss is 0.5239461660385132\n",
      "epoch: 5 step: 629, loss is 0.4663483202457428\n",
      "epoch: 5 step: 630, loss is 0.2795060873031616\n",
      "epoch: 5 step: 631, loss is 0.2585267722606659\n",
      "epoch: 5 step: 632, loss is 0.19303357601165771\n",
      "epoch: 5 step: 633, loss is 0.8471689224243164\n",
      "epoch: 5 step: 634, loss is 0.5815957188606262\n",
      "epoch: 5 step: 635, loss is 0.5737950801849365\n",
      "epoch: 5 step: 636, loss is 0.24514901638031006\n",
      "epoch: 5 step: 637, loss is 0.48161640763282776\n",
      "epoch: 5 step: 638, loss is 0.3006672263145447\n",
      "epoch: 5 step: 639, loss is 0.5734568238258362\n",
      "epoch: 5 step: 640, loss is 0.30527371168136597\n",
      "epoch: 5 step: 641, loss is 0.669682502746582\n",
      "epoch: 5 step: 642, loss is 0.4517804980278015\n",
      "epoch: 5 step: 643, loss is 0.5632967948913574\n",
      "epoch: 5 step: 644, loss is 0.5782107710838318\n",
      "epoch: 5 step: 645, loss is 0.2635001838207245\n",
      "epoch: 5 step: 646, loss is 0.16948159039020538\n",
      "epoch: 5 step: 647, loss is 0.3124125301837921\n",
      "epoch: 5 step: 648, loss is 0.618111252784729\n",
      "epoch: 5 step: 649, loss is 0.41317909955978394\n",
      "epoch: 5 step: 650, loss is 0.2441352903842926\n",
      "epoch: 5 step: 651, loss is 0.44696637988090515\n",
      "epoch: 5 step: 652, loss is 0.21715116500854492\n",
      "epoch: 5 step: 653, loss is 0.35798585414886475\n",
      "epoch: 5 step: 654, loss is 0.4564536213874817\n",
      "epoch: 5 step: 655, loss is 0.3561437726020813\n",
      "epoch: 5 step: 656, loss is 0.5459917783737183\n",
      "epoch: 5 step: 657, loss is 0.4137676954269409\n",
      "epoch: 5 step: 658, loss is 0.34777581691741943\n",
      "epoch: 5 step: 659, loss is 0.2598239779472351\n",
      "epoch: 5 step: 660, loss is 0.29964929819107056\n",
      "epoch: 5 step: 661, loss is 0.29602107405662537\n",
      "epoch: 5 step: 662, loss is 0.3622055947780609\n",
      "epoch: 5 step: 663, loss is 0.43259087204933167\n",
      "epoch: 5 step: 664, loss is 0.6062489748001099\n",
      "epoch: 5 step: 665, loss is 0.7552692890167236\n",
      "epoch: 5 step: 666, loss is 0.4702424705028534\n",
      "epoch: 5 step: 667, loss is 0.4028053283691406\n",
      "epoch: 5 step: 668, loss is 0.09519071131944656\n",
      "epoch: 5 step: 669, loss is 0.2659299969673157\n",
      "epoch: 5 step: 670, loss is 0.49747002124786377\n",
      "epoch: 5 step: 671, loss is 0.35725438594818115\n",
      "epoch: 5 step: 672, loss is 0.1840113252401352\n",
      "epoch: 5 step: 673, loss is 0.506673276424408\n",
      "epoch: 5 step: 674, loss is 0.28338342905044556\n",
      "epoch: 5 step: 675, loss is 0.2376679629087448\n",
      "epoch: 5 step: 676, loss is 0.2843135893344879\n",
      "epoch: 5 step: 677, loss is 0.06662924587726593\n",
      "epoch: 5 step: 678, loss is 0.5979710817337036\n",
      "epoch: 5 step: 679, loss is 0.5043761134147644\n",
      "epoch: 5 step: 680, loss is 0.9511849284172058\n",
      "epoch: 5 step: 681, loss is 0.4300096929073334\n",
      "epoch: 5 step: 682, loss is 0.572288990020752\n",
      "epoch: 5 step: 683, loss is 0.3782542645931244\n",
      "epoch: 5 step: 684, loss is 0.6107915043830872\n",
      "epoch: 5 step: 685, loss is 0.3971847593784332\n",
      "epoch: 5 step: 686, loss is 0.21786987781524658\n",
      "epoch: 5 step: 687, loss is 0.6862766146659851\n",
      "epoch: 5 step: 688, loss is 0.2506992220878601\n",
      "epoch: 5 step: 689, loss is 0.6293490529060364\n",
      "epoch: 5 step: 690, loss is 0.20698003470897675\n",
      "epoch: 5 step: 691, loss is 0.3476012945175171\n",
      "epoch: 5 step: 692, loss is 0.7981700301170349\n",
      "epoch: 5 step: 693, loss is 0.42210131883621216\n",
      "epoch: 5 step: 694, loss is 0.4369128346443176\n",
      "epoch: 5 step: 695, loss is 0.7015127539634705\n",
      "epoch: 5 step: 696, loss is 0.6204264163970947\n",
      "epoch: 5 step: 697, loss is 0.422548770904541\n",
      "epoch: 5 step: 698, loss is 0.30604416131973267\n",
      "epoch: 5 step: 699, loss is 0.5333991646766663\n",
      "epoch: 5 step: 700, loss is 0.3725355267524719\n",
      "epoch: 5 step: 701, loss is 0.6066663265228271\n",
      "epoch: 5 step: 702, loss is 0.3933104872703552\n",
      "epoch: 5 step: 703, loss is 0.5332568883895874\n",
      "epoch: 5 step: 704, loss is 0.7159090638160706\n",
      "epoch: 5 step: 705, loss is 0.26388630270957947\n",
      "epoch: 5 step: 706, loss is 0.5273562669754028\n",
      "epoch: 5 step: 707, loss is 0.5020154714584351\n",
      "epoch: 5 step: 708, loss is 0.09239493310451508\n",
      "epoch: 5 step: 709, loss is 0.3131513297557831\n",
      "epoch: 5 step: 710, loss is 0.5282022356987\n",
      "epoch: 5 step: 711, loss is 0.3829386532306671\n",
      "epoch: 5 step: 712, loss is 0.43464395403862\n",
      "epoch: 5 step: 713, loss is 0.5197742581367493\n",
      "epoch: 5 step: 714, loss is 0.1553315669298172\n",
      "epoch: 5 step: 715, loss is 0.13160687685012817\n",
      "epoch: 5 step: 716, loss is 0.4429245591163635\n",
      "epoch: 5 step: 717, loss is 0.6891372799873352\n",
      "epoch: 5 step: 718, loss is 0.47806525230407715\n",
      "epoch: 5 step: 719, loss is 0.9670594334602356\n",
      "epoch: 5 step: 720, loss is 0.35758692026138306\n",
      "epoch: 5 step: 721, loss is 0.2521620988845825\n",
      "epoch: 5 step: 722, loss is 0.34850025177001953\n",
      "epoch: 5 step: 723, loss is 0.4639662206172943\n",
      "epoch: 5 step: 724, loss is 0.16129131615161896\n",
      "epoch: 5 step: 725, loss is 0.3765903413295746\n",
      "epoch: 5 step: 726, loss is 0.4439631402492523\n",
      "epoch: 5 step: 727, loss is 0.3922714591026306\n",
      "epoch: 5 step: 728, loss is 0.22921384871006012\n",
      "epoch: 5 step: 729, loss is 0.3487605154514313\n",
      "epoch: 5 step: 730, loss is 0.4175037443637848\n",
      "epoch: 5 step: 731, loss is 0.7031335830688477\n",
      "epoch: 5 step: 732, loss is 0.383520245552063\n",
      "epoch: 5 step: 733, loss is 0.2903412878513336\n",
      "epoch: 5 step: 734, loss is 0.5116598606109619\n",
      "epoch: 5 step: 735, loss is 0.3499637544155121\n",
      "epoch: 5 step: 736, loss is 0.5213613510131836\n",
      "epoch: 5 step: 737, loss is 0.31505143642425537\n",
      "epoch: 5 step: 738, loss is 0.9395482540130615\n",
      "epoch: 5 step: 739, loss is 0.2000170648097992\n",
      "epoch: 5 step: 740, loss is 0.3030188977718353\n",
      "epoch: 5 step: 741, loss is 0.40762028098106384\n",
      "epoch: 5 step: 742, loss is 0.2659401595592499\n",
      "epoch: 5 step: 743, loss is 0.3225587010383606\n",
      "epoch: 5 step: 744, loss is 0.21824996173381805\n",
      "epoch: 5 step: 745, loss is 0.1977241188287735\n",
      "epoch: 5 step: 746, loss is 0.5861867070198059\n",
      "epoch: 5 step: 747, loss is 0.4080926477909088\n",
      "epoch: 5 step: 748, loss is 0.36254262924194336\n",
      "epoch: 5 step: 749, loss is 0.22158952057361603\n",
      "epoch: 5 step: 750, loss is 0.3609633445739746\n",
      "epoch: 5 step: 751, loss is 0.3166603744029999\n",
      "epoch: 5 step: 752, loss is 0.4259679317474365\n",
      "epoch: 5 step: 753, loss is 0.30999869108200073\n",
      "epoch: 5 step: 754, loss is 0.5776748061180115\n",
      "epoch: 5 step: 755, loss is 0.5557665824890137\n",
      "epoch: 5 step: 756, loss is 0.38236838579177856\n",
      "epoch: 5 step: 757, loss is 0.6960070729255676\n",
      "epoch: 5 step: 758, loss is 0.5139294266700745\n",
      "epoch: 5 step: 759, loss is 0.26860809326171875\n",
      "epoch: 5 step: 760, loss is 0.23696941137313843\n",
      "epoch: 5 step: 761, loss is 0.8282812833786011\n",
      "epoch: 5 step: 762, loss is 0.47899964451789856\n",
      "epoch: 5 step: 763, loss is 0.24797427654266357\n",
      "epoch: 5 step: 764, loss is 0.23513208329677582\n",
      "epoch: 5 step: 765, loss is 0.3860747218132019\n",
      "epoch: 5 step: 766, loss is 0.8313840627670288\n",
      "epoch: 5 step: 767, loss is 0.3951365649700165\n",
      "epoch: 5 step: 768, loss is 0.4931448698043823\n",
      "epoch: 5 step: 769, loss is 0.787071943283081\n",
      "epoch: 5 step: 770, loss is 0.18515758216381073\n",
      "epoch: 5 step: 771, loss is 0.6463423371315002\n",
      "epoch: 5 step: 772, loss is 0.3716554045677185\n",
      "epoch: 5 step: 773, loss is 0.408900648355484\n",
      "epoch: 5 step: 774, loss is 0.47327369451522827\n",
      "epoch: 5 step: 775, loss is 0.30884939432144165\n",
      "epoch: 5 step: 776, loss is 0.4300738275051117\n",
      "epoch: 5 step: 777, loss is 0.48101070523262024\n",
      "epoch: 5 step: 778, loss is 0.4283257722854614\n",
      "epoch: 5 step: 779, loss is 0.5546880960464478\n",
      "epoch: 5 step: 780, loss is 0.16256536543369293\n",
      "epoch: 5 step: 781, loss is 0.27972742915153503\n",
      "epoch: 5 step: 782, loss is 0.93663090467453\n",
      "epoch: 5 step: 783, loss is 0.8686026930809021\n",
      "epoch: 5 step: 784, loss is 0.9086377024650574\n",
      "epoch: 5 step: 785, loss is 0.4146910011768341\n",
      "epoch: 5 step: 786, loss is 0.6195253729820251\n",
      "epoch: 5 step: 787, loss is 0.5483441948890686\n",
      "epoch: 5 step: 788, loss is 0.8755654692649841\n",
      "epoch: 5 step: 789, loss is 0.5168896317481995\n",
      "epoch: 5 step: 790, loss is 0.26378539204597473\n",
      "epoch: 5 step: 791, loss is 0.5147204399108887\n",
      "epoch: 5 step: 792, loss is 0.3183685541152954\n",
      "epoch: 5 step: 793, loss is 0.46148091554641724\n",
      "epoch: 5 step: 794, loss is 0.3068545162677765\n",
      "epoch: 5 step: 795, loss is 0.15840396285057068\n",
      "epoch: 5 step: 796, loss is 0.31336984038352966\n",
      "epoch: 5 step: 797, loss is 0.39634010195732117\n",
      "epoch: 5 step: 798, loss is 0.4677787125110626\n",
      "epoch: 5 step: 799, loss is 0.4081403613090515\n",
      "epoch: 5 step: 800, loss is 0.2852330207824707\n",
      "epoch: 5 step: 801, loss is 0.3671295642852783\n",
      "epoch: 5 step: 802, loss is 0.2157706916332245\n",
      "epoch: 5 step: 803, loss is 0.5346441268920898\n",
      "epoch: 5 step: 804, loss is 0.28243061900138855\n",
      "epoch: 5 step: 805, loss is 0.30518633127212524\n",
      "epoch: 5 step: 806, loss is 0.32629185914993286\n",
      "epoch: 5 step: 807, loss is 0.5498412251472473\n",
      "epoch: 5 step: 808, loss is 0.44042402505874634\n",
      "epoch: 5 step: 809, loss is 0.8541085720062256\n",
      "epoch: 5 step: 810, loss is 0.44455888867378235\n",
      "epoch: 5 step: 811, loss is 0.2674345374107361\n",
      "epoch: 5 step: 812, loss is 0.24092145264148712\n",
      "epoch: 5 step: 813, loss is 0.4368869364261627\n",
      "epoch: 5 step: 814, loss is 0.2178587168455124\n",
      "epoch: 5 step: 815, loss is 0.541817843914032\n",
      "epoch: 5 step: 816, loss is 0.5150869488716125\n",
      "epoch: 5 step: 817, loss is 0.4755862355232239\n",
      "epoch: 5 step: 818, loss is 0.4281752407550812\n",
      "epoch: 5 step: 819, loss is 0.618861734867096\n",
      "epoch: 5 step: 820, loss is 0.7469332814216614\n",
      "epoch: 5 step: 821, loss is 0.42635607719421387\n",
      "epoch: 5 step: 822, loss is 0.3041834831237793\n",
      "epoch: 5 step: 823, loss is 0.7873318195343018\n",
      "epoch: 5 step: 824, loss is 0.45113277435302734\n",
      "epoch: 5 step: 825, loss is 0.46122464537620544\n",
      "epoch: 5 step: 826, loss is 0.6340111494064331\n",
      "epoch: 5 step: 827, loss is 0.6927716135978699\n",
      "epoch: 5 step: 828, loss is 0.15020272135734558\n",
      "epoch: 5 step: 829, loss is 0.6050136685371399\n",
      "epoch: 5 step: 830, loss is 0.2230941504240036\n",
      "epoch: 5 step: 831, loss is 0.5565090179443359\n",
      "epoch: 5 step: 832, loss is 0.680517315864563\n",
      "epoch: 5 step: 833, loss is 0.46710193157196045\n",
      "epoch: 5 step: 834, loss is 0.31545448303222656\n",
      "epoch: 5 step: 835, loss is 0.7534998655319214\n",
      "epoch: 5 step: 836, loss is 0.44158369302749634\n",
      "epoch: 5 step: 837, loss is 0.4109511077404022\n",
      "epoch: 5 step: 838, loss is 0.7165117859840393\n",
      "epoch: 5 step: 839, loss is 0.4474821984767914\n",
      "epoch: 5 step: 840, loss is 0.4267828166484833\n",
      "epoch: 5 step: 841, loss is 0.5392370820045471\n",
      "epoch: 5 step: 842, loss is 0.3664551079273224\n",
      "epoch: 5 step: 843, loss is 0.739648699760437\n",
      "epoch: 5 step: 844, loss is 0.46685197949409485\n",
      "epoch: 5 step: 845, loss is 0.39527714252471924\n",
      "epoch: 5 step: 846, loss is 0.31787386536598206\n",
      "epoch: 5 step: 847, loss is 0.46373945474624634\n",
      "epoch: 5 step: 848, loss is 0.31114891171455383\n",
      "epoch: 5 step: 849, loss is 0.27651074528694153\n",
      "epoch: 5 step: 850, loss is 0.2749119997024536\n",
      "epoch: 5 step: 851, loss is 0.3014400005340576\n",
      "epoch: 5 step: 852, loss is 0.36569997668266296\n",
      "epoch: 5 step: 853, loss is 0.36738646030426025\n",
      "epoch: 5 step: 854, loss is 0.7844088077545166\n",
      "epoch: 5 step: 855, loss is 0.5607472062110901\n",
      "epoch: 5 step: 856, loss is 0.15851269662380219\n",
      "epoch: 5 step: 857, loss is 0.5477666854858398\n",
      "epoch: 5 step: 858, loss is 0.299608051776886\n",
      "epoch: 5 step: 859, loss is 0.27496543526649475\n",
      "epoch: 5 step: 860, loss is 0.5094395875930786\n",
      "epoch: 5 step: 861, loss is 0.5042455196380615\n",
      "epoch: 5 step: 862, loss is 0.4140470623970032\n",
      "epoch: 5 step: 863, loss is 0.4275591969490051\n",
      "epoch: 5 step: 864, loss is 0.5170783400535583\n",
      "epoch: 5 step: 865, loss is 0.4060197174549103\n",
      "epoch: 5 step: 866, loss is 0.18556350469589233\n",
      "epoch: 5 step: 867, loss is 0.5352483987808228\n",
      "epoch: 5 step: 868, loss is 0.18585985898971558\n",
      "epoch: 5 step: 869, loss is 0.2750900983810425\n",
      "epoch: 5 step: 870, loss is 0.647568941116333\n",
      "epoch: 5 step: 871, loss is 0.5414919257164001\n",
      "epoch: 5 step: 872, loss is 0.9109800457954407\n",
      "epoch: 5 step: 873, loss is 0.36961105465888977\n",
      "epoch: 5 step: 874, loss is 0.8205586075782776\n",
      "epoch: 5 step: 875, loss is 0.22378988564014435\n",
      "epoch: 5 step: 876, loss is 0.8520904779434204\n",
      "epoch: 5 step: 877, loss is 0.3792421519756317\n",
      "epoch: 5 step: 878, loss is 0.318538099527359\n",
      "epoch: 5 step: 879, loss is 0.5577510595321655\n",
      "epoch: 5 step: 880, loss is 0.3337002396583557\n",
      "epoch: 5 step: 881, loss is 0.24779745936393738\n",
      "epoch: 5 step: 882, loss is 0.5778943300247192\n",
      "epoch: 5 step: 883, loss is 0.295147567987442\n",
      "epoch: 5 step: 884, loss is 0.3076256215572357\n",
      "epoch: 5 step: 885, loss is 0.9372835755348206\n",
      "epoch: 5 step: 886, loss is 0.423119455575943\n",
      "epoch: 5 step: 887, loss is 0.7354837656021118\n",
      "epoch: 5 step: 888, loss is 0.5875251293182373\n",
      "epoch: 5 step: 889, loss is 0.4969952702522278\n",
      "epoch: 5 step: 890, loss is 0.3250945806503296\n",
      "epoch: 5 step: 891, loss is 0.5933713912963867\n",
      "epoch: 5 step: 892, loss is 0.406406432390213\n",
      "epoch: 5 step: 893, loss is 0.4363887310028076\n",
      "epoch: 5 step: 894, loss is 0.21549902856349945\n",
      "epoch: 5 step: 895, loss is 0.3281882107257843\n",
      "epoch: 5 step: 896, loss is 0.45984938740730286\n",
      "epoch: 5 step: 897, loss is 0.7532784342765808\n",
      "epoch: 5 step: 898, loss is 0.4435269236564636\n",
      "epoch: 5 step: 899, loss is 0.5977766513824463\n",
      "epoch: 5 step: 900, loss is 0.4200202226638794\n",
      "epoch: 5 step: 901, loss is 0.45948344469070435\n",
      "epoch: 5 step: 902, loss is 0.38314521312713623\n",
      "epoch: 5 step: 903, loss is 0.39938342571258545\n",
      "epoch: 5 step: 904, loss is 0.3800552189350128\n",
      "epoch: 5 step: 905, loss is 0.2538684904575348\n",
      "epoch: 5 step: 906, loss is 0.3080040514469147\n",
      "epoch: 5 step: 907, loss is 0.7967216968536377\n",
      "epoch: 5 step: 908, loss is 0.8813388347625732\n",
      "epoch: 5 step: 909, loss is 0.251231849193573\n",
      "epoch: 5 step: 910, loss is 0.5262326598167419\n",
      "epoch: 5 step: 911, loss is 0.6834703087806702\n",
      "epoch: 5 step: 912, loss is 0.4299295246601105\n",
      "epoch: 5 step: 913, loss is 0.6163027882575989\n",
      "epoch: 5 step: 914, loss is 0.42208656668663025\n",
      "epoch: 5 step: 915, loss is 0.2999047636985779\n",
      "epoch: 5 step: 916, loss is 0.40334463119506836\n",
      "epoch: 5 step: 917, loss is 0.43313437700271606\n",
      "epoch: 5 step: 918, loss is 0.566744863986969\n",
      "epoch: 5 step: 919, loss is 0.3949107229709625\n",
      "epoch: 5 step: 920, loss is 0.5219448208808899\n",
      "epoch: 5 step: 921, loss is 0.26505202054977417\n",
      "epoch: 5 step: 922, loss is 0.5388192534446716\n",
      "epoch: 5 step: 923, loss is 0.2938445508480072\n",
      "epoch: 5 step: 924, loss is 0.5479612946510315\n",
      "epoch: 5 step: 925, loss is 0.17634983360767365\n",
      "epoch: 5 step: 926, loss is 0.3810640871524811\n",
      "epoch: 5 step: 927, loss is 0.2864263653755188\n",
      "epoch: 5 step: 928, loss is 0.45093613862991333\n",
      "epoch: 5 step: 929, loss is 0.2894868552684784\n",
      "epoch: 5 step: 930, loss is 0.22285448014736176\n",
      "epoch: 5 step: 931, loss is 0.6605614423751831\n",
      "epoch: 5 step: 932, loss is 0.32199615240097046\n",
      "epoch: 5 step: 933, loss is 0.2770238518714905\n",
      "epoch: 5 step: 934, loss is 0.3882817029953003\n",
      "epoch: 5 step: 935, loss is 0.4440595805644989\n",
      "epoch: 5 step: 936, loss is 0.7982500791549683\n",
      "epoch: 5 step: 937, loss is 0.6947922706604004\n",
      "epoch: 5 step: 938, loss is 0.1774754524230957\n",
      "epoch: 5 step: 939, loss is 0.26236191391944885\n",
      "epoch: 5 step: 940, loss is 0.41389957070350647\n",
      "epoch: 5 step: 941, loss is 0.5324133634567261\n",
      "epoch: 5 step: 942, loss is 0.14066284894943237\n",
      "epoch: 5 step: 943, loss is 0.3528190851211548\n",
      "epoch: 5 step: 944, loss is 0.16327905654907227\n",
      "epoch: 5 step: 945, loss is 0.7111852765083313\n",
      "epoch: 5 step: 946, loss is 0.15453124046325684\n",
      "epoch: 5 step: 947, loss is 0.5234089493751526\n",
      "epoch: 5 step: 948, loss is 0.1289508193731308\n",
      "epoch: 5 step: 949, loss is 0.6977410316467285\n",
      "epoch: 5 step: 950, loss is 0.2787628769874573\n",
      "epoch: 5 step: 951, loss is 0.36659008264541626\n",
      "epoch: 5 step: 952, loss is 0.6616930961608887\n",
      "epoch: 5 step: 953, loss is 0.2964266240596771\n",
      "epoch: 5 step: 954, loss is 0.5067611336708069\n",
      "epoch: 5 step: 955, loss is 0.329720139503479\n",
      "epoch: 5 step: 956, loss is 0.37289753556251526\n",
      "epoch: 5 step: 957, loss is 0.7223535180091858\n",
      "epoch: 5 step: 958, loss is 0.37253138422966003\n",
      "epoch: 5 step: 959, loss is 0.149186372756958\n",
      "epoch: 5 step: 960, loss is 0.38100704550743103\n",
      "epoch: 5 step: 961, loss is 0.18056772649288177\n",
      "epoch: 5 step: 962, loss is 0.7518520355224609\n",
      "epoch: 5 step: 963, loss is 0.6873746514320374\n",
      "epoch: 5 step: 964, loss is 0.522908627986908\n",
      "epoch: 5 step: 965, loss is 0.17428550124168396\n",
      "epoch: 5 step: 966, loss is 0.6258139610290527\n",
      "epoch: 5 step: 967, loss is 0.7764859795570374\n",
      "epoch: 5 step: 968, loss is 0.6078871488571167\n",
      "epoch: 5 step: 969, loss is 1.136650800704956\n",
      "epoch: 5 step: 970, loss is 1.0747047662734985\n",
      "epoch: 5 step: 971, loss is 0.4688969850540161\n",
      "epoch: 5 step: 972, loss is 0.39031463861465454\n",
      "epoch: 5 step: 973, loss is 0.4678839147090912\n",
      "epoch: 5 step: 974, loss is 0.6801823377609253\n",
      "epoch: 5 step: 975, loss is 0.5297423601150513\n",
      "epoch: 5 step: 976, loss is 0.7489238381385803\n",
      "epoch: 5 step: 977, loss is 0.31081530451774597\n",
      "epoch: 5 step: 978, loss is 0.6622891426086426\n",
      "epoch: 5 step: 979, loss is 0.8414378762245178\n",
      "epoch: 5 step: 980, loss is 0.7251247763633728\n",
      "epoch: 5 step: 981, loss is 0.7292906641960144\n",
      "epoch: 5 step: 982, loss is 0.5370132923126221\n",
      "epoch: 5 step: 983, loss is 0.9127914905548096\n",
      "epoch: 5 step: 984, loss is 0.777582049369812\n",
      "epoch: 5 step: 985, loss is 0.6634604334831238\n",
      "epoch: 5 step: 986, loss is 0.35191670060157776\n",
      "epoch: 5 step: 987, loss is 0.4900537133216858\n",
      "epoch: 5 step: 988, loss is 0.4043293595314026\n",
      "epoch: 5 step: 989, loss is 0.6406173706054688\n",
      "epoch: 5 step: 990, loss is 0.4229881763458252\n",
      "epoch: 5 step: 991, loss is 0.4138784110546112\n",
      "epoch: 5 step: 992, loss is 0.6225575804710388\n",
      "epoch: 5 step: 993, loss is 0.6298367977142334\n",
      "epoch: 5 step: 994, loss is 0.4403170943260193\n",
      "epoch: 5 step: 995, loss is 0.7024631500244141\n",
      "epoch: 5 step: 996, loss is 0.4061386287212372\n",
      "epoch: 5 step: 997, loss is 0.28128549456596375\n",
      "epoch: 5 step: 998, loss is 0.442550927400589\n",
      "epoch: 5 step: 999, loss is 0.5371054410934448\n",
      "epoch: 5 step: 1000, loss is 0.4225773215293884\n",
      "epoch: 5 step: 1001, loss is 0.32804760336875916\n",
      "epoch: 5 step: 1002, loss is 0.39060965180397034\n",
      "epoch: 5 step: 1003, loss is 0.3003349006175995\n",
      "epoch: 5 step: 1004, loss is 0.4267643988132477\n",
      "epoch: 5 step: 1005, loss is 0.5922112464904785\n",
      "epoch: 5 step: 1006, loss is 0.4827776551246643\n",
      "epoch: 5 step: 1007, loss is 0.7065083980560303\n",
      "epoch: 5 step: 1008, loss is 0.45257800817489624\n",
      "epoch: 5 step: 1009, loss is 0.3450699746608734\n",
      "epoch: 5 step: 1010, loss is 0.7277253866195679\n",
      "epoch: 5 step: 1011, loss is 0.6024904251098633\n",
      "epoch: 5 step: 1012, loss is 0.4490963816642761\n",
      "epoch: 5 step: 1013, loss is 0.31974342465400696\n",
      "epoch: 5 step: 1014, loss is 0.26807963848114014\n",
      "epoch: 5 step: 1015, loss is 0.2809065878391266\n",
      "epoch: 5 step: 1016, loss is 0.22655969858169556\n",
      "epoch: 5 step: 1017, loss is 0.5886831879615784\n",
      "epoch: 5 step: 1018, loss is 0.3849451243877411\n",
      "epoch: 5 step: 1019, loss is 0.7236021161079407\n",
      "epoch: 5 step: 1020, loss is 0.3233901858329773\n",
      "epoch: 5 step: 1021, loss is 0.5314182043075562\n",
      "epoch: 5 step: 1022, loss is 0.7120277285575867\n",
      "epoch: 5 step: 1023, loss is 0.2558814585208893\n",
      "epoch: 5 step: 1024, loss is 0.3316698372364044\n",
      "epoch: 5 step: 1025, loss is 0.19161982834339142\n",
      "epoch: 5 step: 1026, loss is 0.3232698142528534\n",
      "epoch: 5 step: 1027, loss is 0.21817316114902496\n",
      "epoch: 5 step: 1028, loss is 0.2959236204624176\n",
      "epoch: 5 step: 1029, loss is 0.31917744874954224\n",
      "epoch: 5 step: 1030, loss is 0.4445180296897888\n",
      "epoch: 5 step: 1031, loss is 0.2212563455104828\n",
      "epoch: 5 step: 1032, loss is 0.3064091205596924\n",
      "epoch: 5 step: 1033, loss is 0.4557962119579315\n",
      "epoch: 5 step: 1034, loss is 0.6075971126556396\n",
      "epoch: 5 step: 1035, loss is 0.5837489366531372\n",
      "epoch: 5 step: 1036, loss is 0.4346057176589966\n",
      "epoch: 5 step: 1037, loss is 0.6650875806808472\n",
      "epoch: 5 step: 1038, loss is 0.3988030254840851\n",
      "epoch: 5 step: 1039, loss is 0.5469107627868652\n",
      "epoch: 5 step: 1040, loss is 0.48470938205718994\n",
      "epoch: 5 step: 1041, loss is 0.35895588994026184\n",
      "epoch: 5 step: 1042, loss is 0.32032471895217896\n",
      "epoch: 5 step: 1043, loss is 0.47003158926963806\n",
      "epoch: 5 step: 1044, loss is 0.17824362218379974\n",
      "epoch: 5 step: 1045, loss is 0.055303823202848434\n",
      "epoch: 5 step: 1046, loss is 0.42228129506111145\n",
      "epoch: 5 step: 1047, loss is 0.20603975653648376\n",
      "epoch: 5 step: 1048, loss is 0.382337361574173\n",
      "epoch: 5 step: 1049, loss is 0.06529088318347931\n",
      "epoch: 5 step: 1050, loss is 0.24245598912239075\n",
      "epoch: 5 step: 1051, loss is 0.49442219734191895\n",
      "epoch: 5 step: 1052, loss is 0.5987585783004761\n",
      "epoch: 5 step: 1053, loss is 0.5826399326324463\n",
      "epoch: 5 step: 1054, loss is 0.16404859721660614\n",
      "epoch: 5 step: 1055, loss is 0.28307589888572693\n",
      "epoch: 5 step: 1056, loss is 0.7370176911354065\n",
      "epoch: 5 step: 1057, loss is 0.25650277733802795\n",
      "epoch: 5 step: 1058, loss is 0.2570851743221283\n",
      "epoch: 5 step: 1059, loss is 0.4254707396030426\n",
      "epoch: 5 step: 1060, loss is 0.18385255336761475\n",
      "epoch: 5 step: 1061, loss is 0.2319583147764206\n",
      "epoch: 5 step: 1062, loss is 0.2866191267967224\n",
      "epoch: 5 step: 1063, loss is 0.5283869504928589\n",
      "epoch: 5 step: 1064, loss is 0.13019143044948578\n",
      "epoch: 5 step: 1065, loss is 0.3399530053138733\n",
      "epoch: 5 step: 1066, loss is 0.5615975856781006\n",
      "epoch: 5 step: 1067, loss is 1.0330369472503662\n",
      "epoch: 5 step: 1068, loss is 0.47827571630477905\n",
      "epoch: 5 step: 1069, loss is 0.5110617280006409\n",
      "epoch: 5 step: 1070, loss is 0.28986358642578125\n",
      "epoch: 5 step: 1071, loss is 0.4735737442970276\n",
      "epoch: 5 step: 1072, loss is 0.1993522346019745\n",
      "epoch: 5 step: 1073, loss is 0.18421290814876556\n",
      "epoch: 5 step: 1074, loss is 0.5016089677810669\n",
      "epoch: 5 step: 1075, loss is 0.2708861231803894\n",
      "epoch: 5 step: 1076, loss is 0.28406137228012085\n",
      "epoch: 5 step: 1077, loss is 0.30944591760635376\n",
      "epoch: 5 step: 1078, loss is 0.6638256311416626\n",
      "epoch: 5 step: 1079, loss is 0.5726576447486877\n",
      "epoch: 5 step: 1080, loss is 0.22137190401554108\n",
      "epoch: 5 step: 1081, loss is 0.5674204230308533\n",
      "epoch: 5 step: 1082, loss is 0.5186688899993896\n",
      "epoch: 5 step: 1083, loss is 0.29237475991249084\n",
      "epoch: 5 step: 1084, loss is 0.09746400266885757\n",
      "epoch: 5 step: 1085, loss is 0.46031221747398376\n",
      "epoch: 5 step: 1086, loss is 0.6267677545547485\n",
      "epoch: 5 step: 1087, loss is 0.7223271727561951\n",
      "epoch: 5 step: 1088, loss is 0.25936514139175415\n",
      "epoch: 5 step: 1089, loss is 0.27403151988983154\n",
      "epoch: 5 step: 1090, loss is 0.20134654641151428\n",
      "epoch: 5 step: 1091, loss is 0.21449217200279236\n",
      "epoch: 5 step: 1092, loss is 0.30671751499176025\n",
      "epoch: 5 step: 1093, loss is 0.588556170463562\n",
      "epoch: 5 step: 1094, loss is 0.4313829839229584\n",
      "epoch: 5 step: 1095, loss is 0.2660350799560547\n",
      "epoch: 5 step: 1096, loss is 0.8456811904907227\n",
      "epoch: 5 step: 1097, loss is 0.30050474405288696\n",
      "epoch: 5 step: 1098, loss is 0.5256673097610474\n",
      "epoch: 5 step: 1099, loss is 0.42420440912246704\n",
      "epoch: 5 step: 1100, loss is 0.5647225379943848\n",
      "epoch: 5 step: 1101, loss is 0.7038346529006958\n",
      "epoch: 5 step: 1102, loss is 0.4641628563404083\n",
      "epoch: 5 step: 1103, loss is 0.8474194407463074\n",
      "epoch: 5 step: 1104, loss is 0.7411016225814819\n",
      "epoch: 5 step: 1105, loss is 0.6349498629570007\n",
      "epoch: 5 step: 1106, loss is 0.458492249250412\n",
      "epoch: 5 step: 1107, loss is 0.6727873682975769\n",
      "epoch: 5 step: 1108, loss is 0.19096435606479645\n",
      "epoch: 5 step: 1109, loss is 0.30146196484565735\n",
      "epoch: 5 step: 1110, loss is 0.6140962839126587\n",
      "epoch: 5 step: 1111, loss is 0.2980797290802002\n",
      "epoch: 5 step: 1112, loss is 0.5320337414741516\n",
      "epoch: 5 step: 1113, loss is 0.3467835485935211\n",
      "epoch: 5 step: 1114, loss is 0.2500026822090149\n",
      "epoch: 5 step: 1115, loss is 0.24763613939285278\n",
      "epoch: 5 step: 1116, loss is 0.41770052909851074\n",
      "epoch: 5 step: 1117, loss is 0.22943177819252014\n",
      "epoch: 5 step: 1118, loss is 0.4034978747367859\n",
      "epoch: 5 step: 1119, loss is 0.5734487175941467\n",
      "epoch: 5 step: 1120, loss is 0.5283536911010742\n",
      "epoch: 5 step: 1121, loss is 0.2582103908061981\n",
      "epoch: 5 step: 1122, loss is 0.2846941649913788\n",
      "epoch: 5 step: 1123, loss is 0.16105255484580994\n",
      "epoch: 5 step: 1124, loss is 0.28796663880348206\n",
      "epoch: 5 step: 1125, loss is 0.4925878345966339\n",
      "epoch: 5 step: 1126, loss is 1.2709813117980957\n",
      "epoch: 5 step: 1127, loss is 0.3650781810283661\n",
      "epoch: 5 step: 1128, loss is 0.31314489245414734\n",
      "epoch: 5 step: 1129, loss is 0.5465424060821533\n",
      "epoch: 5 step: 1130, loss is 0.6474429368972778\n",
      "epoch: 5 step: 1131, loss is 0.25493645668029785\n",
      "epoch: 5 step: 1132, loss is 0.6639513373374939\n",
      "epoch: 5 step: 1133, loss is 0.26962900161743164\n",
      "epoch: 5 step: 1134, loss is 0.49542108178138733\n",
      "epoch: 5 step: 1135, loss is 1.0972347259521484\n",
      "epoch: 5 step: 1136, loss is 0.624395489692688\n",
      "epoch: 5 step: 1137, loss is 0.7000490427017212\n",
      "epoch: 5 step: 1138, loss is 0.27429673075675964\n",
      "epoch: 5 step: 1139, loss is 0.3761328160762787\n",
      "epoch: 5 step: 1140, loss is 0.30353400111198425\n",
      "epoch: 5 step: 1141, loss is 0.26995769143104553\n",
      "epoch: 5 step: 1142, loss is 0.4319446086883545\n",
      "epoch: 5 step: 1143, loss is 0.5133947730064392\n",
      "epoch: 5 step: 1144, loss is 0.6055675745010376\n",
      "epoch: 5 step: 1145, loss is 0.545175313949585\n",
      "epoch: 5 step: 1146, loss is 0.5304692983627319\n",
      "epoch: 5 step: 1147, loss is 0.8107430338859558\n",
      "epoch: 5 step: 1148, loss is 0.19283345341682434\n",
      "epoch: 5 step: 1149, loss is 0.3556531071662903\n",
      "epoch: 5 step: 1150, loss is 0.18215644359588623\n",
      "epoch: 5 step: 1151, loss is 0.3321080505847931\n",
      "epoch: 5 step: 1152, loss is 0.5478085875511169\n",
      "epoch: 5 step: 1153, loss is 0.20073232054710388\n",
      "epoch: 5 step: 1154, loss is 0.26653164625167847\n",
      "epoch: 5 step: 1155, loss is 0.5459014177322388\n",
      "epoch: 5 step: 1156, loss is 0.3247336149215698\n",
      "epoch: 5 step: 1157, loss is 0.41861841082572937\n",
      "epoch: 5 step: 1158, loss is 0.5598005056381226\n",
      "epoch: 5 step: 1159, loss is 0.5924153327941895\n",
      "epoch: 5 step: 1160, loss is 0.18009239435195923\n",
      "epoch: 5 step: 1161, loss is 0.12019603699445724\n",
      "epoch: 5 step: 1162, loss is 0.4770958125591278\n",
      "epoch: 5 step: 1163, loss is 0.30409061908721924\n",
      "epoch: 5 step: 1164, loss is 0.20438942313194275\n",
      "epoch: 5 step: 1165, loss is 0.43888992071151733\n",
      "epoch: 5 step: 1166, loss is 0.386851966381073\n",
      "epoch: 5 step: 1167, loss is 0.3456580638885498\n",
      "epoch: 5 step: 1168, loss is 0.6805437207221985\n",
      "epoch: 5 step: 1169, loss is 0.480657696723938\n",
      "epoch: 5 step: 1170, loss is 1.1458863019943237\n",
      "epoch: 5 step: 1171, loss is 0.5657273530960083\n",
      "epoch: 5 step: 1172, loss is 0.15833552181720734\n",
      "epoch: 5 step: 1173, loss is 0.2124224603176117\n",
      "epoch: 5 step: 1174, loss is 0.33191052079200745\n",
      "epoch: 5 step: 1175, loss is 0.2860909104347229\n",
      "epoch: 5 step: 1176, loss is 1.2637336254119873\n",
      "epoch: 5 step: 1177, loss is 0.4579467475414276\n",
      "epoch: 5 step: 1178, loss is 0.3753189146518707\n",
      "epoch: 5 step: 1179, loss is 0.4450565576553345\n",
      "epoch: 5 step: 1180, loss is 0.5424783229827881\n",
      "epoch: 5 step: 1181, loss is 0.6006805300712585\n",
      "epoch: 5 step: 1182, loss is 0.2965758740901947\n",
      "epoch: 5 step: 1183, loss is 0.1425236463546753\n",
      "epoch: 5 step: 1184, loss is 0.43099790811538696\n",
      "epoch: 5 step: 1185, loss is 0.5668119192123413\n",
      "epoch: 5 step: 1186, loss is 0.616352379322052\n",
      "epoch: 5 step: 1187, loss is 0.48687154054641724\n",
      "epoch: 5 step: 1188, loss is 0.37634339928627014\n",
      "epoch: 5 step: 1189, loss is 0.236175075173378\n",
      "epoch: 5 step: 1190, loss is 0.5009711980819702\n",
      "epoch: 5 step: 1191, loss is 0.4016767144203186\n",
      "epoch: 5 step: 1192, loss is 0.1891881376504898\n",
      "epoch: 5 step: 1193, loss is 0.505607545375824\n",
      "epoch: 5 step: 1194, loss is 0.5720145106315613\n",
      "epoch: 5 step: 1195, loss is 0.43707239627838135\n",
      "epoch: 5 step: 1196, loss is 0.2758946418762207\n",
      "epoch: 5 step: 1197, loss is 0.2909080386161804\n",
      "epoch: 5 step: 1198, loss is 0.70329350233078\n",
      "epoch: 5 step: 1199, loss is 0.3747049570083618\n",
      "epoch: 5 step: 1200, loss is 0.3334544599056244\n",
      "epoch: 5 step: 1201, loss is 0.4376338720321655\n",
      "epoch: 5 step: 1202, loss is 0.5452619791030884\n",
      "epoch: 5 step: 1203, loss is 0.6362048387527466\n",
      "epoch: 5 step: 1204, loss is 0.5166864395141602\n",
      "epoch: 5 step: 1205, loss is 0.5467141270637512\n",
      "epoch: 5 step: 1206, loss is 0.2327616959810257\n",
      "epoch: 5 step: 1207, loss is 0.3718812167644501\n",
      "epoch: 5 step: 1208, loss is 0.6283898949623108\n",
      "epoch: 5 step: 1209, loss is 0.2895392179489136\n",
      "epoch: 5 step: 1210, loss is 0.31514936685562134\n",
      "epoch: 5 step: 1211, loss is 0.6463354825973511\n",
      "epoch: 5 step: 1212, loss is 0.4488873779773712\n",
      "epoch: 5 step: 1213, loss is 0.5377663373947144\n",
      "epoch: 5 step: 1214, loss is 0.17190124094486237\n",
      "epoch: 5 step: 1215, loss is 0.4013538658618927\n",
      "epoch: 5 step: 1216, loss is 0.19379360973834991\n",
      "epoch: 5 step: 1217, loss is 0.3918992280960083\n",
      "epoch: 5 step: 1218, loss is 0.27660441398620605\n",
      "epoch: 5 step: 1219, loss is 0.6932838559150696\n",
      "epoch: 5 step: 1220, loss is 0.3467172384262085\n",
      "epoch: 5 step: 1221, loss is 0.6151799559593201\n",
      "epoch: 5 step: 1222, loss is 0.6941124200820923\n",
      "epoch: 5 step: 1223, loss is 0.30872994661331177\n",
      "epoch: 5 step: 1224, loss is 0.15953175723552704\n",
      "epoch: 5 step: 1225, loss is 0.44927978515625\n",
      "epoch: 5 step: 1226, loss is 0.20724691450595856\n",
      "epoch: 5 step: 1227, loss is 0.2254672646522522\n",
      "epoch: 5 step: 1228, loss is 0.32870492339134216\n",
      "epoch: 5 step: 1229, loss is 0.14275753498077393\n",
      "epoch: 5 step: 1230, loss is 0.7862478494644165\n",
      "epoch: 5 step: 1231, loss is 0.3192133605480194\n",
      "epoch: 5 step: 1232, loss is 0.5698449015617371\n",
      "epoch: 5 step: 1233, loss is 0.3521621823310852\n",
      "epoch: 5 step: 1234, loss is 0.21784788370132446\n",
      "epoch: 5 step: 1235, loss is 0.7156553864479065\n",
      "epoch: 5 step: 1236, loss is 0.08355267345905304\n",
      "epoch: 5 step: 1237, loss is 0.6414079070091248\n",
      "epoch: 5 step: 1238, loss is 0.15022790431976318\n",
      "epoch: 5 step: 1239, loss is 0.13055665791034698\n",
      "epoch: 5 step: 1240, loss is 0.29834288358688354\n",
      "epoch: 5 step: 1241, loss is 0.44082167744636536\n",
      "epoch: 5 step: 1242, loss is 0.6416276097297668\n",
      "epoch: 5 step: 1243, loss is 0.3569765090942383\n",
      "epoch: 5 step: 1244, loss is 0.3688971698284149\n",
      "epoch: 5 step: 1245, loss is 0.7411725521087646\n",
      "epoch: 5 step: 1246, loss is 0.28716835379600525\n",
      "epoch: 5 step: 1247, loss is 0.43217992782592773\n",
      "epoch: 5 step: 1248, loss is 0.6511453986167908\n",
      "epoch: 5 step: 1249, loss is 0.19627386331558228\n",
      "epoch: 5 step: 1250, loss is 0.15966002643108368\n",
      "epoch: 5 step: 1251, loss is 0.21547411382198334\n",
      "epoch: 5 step: 1252, loss is 0.25088927149772644\n",
      "epoch: 5 step: 1253, loss is 0.3647497594356537\n",
      "epoch: 5 step: 1254, loss is 0.24079295992851257\n",
      "epoch: 5 step: 1255, loss is 0.9207889437675476\n",
      "epoch: 5 step: 1256, loss is 0.48138830065727234\n",
      "epoch: 5 step: 1257, loss is 0.2892117202281952\n",
      "epoch: 5 step: 1258, loss is 0.29726746678352356\n",
      "epoch: 5 step: 1259, loss is 0.41893845796585083\n",
      "epoch: 5 step: 1260, loss is 0.246904656291008\n",
      "epoch: 5 step: 1261, loss is 0.2044232040643692\n",
      "epoch: 5 step: 1262, loss is 0.289299875497818\n",
      "epoch: 5 step: 1263, loss is 0.23064753413200378\n",
      "epoch: 5 step: 1264, loss is 0.28402185440063477\n",
      "epoch: 5 step: 1265, loss is 0.4672091007232666\n",
      "epoch: 5 step: 1266, loss is 0.2970644235610962\n",
      "epoch: 5 step: 1267, loss is 0.47120341658592224\n",
      "epoch: 5 step: 1268, loss is 0.14060236513614655\n",
      "epoch: 5 step: 1269, loss is 0.32733920216560364\n",
      "epoch: 5 step: 1270, loss is 0.5546424984931946\n",
      "epoch: 5 step: 1271, loss is 0.545444667339325\n",
      "epoch: 5 step: 1272, loss is 0.404716819524765\n",
      "epoch: 5 step: 1273, loss is 0.34836912155151367\n",
      "epoch: 5 step: 1274, loss is 0.25840306282043457\n",
      "epoch: 5 step: 1275, loss is 0.2455310821533203\n",
      "epoch: 5 step: 1276, loss is 0.4571688175201416\n",
      "epoch: 5 step: 1277, loss is 0.22267019748687744\n",
      "epoch: 5 step: 1278, loss is 0.5769391655921936\n",
      "epoch: 5 step: 1279, loss is 0.6748533248901367\n",
      "epoch: 5 step: 1280, loss is 0.2671676278114319\n",
      "epoch: 5 step: 1281, loss is 0.1372593194246292\n",
      "epoch: 5 step: 1282, loss is 0.7156848311424255\n",
      "epoch: 5 step: 1283, loss is 0.36492934823036194\n",
      "epoch: 5 step: 1284, loss is 0.1980646550655365\n",
      "epoch: 5 step: 1285, loss is 0.3577445447444916\n",
      "epoch: 5 step: 1286, loss is 0.6174309253692627\n",
      "epoch: 5 step: 1287, loss is 0.369343638420105\n",
      "epoch: 5 step: 1288, loss is 0.3004617989063263\n",
      "epoch: 5 step: 1289, loss is 0.8875867128372192\n",
      "epoch: 5 step: 1290, loss is 0.4460863769054413\n",
      "epoch: 5 step: 1291, loss is 0.9466936588287354\n",
      "epoch: 5 step: 1292, loss is 0.18561407923698425\n",
      "epoch: 5 step: 1293, loss is 0.4978470206260681\n",
      "epoch: 5 step: 1294, loss is 0.5706461668014526\n",
      "epoch: 5 step: 1295, loss is 0.5274815559387207\n",
      "epoch: 5 step: 1296, loss is 0.46556806564331055\n",
      "epoch: 5 step: 1297, loss is 0.23851174116134644\n",
      "epoch: 5 step: 1298, loss is 0.118632011115551\n",
      "epoch: 5 step: 1299, loss is 0.5112253427505493\n",
      "epoch: 5 step: 1300, loss is 0.2920655310153961\n",
      "epoch: 5 step: 1301, loss is 0.3954731225967407\n",
      "epoch: 5 step: 1302, loss is 0.23958109319210052\n",
      "epoch: 5 step: 1303, loss is 0.6815062165260315\n",
      "epoch: 5 step: 1304, loss is 0.6058904528617859\n",
      "epoch: 5 step: 1305, loss is 0.7408376932144165\n",
      "epoch: 5 step: 1306, loss is 0.5584632158279419\n",
      "epoch: 5 step: 1307, loss is 0.6795486807823181\n",
      "epoch: 5 step: 1308, loss is 0.5317733287811279\n",
      "epoch: 5 step: 1309, loss is 0.32454997301101685\n",
      "epoch: 5 step: 1310, loss is 0.6815404891967773\n",
      "epoch: 5 step: 1311, loss is 0.18740667402744293\n",
      "epoch: 5 step: 1312, loss is 0.27557748556137085\n",
      "epoch: 5 step: 1313, loss is 0.29642537236213684\n",
      "epoch: 5 step: 1314, loss is 0.44977736473083496\n",
      "epoch: 5 step: 1315, loss is 0.3872663080692291\n",
      "epoch: 5 step: 1316, loss is 0.5576533079147339\n",
      "epoch: 5 step: 1317, loss is 0.2664884328842163\n",
      "epoch: 5 step: 1318, loss is 0.18927158415317535\n",
      "epoch: 5 step: 1319, loss is 0.36844778060913086\n",
      "epoch: 5 step: 1320, loss is 0.2730134129524231\n",
      "epoch: 5 step: 1321, loss is 0.23646360635757446\n",
      "epoch: 5 step: 1322, loss is 0.6503681540489197\n",
      "epoch: 5 step: 1323, loss is 0.7986645102500916\n",
      "epoch: 5 step: 1324, loss is 0.6012346148490906\n",
      "epoch: 5 step: 1325, loss is 0.32190829515457153\n",
      "epoch: 5 step: 1326, loss is 0.595634400844574\n",
      "epoch: 5 step: 1327, loss is 0.4730570316314697\n",
      "epoch: 5 step: 1328, loss is 0.5521941781044006\n",
      "epoch: 5 step: 1329, loss is 0.32059231400489807\n",
      "epoch: 5 step: 1330, loss is 0.3952162563800812\n",
      "epoch: 5 step: 1331, loss is 0.3034607768058777\n",
      "epoch: 5 step: 1332, loss is 0.6712782382965088\n",
      "epoch: 5 step: 1333, loss is 0.7710005044937134\n",
      "epoch: 5 step: 1334, loss is 0.18399310111999512\n",
      "epoch: 5 step: 1335, loss is 0.35024121403694153\n",
      "epoch: 5 step: 1336, loss is 0.4034237563610077\n",
      "epoch: 5 step: 1337, loss is 0.3341559171676636\n",
      "epoch: 5 step: 1338, loss is 0.24904556572437286\n",
      "epoch: 5 step: 1339, loss is 0.8575348854064941\n",
      "epoch: 5 step: 1340, loss is 0.1477736234664917\n",
      "epoch: 5 step: 1341, loss is 0.19326427578926086\n",
      "epoch: 5 step: 1342, loss is 0.22498184442520142\n",
      "epoch: 5 step: 1343, loss is 0.479737251996994\n",
      "epoch: 5 step: 1344, loss is 0.49155497550964355\n",
      "epoch: 5 step: 1345, loss is 0.3572274446487427\n",
      "epoch: 5 step: 1346, loss is 0.6282666921615601\n",
      "epoch: 5 step: 1347, loss is 0.18984414637088776\n",
      "epoch: 5 step: 1348, loss is 0.6326642036437988\n",
      "epoch: 5 step: 1349, loss is 0.9461061954498291\n",
      "epoch: 5 step: 1350, loss is 0.5416823625564575\n",
      "epoch: 5 step: 1351, loss is 0.28637731075286865\n",
      "epoch: 5 step: 1352, loss is 0.521835207939148\n",
      "epoch: 5 step: 1353, loss is 0.3967812657356262\n",
      "epoch: 5 step: 1354, loss is 0.7091952562332153\n",
      "epoch: 5 step: 1355, loss is 0.2882044017314911\n",
      "epoch: 5 step: 1356, loss is 0.2949928045272827\n",
      "epoch: 5 step: 1357, loss is 0.6082583665847778\n",
      "epoch: 5 step: 1358, loss is 0.5462661981582642\n",
      "epoch: 5 step: 1359, loss is 0.42184945940971375\n",
      "epoch: 5 step: 1360, loss is 0.4545309245586395\n",
      "epoch: 5 step: 1361, loss is 0.13199903070926666\n",
      "epoch: 5 step: 1362, loss is 0.3535814583301544\n",
      "epoch: 5 step: 1363, loss is 0.4877503216266632\n",
      "epoch: 5 step: 1364, loss is 0.23494836688041687\n",
      "epoch: 5 step: 1365, loss is 0.27109837532043457\n",
      "epoch: 5 step: 1366, loss is 0.494102418422699\n",
      "epoch: 5 step: 1367, loss is 0.3708726167678833\n",
      "epoch: 5 step: 1368, loss is 0.13481666147708893\n",
      "epoch: 5 step: 1369, loss is 0.45553913712501526\n",
      "epoch: 5 step: 1370, loss is 0.7926503419876099\n",
      "epoch: 5 step: 1371, loss is 0.23861905932426453\n",
      "epoch: 5 step: 1372, loss is 0.2893306612968445\n",
      "epoch: 5 step: 1373, loss is 0.27236589789390564\n",
      "epoch: 5 step: 1374, loss is 0.3378642797470093\n",
      "epoch: 5 step: 1375, loss is 0.37400779128074646\n",
      "epoch: 5 step: 1376, loss is 0.2476082593202591\n",
      "epoch: 5 step: 1377, loss is 0.50240159034729\n",
      "epoch: 5 step: 1378, loss is 0.4762573540210724\n",
      "epoch: 5 step: 1379, loss is 0.41271504759788513\n",
      "epoch: 5 step: 1380, loss is 0.8929932117462158\n",
      "epoch: 5 step: 1381, loss is 0.38972920179367065\n",
      "epoch: 5 step: 1382, loss is 0.7248908877372742\n",
      "epoch: 5 step: 1383, loss is 0.4566746652126312\n",
      "epoch: 5 step: 1384, loss is 0.09324795007705688\n",
      "epoch: 5 step: 1385, loss is 0.385444700717926\n",
      "epoch: 5 step: 1386, loss is 0.6705560088157654\n",
      "epoch: 5 step: 1387, loss is 0.6263325810432434\n",
      "epoch: 5 step: 1388, loss is 0.27097970247268677\n",
      "epoch: 5 step: 1389, loss is 0.4001699686050415\n",
      "epoch: 5 step: 1390, loss is 0.4407459795475006\n",
      "epoch: 5 step: 1391, loss is 0.5274907946586609\n",
      "epoch: 5 step: 1392, loss is 0.3314943015575409\n",
      "epoch: 5 step: 1393, loss is 1.1219978332519531\n",
      "epoch: 5 step: 1394, loss is 0.38513076305389404\n",
      "epoch: 5 step: 1395, loss is 0.3107549250125885\n",
      "epoch: 5 step: 1396, loss is 0.3068747818470001\n",
      "epoch: 5 step: 1397, loss is 0.6487751007080078\n",
      "epoch: 5 step: 1398, loss is 0.5539106726646423\n",
      "epoch: 5 step: 1399, loss is 0.2553142309188843\n",
      "epoch: 5 step: 1400, loss is 0.2400328665971756\n",
      "epoch: 5 step: 1401, loss is 0.5986943244934082\n",
      "epoch: 5 step: 1402, loss is 0.6687933206558228\n",
      "epoch: 5 step: 1403, loss is 0.29320114850997925\n",
      "epoch: 5 step: 1404, loss is 0.553041934967041\n",
      "epoch: 5 step: 1405, loss is 0.4009389281272888\n",
      "epoch: 5 step: 1406, loss is 0.37998878955841064\n",
      "epoch: 5 step: 1407, loss is 0.39533674716949463\n",
      "epoch: 5 step: 1408, loss is 0.5064343214035034\n",
      "epoch: 5 step: 1409, loss is 0.4112478494644165\n",
      "epoch: 5 step: 1410, loss is 0.33028000593185425\n",
      "epoch: 5 step: 1411, loss is 0.36769023537635803\n",
      "epoch: 5 step: 1412, loss is 0.6468175649642944\n",
      "epoch: 5 step: 1413, loss is 0.17759986221790314\n",
      "epoch: 5 step: 1414, loss is 0.30286675691604614\n",
      "epoch: 5 step: 1415, loss is 0.3918692469596863\n",
      "epoch: 5 step: 1416, loss is 0.43423035740852356\n",
      "epoch: 5 step: 1417, loss is 0.16318003833293915\n",
      "epoch: 5 step: 1418, loss is 0.24974703788757324\n",
      "epoch: 5 step: 1419, loss is 0.2759445607662201\n",
      "epoch: 5 step: 1420, loss is 0.31864961981773376\n",
      "epoch: 5 step: 1421, loss is 0.45219388604164124\n",
      "epoch: 5 step: 1422, loss is 0.3462054133415222\n",
      "epoch: 5 step: 1423, loss is 0.22299565374851227\n",
      "epoch: 5 step: 1424, loss is 0.15013299882411957\n",
      "epoch: 5 step: 1425, loss is 0.5726680159568787\n",
      "epoch: 5 step: 1426, loss is 0.26925620436668396\n",
      "epoch: 5 step: 1427, loss is 0.2894498407840729\n",
      "epoch: 5 step: 1428, loss is 0.5395360589027405\n",
      "epoch: 5 step: 1429, loss is 0.5181734561920166\n",
      "epoch: 5 step: 1430, loss is 0.3040611147880554\n",
      "epoch: 5 step: 1431, loss is 0.3547845184803009\n",
      "epoch: 5 step: 1432, loss is 0.34956490993499756\n",
      "epoch: 5 step: 1433, loss is 0.4053521752357483\n",
      "epoch: 5 step: 1434, loss is 0.44767025113105774\n",
      "epoch: 5 step: 1435, loss is 0.10516754537820816\n",
      "epoch: 5 step: 1436, loss is 0.1400090605020523\n",
      "epoch: 5 step: 1437, loss is 0.3445236086845398\n",
      "epoch: 5 step: 1438, loss is 0.3963280916213989\n",
      "epoch: 5 step: 1439, loss is 0.3206813335418701\n",
      "epoch: 5 step: 1440, loss is 0.35937952995300293\n",
      "epoch: 5 step: 1441, loss is 0.2530970573425293\n",
      "epoch: 5 step: 1442, loss is 0.3896998167037964\n",
      "epoch: 5 step: 1443, loss is 0.1330011934041977\n",
      "epoch: 5 step: 1444, loss is 0.7728047370910645\n",
      "epoch: 5 step: 1445, loss is 0.5654048919677734\n",
      "epoch: 5 step: 1446, loss is 0.2796989381313324\n",
      "epoch: 5 step: 1447, loss is 0.37398162484169006\n",
      "epoch: 5 step: 1448, loss is 0.12846380472183228\n",
      "epoch: 5 step: 1449, loss is 0.10663378983736038\n",
      "epoch: 5 step: 1450, loss is 0.7693293690681458\n",
      "epoch: 5 step: 1451, loss is 1.0805447101593018\n",
      "epoch: 5 step: 1452, loss is 0.5981701016426086\n",
      "epoch: 5 step: 1453, loss is 0.3348608911037445\n",
      "epoch: 5 step: 1454, loss is 0.6569631099700928\n",
      "epoch: 5 step: 1455, loss is 0.7485826015472412\n",
      "epoch: 5 step: 1456, loss is 0.5082767605781555\n",
      "epoch: 5 step: 1457, loss is 0.38420137763023376\n",
      "epoch: 5 step: 1458, loss is 0.5255244970321655\n",
      "epoch: 5 step: 1459, loss is 0.40290454030036926\n",
      "epoch: 5 step: 1460, loss is 0.5484399199485779\n",
      "epoch: 5 step: 1461, loss is 0.7539424300193787\n",
      "epoch: 5 step: 1462, loss is 0.13565948605537415\n",
      "epoch: 5 step: 1463, loss is 0.3447082042694092\n",
      "epoch: 5 step: 1464, loss is 0.41602325439453125\n",
      "epoch: 5 step: 1465, loss is 0.3690362274646759\n",
      "epoch: 5 step: 1466, loss is 0.805006742477417\n",
      "epoch: 5 step: 1467, loss is 0.33335426449775696\n",
      "epoch: 5 step: 1468, loss is 0.5295729637145996\n",
      "epoch: 5 step: 1469, loss is 0.8329026103019714\n",
      "epoch: 5 step: 1470, loss is 0.575917661190033\n",
      "epoch: 5 step: 1471, loss is 0.4914119243621826\n",
      "epoch: 5 step: 1472, loss is 0.4621089696884155\n",
      "epoch: 5 step: 1473, loss is 0.6920038461685181\n",
      "epoch: 5 step: 1474, loss is 0.5632569193840027\n",
      "epoch: 5 step: 1475, loss is 0.5954968929290771\n",
      "epoch: 5 step: 1476, loss is 0.2509886920452118\n",
      "epoch: 5 step: 1477, loss is 0.2820328176021576\n",
      "epoch: 5 step: 1478, loss is 0.2954650819301605\n",
      "epoch: 5 step: 1479, loss is 0.47836029529571533\n",
      "epoch: 5 step: 1480, loss is 0.5069307684898376\n",
      "epoch: 5 step: 1481, loss is 0.4634750485420227\n",
      "epoch: 5 step: 1482, loss is 0.4705023169517517\n",
      "epoch: 5 step: 1483, loss is 0.3404388725757599\n",
      "epoch: 5 step: 1484, loss is 1.3093410730361938\n",
      "epoch: 5 step: 1485, loss is 0.46184468269348145\n",
      "epoch: 5 step: 1486, loss is 0.8297947645187378\n",
      "epoch: 5 step: 1487, loss is 0.2133668214082718\n",
      "epoch: 5 step: 1488, loss is 0.5250475406646729\n",
      "epoch: 5 step: 1489, loss is 0.4607037603855133\n",
      "epoch: 5 step: 1490, loss is 0.30715256929397583\n",
      "epoch: 5 step: 1491, loss is 0.18562951683998108\n",
      "epoch: 5 step: 1492, loss is 0.18551671504974365\n",
      "epoch: 5 step: 1493, loss is 0.24553658068180084\n",
      "epoch: 5 step: 1494, loss is 0.1744104027748108\n",
      "epoch: 5 step: 1495, loss is 0.39390304684638977\n",
      "epoch: 5 step: 1496, loss is 0.44436633586883545\n",
      "epoch: 5 step: 1497, loss is 0.7060298919677734\n",
      "epoch: 5 step: 1498, loss is 0.7991836071014404\n",
      "epoch: 5 step: 1499, loss is 0.42681077122688293\n",
      "epoch: 5 step: 1500, loss is 0.28047388792037964\n",
      "epoch: 5 step: 1501, loss is 0.4827796518802643\n",
      "epoch: 5 step: 1502, loss is 0.46795037388801575\n",
      "epoch: 5 step: 1503, loss is 0.48249390721321106\n",
      "epoch: 5 step: 1504, loss is 0.2450075000524521\n",
      "epoch: 5 step: 1505, loss is 0.4254205524921417\n",
      "epoch: 5 step: 1506, loss is 0.39229002594947815\n",
      "epoch: 5 step: 1507, loss is 0.3965654671192169\n",
      "epoch: 5 step: 1508, loss is 0.43627485632896423\n",
      "epoch: 5 step: 1509, loss is 0.4138678014278412\n",
      "epoch: 5 step: 1510, loss is 0.6677788496017456\n",
      "epoch: 5 step: 1511, loss is 0.5242815613746643\n",
      "epoch: 5 step: 1512, loss is 0.46117860078811646\n",
      "epoch: 5 step: 1513, loss is 0.2632196545600891\n",
      "epoch: 5 step: 1514, loss is 0.25801724195480347\n",
      "epoch: 5 step: 1515, loss is 0.2420465648174286\n",
      "epoch: 5 step: 1516, loss is 0.48526427149772644\n",
      "epoch: 5 step: 1517, loss is 0.32089394330978394\n",
      "epoch: 5 step: 1518, loss is 0.2503289580345154\n",
      "epoch: 5 step: 1519, loss is 0.3479267656803131\n",
      "epoch: 5 step: 1520, loss is 0.2727293074131012\n",
      "epoch: 5 step: 1521, loss is 0.38921836018562317\n",
      "epoch: 5 step: 1522, loss is 0.4261670708656311\n",
      "epoch: 5 step: 1523, loss is 0.4626198410987854\n",
      "epoch: 5 step: 1524, loss is 0.6276124119758606\n",
      "epoch: 5 step: 1525, loss is 0.3962593376636505\n",
      "epoch: 5 step: 1526, loss is 0.1518511176109314\n",
      "epoch: 5 step: 1527, loss is 0.34541213512420654\n",
      "epoch: 5 step: 1528, loss is 0.8023319244384766\n",
      "epoch: 5 step: 1529, loss is 0.3393213450908661\n",
      "epoch: 5 step: 1530, loss is 0.174082413315773\n",
      "epoch: 5 step: 1531, loss is 0.4544828534126282\n",
      "epoch: 5 step: 1532, loss is 0.5055227279663086\n",
      "epoch: 5 step: 1533, loss is 0.7406989336013794\n",
      "epoch: 5 step: 1534, loss is 0.5065407156944275\n",
      "epoch: 5 step: 1535, loss is 0.26472023129463196\n",
      "epoch: 5 step: 1536, loss is 0.3175754249095917\n",
      "epoch: 5 step: 1537, loss is 0.7372780442237854\n",
      "epoch: 5 step: 1538, loss is 0.4694376289844513\n",
      "epoch: 5 step: 1539, loss is 0.7656792402267456\n",
      "epoch: 5 step: 1540, loss is 0.7966107130050659\n",
      "epoch: 5 step: 1541, loss is 0.24709580838680267\n",
      "epoch: 5 step: 1542, loss is 0.12590369582176208\n",
      "epoch: 5 step: 1543, loss is 1.1610957384109497\n",
      "epoch: 5 step: 1544, loss is 0.40267491340637207\n",
      "epoch: 5 step: 1545, loss is 0.47985607385635376\n",
      "epoch: 5 step: 1546, loss is 0.6167199611663818\n",
      "epoch: 5 step: 1547, loss is 1.0787211656570435\n",
      "epoch: 5 step: 1548, loss is 0.3799721598625183\n",
      "epoch: 5 step: 1549, loss is 0.6921296715736389\n",
      "epoch: 5 step: 1550, loss is 1.4663156270980835\n",
      "epoch: 5 step: 1551, loss is 0.3219372630119324\n",
      "epoch: 5 step: 1552, loss is 0.5339250564575195\n",
      "epoch: 5 step: 1553, loss is 0.3559463620185852\n",
      "epoch: 5 step: 1554, loss is 0.45927006006240845\n",
      "epoch: 5 step: 1555, loss is 0.2452118694782257\n",
      "epoch: 5 step: 1556, loss is 0.27617180347442627\n",
      "epoch: 5 step: 1557, loss is 0.5470255017280579\n",
      "epoch: 5 step: 1558, loss is 0.2397194653749466\n",
      "epoch: 5 step: 1559, loss is 0.5451042056083679\n",
      "epoch: 5 step: 1560, loss is 0.20223326981067657\n",
      "epoch: 5 step: 1561, loss is 0.4589482843875885\n",
      "epoch: 5 step: 1562, loss is 0.792159378528595\n",
      "epoch: 5 step: 1563, loss is 0.40963706374168396\n",
      "epoch: 5 step: 1564, loss is 0.275417298078537\n",
      "epoch: 5 step: 1565, loss is 0.5548869967460632\n",
      "epoch: 5 step: 1566, loss is 0.19545115530490875\n",
      "epoch: 5 step: 1567, loss is 0.2729140818119049\n",
      "epoch: 5 step: 1568, loss is 0.4291653335094452\n",
      "epoch: 5 step: 1569, loss is 0.3387168049812317\n",
      "epoch: 5 step: 1570, loss is 0.6463589072227478\n",
      "epoch: 5 step: 1571, loss is 0.6598116159439087\n",
      "epoch: 5 step: 1572, loss is 0.29394254088401794\n",
      "epoch: 5 step: 1573, loss is 0.5447803735733032\n",
      "epoch: 5 step: 1574, loss is 0.4361535906791687\n",
      "epoch: 5 step: 1575, loss is 0.2449507862329483\n",
      "epoch: 5 step: 1576, loss is 0.3519698679447174\n",
      "epoch: 5 step: 1577, loss is 0.46394333243370056\n",
      "epoch: 5 step: 1578, loss is 0.26957473158836365\n",
      "epoch: 5 step: 1579, loss is 0.3761342763900757\n",
      "epoch: 5 step: 1580, loss is 0.3040129840373993\n",
      "epoch: 5 step: 1581, loss is 0.3171800971031189\n",
      "epoch: 5 step: 1582, loss is 0.1735266000032425\n",
      "epoch: 5 step: 1583, loss is 0.44495540857315063\n",
      "epoch: 5 step: 1584, loss is 0.536571204662323\n",
      "epoch: 5 step: 1585, loss is 0.2283858209848404\n",
      "epoch: 5 step: 1586, loss is 0.3705679774284363\n",
      "epoch: 5 step: 1587, loss is 0.5382205843925476\n",
      "epoch: 5 step: 1588, loss is 0.1979987919330597\n",
      "epoch: 5 step: 1589, loss is 0.33890944719314575\n",
      "epoch: 5 step: 1590, loss is 0.5452762842178345\n",
      "epoch: 5 step: 1591, loss is 0.21666693687438965\n",
      "epoch: 5 step: 1592, loss is 0.201226606965065\n",
      "epoch: 5 step: 1593, loss is 0.101737380027771\n",
      "epoch: 5 step: 1594, loss is 0.4135962724685669\n",
      "epoch: 5 step: 1595, loss is 0.09920980781316757\n",
      "epoch: 5 step: 1596, loss is 0.4268042743206024\n",
      "epoch: 5 step: 1597, loss is 0.359828919172287\n",
      "epoch: 5 step: 1598, loss is 0.4907510578632355\n",
      "epoch: 5 step: 1599, loss is 0.37981659173965454\n",
      "epoch: 5 step: 1600, loss is 0.22931547462940216\n",
      "epoch: 5 step: 1601, loss is 0.3738105297088623\n",
      "epoch: 5 step: 1602, loss is 0.3074828088283539\n",
      "epoch: 5 step: 1603, loss is 0.5452876687049866\n",
      "epoch: 5 step: 1604, loss is 0.26598161458969116\n",
      "epoch: 5 step: 1605, loss is 0.2844933867454529\n",
      "epoch: 5 step: 1606, loss is 0.13202384114265442\n",
      "epoch: 5 step: 1607, loss is 0.32210156321525574\n",
      "epoch: 5 step: 1608, loss is 0.3478100597858429\n",
      "epoch: 5 step: 1609, loss is 0.38645896315574646\n",
      "epoch: 5 step: 1610, loss is 0.4738912582397461\n",
      "epoch: 5 step: 1611, loss is 0.3897043466567993\n",
      "epoch: 5 step: 1612, loss is 0.8705524206161499\n",
      "epoch: 5 step: 1613, loss is 0.1655111163854599\n",
      "epoch: 5 step: 1614, loss is 0.7513297200202942\n",
      "epoch: 5 step: 1615, loss is 0.499993234872818\n",
      "epoch: 5 step: 1616, loss is 0.2712562680244446\n",
      "epoch: 5 step: 1617, loss is 0.1423785388469696\n",
      "epoch: 5 step: 1618, loss is 0.10751285403966904\n",
      "epoch: 5 step: 1619, loss is 0.14840470254421234\n",
      "epoch: 5 step: 1620, loss is 0.13385064899921417\n",
      "epoch: 5 step: 1621, loss is 0.3138033449649811\n",
      "epoch: 5 step: 1622, loss is 0.26284193992614746\n",
      "epoch: 5 step: 1623, loss is 0.46841686964035034\n",
      "epoch: 5 step: 1624, loss is 0.07866505533456802\n",
      "epoch: 5 step: 1625, loss is 0.8426898121833801\n",
      "epoch: 5 step: 1626, loss is 0.14520488679409027\n",
      "epoch: 5 step: 1627, loss is 0.2604422867298126\n",
      "epoch: 5 step: 1628, loss is 0.09201884269714355\n",
      "epoch: 5 step: 1629, loss is 0.7938612103462219\n",
      "epoch: 5 step: 1630, loss is 0.2285883128643036\n",
      "epoch: 5 step: 1631, loss is 0.09063204377889633\n",
      "epoch: 5 step: 1632, loss is 0.2137502282857895\n",
      "epoch: 5 step: 1633, loss is 0.4129945635795593\n",
      "epoch: 5 step: 1634, loss is 0.7351605892181396\n",
      "epoch: 5 step: 1635, loss is 0.8326656818389893\n",
      "epoch: 5 step: 1636, loss is 0.09092672169208527\n",
      "epoch: 5 step: 1637, loss is 0.48670196533203125\n",
      "epoch: 5 step: 1638, loss is 0.6552776098251343\n",
      "epoch: 5 step: 1639, loss is 0.8439604043960571\n",
      "epoch: 5 step: 1640, loss is 0.733034610748291\n",
      "epoch: 5 step: 1641, loss is 0.5321675539016724\n",
      "epoch: 5 step: 1642, loss is 0.31001245975494385\n",
      "epoch: 5 step: 1643, loss is 0.2002895027399063\n",
      "epoch: 5 step: 1644, loss is 1.0700017213821411\n",
      "epoch: 5 step: 1645, loss is 0.15960495173931122\n",
      "epoch: 5 step: 1646, loss is 0.486164391040802\n",
      "epoch: 5 step: 1647, loss is 0.29300811886787415\n",
      "epoch: 5 step: 1648, loss is 0.26205480098724365\n",
      "epoch: 5 step: 1649, loss is 0.33782511949539185\n",
      "epoch: 5 step: 1650, loss is 0.6587054133415222\n",
      "epoch: 5 step: 1651, loss is 0.335582971572876\n",
      "epoch: 5 step: 1652, loss is 0.994600236415863\n",
      "epoch: 5 step: 1653, loss is 0.25303301215171814\n",
      "epoch: 5 step: 1654, loss is 0.38899293541908264\n",
      "epoch: 5 step: 1655, loss is 0.4338882863521576\n",
      "epoch: 5 step: 1656, loss is 0.21330302953720093\n",
      "epoch: 5 step: 1657, loss is 0.38072308897972107\n",
      "epoch: 5 step: 1658, loss is 0.15704837441444397\n",
      "epoch: 5 step: 1659, loss is 0.524389386177063\n",
      "epoch: 5 step: 1660, loss is 1.0948264598846436\n",
      "epoch: 5 step: 1661, loss is 0.20353654026985168\n",
      "epoch: 5 step: 1662, loss is 0.45989200472831726\n",
      "epoch: 5 step: 1663, loss is 0.5847954750061035\n",
      "epoch: 5 step: 1664, loss is 0.7523579001426697\n",
      "epoch: 5 step: 1665, loss is 0.4511498510837555\n",
      "epoch: 5 step: 1666, loss is 0.7061206102371216\n",
      "epoch: 5 step: 1667, loss is 0.4310944676399231\n",
      "epoch: 5 step: 1668, loss is 0.24927540123462677\n",
      "epoch: 5 step: 1669, loss is 0.4169999957084656\n",
      "epoch: 5 step: 1670, loss is 0.30184102058410645\n",
      "epoch: 5 step: 1671, loss is 0.35875019431114197\n",
      "epoch: 5 step: 1672, loss is 0.2589058578014374\n",
      "epoch: 5 step: 1673, loss is 0.38077253103256226\n",
      "epoch: 5 step: 1674, loss is 0.22341766953468323\n",
      "epoch: 5 step: 1675, loss is 0.6212401986122131\n",
      "epoch: 5 step: 1676, loss is 0.564845860004425\n",
      "epoch: 5 step: 1677, loss is 0.40549618005752563\n",
      "epoch: 5 step: 1678, loss is 0.46869000792503357\n",
      "epoch: 5 step: 1679, loss is 0.5153812170028687\n",
      "epoch: 5 step: 1680, loss is 0.3943561911582947\n",
      "epoch: 5 step: 1681, loss is 0.5307601094245911\n",
      "epoch: 5 step: 1682, loss is 0.33874666690826416\n",
      "epoch: 5 step: 1683, loss is 0.34916168451309204\n",
      "epoch: 5 step: 1684, loss is 0.35749050974845886\n",
      "epoch: 5 step: 1685, loss is 0.17640133202075958\n",
      "epoch: 5 step: 1686, loss is 0.6400217413902283\n",
      "epoch: 5 step: 1687, loss is 0.25056058168411255\n",
      "epoch: 5 step: 1688, loss is 0.4498569071292877\n",
      "epoch: 5 step: 1689, loss is 0.5457490682601929\n",
      "epoch: 5 step: 1690, loss is 0.5869796872138977\n",
      "epoch: 5 step: 1691, loss is 0.6967908143997192\n",
      "epoch: 5 step: 1692, loss is 0.6906095743179321\n",
      "epoch: 5 step: 1693, loss is 0.45786917209625244\n",
      "epoch: 5 step: 1694, loss is 0.3804079592227936\n",
      "epoch: 5 step: 1695, loss is 0.5479551553726196\n",
      "epoch: 5 step: 1696, loss is 0.4469447135925293\n",
      "epoch: 5 step: 1697, loss is 0.46239975094795227\n",
      "epoch: 5 step: 1698, loss is 0.37422963976860046\n",
      "epoch: 5 step: 1699, loss is 0.4721229672431946\n",
      "epoch: 5 step: 1700, loss is 0.7195830345153809\n",
      "epoch: 5 step: 1701, loss is 0.30373772978782654\n",
      "epoch: 5 step: 1702, loss is 0.3530878722667694\n",
      "epoch: 5 step: 1703, loss is 0.7957456707954407\n",
      "epoch: 5 step: 1704, loss is 0.4087589979171753\n",
      "epoch: 5 step: 1705, loss is 0.2738262116909027\n",
      "epoch: 5 step: 1706, loss is 0.6480635404586792\n",
      "epoch: 5 step: 1707, loss is 0.44735464453697205\n",
      "epoch: 5 step: 1708, loss is 0.5508157014846802\n",
      "epoch: 5 step: 1709, loss is 0.28446507453918457\n",
      "epoch: 5 step: 1710, loss is 0.391196608543396\n",
      "epoch: 5 step: 1711, loss is 0.4609786570072174\n",
      "epoch: 5 step: 1712, loss is 0.6313297152519226\n",
      "epoch: 5 step: 1713, loss is 0.7090153098106384\n",
      "epoch: 5 step: 1714, loss is 0.709450900554657\n",
      "epoch: 5 step: 1715, loss is 0.25413769483566284\n",
      "epoch: 5 step: 1716, loss is 0.43813398480415344\n",
      "epoch: 5 step: 1717, loss is 0.43944844603538513\n",
      "epoch: 5 step: 1718, loss is 0.4571295976638794\n",
      "epoch: 5 step: 1719, loss is 0.7658601999282837\n",
      "epoch: 5 step: 1720, loss is 0.43906810879707336\n",
      "epoch: 5 step: 1721, loss is 0.7018951177597046\n",
      "epoch: 5 step: 1722, loss is 0.5042895674705505\n",
      "epoch: 5 step: 1723, loss is 0.37079280614852905\n",
      "epoch: 5 step: 1724, loss is 0.29260239005088806\n",
      "epoch: 5 step: 1725, loss is 0.4310840666294098\n",
      "epoch: 5 step: 1726, loss is 0.4299958646297455\n",
      "epoch: 5 step: 1727, loss is 0.49141302704811096\n",
      "epoch: 5 step: 1728, loss is 0.3502110540866852\n",
      "epoch: 5 step: 1729, loss is 0.27582815289497375\n",
      "epoch: 5 step: 1730, loss is 0.3634175956249237\n",
      "epoch: 5 step: 1731, loss is 0.7824565768241882\n",
      "epoch: 5 step: 1732, loss is 0.6938278079032898\n",
      "epoch: 5 step: 1733, loss is 0.31019988656044006\n",
      "epoch: 5 step: 1734, loss is 0.5161057114601135\n",
      "epoch: 5 step: 1735, loss is 0.2118622362613678\n",
      "epoch: 5 step: 1736, loss is 0.27701714634895325\n",
      "epoch: 5 step: 1737, loss is 0.40480831265449524\n",
      "epoch: 5 step: 1738, loss is 0.4011661410331726\n",
      "epoch: 5 step: 1739, loss is 0.3721449077129364\n",
      "epoch: 5 step: 1740, loss is 0.6048126816749573\n",
      "epoch: 5 step: 1741, loss is 0.7916207909584045\n",
      "epoch: 5 step: 1742, loss is 0.48860883712768555\n",
      "epoch: 5 step: 1743, loss is 0.6907496452331543\n",
      "epoch: 5 step: 1744, loss is 0.42812874913215637\n",
      "epoch: 5 step: 1745, loss is 0.8474665880203247\n",
      "epoch: 5 step: 1746, loss is 0.5981758236885071\n",
      "epoch: 5 step: 1747, loss is 0.5680261254310608\n",
      "epoch: 5 step: 1748, loss is 0.49104613065719604\n",
      "epoch: 5 step: 1749, loss is 0.1837085783481598\n",
      "epoch: 5 step: 1750, loss is 0.5586932301521301\n",
      "epoch: 5 step: 1751, loss is 0.46576571464538574\n",
      "epoch: 5 step: 1752, loss is 0.2659231126308441\n",
      "epoch: 5 step: 1753, loss is 0.2823330760002136\n",
      "epoch: 5 step: 1754, loss is 0.3588062524795532\n",
      "epoch: 5 step: 1755, loss is 0.3945172131061554\n",
      "epoch: 5 step: 1756, loss is 0.362170934677124\n",
      "epoch: 5 step: 1757, loss is 0.5299205183982849\n",
      "epoch: 5 step: 1758, loss is 0.46935826539993286\n",
      "epoch: 5 step: 1759, loss is 0.2772827744483948\n",
      "epoch: 5 step: 1760, loss is 0.20844513177871704\n",
      "epoch: 5 step: 1761, loss is 0.5771018862724304\n",
      "epoch: 5 step: 1762, loss is 0.8265461325645447\n",
      "epoch: 5 step: 1763, loss is 0.7466938495635986\n",
      "epoch: 5 step: 1764, loss is 0.42533427476882935\n",
      "epoch: 5 step: 1765, loss is 0.15312820672988892\n",
      "epoch: 5 step: 1766, loss is 0.45048096776008606\n",
      "epoch: 5 step: 1767, loss is 0.6548035144805908\n",
      "epoch: 5 step: 1768, loss is 0.548664927482605\n",
      "epoch: 5 step: 1769, loss is 0.698354959487915\n",
      "epoch: 5 step: 1770, loss is 0.34741777181625366\n",
      "epoch: 5 step: 1771, loss is 0.27503591775894165\n",
      "epoch: 5 step: 1772, loss is 0.318048357963562\n",
      "epoch: 5 step: 1773, loss is 0.19007530808448792\n",
      "epoch: 5 step: 1774, loss is 0.34724825620651245\n",
      "epoch: 5 step: 1775, loss is 0.41725027561187744\n",
      "epoch: 5 step: 1776, loss is 0.231047123670578\n",
      "epoch: 5 step: 1777, loss is 0.3860662579536438\n",
      "epoch: 5 step: 1778, loss is 0.6074618101119995\n",
      "epoch: 5 step: 1779, loss is 0.2342858463525772\n",
      "epoch: 5 step: 1780, loss is 0.7439302802085876\n",
      "epoch: 5 step: 1781, loss is 0.3023013174533844\n",
      "epoch: 5 step: 1782, loss is 0.3908158242702484\n",
      "epoch: 5 step: 1783, loss is 0.15222476422786713\n",
      "epoch: 5 step: 1784, loss is 0.4114820063114166\n",
      "epoch: 5 step: 1785, loss is 0.3941028416156769\n",
      "epoch: 5 step: 1786, loss is 0.7494606375694275\n",
      "epoch: 5 step: 1787, loss is 0.3916585147380829\n",
      "epoch: 5 step: 1788, loss is 0.25981563329696655\n",
      "epoch: 5 step: 1789, loss is 0.6011105179786682\n",
      "epoch: 5 step: 1790, loss is 0.6481349468231201\n",
      "epoch: 5 step: 1791, loss is 0.3026961088180542\n",
      "epoch: 5 step: 1792, loss is 0.2487565129995346\n",
      "epoch: 5 step: 1793, loss is 0.9182345271110535\n",
      "epoch: 5 step: 1794, loss is 0.4797307252883911\n",
      "epoch: 5 step: 1795, loss is 0.48091498017311096\n",
      "epoch: 5 step: 1796, loss is 0.2375895082950592\n",
      "epoch: 5 step: 1797, loss is 0.7218586206436157\n",
      "epoch: 5 step: 1798, loss is 0.5832147598266602\n",
      "epoch: 5 step: 1799, loss is 0.42194145917892456\n",
      "epoch: 5 step: 1800, loss is 0.21642889082431793\n",
      "epoch: 5 step: 1801, loss is 0.11989874392747879\n",
      "epoch: 5 step: 1802, loss is 0.23596608638763428\n",
      "epoch: 5 step: 1803, loss is 0.40115901827812195\n",
      "epoch: 5 step: 1804, loss is 0.3006076216697693\n",
      "epoch: 5 step: 1805, loss is 0.8870381116867065\n",
      "epoch: 5 step: 1806, loss is 0.29062241315841675\n",
      "epoch: 5 step: 1807, loss is 0.41933998465538025\n",
      "epoch: 5 step: 1808, loss is 0.32490864396095276\n",
      "epoch: 5 step: 1809, loss is 0.25481539964675903\n",
      "epoch: 5 step: 1810, loss is 0.21624936163425446\n",
      "epoch: 5 step: 1811, loss is 0.34551334381103516\n",
      "epoch: 5 step: 1812, loss is 0.5466576814651489\n",
      "epoch: 5 step: 1813, loss is 0.45514756441116333\n",
      "epoch: 5 step: 1814, loss is 0.4648420810699463\n",
      "epoch: 5 step: 1815, loss is 0.30738282203674316\n",
      "epoch: 5 step: 1816, loss is 0.2639618515968323\n",
      "epoch: 5 step: 1817, loss is 1.2107493877410889\n",
      "epoch: 5 step: 1818, loss is 0.46793320775032043\n",
      "epoch: 5 step: 1819, loss is 0.355301171541214\n",
      "epoch: 5 step: 1820, loss is 0.33202022314071655\n",
      "epoch: 5 step: 1821, loss is 0.40335291624069214\n",
      "epoch: 5 step: 1822, loss is 0.22230862081050873\n",
      "epoch: 5 step: 1823, loss is 0.3597636818885803\n",
      "epoch: 5 step: 1824, loss is 0.4882160425186157\n",
      "epoch: 5 step: 1825, loss is 0.28734010457992554\n",
      "epoch: 5 step: 1826, loss is 0.7699877023696899\n",
      "epoch: 5 step: 1827, loss is 0.27923813462257385\n",
      "epoch: 5 step: 1828, loss is 0.29794156551361084\n",
      "epoch: 5 step: 1829, loss is 0.41111600399017334\n",
      "epoch: 5 step: 1830, loss is 0.513734757900238\n",
      "epoch: 5 step: 1831, loss is 0.19180896878242493\n",
      "epoch: 5 step: 1832, loss is 0.2764569818973541\n",
      "epoch: 5 step: 1833, loss is 0.242487832903862\n",
      "epoch: 5 step: 1834, loss is 0.3508279025554657\n",
      "epoch: 5 step: 1835, loss is 0.38606545329093933\n",
      "epoch: 5 step: 1836, loss is 0.9720908999443054\n",
      "epoch: 5 step: 1837, loss is 0.31063762307167053\n",
      "epoch: 5 step: 1838, loss is 0.8563570976257324\n",
      "epoch: 5 step: 1839, loss is 0.3934451937675476\n",
      "epoch: 5 step: 1840, loss is 0.5058814287185669\n",
      "epoch: 5 step: 1841, loss is 0.20825275778770447\n",
      "epoch: 5 step: 1842, loss is 0.8956099152565002\n",
      "epoch: 5 step: 1843, loss is 0.2460229992866516\n",
      "epoch: 5 step: 1844, loss is 0.44472363591194153\n",
      "epoch: 5 step: 1845, loss is 0.5706722140312195\n",
      "epoch: 5 step: 1846, loss is 0.5592515468597412\n",
      "epoch: 5 step: 1847, loss is 0.5783146023750305\n",
      "epoch: 5 step: 1848, loss is 0.2727397680282593\n",
      "epoch: 5 step: 1849, loss is 0.2000357061624527\n",
      "epoch: 5 step: 1850, loss is 0.38795986771583557\n",
      "epoch: 5 step: 1851, loss is 0.17593194544315338\n",
      "epoch: 5 step: 1852, loss is 0.5899994373321533\n",
      "epoch: 5 step: 1853, loss is 0.40050241351127625\n",
      "epoch: 5 step: 1854, loss is 0.5856744647026062\n",
      "epoch: 5 step: 1855, loss is 0.6134319305419922\n",
      "epoch: 5 step: 1856, loss is 0.43526047468185425\n",
      "epoch: 5 step: 1857, loss is 0.472154438495636\n",
      "epoch: 5 step: 1858, loss is 0.2242356836795807\n",
      "epoch: 5 step: 1859, loss is 0.3761370778083801\n",
      "epoch: 5 step: 1860, loss is 0.2111535370349884\n",
      "epoch: 5 step: 1861, loss is 0.5869803428649902\n",
      "epoch: 5 step: 1862, loss is 1.0497061014175415\n",
      "epoch: 5 step: 1863, loss is 0.26341453194618225\n",
      "epoch: 5 step: 1864, loss is 0.39764654636383057\n",
      "epoch: 5 step: 1865, loss is 0.6781742572784424\n",
      "epoch: 5 step: 1866, loss is 0.24266670644283295\n",
      "epoch: 5 step: 1867, loss is 0.23046176135540009\n",
      "epoch: 5 step: 1868, loss is 0.5323665142059326\n",
      "epoch: 5 step: 1869, loss is 0.5721062421798706\n",
      "epoch: 5 step: 1870, loss is 0.5706022381782532\n",
      "epoch: 5 step: 1871, loss is 0.48300647735595703\n",
      "epoch: 5 step: 1872, loss is 0.5667101144790649\n",
      "epoch: 5 step: 1873, loss is 0.17877399921417236\n",
      "epoch: 5 step: 1874, loss is 0.2549782991409302\n",
      "epoch: 5 step: 1875, loss is 0.6178134679794312\n",
      "epoch: 5 step: 1876, loss is 0.24496737122535706\n",
      "epoch: 5 step: 1877, loss is 0.5115540027618408\n",
      "epoch: 5 step: 1878, loss is 0.4510877728462219\n",
      "epoch: 5 step: 1879, loss is 0.9948478937149048\n",
      "epoch: 5 step: 1880, loss is 0.3426229953765869\n",
      "epoch: 5 step: 1881, loss is 0.48794180154800415\n",
      "epoch: 5 step: 1882, loss is 0.3350434899330139\n",
      "epoch: 5 step: 1883, loss is 0.4399319887161255\n",
      "epoch: 5 step: 1884, loss is 0.27817782759666443\n",
      "epoch: 5 step: 1885, loss is 0.6172017455101013\n",
      "epoch: 5 step: 1886, loss is 0.25868916511535645\n",
      "epoch: 5 step: 1887, loss is 0.3732384443283081\n",
      "epoch: 5 step: 1888, loss is 0.47820454835891724\n",
      "epoch: 5 step: 1889, loss is 0.26803460717201233\n",
      "epoch: 5 step: 1890, loss is 0.4096810817718506\n",
      "epoch: 5 step: 1891, loss is 0.41260817646980286\n",
      "epoch: 5 step: 1892, loss is 0.3988211154937744\n",
      "epoch: 5 step: 1893, loss is 0.26310792565345764\n",
      "epoch: 5 step: 1894, loss is 0.1824415624141693\n",
      "epoch: 5 step: 1895, loss is 0.616407036781311\n",
      "epoch: 5 step: 1896, loss is 0.2342246174812317\n",
      "epoch: 5 step: 1897, loss is 0.8231196403503418\n",
      "epoch: 5 step: 1898, loss is 0.33145084977149963\n",
      "epoch: 5 step: 1899, loss is 0.23375141620635986\n",
      "epoch: 5 step: 1900, loss is 0.16117820143699646\n",
      "epoch: 5 step: 1901, loss is 0.30229565501213074\n",
      "epoch: 5 step: 1902, loss is 0.9528663158416748\n",
      "epoch: 5 step: 1903, loss is 0.13100747764110565\n",
      "epoch: 5 step: 1904, loss is 0.4692239463329315\n",
      "epoch: 5 step: 1905, loss is 0.3696421682834625\n",
      "epoch: 5 step: 1906, loss is 0.5032892823219299\n",
      "epoch: 5 step: 1907, loss is 0.47333428263664246\n",
      "epoch: 5 step: 1908, loss is 0.5125413537025452\n",
      "epoch: 5 step: 1909, loss is 0.14843176305294037\n",
      "epoch: 5 step: 1910, loss is 0.14583101868629456\n",
      "epoch: 5 step: 1911, loss is 0.3274691104888916\n",
      "epoch: 5 step: 1912, loss is 0.2911692261695862\n",
      "epoch: 5 step: 1913, loss is 0.3571413457393646\n",
      "epoch: 5 step: 1914, loss is 0.3849009871482849\n",
      "epoch: 5 step: 1915, loss is 0.7138844728469849\n",
      "epoch: 5 step: 1916, loss is 0.21051998436450958\n",
      "epoch: 5 step: 1917, loss is 0.5493823289871216\n",
      "epoch: 5 step: 1918, loss is 0.9089310169219971\n",
      "epoch: 5 step: 1919, loss is 0.20974914729595184\n",
      "epoch: 5 step: 1920, loss is 0.3667837679386139\n",
      "epoch: 5 step: 1921, loss is 0.2624638080596924\n",
      "epoch: 5 step: 1922, loss is 0.848090410232544\n",
      "epoch: 5 step: 1923, loss is 1.026759147644043\n",
      "epoch: 5 step: 1924, loss is 0.27585768699645996\n",
      "epoch: 5 step: 1925, loss is 0.45475926995277405\n",
      "epoch: 5 step: 1926, loss is 0.2705639600753784\n",
      "epoch: 5 step: 1927, loss is 0.14948967099189758\n",
      "epoch: 5 step: 1928, loss is 0.6447830200195312\n",
      "epoch: 5 step: 1929, loss is 0.5477993488311768\n",
      "epoch: 5 step: 1930, loss is 0.5755136013031006\n",
      "epoch: 5 step: 1931, loss is 0.17456918954849243\n",
      "epoch: 5 step: 1932, loss is 0.5114511847496033\n",
      "epoch: 5 step: 1933, loss is 0.6761852502822876\n",
      "epoch: 5 step: 1934, loss is 0.6053913831710815\n",
      "epoch: 5 step: 1935, loss is 0.6104549169540405\n",
      "epoch: 5 step: 1936, loss is 0.6143883466720581\n",
      "epoch: 5 step: 1937, loss is 0.37606143951416016\n",
      "epoch: 5 step: 1938, loss is 0.22012032568454742\n",
      "epoch: 5 step: 1939, loss is 0.391436904668808\n",
      "epoch: 5 step: 1940, loss is 0.27457165718078613\n",
      "epoch: 5 step: 1941, loss is 0.34119120240211487\n",
      "epoch: 5 step: 1942, loss is 0.1657242327928543\n",
      "epoch: 5 step: 1943, loss is 0.3413379192352295\n",
      "epoch: 5 step: 1944, loss is 0.4114585220813751\n",
      "epoch: 5 step: 1945, loss is 0.17919453978538513\n",
      "epoch: 5 step: 1946, loss is 0.37963131070137024\n",
      "epoch: 5 step: 1947, loss is 0.2831904888153076\n",
      "epoch: 5 step: 1948, loss is 0.2679918110370636\n",
      "epoch: 5 step: 1949, loss is 0.40040087699890137\n",
      "epoch: 5 step: 1950, loss is 0.5016595125198364\n",
      "epoch: 5 step: 1951, loss is 0.4845817983150482\n",
      "epoch: 5 step: 1952, loss is 0.11542677879333496\n",
      "epoch: 5 step: 1953, loss is 0.33086085319519043\n",
      "epoch: 5 step: 1954, loss is 0.13693393766880035\n",
      "epoch: 5 step: 1955, loss is 0.5574686527252197\n",
      "epoch: 5 step: 1956, loss is 0.22613435983657837\n",
      "epoch: 5 step: 1957, loss is 0.540759265422821\n",
      "epoch: 5 step: 1958, loss is 0.4299941658973694\n",
      "epoch: 5 step: 1959, loss is 0.2945786118507385\n",
      "epoch: 5 step: 1960, loss is 0.5599277019500732\n",
      "epoch: 5 step: 1961, loss is 0.5509162545204163\n",
      "epoch: 5 step: 1962, loss is 0.8205165266990662\n",
      "epoch: 5 step: 1963, loss is 0.9339029788970947\n",
      "epoch: 5 step: 1964, loss is 0.3333445191383362\n",
      "epoch: 5 step: 1965, loss is 0.3757774233818054\n",
      "epoch: 5 step: 1966, loss is 0.1891622245311737\n",
      "epoch: 5 step: 1967, loss is 0.42198488116264343\n",
      "epoch: 5 step: 1968, loss is 0.5063717365264893\n",
      "epoch: 5 step: 1969, loss is 0.8429977297782898\n",
      "epoch: 5 step: 1970, loss is 0.2754364013671875\n",
      "epoch: 5 step: 1971, loss is 0.8256744146347046\n",
      "epoch: 5 step: 1972, loss is 0.29088813066482544\n",
      "epoch: 5 step: 1973, loss is 0.6175944209098816\n",
      "epoch: 5 step: 1974, loss is 0.16402195394039154\n",
      "epoch: 5 step: 1975, loss is 0.6018593311309814\n",
      "epoch: 5 step: 1976, loss is 0.4267702102661133\n",
      "epoch: 5 step: 1977, loss is 0.3801988661289215\n",
      "epoch: 5 step: 1978, loss is 0.5703659057617188\n",
      "epoch: 5 step: 1979, loss is 0.5148608088493347\n",
      "epoch: 5 step: 1980, loss is 0.6033748984336853\n",
      "epoch: 5 step: 1981, loss is 0.22746898233890533\n",
      "epoch: 5 step: 1982, loss is 0.3991762399673462\n",
      "epoch: 5 step: 1983, loss is 0.34583479166030884\n",
      "epoch: 5 step: 1984, loss is 0.48601219058036804\n",
      "epoch: 5 step: 1985, loss is 0.5046440958976746\n",
      "epoch: 5 step: 1986, loss is 0.5822356343269348\n",
      "epoch: 5 step: 1987, loss is 0.6585864424705505\n",
      "epoch: 5 step: 1988, loss is 0.3321588933467865\n",
      "epoch: 5 step: 1989, loss is 0.7826439142227173\n",
      "epoch: 5 step: 1990, loss is 0.22546422481536865\n",
      "epoch: 5 step: 1991, loss is 0.3547447919845581\n",
      "epoch: 5 step: 1992, loss is 0.1809900403022766\n",
      "epoch: 5 step: 1993, loss is 0.5389416217803955\n",
      "epoch: 5 step: 1994, loss is 0.24079519510269165\n",
      "epoch: 5 step: 1995, loss is 0.24442824721336365\n",
      "epoch: 5 step: 1996, loss is 0.4108915627002716\n",
      "epoch: 5 step: 1997, loss is 0.39737480878829956\n",
      "epoch: 5 step: 1998, loss is 0.392080694437027\n",
      "epoch: 5 step: 1999, loss is 0.5289878845214844\n",
      "epoch: 5 step: 2000, loss is 0.3123399615287781\n",
      "epoch: 5 step: 2001, loss is 0.12972088158130646\n",
      "epoch: 5 step: 2002, loss is 0.2975316345691681\n",
      "epoch: 5 step: 2003, loss is 0.32321810722351074\n",
      "epoch: 5 step: 2004, loss is 0.3824329972267151\n",
      "epoch: 5 step: 2005, loss is 0.24796883761882782\n",
      "epoch: 5 step: 2006, loss is 0.36359480023384094\n",
      "epoch: 5 step: 2007, loss is 0.30317071080207825\n",
      "epoch: 5 step: 2008, loss is 0.31677427887916565\n",
      "epoch: 5 step: 2009, loss is 0.21318042278289795\n",
      "epoch: 5 step: 2010, loss is 0.2908767759799957\n",
      "epoch: 5 step: 2011, loss is 0.41596293449401855\n",
      "epoch: 5 step: 2012, loss is 0.27743783593177795\n",
      "epoch: 5 step: 2013, loss is 0.19892480969429016\n",
      "epoch: 5 step: 2014, loss is 0.41198036074638367\n",
      "epoch: 5 step: 2015, loss is 0.40800029039382935\n",
      "epoch: 5 step: 2016, loss is 0.24932119250297546\n",
      "epoch: 5 step: 2017, loss is 0.3922046422958374\n",
      "epoch: 5 step: 2018, loss is 0.5383268594741821\n",
      "epoch: 5 step: 2019, loss is 0.4316549599170685\n",
      "epoch: 5 step: 2020, loss is 0.5495355725288391\n",
      "epoch: 5 step: 2021, loss is 0.1458473950624466\n",
      "epoch: 5 step: 2022, loss is 0.6971134543418884\n",
      "epoch: 5 step: 2023, loss is 0.9020252823829651\n",
      "epoch: 5 step: 2024, loss is 0.3820247948169708\n",
      "epoch: 5 step: 2025, loss is 0.26617667078971863\n",
      "epoch: 5 step: 2026, loss is 0.20439518988132477\n",
      "epoch: 5 step: 2027, loss is 0.1420348584651947\n",
      "epoch: 5 step: 2028, loss is 0.48951107263565063\n",
      "epoch: 5 step: 2029, loss is 0.13962580263614655\n",
      "epoch: 5 step: 2030, loss is 0.4640118479728699\n",
      "epoch: 5 step: 2031, loss is 0.4313284456729889\n",
      "epoch: 5 step: 2032, loss is 0.4835570752620697\n",
      "epoch: 5 step: 2033, loss is 0.3286499083042145\n",
      "epoch: 5 step: 2034, loss is 0.2871949076652527\n",
      "epoch: 5 step: 2035, loss is 0.32508185505867004\n",
      "epoch: 5 step: 2036, loss is 0.262398362159729\n",
      "epoch: 5 step: 2037, loss is 0.632591724395752\n",
      "epoch: 5 step: 2038, loss is 0.3838239908218384\n",
      "epoch: 5 step: 2039, loss is 0.8336756229400635\n",
      "epoch: 5 step: 2040, loss is 0.3633006811141968\n",
      "epoch: 5 step: 2041, loss is 0.8988364934921265\n",
      "epoch: 5 step: 2042, loss is 0.5234313607215881\n",
      "epoch: 5 step: 2043, loss is 0.3530486524105072\n",
      "epoch: 5 step: 2044, loss is 0.7118883728981018\n",
      "epoch: 5 step: 2045, loss is 0.2787177860736847\n",
      "epoch: 5 step: 2046, loss is 0.4172862768173218\n",
      "epoch: 5 step: 2047, loss is 0.37237703800201416\n",
      "epoch: 5 step: 2048, loss is 0.5245981216430664\n",
      "epoch: 5 step: 2049, loss is 0.48748132586479187\n",
      "epoch: 5 step: 2050, loss is 0.38205888867378235\n",
      "epoch: 5 step: 2051, loss is 0.2641279101371765\n",
      "epoch: 5 step: 2052, loss is 0.10463139414787292\n",
      "epoch: 5 step: 2053, loss is 0.48925846815109253\n",
      "epoch: 5 step: 2054, loss is 0.8832176923751831\n",
      "epoch: 5 step: 2055, loss is 0.32325148582458496\n",
      "epoch: 5 step: 2056, loss is 0.4119000732898712\n",
      "epoch: 5 step: 2057, loss is 0.39784306287765503\n",
      "epoch: 5 step: 2058, loss is 0.617051899433136\n",
      "epoch: 5 step: 2059, loss is 0.32780569791793823\n",
      "epoch: 5 step: 2060, loss is 0.34449759125709534\n",
      "epoch: 5 step: 2061, loss is 0.39133960008621216\n",
      "epoch: 5 step: 2062, loss is 0.2597636580467224\n",
      "epoch: 5 step: 2063, loss is 0.5167607665061951\n",
      "epoch: 5 step: 2064, loss is 0.3006630539894104\n",
      "epoch: 5 step: 2065, loss is 0.32199448347091675\n",
      "epoch: 5 step: 2066, loss is 0.37193763256073\n",
      "epoch: 5 step: 2067, loss is 0.390836626291275\n",
      "epoch: 5 step: 2068, loss is 0.2312593162059784\n",
      "epoch: 5 step: 2069, loss is 0.2434031367301941\n",
      "epoch: 5 step: 2070, loss is 0.28935790061950684\n",
      "epoch: 5 step: 2071, loss is 0.13652050495147705\n",
      "epoch: 5 step: 2072, loss is 0.2999822497367859\n",
      "epoch: 5 step: 2073, loss is 0.5893159508705139\n",
      "epoch: 5 step: 2074, loss is 0.7220221161842346\n",
      "epoch: 5 step: 2075, loss is 0.37301066517829895\n",
      "epoch: 5 step: 2076, loss is 0.36930179595947266\n",
      "epoch: 5 step: 2077, loss is 0.5106609463691711\n",
      "epoch: 5 step: 2078, loss is 0.44681790471076965\n",
      "epoch: 5 step: 2079, loss is 0.6163124442100525\n",
      "epoch: 5 step: 2080, loss is 0.36813101172447205\n",
      "epoch: 5 step: 2081, loss is 0.15770363807678223\n",
      "epoch: 5 step: 2082, loss is 0.440547913312912\n",
      "epoch: 5 step: 2083, loss is 0.5941779613494873\n",
      "epoch: 5 step: 2084, loss is 0.38093024492263794\n",
      "epoch: 5 step: 2085, loss is 0.24929997324943542\n",
      "epoch: 5 step: 2086, loss is 0.22825928032398224\n",
      "epoch: 5 step: 2087, loss is 0.5330029726028442\n",
      "epoch: 5 step: 2088, loss is 0.42063671350479126\n",
      "epoch: 5 step: 2089, loss is 0.4395911693572998\n",
      "epoch: 5 step: 2090, loss is 0.5882105827331543\n",
      "epoch: 5 step: 2091, loss is 0.12288567423820496\n",
      "epoch: 5 step: 2092, loss is 0.21335221827030182\n",
      "epoch: 5 step: 2093, loss is 0.8346050381660461\n",
      "epoch: 5 step: 2094, loss is 0.4818572700023651\n",
      "epoch: 5 step: 2095, loss is 0.2571048140525818\n",
      "epoch: 5 step: 2096, loss is 0.35896462202072144\n",
      "epoch: 5 step: 2097, loss is 0.4483904540538788\n",
      "epoch: 5 step: 2098, loss is 0.29980605840682983\n",
      "epoch: 5 step: 2099, loss is 0.5771822929382324\n",
      "epoch: 5 step: 2100, loss is 0.8248283267021179\n",
      "epoch: 5 step: 2101, loss is 0.5310056209564209\n",
      "epoch: 5 step: 2102, loss is 0.2710270881652832\n",
      "epoch: 5 step: 2103, loss is 0.2608596384525299\n",
      "epoch: 5 step: 2104, loss is 0.15170666575431824\n",
      "epoch: 5 step: 2105, loss is 0.1623629927635193\n",
      "epoch: 5 step: 2106, loss is 1.0384467840194702\n",
      "epoch: 5 step: 2107, loss is 0.3032515048980713\n",
      "epoch: 5 step: 2108, loss is 0.26163288950920105\n",
      "epoch: 5 step: 2109, loss is 0.5825256705284119\n",
      "epoch: 5 step: 2110, loss is 0.26202392578125\n",
      "epoch: 5 step: 2111, loss is 0.36072373390197754\n",
      "epoch: 5 step: 2112, loss is 0.9016828536987305\n",
      "epoch: 5 step: 2113, loss is 0.32277265191078186\n",
      "epoch: 5 step: 2114, loss is 0.22727954387664795\n",
      "epoch: 5 step: 2115, loss is 0.2644502520561218\n",
      "epoch: 5 step: 2116, loss is 0.7304635643959045\n",
      "epoch: 5 step: 2117, loss is 0.5305675268173218\n",
      "epoch: 5 step: 2118, loss is 0.7798829078674316\n",
      "epoch: 5 step: 2119, loss is 0.20385174453258514\n",
      "epoch: 5 step: 2120, loss is 0.5029124021530151\n",
      "epoch: 5 step: 2121, loss is 0.5145361423492432\n",
      "epoch: 5 step: 2122, loss is 0.3175409138202667\n",
      "epoch: 5 step: 2123, loss is 0.25467100739479065\n",
      "epoch: 5 step: 2124, loss is 0.5003082752227783\n",
      "epoch: 5 step: 2125, loss is 0.23405194282531738\n",
      "epoch: 5 step: 2126, loss is 0.44701677560806274\n",
      "epoch: 5 step: 2127, loss is 0.5809750556945801\n",
      "epoch: 5 step: 2128, loss is 0.35371387004852295\n",
      "epoch: 5 step: 2129, loss is 0.46348798274993896\n",
      "epoch: 5 step: 2130, loss is 0.2389642596244812\n",
      "epoch: 5 step: 2131, loss is 0.372614324092865\n",
      "epoch: 5 step: 2132, loss is 0.44744858145713806\n",
      "epoch: 5 step: 2133, loss is 0.6668758988380432\n",
      "epoch: 5 step: 2134, loss is 0.5900804400444031\n",
      "epoch: 5 step: 2135, loss is 0.25722119212150574\n",
      "epoch: 5 step: 2136, loss is 0.4803321957588196\n",
      "epoch: 5 step: 2137, loss is 0.4324100613594055\n",
      "epoch: 5 step: 2138, loss is 0.5225071907043457\n",
      "epoch: 5 step: 2139, loss is 0.6077849864959717\n",
      "epoch: 5 step: 2140, loss is 0.3328916132450104\n",
      "epoch: 5 step: 2141, loss is 0.6595891714096069\n",
      "epoch: 5 step: 2142, loss is 0.6011168360710144\n",
      "epoch: 5 step: 2143, loss is 0.3437346816062927\n",
      "epoch: 5 step: 2144, loss is 0.5788348913192749\n",
      "epoch: 5 step: 2145, loss is 0.3340180516242981\n",
      "epoch: 5 step: 2146, loss is 0.4251825511455536\n",
      "epoch: 5 step: 2147, loss is 0.531747043132782\n",
      "epoch: 5 step: 2148, loss is 0.38861578702926636\n",
      "epoch: 5 step: 2149, loss is 0.571975827217102\n",
      "epoch: 5 step: 2150, loss is 0.31749990582466125\n",
      "epoch: 5 step: 2151, loss is 0.11982209980487823\n",
      "epoch: 5 step: 2152, loss is 0.8169149160385132\n",
      "epoch: 5 step: 2153, loss is 0.1698320358991623\n",
      "epoch: 5 step: 2154, loss is 0.3445669114589691\n",
      "epoch: 5 step: 2155, loss is 0.42236819863319397\n",
      "epoch: 5 step: 2156, loss is 0.7444928884506226\n",
      "epoch: 5 step: 2157, loss is 0.24815396964550018\n",
      "epoch: 5 step: 2158, loss is 0.669126570224762\n",
      "epoch: 5 step: 2159, loss is 0.2329428493976593\n",
      "epoch: 5 step: 2160, loss is 0.4304286539554596\n",
      "epoch: 5 step: 2161, loss is 0.35387223958969116\n",
      "epoch: 5 step: 2162, loss is 0.4700053632259369\n",
      "epoch: 5 step: 2163, loss is 0.29171887040138245\n",
      "epoch: 5 step: 2164, loss is 0.28853076696395874\n",
      "epoch: 5 step: 2165, loss is 0.6388535499572754\n",
      "epoch: 5 step: 2166, loss is 0.3970450758934021\n",
      "epoch: 5 step: 2167, loss is 0.6018869876861572\n",
      "epoch: 5 step: 2168, loss is 0.6310102343559265\n",
      "epoch: 5 step: 2169, loss is 0.6393724083900452\n",
      "epoch: 5 step: 2170, loss is 0.446318656206131\n",
      "epoch: 5 step: 2171, loss is 0.38394424319267273\n",
      "epoch: 5 step: 2172, loss is 0.27300238609313965\n",
      "epoch: 5 step: 2173, loss is 0.072241872549057\n",
      "epoch: 5 step: 2174, loss is 0.22550132870674133\n",
      "epoch: 5 step: 2175, loss is 0.49437350034713745\n",
      "epoch: 5 step: 2176, loss is 0.27936074137687683\n",
      "epoch: 5 step: 2177, loss is 0.12318779528141022\n",
      "epoch: 5 step: 2178, loss is 0.29886317253112793\n",
      "epoch: 5 step: 2179, loss is 0.16751596331596375\n",
      "epoch: 5 step: 2180, loss is 0.7228013277053833\n",
      "epoch: 5 step: 2181, loss is 0.5691102743148804\n",
      "epoch: 5 step: 2182, loss is 0.6169365644454956\n",
      "epoch: 5 step: 2183, loss is 0.5102627277374268\n",
      "epoch: 5 step: 2184, loss is 0.6655515432357788\n",
      "epoch: 5 step: 2185, loss is 0.24906446039676666\n",
      "epoch: 5 step: 2186, loss is 0.24910971522331238\n",
      "epoch: 5 step: 2187, loss is 0.24080266058444977\n",
      "epoch: 5 step: 2188, loss is 0.3098453879356384\n",
      "epoch: 5 step: 2189, loss is 0.3143192231655121\n",
      "epoch: 5 step: 2190, loss is 0.30402228236198425\n",
      "epoch: 5 step: 2191, loss is 0.28435418009757996\n",
      "epoch: 5 step: 2192, loss is 0.22174601256847382\n",
      "epoch: 5 step: 2193, loss is 0.3380715847015381\n",
      "epoch: 5 step: 2194, loss is 0.5240301489830017\n",
      "epoch: 5 step: 2195, loss is 0.23275704681873322\n",
      "epoch: 5 step: 2196, loss is 0.27949947118759155\n",
      "epoch: 5 step: 2197, loss is 0.7131070494651794\n",
      "epoch: 5 step: 2198, loss is 0.20025424659252167\n",
      "epoch: 5 step: 2199, loss is 0.19880233705043793\n",
      "epoch: 5 step: 2200, loss is 0.4595402181148529\n",
      "epoch: 5 step: 2201, loss is 0.5098370909690857\n",
      "epoch: 5 step: 2202, loss is 0.17956121265888214\n",
      "epoch: 5 step: 2203, loss is 0.5869811773300171\n",
      "epoch: 5 step: 2204, loss is 0.40319615602493286\n",
      "epoch: 5 step: 2205, loss is 0.41391465067863464\n",
      "epoch: 5 step: 2206, loss is 0.08320676535367966\n",
      "epoch: 5 step: 2207, loss is 0.2380620539188385\n",
      "epoch: 5 step: 2208, loss is 0.22252874076366425\n",
      "epoch: 5 step: 2209, loss is 0.08956829458475113\n",
      "epoch: 5 step: 2210, loss is 0.1417987197637558\n",
      "epoch: 5 step: 2211, loss is 0.45616206526756287\n",
      "epoch: 5 step: 2212, loss is 0.0935446172952652\n",
      "epoch: 5 step: 2213, loss is 0.5355842113494873\n",
      "epoch: 5 step: 2214, loss is 0.6006704568862915\n",
      "epoch: 5 step: 2215, loss is 0.33480095863342285\n",
      "epoch: 5 step: 2216, loss is 0.3344842493534088\n",
      "epoch: 5 step: 2217, loss is 0.223580464720726\n",
      "epoch: 5 step: 2218, loss is 0.15018507838249207\n",
      "epoch: 5 step: 2219, loss is 0.6373600363731384\n",
      "epoch: 5 step: 2220, loss is 0.5912887454032898\n",
      "epoch: 5 step: 2221, loss is 0.09573332220315933\n",
      "epoch: 5 step: 2222, loss is 0.46779027581214905\n",
      "epoch: 5 step: 2223, loss is 0.27501145005226135\n",
      "epoch: 5 step: 2224, loss is 0.23942452669143677\n",
      "epoch: 5 step: 2225, loss is 0.5739200115203857\n",
      "epoch: 5 step: 2226, loss is 0.08752112090587616\n",
      "epoch: 5 step: 2227, loss is 0.9177649617195129\n",
      "epoch: 5 step: 2228, loss is 0.7586555480957031\n",
      "epoch: 5 step: 2229, loss is 0.9041089415550232\n",
      "epoch: 5 step: 2230, loss is 0.3391881287097931\n",
      "epoch: 5 step: 2231, loss is 0.42762646079063416\n",
      "epoch: 5 step: 2232, loss is 0.3608627915382385\n",
      "epoch: 5 step: 2233, loss is 0.3933959901332855\n",
      "epoch: 5 step: 2234, loss is 0.34427058696746826\n",
      "epoch: 5 step: 2235, loss is 0.15340498089790344\n",
      "epoch: 5 step: 2236, loss is 0.3449104428291321\n",
      "epoch: 5 step: 2237, loss is 0.86777663230896\n",
      "epoch: 5 step: 2238, loss is 0.168600395321846\n",
      "epoch: 5 step: 2239, loss is 0.18143463134765625\n",
      "epoch: 5 step: 2240, loss is 0.24011270701885223\n",
      "epoch: 5 step: 2241, loss is 0.1467498242855072\n",
      "epoch: 5 step: 2242, loss is 0.5809555053710938\n",
      "epoch: 5 step: 2243, loss is 0.4896310865879059\n",
      "epoch: 5 step: 2244, loss is 0.1800650954246521\n",
      "epoch: 5 step: 2245, loss is 0.8137070536613464\n",
      "epoch: 5 step: 2246, loss is 0.30718088150024414\n",
      "epoch: 5 step: 2247, loss is 0.2583727538585663\n",
      "epoch: 5 step: 2248, loss is 0.31299737095832825\n",
      "epoch: 5 step: 2249, loss is 0.2114783525466919\n",
      "epoch: 5 step: 2250, loss is 0.41060081124305725\n",
      "epoch: 5 step: 2251, loss is 0.42896321415901184\n",
      "epoch: 5 step: 2252, loss is 0.1265994906425476\n",
      "epoch: 5 step: 2253, loss is 0.18893270194530487\n",
      "epoch: 5 step: 2254, loss is 0.16781672835350037\n",
      "epoch: 5 step: 2255, loss is 0.9320392608642578\n",
      "epoch: 5 step: 2256, loss is 0.22881300747394562\n",
      "epoch: 5 step: 2257, loss is 0.18050846457481384\n",
      "epoch: 5 step: 2258, loss is 0.6641712188720703\n",
      "epoch: 5 step: 2259, loss is 0.2988213002681732\n",
      "epoch: 5 step: 2260, loss is 0.7021133303642273\n",
      "epoch: 5 step: 2261, loss is 0.4150486886501312\n",
      "epoch: 5 step: 2262, loss is 0.4367385804653168\n",
      "epoch: 5 step: 2263, loss is 0.5529182553291321\n",
      "epoch: 5 step: 2264, loss is 0.17318056523799896\n",
      "epoch: 5 step: 2265, loss is 0.4460754990577698\n",
      "epoch: 5 step: 2266, loss is 0.3327803313732147\n",
      "epoch: 5 step: 2267, loss is 0.4945628345012665\n",
      "epoch: 5 step: 2268, loss is 0.3891867399215698\n",
      "epoch: 5 step: 2269, loss is 0.6923101544380188\n",
      "epoch: 5 step: 2270, loss is 0.8585392832756042\n",
      "epoch: 5 step: 2271, loss is 1.0043655633926392\n",
      "epoch: 5 step: 2272, loss is 0.13769234716892242\n",
      "epoch: 5 step: 2273, loss is 0.6943187713623047\n",
      "epoch: 5 step: 2274, loss is 0.5949236750602722\n",
      "epoch: 5 step: 2275, loss is 0.3205687403678894\n",
      "epoch: 5 step: 2276, loss is 0.21581017971038818\n",
      "epoch: 5 step: 2277, loss is 0.3052036464214325\n",
      "epoch: 5 step: 2278, loss is 0.4081610143184662\n",
      "epoch: 5 step: 2279, loss is 0.16185468435287476\n",
      "epoch: 5 step: 2280, loss is 0.4877668619155884\n",
      "epoch: 5 step: 2281, loss is 0.8631089925765991\n",
      "epoch: 5 step: 2282, loss is 0.4254723787307739\n",
      "epoch: 5 step: 2283, loss is 0.31704506278038025\n",
      "epoch: 5 step: 2284, loss is 1.2147961854934692\n",
      "epoch: 5 step: 2285, loss is 0.3189743757247925\n",
      "epoch: 5 step: 2286, loss is 0.4879592955112457\n",
      "epoch: 5 step: 2287, loss is 0.3449580669403076\n",
      "epoch: 5 step: 2288, loss is 0.2326108068227768\n",
      "epoch: 5 step: 2289, loss is 0.7775784134864807\n",
      "epoch: 5 step: 2290, loss is 0.5170316696166992\n",
      "epoch: 5 step: 2291, loss is 0.44503092765808105\n",
      "epoch: 5 step: 2292, loss is 0.6192857027053833\n",
      "epoch: 5 step: 2293, loss is 0.3462505638599396\n",
      "epoch: 5 step: 2294, loss is 0.4925229847431183\n",
      "epoch: 5 step: 2295, loss is 0.3556087017059326\n",
      "epoch: 5 step: 2296, loss is 0.47077393531799316\n",
      "epoch: 5 step: 2297, loss is 0.5266635417938232\n",
      "epoch: 5 step: 2298, loss is 0.45307138562202454\n",
      "epoch: 5 step: 2299, loss is 0.802975594997406\n",
      "epoch: 5 step: 2300, loss is 0.24266888201236725\n",
      "epoch: 5 step: 2301, loss is 0.26540836691856384\n",
      "epoch: 5 step: 2302, loss is 0.3764055371284485\n",
      "epoch: 5 step: 2303, loss is 0.1395261436700821\n",
      "epoch: 5 step: 2304, loss is 0.37306728959083557\n",
      "epoch: 5 step: 2305, loss is 0.6915895342826843\n",
      "epoch: 5 step: 2306, loss is 0.31842654943466187\n",
      "epoch: 5 step: 2307, loss is 0.3590026795864105\n",
      "epoch: 5 step: 2308, loss is 0.37158524990081787\n",
      "epoch: 5 step: 2309, loss is 0.29850122332572937\n",
      "epoch: 5 step: 2310, loss is 0.8119511604309082\n",
      "epoch: 5 step: 2311, loss is 0.201635479927063\n",
      "epoch: 5 step: 2312, loss is 0.36354660987854004\n",
      "epoch: 5 step: 2313, loss is 0.5227870345115662\n",
      "epoch: 5 step: 2314, loss is 0.2312365174293518\n",
      "epoch: 5 step: 2315, loss is 0.3588849902153015\n",
      "epoch: 5 step: 2316, loss is 0.27167296409606934\n",
      "epoch: 5 step: 2317, loss is 0.5559473633766174\n",
      "epoch: 5 step: 2318, loss is 0.41086244583129883\n",
      "epoch: 5 step: 2319, loss is 0.22599630057811737\n",
      "epoch: 5 step: 2320, loss is 0.570085883140564\n",
      "epoch: 5 step: 2321, loss is 0.26311662793159485\n",
      "epoch: 5 step: 2322, loss is 0.5181282758712769\n",
      "epoch: 5 step: 2323, loss is 0.29446324706077576\n",
      "epoch: 5 step: 2324, loss is 0.4762880802154541\n",
      "epoch: 5 step: 2325, loss is 0.3264906704425812\n",
      "epoch: 5 step: 2326, loss is 0.2310226559638977\n",
      "epoch: 5 step: 2327, loss is 0.39122888445854187\n",
      "epoch: 5 step: 2328, loss is 0.21329621970653534\n",
      "epoch: 5 step: 2329, loss is 0.20513330399990082\n",
      "epoch: 5 step: 2330, loss is 0.26557403802871704\n",
      "epoch: 5 step: 2331, loss is 1.1535392999649048\n",
      "epoch: 5 step: 2332, loss is 0.22879265248775482\n",
      "epoch: 5 step: 2333, loss is 1.0227526426315308\n",
      "epoch: 5 step: 2334, loss is 0.3844096064567566\n",
      "epoch: 5 step: 2335, loss is 0.2911740243434906\n",
      "epoch: 5 step: 2336, loss is 0.23775754868984222\n",
      "epoch: 5 step: 2337, loss is 0.170774444937706\n",
      "epoch: 5 step: 2338, loss is 0.4977273643016815\n",
      "epoch: 5 step: 2339, loss is 0.2386726588010788\n",
      "epoch: 5 step: 2340, loss is 0.3422102630138397\n",
      "epoch: 5 step: 2341, loss is 0.5967538356781006\n",
      "epoch: 5 step: 2342, loss is 0.5728371739387512\n",
      "epoch: 5 step: 2343, loss is 0.09992105513811111\n",
      "epoch: 5 step: 2344, loss is 0.12376131117343903\n",
      "epoch: 5 step: 2345, loss is 0.6169520020484924\n",
      "epoch: 5 step: 2346, loss is 0.62729811668396\n",
      "epoch: 5 step: 2347, loss is 0.2084398716688156\n",
      "epoch: 5 step: 2348, loss is 0.34457722306251526\n",
      "epoch: 5 step: 2349, loss is 0.42651793360710144\n",
      "epoch: 5 step: 2350, loss is 0.07870360463857651\n",
      "epoch: 5 step: 2351, loss is 0.33485329151153564\n",
      "epoch: 5 step: 2352, loss is 0.44160357117652893\n",
      "epoch: 5 step: 2353, loss is 0.27351850271224976\n",
      "epoch: 5 step: 2354, loss is 0.601586639881134\n",
      "epoch: 5 step: 2355, loss is 0.34172898530960083\n",
      "epoch: 5 step: 2356, loss is 0.40566307306289673\n",
      "epoch: 5 step: 2357, loss is 0.3204326033592224\n",
      "epoch: 5 step: 2358, loss is 0.4586731195449829\n",
      "epoch: 5 step: 2359, loss is 0.6048794388771057\n",
      "epoch: 5 step: 2360, loss is 0.3965740501880646\n",
      "epoch: 5 step: 2361, loss is 0.16057664155960083\n",
      "epoch: 5 step: 2362, loss is 0.5363293886184692\n",
      "epoch: 5 step: 2363, loss is 0.5479090213775635\n",
      "epoch: 5 step: 2364, loss is 0.3082334101200104\n",
      "epoch: 5 step: 2365, loss is 0.27940648794174194\n",
      "epoch: 5 step: 2366, loss is 0.6547278761863708\n",
      "epoch: 5 step: 2367, loss is 0.21256963908672333\n",
      "epoch: 5 step: 2368, loss is 0.45946377515792847\n",
      "epoch: 5 step: 2369, loss is 0.39871805906295776\n",
      "epoch: 5 step: 2370, loss is 0.23362745344638824\n",
      "epoch: 5 step: 2371, loss is 0.2973455786705017\n",
      "epoch: 5 step: 2372, loss is 0.3540363311767578\n",
      "epoch: 5 step: 2373, loss is 0.33313024044036865\n",
      "epoch: 5 step: 2374, loss is 0.6982899308204651\n",
      "epoch: 5 step: 2375, loss is 0.788722038269043\n",
      "epoch: 5 step: 2376, loss is 0.2825949490070343\n",
      "epoch: 5 step: 2377, loss is 0.1616625040769577\n",
      "epoch: 5 step: 2378, loss is 0.7720310688018799\n",
      "epoch: 5 step: 2379, loss is 0.14800673723220825\n",
      "epoch: 5 step: 2380, loss is 0.45279011130332947\n",
      "epoch: 5 step: 2381, loss is 0.370388925075531\n",
      "epoch: 5 step: 2382, loss is 0.43848320841789246\n",
      "epoch: 5 step: 2383, loss is 0.14801959693431854\n",
      "epoch: 5 step: 2384, loss is 0.8135629296302795\n",
      "epoch: 5 step: 2385, loss is 0.17405006289482117\n",
      "epoch: 5 step: 2386, loss is 0.3494415581226349\n",
      "epoch: 5 step: 2387, loss is 0.37994271516799927\n",
      "epoch: 5 step: 2388, loss is 0.44590622186660767\n",
      "epoch: 5 step: 2389, loss is 0.4386138916015625\n",
      "epoch: 5 step: 2390, loss is 0.16236595809459686\n",
      "epoch: 5 step: 2391, loss is 0.29179829359054565\n",
      "epoch: 5 step: 2392, loss is 0.22776193916797638\n",
      "epoch: 5 step: 2393, loss is 0.217403382062912\n",
      "epoch: 5 step: 2394, loss is 0.40360742807388306\n",
      "epoch: 5 step: 2395, loss is 0.32040026783943176\n",
      "epoch: 5 step: 2396, loss is 0.11853640526533127\n",
      "epoch: 5 step: 2397, loss is 0.33178597688674927\n",
      "epoch: 5 step: 2398, loss is 0.5875883102416992\n",
      "epoch: 5 step: 2399, loss is 0.70975261926651\n",
      "epoch: 5 step: 2400, loss is 0.6633394360542297\n",
      "epoch: 5 step: 2401, loss is 0.08355487883090973\n",
      "epoch: 5 step: 2402, loss is 0.24983452260494232\n",
      "epoch: 5 step: 2403, loss is 0.25219976902008057\n",
      "epoch: 5 step: 2404, loss is 0.2702736556529999\n",
      "epoch: 5 step: 2405, loss is 0.4854406416416168\n",
      "epoch: 5 step: 2406, loss is 0.338584840297699\n",
      "epoch: 5 step: 2407, loss is 0.4036642909049988\n",
      "epoch: 5 step: 2408, loss is 0.28981852531433105\n",
      "epoch: 5 step: 2409, loss is 0.37991178035736084\n",
      "epoch: 5 step: 2410, loss is 0.30706602334976196\n",
      "epoch: 5 step: 2411, loss is 0.07329712063074112\n",
      "epoch: 5 step: 2412, loss is 0.32387644052505493\n",
      "epoch: 5 step: 2413, loss is 0.2728215456008911\n",
      "epoch: 5 step: 2414, loss is 0.23790928721427917\n",
      "epoch: 5 step: 2415, loss is 0.6012924313545227\n",
      "epoch: 5 step: 2416, loss is 0.2543447017669678\n",
      "epoch: 5 step: 2417, loss is 0.3347248435020447\n",
      "epoch: 5 step: 2418, loss is 0.2587531805038452\n",
      "epoch: 5 step: 2419, loss is 0.2795819938182831\n",
      "epoch: 5 step: 2420, loss is 0.39363786578178406\n",
      "epoch: 5 step: 2421, loss is 0.5777801275253296\n",
      "epoch: 5 step: 2422, loss is 0.7661471962928772\n",
      "epoch: 5 step: 2423, loss is 0.26420465111732483\n",
      "epoch: 5 step: 2424, loss is 0.3071901202201843\n",
      "epoch: 5 step: 2425, loss is 0.18111827969551086\n",
      "epoch: 5 step: 2426, loss is 0.15104861557483673\n",
      "epoch: 5 step: 2427, loss is 1.1093727350234985\n",
      "epoch: 5 step: 2428, loss is 0.1249849945306778\n",
      "epoch: 5 step: 2429, loss is 0.17461323738098145\n",
      "epoch: 5 step: 2430, loss is 1.0082091093063354\n",
      "epoch: 5 step: 2431, loss is 0.06346186250448227\n",
      "epoch: 5 step: 2432, loss is 0.6092110872268677\n",
      "epoch: 5 step: 2433, loss is 0.36479637026786804\n",
      "epoch: 5 step: 2434, loss is 0.22336548566818237\n",
      "epoch: 5 step: 2435, loss is 0.22651848196983337\n",
      "epoch: 5 step: 2436, loss is 0.18543913960456848\n",
      "epoch: 5 step: 2437, loss is 0.4636194407939911\n",
      "epoch: 5 step: 2438, loss is 0.37642902135849\n",
      "epoch: 5 step: 2439, loss is 0.2513650953769684\n",
      "epoch: 5 step: 2440, loss is 0.4417875409126282\n",
      "epoch: 5 step: 2441, loss is 0.6233602166175842\n",
      "epoch: 5 step: 2442, loss is 0.3634698688983917\n",
      "epoch: 5 step: 2443, loss is 0.20376548171043396\n",
      "epoch: 5 step: 2444, loss is 0.1345064342021942\n",
      "epoch: 5 step: 2445, loss is 0.2660282552242279\n",
      "epoch: 5 step: 2446, loss is 0.40945926308631897\n",
      "epoch: 5 step: 2447, loss is 0.783774197101593\n",
      "epoch: 5 step: 2448, loss is 0.24140289425849915\n",
      "epoch: 5 step: 2449, loss is 0.15269899368286133\n",
      "epoch: 5 step: 2450, loss is 0.3660716116428375\n",
      "epoch: 5 step: 2451, loss is 0.4641478359699249\n",
      "epoch: 5 step: 2452, loss is 0.5578460693359375\n",
      "epoch: 5 step: 2453, loss is 0.2310679405927658\n",
      "epoch: 5 step: 2454, loss is 1.1244970560073853\n",
      "epoch: 5 step: 2455, loss is 0.2628456652164459\n",
      "epoch: 5 step: 2456, loss is 0.26930272579193115\n",
      "epoch: 5 step: 2457, loss is 0.1701871156692505\n",
      "epoch: 5 step: 2458, loss is 0.23527328670024872\n",
      "epoch: 5 step: 2459, loss is 0.14211222529411316\n",
      "epoch: 5 step: 2460, loss is 0.47046616673469543\n",
      "epoch: 5 step: 2461, loss is 0.17001402378082275\n",
      "epoch: 5 step: 2462, loss is 0.17229387164115906\n",
      "epoch: 5 step: 2463, loss is 0.543131411075592\n",
      "epoch: 5 step: 2464, loss is 0.2813290059566498\n",
      "epoch: 5 step: 2465, loss is 0.1332128345966339\n",
      "epoch: 5 step: 2466, loss is 0.16198287904262543\n",
      "epoch: 5 step: 2467, loss is 0.527727484703064\n",
      "epoch: 5 step: 2468, loss is 0.8296769261360168\n",
      "epoch: 5 step: 2469, loss is 0.28534644842147827\n",
      "epoch: 5 step: 2470, loss is 0.5010528564453125\n",
      "epoch: 5 step: 2471, loss is 0.14709167182445526\n",
      "epoch: 5 step: 2472, loss is 0.0845361277461052\n",
      "epoch: 5 step: 2473, loss is 0.44127023220062256\n",
      "epoch: 5 step: 2474, loss is 0.27726617455482483\n",
      "epoch: 5 step: 2475, loss is 0.5958830118179321\n",
      "epoch: 5 step: 2476, loss is 0.42818865180015564\n",
      "epoch: 5 step: 2477, loss is 0.13605624437332153\n",
      "epoch: 5 step: 2478, loss is 0.6683094501495361\n",
      "epoch: 5 step: 2479, loss is 0.7034964561462402\n",
      "epoch: 5 step: 2480, loss is 0.39030447602272034\n",
      "epoch: 5 step: 2481, loss is 0.5610010027885437\n",
      "epoch: 5 step: 2482, loss is 0.6841892004013062\n",
      "epoch: 5 step: 2483, loss is 0.6612298488616943\n",
      "epoch: 5 step: 2484, loss is 0.4440188407897949\n",
      "epoch: 5 step: 2485, loss is 0.17762504518032074\n",
      "epoch: 5 step: 2486, loss is 0.29365265369415283\n",
      "epoch: 5 step: 2487, loss is 0.6581416130065918\n",
      "epoch: 5 step: 2488, loss is 0.37272003293037415\n",
      "epoch: 5 step: 2489, loss is 0.7010520100593567\n",
      "epoch: 5 step: 2490, loss is 0.5026299953460693\n",
      "epoch: 5 step: 2491, loss is 0.5459182262420654\n",
      "epoch: 5 step: 2492, loss is 1.1166820526123047\n",
      "epoch: 5 step: 2493, loss is 0.31880244612693787\n",
      "epoch: 5 step: 2494, loss is 0.3906496465206146\n",
      "epoch: 5 step: 2495, loss is 0.3444322645664215\n",
      "epoch: 5 step: 2496, loss is 0.3757142424583435\n",
      "epoch: 5 step: 2497, loss is 0.12017568945884705\n",
      "epoch: 5 step: 2498, loss is 0.5311394333839417\n",
      "epoch: 5 step: 2499, loss is 0.8051532506942749\n",
      "epoch: 5 step: 2500, loss is 0.22596031427383423\n",
      "epoch: 5 step: 2501, loss is 0.19969502091407776\n",
      "epoch: 5 step: 2502, loss is 0.26392218470573425\n",
      "epoch: 5 step: 2503, loss is 0.455554723739624\n",
      "epoch: 5 step: 2504, loss is 0.44177940487861633\n",
      "epoch: 5 step: 2505, loss is 0.277578741312027\n",
      "epoch: 5 step: 2506, loss is 0.23892907798290253\n",
      "epoch: 5 step: 2507, loss is 0.25036942958831787\n",
      "epoch: 5 step: 2508, loss is 0.861408531665802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: unknown JFIF revision number 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 1, loss is 0.37908801436424255\n",
      "epoch: 6 step: 2, loss is 0.6397701501846313\n",
      "epoch: 6 step: 3, loss is 0.1932392120361328\n",
      "epoch: 6 step: 4, loss is 0.5709540247917175\n",
      "epoch: 6 step: 5, loss is 0.3407653272151947\n",
      "epoch: 6 step: 6, loss is 0.32193055748939514\n",
      "epoch: 6 step: 7, loss is 0.28332170844078064\n",
      "epoch: 6 step: 8, loss is 0.29434865713119507\n",
      "epoch: 6 step: 9, loss is 0.7570362091064453\n",
      "epoch: 6 step: 10, loss is 0.3674922287464142\n",
      "epoch: 6 step: 11, loss is 0.6332653164863586\n",
      "epoch: 6 step: 12, loss is 0.36102285981178284\n",
      "epoch: 6 step: 13, loss is 0.7055620551109314\n",
      "epoch: 6 step: 14, loss is 0.4602992534637451\n",
      "epoch: 6 step: 15, loss is 0.4771195352077484\n",
      "epoch: 6 step: 16, loss is 0.2933010160923004\n",
      "epoch: 6 step: 17, loss is 0.26521360874176025\n",
      "epoch: 6 step: 18, loss is 0.7057885527610779\n",
      "epoch: 6 step: 19, loss is 0.2394437938928604\n",
      "epoch: 6 step: 20, loss is 0.39358535408973694\n",
      "epoch: 6 step: 21, loss is 0.3931790590286255\n",
      "epoch: 6 step: 22, loss is 0.46314045786857605\n",
      "epoch: 6 step: 23, loss is 0.3224315643310547\n",
      "epoch: 6 step: 24, loss is 0.24465830624103546\n",
      "epoch: 6 step: 25, loss is 0.2579263150691986\n",
      "epoch: 6 step: 26, loss is 0.36639487743377686\n",
      "epoch: 6 step: 27, loss is 0.45801132917404175\n",
      "epoch: 6 step: 28, loss is 0.2189958393573761\n",
      "epoch: 6 step: 29, loss is 0.12268970906734467\n",
      "epoch: 6 step: 30, loss is 0.44104060530662537\n",
      "epoch: 6 step: 31, loss is 0.3346852660179138\n",
      "epoch: 6 step: 32, loss is 0.5006201863288879\n",
      "epoch: 6 step: 33, loss is 0.10663796961307526\n",
      "epoch: 6 step: 34, loss is 0.4175459146499634\n",
      "epoch: 6 step: 35, loss is 0.46312952041625977\n",
      "epoch: 6 step: 36, loss is 0.3292294144630432\n",
      "epoch: 6 step: 37, loss is 0.15531407296657562\n",
      "epoch: 6 step: 38, loss is 0.36901170015335083\n",
      "epoch: 6 step: 39, loss is 0.27185240387916565\n",
      "epoch: 6 step: 40, loss is 0.13080252707004547\n",
      "epoch: 6 step: 41, loss is 0.27663177251815796\n",
      "epoch: 6 step: 42, loss is 0.3667038381099701\n",
      "epoch: 6 step: 43, loss is 0.21968074142932892\n",
      "epoch: 6 step: 44, loss is 0.1606471687555313\n",
      "epoch: 6 step: 45, loss is 0.3769310712814331\n",
      "epoch: 6 step: 46, loss is 0.5078331828117371\n",
      "epoch: 6 step: 47, loss is 0.2298140525817871\n",
      "epoch: 6 step: 48, loss is 0.4447084665298462\n",
      "epoch: 6 step: 49, loss is 1.2804152965545654\n",
      "epoch: 6 step: 50, loss is 0.3525824546813965\n",
      "epoch: 6 step: 51, loss is 0.49249181151390076\n",
      "epoch: 6 step: 52, loss is 0.2438696175813675\n",
      "epoch: 6 step: 53, loss is 0.29083961248397827\n",
      "epoch: 6 step: 54, loss is 0.529979407787323\n",
      "epoch: 6 step: 55, loss is 0.07599753886461258\n",
      "epoch: 6 step: 56, loss is 0.30424875020980835\n",
      "epoch: 6 step: 57, loss is 0.19960665702819824\n",
      "epoch: 6 step: 58, loss is 0.3871736526489258\n",
      "epoch: 6 step: 59, loss is 0.22462482750415802\n",
      "epoch: 6 step: 60, loss is 0.3466191589832306\n",
      "epoch: 6 step: 61, loss is 0.4030483663082123\n",
      "epoch: 6 step: 62, loss is 0.3971004784107208\n",
      "epoch: 6 step: 63, loss is 0.11354601383209229\n",
      "epoch: 6 step: 64, loss is 0.4568033218383789\n",
      "epoch: 6 step: 65, loss is 0.2826046943664551\n",
      "epoch: 6 step: 66, loss is 0.4414311647415161\n",
      "epoch: 6 step: 67, loss is 0.5932466387748718\n",
      "epoch: 6 step: 68, loss is 0.6104631423950195\n",
      "epoch: 6 step: 69, loss is 0.4475148916244507\n",
      "epoch: 6 step: 70, loss is 0.389350563287735\n",
      "epoch: 6 step: 71, loss is 0.213653564453125\n",
      "epoch: 6 step: 72, loss is 0.33354365825653076\n",
      "epoch: 6 step: 73, loss is 0.31129908561706543\n",
      "epoch: 6 step: 74, loss is 0.37138262391090393\n",
      "epoch: 6 step: 75, loss is 0.22422049939632416\n",
      "epoch: 6 step: 76, loss is 0.4419947564601898\n",
      "epoch: 6 step: 77, loss is 0.16128747165203094\n",
      "epoch: 6 step: 78, loss is 0.3710000216960907\n",
      "epoch: 6 step: 79, loss is 0.27595627307891846\n",
      "epoch: 6 step: 80, loss is 0.5565123558044434\n",
      "epoch: 6 step: 81, loss is 0.40852272510528564\n",
      "epoch: 6 step: 82, loss is 0.15231233835220337\n",
      "epoch: 6 step: 83, loss is 0.23909471929073334\n",
      "epoch: 6 step: 84, loss is 0.3674960136413574\n",
      "epoch: 6 step: 85, loss is 0.311332106590271\n",
      "epoch: 6 step: 86, loss is 0.21663056313991547\n",
      "epoch: 6 step: 87, loss is 0.24127966165542603\n",
      "epoch: 6 step: 88, loss is 0.6247109770774841\n",
      "epoch: 6 step: 89, loss is 0.901806652545929\n",
      "epoch: 6 step: 90, loss is 0.3949095904827118\n",
      "epoch: 6 step: 91, loss is 0.18989229202270508\n",
      "epoch: 6 step: 92, loss is 0.6006262302398682\n",
      "epoch: 6 step: 93, loss is 0.9047998785972595\n",
      "epoch: 6 step: 94, loss is 0.444582998752594\n",
      "epoch: 6 step: 95, loss is 0.4107186794281006\n",
      "epoch: 6 step: 96, loss is 0.40177300572395325\n",
      "epoch: 6 step: 97, loss is 0.5392893552780151\n",
      "epoch: 6 step: 98, loss is 0.5429928302764893\n",
      "epoch: 6 step: 99, loss is 0.13819292187690735\n",
      "epoch: 6 step: 100, loss is 0.6026873588562012\n",
      "epoch: 6 step: 101, loss is 0.15698792040348053\n",
      "epoch: 6 step: 102, loss is 0.4201052188873291\n",
      "epoch: 6 step: 103, loss is 0.44226396083831787\n",
      "epoch: 6 step: 104, loss is 0.7741550803184509\n",
      "epoch: 6 step: 105, loss is 0.29926222562789917\n",
      "epoch: 6 step: 106, loss is 0.1712825447320938\n",
      "epoch: 6 step: 107, loss is 0.23011136054992676\n",
      "epoch: 6 step: 108, loss is 0.3256823718547821\n",
      "epoch: 6 step: 109, loss is 0.34046754240989685\n",
      "epoch: 6 step: 110, loss is 0.5894534587860107\n",
      "epoch: 6 step: 111, loss is 0.33270296454429626\n",
      "epoch: 6 step: 112, loss is 0.22532446682453156\n",
      "epoch: 6 step: 113, loss is 0.35725101828575134\n",
      "epoch: 6 step: 114, loss is 0.3657836616039276\n",
      "epoch: 6 step: 115, loss is 0.26192036271095276\n",
      "epoch: 6 step: 116, loss is 0.5011482238769531\n",
      "epoch: 6 step: 117, loss is 1.2939621210098267\n",
      "epoch: 6 step: 118, loss is 0.3116004168987274\n",
      "epoch: 6 step: 119, loss is 0.22311387956142426\n",
      "epoch: 6 step: 120, loss is 0.5368413925170898\n",
      "epoch: 6 step: 121, loss is 0.4224398732185364\n",
      "epoch: 6 step: 122, loss is 0.18869395554065704\n",
      "epoch: 6 step: 123, loss is 0.2232913225889206\n",
      "epoch: 6 step: 124, loss is 0.6015504002571106\n",
      "epoch: 6 step: 125, loss is 0.3797636926174164\n",
      "epoch: 6 step: 126, loss is 0.39940041303634644\n",
      "epoch: 6 step: 127, loss is 0.2677006721496582\n",
      "epoch: 6 step: 128, loss is 0.5929045081138611\n",
      "epoch: 6 step: 129, loss is 0.7930445671081543\n",
      "epoch: 6 step: 130, loss is 0.3478281795978546\n",
      "epoch: 6 step: 131, loss is 0.18990665674209595\n",
      "epoch: 6 step: 132, loss is 0.39024585485458374\n",
      "epoch: 6 step: 133, loss is 0.33638137578964233\n",
      "epoch: 6 step: 134, loss is 0.3980715572834015\n",
      "epoch: 6 step: 135, loss is 0.7644367218017578\n",
      "epoch: 6 step: 136, loss is 0.44279322028160095\n",
      "epoch: 6 step: 137, loss is 0.6092431545257568\n",
      "epoch: 6 step: 138, loss is 0.6682993769645691\n",
      "epoch: 6 step: 139, loss is 0.3698645830154419\n",
      "epoch: 6 step: 140, loss is 0.23923389613628387\n",
      "epoch: 6 step: 141, loss is 0.2485196441411972\n",
      "epoch: 6 step: 142, loss is 0.2305835336446762\n",
      "epoch: 6 step: 143, loss is 0.44042232632637024\n",
      "epoch: 6 step: 144, loss is 0.29769378900527954\n",
      "epoch: 6 step: 145, loss is 0.19476573169231415\n",
      "epoch: 6 step: 146, loss is 0.4236454367637634\n",
      "epoch: 6 step: 147, loss is 0.2383725345134735\n",
      "epoch: 6 step: 148, loss is 0.5679392218589783\n",
      "epoch: 6 step: 149, loss is 0.48046016693115234\n",
      "epoch: 6 step: 150, loss is 0.5461606383323669\n",
      "epoch: 6 step: 151, loss is 0.2255764752626419\n",
      "epoch: 6 step: 152, loss is 0.2750871479511261\n",
      "epoch: 6 step: 153, loss is 0.4401961863040924\n",
      "epoch: 6 step: 154, loss is 0.17273004353046417\n",
      "epoch: 6 step: 155, loss is 0.2707076370716095\n",
      "epoch: 6 step: 156, loss is 0.3346484303474426\n",
      "epoch: 6 step: 157, loss is 0.18308141827583313\n",
      "epoch: 6 step: 158, loss is 0.17410966753959656\n",
      "epoch: 6 step: 159, loss is 0.32784798741340637\n",
      "epoch: 6 step: 160, loss is 0.4206588864326477\n",
      "epoch: 6 step: 161, loss is 0.07588628679513931\n",
      "epoch: 6 step: 162, loss is 0.8733391761779785\n",
      "epoch: 6 step: 163, loss is 1.158713698387146\n",
      "epoch: 6 step: 164, loss is 0.3366777300834656\n",
      "epoch: 6 step: 165, loss is 0.3436794877052307\n",
      "epoch: 6 step: 166, loss is 0.5647216439247131\n",
      "epoch: 6 step: 167, loss is 0.2534022033214569\n",
      "epoch: 6 step: 168, loss is 0.4360787272453308\n",
      "epoch: 6 step: 169, loss is 0.13540668785572052\n",
      "epoch: 6 step: 170, loss is 0.16142955422401428\n",
      "epoch: 6 step: 171, loss is 0.20546795427799225\n",
      "epoch: 6 step: 172, loss is 0.5529345273971558\n",
      "epoch: 6 step: 173, loss is 0.3266754448413849\n",
      "epoch: 6 step: 174, loss is 0.6090814471244812\n",
      "epoch: 6 step: 175, loss is 0.1343090683221817\n",
      "epoch: 6 step: 176, loss is 0.3221116065979004\n",
      "epoch: 6 step: 177, loss is 0.5714443922042847\n",
      "epoch: 6 step: 178, loss is 0.255279004573822\n",
      "epoch: 6 step: 179, loss is 0.37904560565948486\n",
      "epoch: 6 step: 180, loss is 0.18245506286621094\n",
      "epoch: 6 step: 181, loss is 0.6308032274246216\n",
      "epoch: 6 step: 182, loss is 0.39515477418899536\n",
      "epoch: 6 step: 183, loss is 0.3135428726673126\n",
      "epoch: 6 step: 184, loss is 0.19066181778907776\n",
      "epoch: 6 step: 185, loss is 0.3038547933101654\n",
      "epoch: 6 step: 186, loss is 0.14623171091079712\n",
      "epoch: 6 step: 187, loss is 0.32076433300971985\n",
      "epoch: 6 step: 188, loss is 0.2747834324836731\n",
      "epoch: 6 step: 189, loss is 0.3510676622390747\n",
      "epoch: 6 step: 190, loss is 0.1772857904434204\n",
      "epoch: 6 step: 191, loss is 0.354472279548645\n",
      "epoch: 6 step: 192, loss is 0.2969382703304291\n",
      "epoch: 6 step: 193, loss is 0.7538206577301025\n",
      "epoch: 6 step: 194, loss is 0.15818209946155548\n",
      "epoch: 6 step: 195, loss is 0.20728649199008942\n",
      "epoch: 6 step: 196, loss is 0.16726061701774597\n",
      "epoch: 6 step: 197, loss is 0.4370695650577545\n",
      "epoch: 6 step: 198, loss is 0.07696394622325897\n",
      "epoch: 6 step: 199, loss is 0.34954968094825745\n",
      "epoch: 6 step: 200, loss is 0.16988959908485413\n",
      "epoch: 6 step: 201, loss is 0.40739160776138306\n",
      "epoch: 6 step: 202, loss is 1.1106791496276855\n",
      "epoch: 6 step: 203, loss is 0.16002000868320465\n",
      "epoch: 6 step: 204, loss is 0.5385217666625977\n",
      "epoch: 6 step: 205, loss is 0.7236093878746033\n",
      "epoch: 6 step: 206, loss is 0.38451674580574036\n",
      "epoch: 6 step: 207, loss is 0.7473627328872681\n",
      "epoch: 6 step: 208, loss is 0.42066237330436707\n",
      "epoch: 6 step: 209, loss is 0.7604058384895325\n",
      "epoch: 6 step: 210, loss is 0.2000274509191513\n",
      "epoch: 6 step: 211, loss is 0.33666369318962097\n",
      "epoch: 6 step: 212, loss is 0.6354759335517883\n",
      "epoch: 6 step: 213, loss is 0.8297334313392639\n",
      "epoch: 6 step: 214, loss is 0.2703676223754883\n",
      "epoch: 6 step: 215, loss is 0.49096015095710754\n",
      "epoch: 6 step: 216, loss is 0.2077742964029312\n",
      "epoch: 6 step: 217, loss is 0.6595317721366882\n",
      "epoch: 6 step: 218, loss is 0.5162993669509888\n",
      "epoch: 6 step: 219, loss is 0.46440356969833374\n",
      "epoch: 6 step: 220, loss is 0.4258631765842438\n",
      "epoch: 6 step: 221, loss is 0.362877756357193\n",
      "epoch: 6 step: 222, loss is 0.4057423770427704\n",
      "epoch: 6 step: 223, loss is 0.29810047149658203\n",
      "epoch: 6 step: 224, loss is 0.623671293258667\n",
      "epoch: 6 step: 225, loss is 0.4311346709728241\n",
      "epoch: 6 step: 226, loss is 0.5920386910438538\n",
      "epoch: 6 step: 227, loss is 0.6057912111282349\n",
      "epoch: 6 step: 228, loss is 0.9012101292610168\n",
      "epoch: 6 step: 229, loss is 0.14347192645072937\n",
      "epoch: 6 step: 230, loss is 0.4561637043952942\n",
      "epoch: 6 step: 231, loss is 0.7806106209754944\n",
      "epoch: 6 step: 232, loss is 0.3517349064350128\n",
      "epoch: 6 step: 233, loss is 0.5887802243232727\n",
      "epoch: 6 step: 234, loss is 0.4583399295806885\n",
      "epoch: 6 step: 235, loss is 0.18157169222831726\n",
      "epoch: 6 step: 236, loss is 0.29114848375320435\n",
      "epoch: 6 step: 237, loss is 0.2821214199066162\n",
      "epoch: 6 step: 238, loss is 0.3477705717086792\n",
      "epoch: 6 step: 239, loss is 0.3044233024120331\n",
      "epoch: 6 step: 240, loss is 0.7203373908996582\n",
      "epoch: 6 step: 241, loss is 0.30588027834892273\n",
      "epoch: 6 step: 242, loss is 0.39787212014198303\n",
      "epoch: 6 step: 243, loss is 0.6440637707710266\n",
      "epoch: 6 step: 244, loss is 0.22930069267749786\n",
      "epoch: 6 step: 245, loss is 0.38741225004196167\n",
      "epoch: 6 step: 246, loss is 0.4230652451515198\n",
      "epoch: 6 step: 247, loss is 0.14608129858970642\n",
      "epoch: 6 step: 248, loss is 0.2573580741882324\n",
      "epoch: 6 step: 249, loss is 0.4168111979961395\n",
      "epoch: 6 step: 250, loss is 0.19353583455085754\n",
      "epoch: 6 step: 251, loss is 0.42707309126853943\n",
      "epoch: 6 step: 252, loss is 0.5297561287879944\n",
      "epoch: 6 step: 253, loss is 0.07961354404687881\n",
      "epoch: 6 step: 254, loss is 0.1614360362291336\n",
      "epoch: 6 step: 255, loss is 0.3518292307853699\n",
      "epoch: 6 step: 256, loss is 0.15685057640075684\n",
      "epoch: 6 step: 257, loss is 0.4550652801990509\n",
      "epoch: 6 step: 258, loss is 0.5607749223709106\n",
      "epoch: 6 step: 259, loss is 0.34790292382240295\n",
      "epoch: 6 step: 260, loss is 0.1305263787508011\n",
      "epoch: 6 step: 261, loss is 0.37575793266296387\n",
      "epoch: 6 step: 262, loss is 0.0902537927031517\n",
      "epoch: 6 step: 263, loss is 0.5441644191741943\n",
      "epoch: 6 step: 264, loss is 0.23594912886619568\n",
      "epoch: 6 step: 265, loss is 0.34598100185394287\n",
      "epoch: 6 step: 266, loss is 0.19770202040672302\n",
      "epoch: 6 step: 267, loss is 0.3562031686306\n",
      "epoch: 6 step: 268, loss is 0.3034619390964508\n",
      "epoch: 6 step: 269, loss is 0.1125887930393219\n",
      "epoch: 6 step: 270, loss is 0.4570271074771881\n",
      "epoch: 6 step: 271, loss is 0.17534717917442322\n",
      "epoch: 6 step: 272, loss is 0.18078330159187317\n",
      "epoch: 6 step: 273, loss is 0.4310332238674164\n",
      "epoch: 6 step: 274, loss is 0.5995478630065918\n",
      "epoch: 6 step: 275, loss is 0.44463714957237244\n",
      "epoch: 6 step: 276, loss is 1.1465758085250854\n",
      "epoch: 6 step: 277, loss is 0.504976212978363\n",
      "epoch: 6 step: 278, loss is 0.20552194118499756\n",
      "epoch: 6 step: 279, loss is 0.24316690862178802\n",
      "epoch: 6 step: 280, loss is 0.24288254976272583\n",
      "epoch: 6 step: 281, loss is 0.23085327446460724\n",
      "epoch: 6 step: 282, loss is 0.15466094017028809\n",
      "epoch: 6 step: 283, loss is 0.412474125623703\n",
      "epoch: 6 step: 284, loss is 0.46501418948173523\n",
      "epoch: 6 step: 285, loss is 0.7167836427688599\n",
      "epoch: 6 step: 286, loss is 0.7245670557022095\n",
      "epoch: 6 step: 287, loss is 0.20054380595684052\n",
      "epoch: 6 step: 288, loss is 0.16498571634292603\n",
      "epoch: 6 step: 289, loss is 0.34375274181365967\n",
      "epoch: 6 step: 290, loss is 0.3684404790401459\n",
      "epoch: 6 step: 291, loss is 0.20365938544273376\n",
      "epoch: 6 step: 292, loss is 0.14488279819488525\n",
      "epoch: 6 step: 293, loss is 0.3090205192565918\n",
      "epoch: 6 step: 294, loss is 0.391010046005249\n",
      "epoch: 6 step: 295, loss is 0.16882744431495667\n",
      "epoch: 6 step: 296, loss is 0.5277694463729858\n",
      "epoch: 6 step: 297, loss is 0.4066530466079712\n",
      "epoch: 6 step: 298, loss is 0.12192685157060623\n",
      "epoch: 6 step: 299, loss is 0.31058269739151\n",
      "epoch: 6 step: 300, loss is 0.5876264572143555\n",
      "epoch: 6 step: 301, loss is 0.21095243096351624\n",
      "epoch: 6 step: 302, loss is 0.30885007977485657\n",
      "epoch: 6 step: 303, loss is 0.10667120665311813\n",
      "epoch: 6 step: 304, loss is 0.14563757181167603\n",
      "epoch: 6 step: 305, loss is 0.37680596113204956\n",
      "epoch: 6 step: 306, loss is 0.5891914963722229\n",
      "epoch: 6 step: 307, loss is 0.10582266747951508\n",
      "epoch: 6 step: 308, loss is 0.25064408779144287\n",
      "epoch: 6 step: 309, loss is 0.3329281508922577\n",
      "epoch: 6 step: 310, loss is 0.22418300807476044\n",
      "epoch: 6 step: 311, loss is 0.07554417103528976\n",
      "epoch: 6 step: 312, loss is 0.3125467300415039\n",
      "epoch: 6 step: 313, loss is 0.24679440259933472\n",
      "epoch: 6 step: 314, loss is 0.8274816870689392\n",
      "epoch: 6 step: 315, loss is 0.44644132256507874\n",
      "epoch: 6 step: 316, loss is 0.22961339354515076\n",
      "epoch: 6 step: 317, loss is 0.17761188745498657\n",
      "epoch: 6 step: 318, loss is 0.8305210471153259\n",
      "epoch: 6 step: 319, loss is 0.37047284841537476\n",
      "epoch: 6 step: 320, loss is 0.42408502101898193\n",
      "epoch: 6 step: 321, loss is 0.186640202999115\n",
      "epoch: 6 step: 322, loss is 0.7302263379096985\n",
      "epoch: 6 step: 323, loss is 0.41752859950065613\n",
      "epoch: 6 step: 324, loss is 0.11012957990169525\n",
      "epoch: 6 step: 325, loss is 0.5038015842437744\n",
      "epoch: 6 step: 326, loss is 0.4083929657936096\n",
      "epoch: 6 step: 327, loss is 0.2129160612821579\n",
      "epoch: 6 step: 328, loss is 0.3316386640071869\n",
      "epoch: 6 step: 329, loss is 0.42408496141433716\n",
      "epoch: 6 step: 330, loss is 0.6141796112060547\n",
      "epoch: 6 step: 331, loss is 0.3191062808036804\n",
      "epoch: 6 step: 332, loss is 0.2609242796897888\n",
      "epoch: 6 step: 333, loss is 0.04305749759078026\n",
      "epoch: 6 step: 334, loss is 0.4494384825229645\n",
      "epoch: 6 step: 335, loss is 0.5710890889167786\n",
      "epoch: 6 step: 336, loss is 0.6852232813835144\n",
      "epoch: 6 step: 337, loss is 0.1383383721113205\n",
      "epoch: 6 step: 338, loss is 0.37241330742836\n",
      "epoch: 6 step: 339, loss is 0.32097530364990234\n",
      "epoch: 6 step: 340, loss is 0.3746064305305481\n",
      "epoch: 6 step: 341, loss is 0.20971351861953735\n",
      "epoch: 6 step: 342, loss is 0.40894973278045654\n",
      "epoch: 6 step: 343, loss is 0.5363307595252991\n",
      "epoch: 6 step: 344, loss is 0.5998407602310181\n",
      "epoch: 6 step: 345, loss is 0.552295982837677\n",
      "epoch: 6 step: 346, loss is 0.6646756529808044\n",
      "epoch: 6 step: 347, loss is 0.5998558402061462\n",
      "epoch: 6 step: 348, loss is 0.25213423371315\n",
      "epoch: 6 step: 349, loss is 0.20428285002708435\n",
      "epoch: 6 step: 350, loss is 0.2320062518119812\n",
      "epoch: 6 step: 351, loss is 0.3014078140258789\n",
      "epoch: 6 step: 352, loss is 0.4002628028392792\n",
      "epoch: 6 step: 353, loss is 0.16608399152755737\n",
      "epoch: 6 step: 354, loss is 0.6321983933448792\n",
      "epoch: 6 step: 355, loss is 0.19508673250675201\n",
      "epoch: 6 step: 356, loss is 0.14868661761283875\n",
      "epoch: 6 step: 357, loss is 0.5455566644668579\n",
      "epoch: 6 step: 358, loss is 0.49183058738708496\n",
      "epoch: 6 step: 359, loss is 0.3054381012916565\n",
      "epoch: 6 step: 360, loss is 0.6864072680473328\n",
      "epoch: 6 step: 361, loss is 0.2215033620595932\n",
      "epoch: 6 step: 362, loss is 0.3422791659832001\n",
      "epoch: 6 step: 363, loss is 0.522415816783905\n",
      "epoch: 6 step: 364, loss is 0.5301131010055542\n",
      "epoch: 6 step: 365, loss is 0.18509964644908905\n",
      "epoch: 6 step: 366, loss is 0.08643069118261337\n",
      "epoch: 6 step: 367, loss is 0.42274901270866394\n",
      "epoch: 6 step: 368, loss is 0.2880431115627289\n",
      "epoch: 6 step: 369, loss is 0.21776089072227478\n",
      "epoch: 6 step: 370, loss is 0.39013174176216125\n",
      "epoch: 6 step: 371, loss is 0.91794353723526\n",
      "epoch: 6 step: 372, loss is 0.15319637954235077\n",
      "epoch: 6 step: 373, loss is 0.6408140063285828\n",
      "epoch: 6 step: 374, loss is 0.2825658619403839\n",
      "epoch: 6 step: 375, loss is 0.2672702968120575\n",
      "epoch: 6 step: 376, loss is 0.5502895712852478\n",
      "epoch: 6 step: 377, loss is 0.3854372501373291\n",
      "epoch: 6 step: 378, loss is 0.2770545482635498\n",
      "epoch: 6 step: 379, loss is 0.4476120173931122\n",
      "epoch: 6 step: 380, loss is 0.18644855916500092\n",
      "epoch: 6 step: 381, loss is 0.33108770847320557\n",
      "epoch: 6 step: 382, loss is 0.14741700887680054\n",
      "epoch: 6 step: 383, loss is 0.5785183906555176\n",
      "epoch: 6 step: 384, loss is 0.29359209537506104\n",
      "epoch: 6 step: 385, loss is 0.30996033549308777\n",
      "epoch: 6 step: 386, loss is 0.3853835165500641\n",
      "epoch: 6 step: 387, loss is 0.5395225882530212\n",
      "epoch: 6 step: 388, loss is 0.27693554759025574\n",
      "epoch: 6 step: 389, loss is 0.5012239217758179\n",
      "epoch: 6 step: 390, loss is 0.2649308443069458\n",
      "epoch: 6 step: 391, loss is 0.30851441621780396\n",
      "epoch: 6 step: 392, loss is 0.3725031614303589\n",
      "epoch: 6 step: 393, loss is 0.42037123441696167\n",
      "epoch: 6 step: 394, loss is 0.3871699869632721\n",
      "epoch: 6 step: 395, loss is 0.43176600337028503\n",
      "epoch: 6 step: 396, loss is 0.4983486831188202\n",
      "epoch: 6 step: 397, loss is 0.1362370103597641\n",
      "epoch: 6 step: 398, loss is 0.3252391815185547\n",
      "epoch: 6 step: 399, loss is 0.26075634360313416\n",
      "epoch: 6 step: 400, loss is 0.32636216282844543\n",
      "epoch: 6 step: 401, loss is 0.3991972804069519\n",
      "epoch: 6 step: 402, loss is 0.2342909276485443\n",
      "epoch: 6 step: 403, loss is 0.11892111599445343\n",
      "epoch: 6 step: 404, loss is 0.6660120487213135\n",
      "epoch: 6 step: 405, loss is 0.3369770348072052\n",
      "epoch: 6 step: 406, loss is 0.34890109300613403\n",
      "epoch: 6 step: 407, loss is 0.2290600687265396\n",
      "epoch: 6 step: 408, loss is 0.15518462657928467\n",
      "epoch: 6 step: 409, loss is 0.48860281705856323\n",
      "epoch: 6 step: 410, loss is 0.6575267314910889\n",
      "epoch: 6 step: 411, loss is 0.3344811201095581\n",
      "epoch: 6 step: 412, loss is 0.7906421422958374\n",
      "epoch: 6 step: 413, loss is 0.44455626606941223\n",
      "epoch: 6 step: 414, loss is 0.19240827858448029\n",
      "epoch: 6 step: 415, loss is 0.16299885511398315\n",
      "epoch: 6 step: 416, loss is 0.8476482033729553\n",
      "epoch: 6 step: 417, loss is 0.461966872215271\n",
      "epoch: 6 step: 418, loss is 1.0395538806915283\n",
      "epoch: 6 step: 419, loss is 0.17381522059440613\n",
      "epoch: 6 step: 420, loss is 0.1469532996416092\n",
      "epoch: 6 step: 421, loss is 0.16659116744995117\n",
      "epoch: 6 step: 422, loss is 0.3101879954338074\n",
      "epoch: 6 step: 423, loss is 0.4430525600910187\n",
      "epoch: 6 step: 424, loss is 0.32860302925109863\n",
      "epoch: 6 step: 425, loss is 0.31862717866897583\n",
      "epoch: 6 step: 426, loss is 0.42753124237060547\n",
      "epoch: 6 step: 427, loss is 0.3892311453819275\n",
      "epoch: 6 step: 428, loss is 0.38807639479637146\n",
      "epoch: 6 step: 429, loss is 0.9137474894523621\n",
      "epoch: 6 step: 430, loss is 0.3373558521270752\n",
      "epoch: 6 step: 431, loss is 0.2383965253829956\n",
      "epoch: 6 step: 432, loss is 0.2047303318977356\n",
      "epoch: 6 step: 433, loss is 0.4081319272518158\n",
      "epoch: 6 step: 434, loss is 0.2224148064851761\n",
      "epoch: 6 step: 435, loss is 0.14669612050056458\n",
      "epoch: 6 step: 436, loss is 0.540809690952301\n",
      "epoch: 6 step: 437, loss is 0.3371628224849701\n",
      "epoch: 6 step: 438, loss is 0.10829848796129227\n",
      "epoch: 6 step: 439, loss is 0.2997491955757141\n",
      "epoch: 6 step: 440, loss is 0.2816053032875061\n",
      "epoch: 6 step: 441, loss is 0.24971750378608704\n",
      "epoch: 6 step: 442, loss is 0.5586697459220886\n",
      "epoch: 6 step: 443, loss is 0.17601856589317322\n",
      "epoch: 6 step: 444, loss is 0.8594458699226379\n",
      "epoch: 6 step: 445, loss is 0.5562082529067993\n",
      "epoch: 6 step: 446, loss is 0.4693908989429474\n",
      "epoch: 6 step: 447, loss is 0.303627073764801\n",
      "epoch: 6 step: 448, loss is 0.38538116216659546\n",
      "epoch: 6 step: 449, loss is 0.3823571801185608\n",
      "epoch: 6 step: 450, loss is 0.08859623223543167\n",
      "epoch: 6 step: 451, loss is 0.5227885246276855\n",
      "epoch: 6 step: 452, loss is 0.28113579750061035\n",
      "epoch: 6 step: 453, loss is 0.19485558569431305\n",
      "epoch: 6 step: 454, loss is 0.7004895210266113\n",
      "epoch: 6 step: 455, loss is 0.3386668860912323\n",
      "epoch: 6 step: 456, loss is 0.5733711123466492\n",
      "epoch: 6 step: 457, loss is 0.38607344031333923\n",
      "epoch: 6 step: 458, loss is 0.21286456286907196\n",
      "epoch: 6 step: 459, loss is 0.537189245223999\n",
      "epoch: 6 step: 460, loss is 0.38038650155067444\n",
      "epoch: 6 step: 461, loss is 0.5667207837104797\n",
      "epoch: 6 step: 462, loss is 0.07841004431247711\n",
      "epoch: 6 step: 463, loss is 0.3003551959991455\n",
      "epoch: 6 step: 464, loss is 0.40160995721817017\n",
      "epoch: 6 step: 465, loss is 0.40191835165023804\n",
      "epoch: 6 step: 466, loss is 0.5888628959655762\n",
      "epoch: 6 step: 467, loss is 0.28843602538108826\n",
      "epoch: 6 step: 468, loss is 0.3657190799713135\n",
      "epoch: 6 step: 469, loss is 0.30868417024612427\n",
      "epoch: 6 step: 470, loss is 0.15580302476882935\n",
      "epoch: 6 step: 471, loss is 0.17738625407218933\n",
      "epoch: 6 step: 472, loss is 0.14281153678894043\n",
      "epoch: 6 step: 473, loss is 0.41247594356536865\n",
      "epoch: 6 step: 474, loss is 0.2643885910511017\n",
      "epoch: 6 step: 475, loss is 0.14222073554992676\n",
      "epoch: 6 step: 476, loss is 0.28011345863342285\n",
      "epoch: 6 step: 477, loss is 0.5003569722175598\n",
      "epoch: 6 step: 478, loss is 0.26534730195999146\n",
      "epoch: 6 step: 479, loss is 0.4182800054550171\n",
      "epoch: 6 step: 480, loss is 0.3762248754501343\n",
      "epoch: 6 step: 481, loss is 0.28042495250701904\n",
      "epoch: 6 step: 482, loss is 0.26008284091949463\n",
      "epoch: 6 step: 483, loss is 0.29572179913520813\n",
      "epoch: 6 step: 484, loss is 0.3880995213985443\n",
      "epoch: 6 step: 485, loss is 0.4878215193748474\n",
      "epoch: 6 step: 486, loss is 0.2857914865016937\n",
      "epoch: 6 step: 487, loss is 0.6201590299606323\n",
      "epoch: 6 step: 488, loss is 0.5327869057655334\n",
      "epoch: 6 step: 489, loss is 0.27200913429260254\n",
      "epoch: 6 step: 490, loss is 0.3153766393661499\n",
      "epoch: 6 step: 491, loss is 0.2566913962364197\n",
      "epoch: 6 step: 492, loss is 0.2739770710468292\n",
      "epoch: 6 step: 493, loss is 0.2865922451019287\n",
      "epoch: 6 step: 494, loss is 0.5005195736885071\n",
      "epoch: 6 step: 495, loss is 0.5193324685096741\n",
      "epoch: 6 step: 496, loss is 0.1333247870206833\n",
      "epoch: 6 step: 497, loss is 0.24524544179439545\n",
      "epoch: 6 step: 498, loss is 0.6066153049468994\n",
      "epoch: 6 step: 499, loss is 0.334403395652771\n",
      "epoch: 6 step: 500, loss is 0.5404380559921265\n",
      "epoch: 6 step: 501, loss is 0.5362125635147095\n",
      "epoch: 6 step: 502, loss is 0.46019306778907776\n",
      "epoch: 6 step: 503, loss is 0.31224241852760315\n",
      "epoch: 6 step: 504, loss is 0.20615334808826447\n",
      "epoch: 6 step: 505, loss is 0.5198881030082703\n",
      "epoch: 6 step: 506, loss is 0.1923673152923584\n",
      "epoch: 6 step: 507, loss is 0.43321722745895386\n",
      "epoch: 6 step: 508, loss is 0.4095761179924011\n",
      "epoch: 6 step: 509, loss is 0.2619533836841583\n",
      "epoch: 6 step: 510, loss is 0.16518843173980713\n",
      "epoch: 6 step: 511, loss is 0.1939486414194107\n",
      "epoch: 6 step: 512, loss is 0.18834762275218964\n",
      "epoch: 6 step: 513, loss is 0.42448708415031433\n",
      "epoch: 6 step: 514, loss is 0.12794162333011627\n",
      "epoch: 6 step: 515, loss is 0.19876880943775177\n",
      "epoch: 6 step: 516, loss is 0.3573145866394043\n",
      "epoch: 6 step: 517, loss is 0.5141521692276001\n",
      "epoch: 6 step: 518, loss is 0.25745177268981934\n",
      "epoch: 6 step: 519, loss is 0.9321824312210083\n",
      "epoch: 6 step: 520, loss is 0.3169988691806793\n",
      "epoch: 6 step: 521, loss is 0.1558791697025299\n",
      "epoch: 6 step: 522, loss is 0.255535364151001\n",
      "epoch: 6 step: 523, loss is 0.3420521020889282\n",
      "epoch: 6 step: 524, loss is 0.14164823293685913\n",
      "epoch: 6 step: 525, loss is 0.1422385424375534\n",
      "epoch: 6 step: 526, loss is 0.6604067087173462\n",
      "epoch: 6 step: 527, loss is 0.39102256298065186\n",
      "epoch: 6 step: 528, loss is 0.5082715749740601\n",
      "epoch: 6 step: 529, loss is 0.3559867739677429\n",
      "epoch: 6 step: 530, loss is 0.1581866443157196\n",
      "epoch: 6 step: 531, loss is 0.2519284188747406\n",
      "epoch: 6 step: 532, loss is 1.094279408454895\n",
      "epoch: 6 step: 533, loss is 0.23795127868652344\n",
      "epoch: 6 step: 534, loss is 0.6931190490722656\n",
      "epoch: 6 step: 535, loss is 0.2606165409088135\n",
      "epoch: 6 step: 536, loss is 0.42272892594337463\n",
      "epoch: 6 step: 537, loss is 0.16897182166576385\n",
      "epoch: 6 step: 538, loss is 0.1699230819940567\n",
      "epoch: 6 step: 539, loss is 0.22262072563171387\n",
      "epoch: 6 step: 540, loss is 0.29340776801109314\n",
      "epoch: 6 step: 541, loss is 0.20328691601753235\n",
      "epoch: 6 step: 542, loss is 0.23788274824619293\n",
      "epoch: 6 step: 543, loss is 0.3163135051727295\n",
      "epoch: 6 step: 544, loss is 0.2883771061897278\n",
      "epoch: 6 step: 545, loss is 0.30076780915260315\n",
      "epoch: 6 step: 546, loss is 0.10626361519098282\n",
      "epoch: 6 step: 547, loss is 0.6437097191810608\n",
      "epoch: 6 step: 548, loss is 0.17964699864387512\n",
      "epoch: 6 step: 549, loss is 0.08644397556781769\n",
      "epoch: 6 step: 550, loss is 0.15646803379058838\n",
      "epoch: 6 step: 551, loss is 0.2939240336418152\n",
      "epoch: 6 step: 552, loss is 0.536020815372467\n",
      "epoch: 6 step: 553, loss is 0.24693918228149414\n",
      "epoch: 6 step: 554, loss is 0.1963527798652649\n",
      "epoch: 6 step: 555, loss is 0.3902742564678192\n",
      "epoch: 6 step: 556, loss is 0.16495558619499207\n",
      "epoch: 6 step: 557, loss is 0.3814586400985718\n",
      "epoch: 6 step: 558, loss is 0.0591784343123436\n",
      "epoch: 6 step: 559, loss is 0.28958120942115784\n",
      "epoch: 6 step: 560, loss is 0.13270600140094757\n",
      "epoch: 6 step: 561, loss is 0.07901885360479355\n",
      "epoch: 6 step: 562, loss is 0.5848352313041687\n",
      "epoch: 6 step: 563, loss is 0.42454612255096436\n",
      "epoch: 6 step: 564, loss is 0.35833191871643066\n",
      "epoch: 6 step: 565, loss is 0.1799764633178711\n",
      "epoch: 6 step: 566, loss is 0.21373644471168518\n",
      "epoch: 6 step: 567, loss is 0.3703720271587372\n",
      "epoch: 6 step: 568, loss is 0.15899547934532166\n",
      "epoch: 6 step: 569, loss is 0.09220542758703232\n",
      "epoch: 6 step: 570, loss is 0.17353342473506927\n",
      "epoch: 6 step: 571, loss is 0.32678982615470886\n",
      "epoch: 6 step: 572, loss is 0.35191410779953003\n",
      "epoch: 6 step: 573, loss is 0.1416500359773636\n",
      "epoch: 6 step: 574, loss is 0.2539454400539398\n",
      "epoch: 6 step: 575, loss is 0.36339959502220154\n",
      "epoch: 6 step: 576, loss is 0.4654298722743988\n",
      "epoch: 6 step: 577, loss is 0.5261222124099731\n",
      "epoch: 6 step: 578, loss is 0.5479268431663513\n",
      "epoch: 6 step: 579, loss is 0.23012377321720123\n",
      "epoch: 6 step: 580, loss is 0.4831368327140808\n",
      "epoch: 6 step: 581, loss is 0.47707998752593994\n",
      "epoch: 6 step: 582, loss is 0.14811934530735016\n",
      "epoch: 6 step: 583, loss is 0.8257774710655212\n",
      "epoch: 6 step: 584, loss is 0.10411036014556885\n",
      "epoch: 6 step: 585, loss is 0.2599165439605713\n",
      "epoch: 6 step: 586, loss is 0.1561785489320755\n",
      "epoch: 6 step: 587, loss is 0.08741341531276703\n",
      "epoch: 6 step: 588, loss is 0.4118427336215973\n",
      "epoch: 6 step: 589, loss is 0.22192591428756714\n",
      "epoch: 6 step: 590, loss is 0.28237032890319824\n",
      "epoch: 6 step: 591, loss is 0.1481173187494278\n",
      "epoch: 6 step: 592, loss is 0.3216095566749573\n",
      "epoch: 6 step: 593, loss is 0.3006468713283539\n",
      "epoch: 6 step: 594, loss is 0.30344733595848083\n",
      "epoch: 6 step: 595, loss is 0.6318509578704834\n",
      "epoch: 6 step: 596, loss is 1.2154475450515747\n",
      "epoch: 6 step: 597, loss is 0.3400295376777649\n",
      "epoch: 6 step: 598, loss is 0.22817818820476532\n",
      "epoch: 6 step: 599, loss is 0.15834619104862213\n",
      "epoch: 6 step: 600, loss is 0.12858356535434723\n",
      "epoch: 6 step: 601, loss is 0.3683536946773529\n",
      "epoch: 6 step: 602, loss is 0.4239664077758789\n",
      "epoch: 6 step: 603, loss is 0.41948243975639343\n",
      "epoch: 6 step: 604, loss is 0.87575364112854\n",
      "epoch: 6 step: 605, loss is 0.6060434579849243\n",
      "epoch: 6 step: 606, loss is 0.2374543845653534\n",
      "epoch: 6 step: 607, loss is 0.5309046506881714\n",
      "epoch: 6 step: 608, loss is 0.6324042081832886\n",
      "epoch: 6 step: 609, loss is 0.4511236846446991\n",
      "epoch: 6 step: 610, loss is 0.11546123027801514\n",
      "epoch: 6 step: 611, loss is 0.21830615401268005\n",
      "epoch: 6 step: 612, loss is 0.2668623626232147\n",
      "epoch: 6 step: 613, loss is 0.09296490252017975\n",
      "epoch: 6 step: 614, loss is 0.3211911618709564\n",
      "epoch: 6 step: 615, loss is 0.26148372888565063\n",
      "epoch: 6 step: 616, loss is 0.4117385447025299\n",
      "epoch: 6 step: 617, loss is 0.16950348019599915\n",
      "epoch: 6 step: 618, loss is 0.35063862800598145\n",
      "epoch: 6 step: 619, loss is 0.4847005009651184\n",
      "epoch: 6 step: 620, loss is 0.20852504670619965\n",
      "epoch: 6 step: 621, loss is 0.16383293271064758\n",
      "epoch: 6 step: 622, loss is 0.1578337699174881\n",
      "epoch: 6 step: 623, loss is 0.29113197326660156\n",
      "epoch: 6 step: 624, loss is 0.6312930583953857\n",
      "epoch: 6 step: 625, loss is 0.5717727541923523\n",
      "epoch: 6 step: 626, loss is 0.365833044052124\n",
      "epoch: 6 step: 627, loss is 0.3573577105998993\n",
      "epoch: 6 step: 628, loss is 0.6529757380485535\n",
      "epoch: 6 step: 629, loss is 0.155724436044693\n",
      "epoch: 6 step: 630, loss is 0.3385021686553955\n",
      "epoch: 6 step: 631, loss is 0.14874115586280823\n",
      "epoch: 6 step: 632, loss is 0.9722151160240173\n",
      "epoch: 6 step: 633, loss is 0.29561886191368103\n",
      "epoch: 6 step: 634, loss is 0.41740405559539795\n",
      "epoch: 6 step: 635, loss is 0.5117461681365967\n",
      "epoch: 6 step: 636, loss is 0.29088130593299866\n",
      "epoch: 6 step: 637, loss is 0.25091883540153503\n",
      "epoch: 6 step: 638, loss is 0.211867555975914\n",
      "epoch: 6 step: 639, loss is 0.284231960773468\n",
      "epoch: 6 step: 640, loss is 0.4735385775566101\n",
      "epoch: 6 step: 641, loss is 0.4642940163612366\n",
      "epoch: 6 step: 642, loss is 0.5775569677352905\n",
      "epoch: 6 step: 643, loss is 0.664012610912323\n",
      "epoch: 6 step: 644, loss is 0.6227452158927917\n",
      "epoch: 6 step: 645, loss is 0.680165708065033\n",
      "epoch: 6 step: 646, loss is 0.30875375866889954\n",
      "epoch: 6 step: 647, loss is 0.3719691038131714\n",
      "epoch: 6 step: 648, loss is 0.5495780110359192\n",
      "epoch: 6 step: 649, loss is 0.6476603150367737\n",
      "epoch: 6 step: 650, loss is 0.2943165898323059\n",
      "epoch: 6 step: 651, loss is 0.3299941420555115\n",
      "epoch: 6 step: 652, loss is 0.30653759837150574\n",
      "epoch: 6 step: 653, loss is 0.36488109827041626\n",
      "epoch: 6 step: 654, loss is 0.24953493475914001\n",
      "epoch: 6 step: 655, loss is 0.3840469419956207\n",
      "epoch: 6 step: 656, loss is 0.3876289129257202\n",
      "epoch: 6 step: 657, loss is 0.3946845531463623\n",
      "epoch: 6 step: 658, loss is 0.3676130175590515\n",
      "epoch: 6 step: 659, loss is 0.28272777795791626\n",
      "epoch: 6 step: 660, loss is 0.26435646414756775\n",
      "epoch: 6 step: 661, loss is 0.1831817477941513\n",
      "epoch: 6 step: 662, loss is 0.38880476355552673\n",
      "epoch: 6 step: 663, loss is 0.2710416615009308\n",
      "epoch: 6 step: 664, loss is 0.17477965354919434\n",
      "epoch: 6 step: 665, loss is 0.4999257028102875\n",
      "epoch: 6 step: 666, loss is 0.32977280020713806\n",
      "epoch: 6 step: 667, loss is 0.4411216080188751\n",
      "epoch: 6 step: 668, loss is 0.38168102502822876\n",
      "epoch: 6 step: 669, loss is 0.3766961991786957\n",
      "epoch: 6 step: 670, loss is 0.269799143075943\n",
      "epoch: 6 step: 671, loss is 0.3903992772102356\n",
      "epoch: 6 step: 672, loss is 0.5597608685493469\n",
      "epoch: 6 step: 673, loss is 0.37024983763694763\n",
      "epoch: 6 step: 674, loss is 0.5497506856918335\n",
      "epoch: 6 step: 675, loss is 0.26105761528015137\n",
      "epoch: 6 step: 676, loss is 0.3876405656337738\n",
      "epoch: 6 step: 677, loss is 0.40893638134002686\n",
      "epoch: 6 step: 678, loss is 0.6787963509559631\n",
      "epoch: 6 step: 679, loss is 0.40471094846725464\n",
      "epoch: 6 step: 680, loss is 0.45642954111099243\n",
      "epoch: 6 step: 681, loss is 0.2487216740846634\n",
      "epoch: 6 step: 682, loss is 0.33191144466400146\n",
      "epoch: 6 step: 683, loss is 0.7845803499221802\n",
      "epoch: 6 step: 684, loss is 0.3155045509338379\n",
      "epoch: 6 step: 685, loss is 0.2824387550354004\n",
      "epoch: 6 step: 686, loss is 0.771819531917572\n",
      "epoch: 6 step: 687, loss is 0.46483492851257324\n",
      "epoch: 6 step: 688, loss is 0.5975320339202881\n",
      "epoch: 6 step: 689, loss is 0.18315306305885315\n",
      "epoch: 6 step: 690, loss is 0.26436567306518555\n",
      "epoch: 6 step: 691, loss is 0.19014917314052582\n",
      "epoch: 6 step: 692, loss is 0.18148279190063477\n",
      "epoch: 6 step: 693, loss is 0.49257877469062805\n",
      "epoch: 6 step: 694, loss is 0.4439413249492645\n",
      "epoch: 6 step: 695, loss is 0.6054218411445618\n",
      "epoch: 6 step: 696, loss is 0.14437180757522583\n",
      "epoch: 6 step: 697, loss is 0.3589586615562439\n",
      "epoch: 6 step: 698, loss is 0.37043049931526184\n",
      "epoch: 6 step: 699, loss is 0.2490762323141098\n",
      "epoch: 6 step: 700, loss is 0.15241311490535736\n",
      "epoch: 6 step: 701, loss is 0.36950865387916565\n",
      "epoch: 6 step: 702, loss is 0.3603282868862152\n",
      "epoch: 6 step: 703, loss is 0.48083558678627014\n",
      "epoch: 6 step: 704, loss is 0.3017662465572357\n",
      "epoch: 6 step: 705, loss is 0.4590069651603699\n",
      "epoch: 6 step: 706, loss is 0.3138692080974579\n",
      "epoch: 6 step: 707, loss is 0.27186867594718933\n",
      "epoch: 6 step: 708, loss is 0.3052998185157776\n",
      "epoch: 6 step: 709, loss is 0.21856039762496948\n",
      "epoch: 6 step: 710, loss is 0.31197693943977356\n",
      "epoch: 6 step: 711, loss is 0.8030156493186951\n",
      "epoch: 6 step: 712, loss is 0.29661113023757935\n",
      "epoch: 6 step: 713, loss is 0.45189350843429565\n",
      "epoch: 6 step: 714, loss is 0.3927311599254608\n",
      "epoch: 6 step: 715, loss is 0.47909417748451233\n",
      "epoch: 6 step: 716, loss is 0.34558966755867004\n",
      "epoch: 6 step: 717, loss is 0.20040631294250488\n",
      "epoch: 6 step: 718, loss is 0.2057560235261917\n",
      "epoch: 6 step: 719, loss is 0.5943347215652466\n",
      "epoch: 6 step: 720, loss is 0.40223824977874756\n",
      "epoch: 6 step: 721, loss is 0.24255751073360443\n",
      "epoch: 6 step: 722, loss is 0.4688093364238739\n",
      "epoch: 6 step: 723, loss is 0.29011377692222595\n",
      "epoch: 6 step: 724, loss is 0.25632306933403015\n",
      "epoch: 6 step: 725, loss is 0.5016986131668091\n",
      "epoch: 6 step: 726, loss is 0.23510479927062988\n",
      "epoch: 6 step: 727, loss is 0.2746056020259857\n",
      "epoch: 6 step: 728, loss is 0.9217950701713562\n",
      "epoch: 6 step: 729, loss is 0.617585301399231\n",
      "epoch: 6 step: 730, loss is 0.054537538439035416\n",
      "epoch: 6 step: 731, loss is 0.07554201036691666\n",
      "epoch: 6 step: 732, loss is 0.5186514854431152\n",
      "epoch: 6 step: 733, loss is 0.6190400123596191\n",
      "epoch: 6 step: 734, loss is 0.29283127188682556\n",
      "epoch: 6 step: 735, loss is 0.480234295129776\n",
      "epoch: 6 step: 736, loss is 0.1531514823436737\n",
      "epoch: 6 step: 737, loss is 0.33491864800453186\n",
      "epoch: 6 step: 738, loss is 0.4188907742500305\n",
      "epoch: 6 step: 739, loss is 1.0260931253433228\n",
      "epoch: 6 step: 740, loss is 0.33298635482788086\n",
      "epoch: 6 step: 741, loss is 0.19661802053451538\n",
      "epoch: 6 step: 742, loss is 0.545870840549469\n",
      "epoch: 6 step: 743, loss is 0.6085883378982544\n",
      "epoch: 6 step: 744, loss is 0.5386523604393005\n",
      "epoch: 6 step: 745, loss is 0.47148850560188293\n",
      "epoch: 6 step: 746, loss is 0.14988869428634644\n",
      "epoch: 6 step: 747, loss is 0.5302378535270691\n",
      "epoch: 6 step: 748, loss is 0.24061250686645508\n",
      "epoch: 6 step: 749, loss is 0.49920642375946045\n",
      "epoch: 6 step: 750, loss is 0.313541054725647\n",
      "epoch: 6 step: 751, loss is 0.2720203101634979\n",
      "epoch: 6 step: 752, loss is 0.47098955512046814\n",
      "epoch: 6 step: 753, loss is 0.42792537808418274\n",
      "epoch: 6 step: 754, loss is 0.43572500348091125\n",
      "epoch: 6 step: 755, loss is 0.532697856426239\n",
      "epoch: 6 step: 756, loss is 0.27296048402786255\n",
      "epoch: 6 step: 757, loss is 0.48710164427757263\n",
      "epoch: 6 step: 758, loss is 0.3711070120334625\n",
      "epoch: 6 step: 759, loss is 0.45342522859573364\n",
      "epoch: 6 step: 760, loss is 0.448821485042572\n",
      "epoch: 6 step: 761, loss is 0.3426362872123718\n",
      "epoch: 6 step: 762, loss is 0.16065222024917603\n",
      "epoch: 6 step: 763, loss is 0.2672478258609772\n",
      "epoch: 6 step: 764, loss is 0.4594304859638214\n",
      "epoch: 6 step: 765, loss is 0.3953513205051422\n",
      "epoch: 6 step: 766, loss is 0.44545578956604004\n",
      "epoch: 6 step: 767, loss is 0.3089427649974823\n",
      "epoch: 6 step: 768, loss is 0.35115525126457214\n",
      "epoch: 6 step: 769, loss is 0.23616768419742584\n",
      "epoch: 6 step: 770, loss is 0.36449360847473145\n",
      "epoch: 6 step: 771, loss is 0.27191755175590515\n",
      "epoch: 6 step: 772, loss is 0.35648593306541443\n",
      "epoch: 6 step: 773, loss is 0.24938100576400757\n",
      "epoch: 6 step: 774, loss is 0.18429550528526306\n",
      "epoch: 6 step: 775, loss is 0.27557116746902466\n",
      "epoch: 6 step: 776, loss is 0.17184042930603027\n",
      "epoch: 6 step: 777, loss is 0.28226009011268616\n",
      "epoch: 6 step: 778, loss is 0.1858922839164734\n",
      "epoch: 6 step: 779, loss is 0.19910842180252075\n",
      "epoch: 6 step: 780, loss is 0.4679289162158966\n",
      "epoch: 6 step: 781, loss is 0.24467764794826508\n",
      "epoch: 6 step: 782, loss is 0.12125509977340698\n",
      "epoch: 6 step: 783, loss is 0.9553491473197937\n",
      "epoch: 6 step: 784, loss is 0.8195552825927734\n",
      "epoch: 6 step: 785, loss is 0.1939130425453186\n",
      "epoch: 6 step: 786, loss is 0.7493075132369995\n",
      "epoch: 6 step: 787, loss is 0.9635886549949646\n",
      "epoch: 6 step: 788, loss is 0.47968050837516785\n",
      "epoch: 6 step: 789, loss is 0.0813257172703743\n",
      "epoch: 6 step: 790, loss is 0.291081964969635\n",
      "epoch: 6 step: 791, loss is 0.30955803394317627\n",
      "epoch: 6 step: 792, loss is 0.7968146204948425\n",
      "epoch: 6 step: 793, loss is 0.8575377464294434\n",
      "epoch: 6 step: 794, loss is 0.5130124688148499\n",
      "epoch: 6 step: 795, loss is 0.5416290760040283\n",
      "epoch: 6 step: 796, loss is 0.25360095500946045\n",
      "epoch: 6 step: 797, loss is 0.23479251563549042\n",
      "epoch: 6 step: 798, loss is 0.2969200909137726\n",
      "epoch: 6 step: 799, loss is 0.17339076101779938\n",
      "epoch: 6 step: 800, loss is 0.6140057444572449\n",
      "epoch: 6 step: 801, loss is 0.10697071254253387\n",
      "epoch: 6 step: 802, loss is 0.5880803465843201\n",
      "epoch: 6 step: 803, loss is 0.1686430424451828\n",
      "epoch: 6 step: 804, loss is 0.3275117874145508\n",
      "epoch: 6 step: 805, loss is 0.18463639914989471\n",
      "epoch: 6 step: 806, loss is 0.26852142810821533\n",
      "epoch: 6 step: 807, loss is 0.3468112051486969\n",
      "epoch: 6 step: 808, loss is 0.17617878317832947\n",
      "epoch: 6 step: 809, loss is 0.29163768887519836\n",
      "epoch: 6 step: 810, loss is 0.6718654036521912\n",
      "epoch: 6 step: 811, loss is 0.33244413137435913\n",
      "epoch: 6 step: 812, loss is 0.23971645534038544\n",
      "epoch: 6 step: 813, loss is 0.13024526834487915\n",
      "epoch: 6 step: 814, loss is 0.19059060513973236\n",
      "epoch: 6 step: 815, loss is 0.599769651889801\n",
      "epoch: 6 step: 816, loss is 0.3848045766353607\n",
      "epoch: 6 step: 817, loss is 0.29354584217071533\n",
      "epoch: 6 step: 818, loss is 0.23347532749176025\n",
      "epoch: 6 step: 819, loss is 0.16192641854286194\n",
      "epoch: 6 step: 820, loss is 0.21286432445049286\n",
      "epoch: 6 step: 821, loss is 0.28582829236984253\n",
      "epoch: 6 step: 822, loss is 0.053950853645801544\n",
      "epoch: 6 step: 823, loss is 0.5484444499015808\n",
      "epoch: 6 step: 824, loss is 0.9074512720108032\n",
      "epoch: 6 step: 825, loss is 0.18149809539318085\n",
      "epoch: 6 step: 826, loss is 0.43966978788375854\n",
      "epoch: 6 step: 827, loss is 0.1544601023197174\n",
      "epoch: 6 step: 828, loss is 0.340047150850296\n",
      "epoch: 6 step: 829, loss is 0.2575112581253052\n",
      "epoch: 6 step: 830, loss is 0.4437055289745331\n",
      "epoch: 6 step: 831, loss is 0.2822243571281433\n",
      "epoch: 6 step: 832, loss is 0.18507006764411926\n",
      "epoch: 6 step: 833, loss is 0.20555418729782104\n",
      "epoch: 6 step: 834, loss is 0.2796768844127655\n",
      "epoch: 6 step: 835, loss is 0.3412388265132904\n",
      "epoch: 6 step: 836, loss is 0.12095195055007935\n",
      "epoch: 6 step: 837, loss is 0.09883563220500946\n",
      "epoch: 6 step: 838, loss is 0.14270679652690887\n",
      "epoch: 6 step: 839, loss is 0.312776118516922\n",
      "epoch: 6 step: 840, loss is 0.10682578384876251\n",
      "epoch: 6 step: 841, loss is 0.2797391414642334\n",
      "epoch: 6 step: 842, loss is 0.8353028297424316\n",
      "epoch: 6 step: 843, loss is 0.4466606676578522\n",
      "epoch: 6 step: 844, loss is 0.20405954122543335\n",
      "epoch: 6 step: 845, loss is 0.5367031097412109\n",
      "epoch: 6 step: 846, loss is 0.3664250373840332\n",
      "epoch: 6 step: 847, loss is 0.553849458694458\n",
      "epoch: 6 step: 848, loss is 0.2991466522216797\n",
      "epoch: 6 step: 849, loss is 0.2531954050064087\n",
      "epoch: 6 step: 850, loss is 0.38020065426826477\n",
      "epoch: 6 step: 851, loss is 0.06605923175811768\n",
      "epoch: 6 step: 852, loss is 0.34748008847236633\n",
      "epoch: 6 step: 853, loss is 0.493062287569046\n",
      "epoch: 6 step: 854, loss is 0.1863364577293396\n",
      "epoch: 6 step: 855, loss is 0.250890851020813\n",
      "epoch: 6 step: 856, loss is 0.5079286098480225\n",
      "epoch: 6 step: 857, loss is 0.4505416452884674\n",
      "epoch: 6 step: 858, loss is 0.4972912669181824\n",
      "epoch: 6 step: 859, loss is 0.151689350605011\n",
      "epoch: 6 step: 860, loss is 0.3506060838699341\n",
      "epoch: 6 step: 861, loss is 0.07899919152259827\n",
      "epoch: 6 step: 862, loss is 0.5855934023857117\n",
      "epoch: 6 step: 863, loss is 0.39521002769470215\n",
      "epoch: 6 step: 864, loss is 0.08897153288125992\n",
      "epoch: 6 step: 865, loss is 0.3701068162918091\n",
      "epoch: 6 step: 866, loss is 0.06858673691749573\n",
      "epoch: 6 step: 867, loss is 0.17055487632751465\n",
      "epoch: 6 step: 868, loss is 0.15649011731147766\n",
      "epoch: 6 step: 869, loss is 0.13199643790721893\n",
      "epoch: 6 step: 870, loss is 0.3813457787036896\n",
      "epoch: 6 step: 871, loss is 0.7052027583122253\n",
      "epoch: 6 step: 872, loss is 0.22442834079265594\n",
      "epoch: 6 step: 873, loss is 0.3621138036251068\n",
      "epoch: 6 step: 874, loss is 0.5108742117881775\n",
      "epoch: 6 step: 875, loss is 0.20896676182746887\n",
      "epoch: 6 step: 876, loss is 0.3801037073135376\n",
      "epoch: 6 step: 877, loss is 0.12351153790950775\n",
      "epoch: 6 step: 878, loss is 0.2512381374835968\n",
      "epoch: 6 step: 879, loss is 0.2123357653617859\n",
      "epoch: 6 step: 880, loss is 0.1233510971069336\n",
      "epoch: 6 step: 881, loss is 0.6795315742492676\n",
      "epoch: 6 step: 882, loss is 0.45442700386047363\n",
      "epoch: 6 step: 883, loss is 0.24517382681369781\n",
      "epoch: 6 step: 884, loss is 0.5040898323059082\n",
      "epoch: 6 step: 885, loss is 0.46578872203826904\n",
      "epoch: 6 step: 886, loss is 0.7848692536354065\n",
      "epoch: 6 step: 887, loss is 0.3739616870880127\n",
      "epoch: 6 step: 888, loss is 0.4662075936794281\n",
      "epoch: 6 step: 889, loss is 0.48804304003715515\n",
      "epoch: 6 step: 890, loss is 0.3673244118690491\n",
      "epoch: 6 step: 891, loss is 0.24826876819133759\n",
      "epoch: 6 step: 892, loss is 0.3881998360157013\n",
      "epoch: 6 step: 893, loss is 0.4237912893295288\n",
      "epoch: 6 step: 894, loss is 0.384135901927948\n",
      "epoch: 6 step: 895, loss is 0.38899052143096924\n",
      "epoch: 6 step: 896, loss is 0.48865896463394165\n",
      "epoch: 6 step: 897, loss is 0.3292655348777771\n",
      "epoch: 6 step: 898, loss is 0.31257420778274536\n",
      "epoch: 6 step: 899, loss is 0.4539134204387665\n",
      "epoch: 6 step: 900, loss is 0.25911542773246765\n",
      "epoch: 6 step: 901, loss is 0.33404895663261414\n",
      "epoch: 6 step: 902, loss is 0.10096951574087143\n",
      "epoch: 6 step: 903, loss is 0.2196405678987503\n",
      "epoch: 6 step: 904, loss is 0.31816428899765015\n",
      "epoch: 6 step: 905, loss is 0.4940454661846161\n",
      "epoch: 6 step: 906, loss is 0.2803957462310791\n",
      "epoch: 6 step: 907, loss is 0.4442650377750397\n",
      "epoch: 6 step: 908, loss is 0.9789961576461792\n",
      "epoch: 6 step: 909, loss is 0.2065315991640091\n",
      "epoch: 6 step: 910, loss is 0.28601694107055664\n",
      "epoch: 6 step: 911, loss is 0.4166824221611023\n",
      "epoch: 6 step: 912, loss is 0.7792456150054932\n",
      "epoch: 6 step: 913, loss is 0.4636986553668976\n",
      "epoch: 6 step: 914, loss is 0.3510357439517975\n",
      "epoch: 6 step: 915, loss is 0.27533143758773804\n",
      "epoch: 6 step: 916, loss is 0.27551642060279846\n",
      "epoch: 6 step: 917, loss is 0.3033425509929657\n",
      "epoch: 6 step: 918, loss is 0.4777514934539795\n",
      "epoch: 6 step: 919, loss is 0.4727438986301422\n",
      "epoch: 6 step: 920, loss is 0.10908285528421402\n",
      "epoch: 6 step: 921, loss is 0.23621512949466705\n",
      "epoch: 6 step: 922, loss is 0.19250176846981049\n",
      "epoch: 6 step: 923, loss is 0.40379688143730164\n",
      "epoch: 6 step: 924, loss is 0.5115978121757507\n",
      "epoch: 6 step: 925, loss is 0.21524541079998016\n",
      "epoch: 6 step: 926, loss is 0.13814564049243927\n",
      "epoch: 6 step: 927, loss is 0.2830330431461334\n",
      "epoch: 6 step: 928, loss is 0.6721081137657166\n",
      "epoch: 6 step: 929, loss is 0.3546831011772156\n",
      "epoch: 6 step: 930, loss is 0.48013436794281006\n",
      "epoch: 6 step: 931, loss is 0.827875554561615\n",
      "epoch: 6 step: 932, loss is 0.36606109142303467\n",
      "epoch: 6 step: 933, loss is 0.26649805903434753\n",
      "epoch: 6 step: 934, loss is 0.21986719965934753\n",
      "epoch: 6 step: 935, loss is 0.24395892024040222\n",
      "epoch: 6 step: 936, loss is 0.4645260274410248\n",
      "epoch: 6 step: 937, loss is 0.20354503393173218\n",
      "epoch: 6 step: 938, loss is 0.3132385015487671\n",
      "epoch: 6 step: 939, loss is 0.10760163515806198\n",
      "epoch: 6 step: 940, loss is 0.24606259167194366\n",
      "epoch: 6 step: 941, loss is 0.38690924644470215\n",
      "epoch: 6 step: 942, loss is 0.5851107239723206\n",
      "epoch: 6 step: 943, loss is 0.312883585691452\n",
      "epoch: 6 step: 944, loss is 0.27549225091934204\n",
      "epoch: 6 step: 945, loss is 0.13786326348781586\n",
      "epoch: 6 step: 946, loss is 0.3412918746471405\n",
      "epoch: 6 step: 947, loss is 0.6985787749290466\n",
      "epoch: 6 step: 948, loss is 0.5933154225349426\n",
      "epoch: 6 step: 949, loss is 0.6500735878944397\n",
      "epoch: 6 step: 950, loss is 0.6689199209213257\n",
      "epoch: 6 step: 951, loss is 0.4583864212036133\n",
      "epoch: 6 step: 952, loss is 0.32517537474632263\n",
      "epoch: 6 step: 953, loss is 0.2476557046175003\n",
      "epoch: 6 step: 954, loss is 0.18238823115825653\n",
      "epoch: 6 step: 955, loss is 0.4261961281299591\n",
      "epoch: 6 step: 956, loss is 0.15974226593971252\n",
      "epoch: 6 step: 957, loss is 0.33175766468048096\n",
      "epoch: 6 step: 958, loss is 0.6339372396469116\n",
      "epoch: 6 step: 959, loss is 0.5299555659294128\n",
      "epoch: 6 step: 960, loss is 0.5847657918930054\n",
      "epoch: 6 step: 961, loss is 0.44288331270217896\n",
      "epoch: 6 step: 962, loss is 1.043324589729309\n",
      "epoch: 6 step: 963, loss is 0.369868665933609\n",
      "epoch: 6 step: 964, loss is 0.3989894986152649\n",
      "epoch: 6 step: 965, loss is 0.590200662612915\n",
      "epoch: 6 step: 966, loss is 0.36814674735069275\n",
      "epoch: 6 step: 967, loss is 0.35244888067245483\n",
      "epoch: 6 step: 968, loss is 0.3017325699329376\n",
      "epoch: 6 step: 969, loss is 0.49469050765037537\n",
      "epoch: 6 step: 970, loss is 0.48656123876571655\n",
      "epoch: 6 step: 971, loss is 0.1440824270248413\n",
      "epoch: 6 step: 972, loss is 0.598139762878418\n",
      "epoch: 6 step: 973, loss is 0.46578580141067505\n",
      "epoch: 6 step: 974, loss is 0.18320442736148834\n",
      "epoch: 6 step: 975, loss is 0.34349381923675537\n",
      "epoch: 6 step: 976, loss is 0.45255136489868164\n",
      "epoch: 6 step: 977, loss is 0.21059636771678925\n",
      "epoch: 6 step: 978, loss is 0.3361707031726837\n",
      "epoch: 6 step: 979, loss is 0.22790811955928802\n",
      "epoch: 6 step: 980, loss is 0.34175172448158264\n",
      "epoch: 6 step: 981, loss is 0.3043626844882965\n",
      "epoch: 6 step: 982, loss is 0.34180372953414917\n",
      "epoch: 6 step: 983, loss is 0.6803117394447327\n",
      "epoch: 6 step: 984, loss is 0.33554819226264954\n",
      "epoch: 6 step: 985, loss is 0.3952163755893707\n",
      "epoch: 6 step: 986, loss is 0.46131646633148193\n",
      "epoch: 6 step: 987, loss is 0.1922813504934311\n",
      "epoch: 6 step: 988, loss is 0.48691704869270325\n",
      "epoch: 6 step: 989, loss is 0.20696155726909637\n",
      "epoch: 6 step: 990, loss is 0.488288551568985\n",
      "epoch: 6 step: 991, loss is 0.4478352665901184\n",
      "epoch: 6 step: 992, loss is 0.2359827756881714\n",
      "epoch: 6 step: 993, loss is 0.3353623151779175\n",
      "epoch: 6 step: 994, loss is 0.12715186178684235\n",
      "epoch: 6 step: 995, loss is 0.33304905891418457\n",
      "epoch: 6 step: 996, loss is 0.30545204877853394\n",
      "epoch: 6 step: 997, loss is 0.4561718702316284\n",
      "epoch: 6 step: 998, loss is 0.2738468647003174\n",
      "epoch: 6 step: 999, loss is 0.11911633610725403\n",
      "epoch: 6 step: 1000, loss is 0.10108774900436401\n",
      "epoch: 6 step: 1001, loss is 0.5029831528663635\n",
      "epoch: 6 step: 1002, loss is 0.5921273231506348\n",
      "epoch: 6 step: 1003, loss is 0.2692304849624634\n",
      "epoch: 6 step: 1004, loss is 0.26745742559432983\n",
      "epoch: 6 step: 1005, loss is 0.47808289527893066\n",
      "epoch: 6 step: 1006, loss is 0.32615262269973755\n",
      "epoch: 6 step: 1007, loss is 0.4881252348423004\n",
      "epoch: 6 step: 1008, loss is 0.18921341001987457\n",
      "epoch: 6 step: 1009, loss is 0.288244366645813\n",
      "epoch: 6 step: 1010, loss is 0.12001203745603561\n",
      "epoch: 6 step: 1011, loss is 0.3223658502101898\n",
      "epoch: 6 step: 1012, loss is 0.25312283635139465\n",
      "epoch: 6 step: 1013, loss is 0.8637498617172241\n",
      "epoch: 6 step: 1014, loss is 0.08470942080020905\n",
      "epoch: 6 step: 1015, loss is 0.220737487077713\n",
      "epoch: 6 step: 1016, loss is 0.6654151678085327\n",
      "epoch: 6 step: 1017, loss is 0.8312205672264099\n",
      "epoch: 6 step: 1018, loss is 0.25102290511131287\n",
      "epoch: 6 step: 1019, loss is 0.9479202032089233\n",
      "epoch: 6 step: 1020, loss is 0.27637505531311035\n",
      "epoch: 6 step: 1021, loss is 0.14636465907096863\n",
      "epoch: 6 step: 1022, loss is 0.30337557196617126\n",
      "epoch: 6 step: 1023, loss is 0.16656611859798431\n",
      "epoch: 6 step: 1024, loss is 0.19953767955303192\n",
      "epoch: 6 step: 1025, loss is 0.5849968791007996\n",
      "epoch: 6 step: 1026, loss is 1.296546459197998\n",
      "epoch: 6 step: 1027, loss is 0.3669207990169525\n",
      "epoch: 6 step: 1028, loss is 0.4989495873451233\n",
      "epoch: 6 step: 1029, loss is 0.4393065869808197\n",
      "epoch: 6 step: 1030, loss is 0.16021256148815155\n",
      "epoch: 6 step: 1031, loss is 0.25833994150161743\n",
      "epoch: 6 step: 1032, loss is 0.10564664751291275\n",
      "epoch: 6 step: 1033, loss is 0.11428011208772659\n",
      "epoch: 6 step: 1034, loss is 0.6219410300254822\n",
      "epoch: 6 step: 1035, loss is 0.3225885331630707\n",
      "epoch: 6 step: 1036, loss is 0.22444194555282593\n",
      "epoch: 6 step: 1037, loss is 0.8016819953918457\n",
      "epoch: 6 step: 1038, loss is 0.4922732412815094\n",
      "epoch: 6 step: 1039, loss is 0.48389071226119995\n",
      "epoch: 6 step: 1040, loss is 0.18345043063163757\n",
      "epoch: 6 step: 1041, loss is 0.16316768527030945\n",
      "epoch: 6 step: 1042, loss is 0.21081078052520752\n",
      "epoch: 6 step: 1043, loss is 0.32829898595809937\n",
      "epoch: 6 step: 1044, loss is 0.4222193956375122\n",
      "epoch: 6 step: 1045, loss is 0.2819703221321106\n",
      "epoch: 6 step: 1046, loss is 0.231552854180336\n",
      "epoch: 6 step: 1047, loss is 0.21229542791843414\n",
      "epoch: 6 step: 1048, loss is 0.3200514316558838\n",
      "epoch: 6 step: 1049, loss is 0.30285733938217163\n",
      "epoch: 6 step: 1050, loss is 0.10133180022239685\n",
      "epoch: 6 step: 1051, loss is 0.2357199341058731\n",
      "epoch: 6 step: 1052, loss is 0.12496640533208847\n",
      "epoch: 6 step: 1053, loss is 0.4968951642513275\n",
      "epoch: 6 step: 1054, loss is 0.8032974600791931\n",
      "epoch: 6 step: 1055, loss is 0.36071667075157166\n",
      "epoch: 6 step: 1056, loss is 0.6144964098930359\n",
      "epoch: 6 step: 1057, loss is 0.34916409850120544\n",
      "epoch: 6 step: 1058, loss is 0.5330623984336853\n",
      "epoch: 6 step: 1059, loss is 0.18651284277439117\n",
      "epoch: 6 step: 1060, loss is 0.2714805603027344\n",
      "epoch: 6 step: 1061, loss is 0.08357010036706924\n",
      "epoch: 6 step: 1062, loss is 0.5487664341926575\n",
      "epoch: 6 step: 1063, loss is 0.7207401990890503\n",
      "epoch: 6 step: 1064, loss is 0.22991012036800385\n",
      "epoch: 6 step: 1065, loss is 0.24184350669384003\n",
      "epoch: 6 step: 1066, loss is 0.08494593948125839\n",
      "epoch: 6 step: 1067, loss is 0.4709798991680145\n",
      "epoch: 6 step: 1068, loss is 0.1458224058151245\n",
      "epoch: 6 step: 1069, loss is 0.08430498838424683\n",
      "epoch: 6 step: 1070, loss is 0.20178380608558655\n",
      "epoch: 6 step: 1071, loss is 0.32955828309059143\n",
      "epoch: 6 step: 1072, loss is 0.3690580129623413\n",
      "epoch: 6 step: 1073, loss is 0.39002302289009094\n",
      "epoch: 6 step: 1074, loss is 0.42787671089172363\n",
      "epoch: 6 step: 1075, loss is 0.4188176393508911\n",
      "epoch: 6 step: 1076, loss is 1.0318410396575928\n",
      "epoch: 6 step: 1077, loss is 0.4882889986038208\n",
      "epoch: 6 step: 1078, loss is 0.5771074891090393\n",
      "epoch: 6 step: 1079, loss is 0.28889328241348267\n",
      "epoch: 6 step: 1080, loss is 0.38656023144721985\n",
      "epoch: 6 step: 1081, loss is 0.09458144754171371\n",
      "epoch: 6 step: 1082, loss is 0.19848768413066864\n",
      "epoch: 6 step: 1083, loss is 0.8749933242797852\n",
      "epoch: 6 step: 1084, loss is 0.8285060524940491\n",
      "epoch: 6 step: 1085, loss is 0.14976459741592407\n",
      "epoch: 6 step: 1086, loss is 0.5550236701965332\n",
      "epoch: 6 step: 1087, loss is 0.1599733680486679\n",
      "epoch: 6 step: 1088, loss is 0.2104664295911789\n",
      "epoch: 6 step: 1089, loss is 0.15692712366580963\n",
      "epoch: 6 step: 1090, loss is 0.23736244440078735\n",
      "epoch: 6 step: 1091, loss is 0.42236021161079407\n",
      "epoch: 6 step: 1092, loss is 0.4865065813064575\n",
      "epoch: 6 step: 1093, loss is 0.7481805682182312\n",
      "epoch: 6 step: 1094, loss is 0.31758949160575867\n",
      "epoch: 6 step: 1095, loss is 0.42162901163101196\n",
      "epoch: 6 step: 1096, loss is 0.46015486121177673\n",
      "epoch: 6 step: 1097, loss is 0.14669285714626312\n",
      "epoch: 6 step: 1098, loss is 0.39341405034065247\n",
      "epoch: 6 step: 1099, loss is 0.34037676453590393\n",
      "epoch: 6 step: 1100, loss is 0.2426603138446808\n",
      "epoch: 6 step: 1101, loss is 0.6245466470718384\n",
      "epoch: 6 step: 1102, loss is 0.2478458285331726\n",
      "epoch: 6 step: 1103, loss is 0.08747626841068268\n",
      "epoch: 6 step: 1104, loss is 0.9762685894966125\n",
      "epoch: 6 step: 1105, loss is 0.27382397651672363\n",
      "epoch: 6 step: 1106, loss is 1.0413457155227661\n",
      "epoch: 6 step: 1107, loss is 0.40776312351226807\n",
      "epoch: 6 step: 1108, loss is 0.43285879492759705\n",
      "epoch: 6 step: 1109, loss is 0.648368775844574\n",
      "epoch: 6 step: 1110, loss is 0.3032112419605255\n",
      "epoch: 6 step: 1111, loss is 0.1733545958995819\n",
      "epoch: 6 step: 1112, loss is 0.4698525071144104\n",
      "epoch: 6 step: 1113, loss is 0.600798487663269\n",
      "epoch: 6 step: 1114, loss is 0.5856872200965881\n",
      "epoch: 6 step: 1115, loss is 0.2053803652524948\n",
      "epoch: 6 step: 1116, loss is 0.4057648479938507\n",
      "epoch: 6 step: 1117, loss is 0.1727444976568222\n",
      "epoch: 6 step: 1118, loss is 0.1813543140888214\n",
      "epoch: 6 step: 1119, loss is 0.5562703013420105\n",
      "epoch: 6 step: 1120, loss is 0.5787907838821411\n",
      "epoch: 6 step: 1121, loss is 0.40032631158828735\n",
      "epoch: 6 step: 1122, loss is 0.24353180825710297\n",
      "epoch: 6 step: 1123, loss is 0.6697662472724915\n",
      "epoch: 6 step: 1124, loss is 0.38766342401504517\n",
      "epoch: 6 step: 1125, loss is 0.6292404532432556\n",
      "epoch: 6 step: 1126, loss is 0.23021605610847473\n",
      "epoch: 6 step: 1127, loss is 0.4446066915988922\n",
      "epoch: 6 step: 1128, loss is 0.14126531779766083\n",
      "epoch: 6 step: 1129, loss is 0.231514573097229\n",
      "epoch: 6 step: 1130, loss is 0.19768545031547546\n",
      "epoch: 6 step: 1131, loss is 0.41187557578086853\n",
      "epoch: 6 step: 1132, loss is 0.2512001395225525\n",
      "epoch: 6 step: 1133, loss is 0.9308217167854309\n",
      "epoch: 6 step: 1134, loss is 0.42169350385665894\n",
      "epoch: 6 step: 1135, loss is 0.26310357451438904\n",
      "epoch: 6 step: 1136, loss is 0.06487376242876053\n",
      "epoch: 6 step: 1137, loss is 0.4280398488044739\n",
      "epoch: 6 step: 1138, loss is 0.5197811126708984\n",
      "epoch: 6 step: 1139, loss is 0.14566902816295624\n",
      "epoch: 6 step: 1140, loss is 0.16994255781173706\n",
      "epoch: 6 step: 1141, loss is 0.1364767998456955\n",
      "epoch: 6 step: 1142, loss is 0.17939452826976776\n",
      "epoch: 6 step: 1143, loss is 0.502470076084137\n",
      "epoch: 6 step: 1144, loss is 0.8173616528511047\n",
      "epoch: 6 step: 1145, loss is 0.1263013631105423\n",
      "epoch: 6 step: 1146, loss is 0.1536320596933365\n",
      "epoch: 6 step: 1147, loss is 0.1292557567358017\n",
      "epoch: 6 step: 1148, loss is 0.1406741440296173\n",
      "epoch: 6 step: 1149, loss is 0.10260342806577682\n",
      "epoch: 6 step: 1150, loss is 0.5393102765083313\n",
      "epoch: 6 step: 1151, loss is 0.4291202425956726\n",
      "epoch: 6 step: 1152, loss is 0.33001020550727844\n",
      "epoch: 6 step: 1153, loss is 0.39869120717048645\n",
      "epoch: 6 step: 1154, loss is 0.31386101245880127\n",
      "epoch: 6 step: 1155, loss is 0.12975351512432098\n",
      "epoch: 6 step: 1156, loss is 0.7965862154960632\n",
      "epoch: 6 step: 1157, loss is 0.17823387682437897\n",
      "epoch: 6 step: 1158, loss is 0.23346847295761108\n",
      "epoch: 6 step: 1159, loss is 0.12902212142944336\n",
      "epoch: 6 step: 1160, loss is 0.49628257751464844\n",
      "epoch: 6 step: 1161, loss is 0.2621954679489136\n",
      "epoch: 6 step: 1162, loss is 0.2070102095603943\n",
      "epoch: 6 step: 1163, loss is 0.3249129354953766\n",
      "epoch: 6 step: 1164, loss is 0.13902942836284637\n",
      "epoch: 6 step: 1165, loss is 0.6263707876205444\n",
      "epoch: 6 step: 1166, loss is 0.5957041382789612\n",
      "epoch: 6 step: 1167, loss is 0.09570521861314774\n",
      "epoch: 6 step: 1168, loss is 0.437825471162796\n",
      "epoch: 6 step: 1169, loss is 0.18093785643577576\n",
      "epoch: 6 step: 1170, loss is 0.27957281470298767\n",
      "epoch: 6 step: 1171, loss is 0.2870340645313263\n",
      "epoch: 6 step: 1172, loss is 0.14289933443069458\n",
      "epoch: 6 step: 1173, loss is 0.3946531414985657\n",
      "epoch: 6 step: 1174, loss is 0.312406450510025\n",
      "epoch: 6 step: 1175, loss is 0.29234975576400757\n",
      "epoch: 6 step: 1176, loss is 0.36022818088531494\n",
      "epoch: 6 step: 1177, loss is 0.5479670166969299\n",
      "epoch: 6 step: 1178, loss is 0.1750422865152359\n",
      "epoch: 6 step: 1179, loss is 0.43708735704421997\n",
      "epoch: 6 step: 1180, loss is 0.2547006607055664\n",
      "epoch: 6 step: 1181, loss is 0.44120967388153076\n",
      "epoch: 6 step: 1182, loss is 0.09294235706329346\n",
      "epoch: 6 step: 1183, loss is 0.26778435707092285\n",
      "epoch: 6 step: 1184, loss is 0.3627542555332184\n",
      "epoch: 6 step: 1185, loss is 0.29200026392936707\n",
      "epoch: 6 step: 1186, loss is 0.5455270409584045\n",
      "epoch: 6 step: 1187, loss is 0.4572218060493469\n",
      "epoch: 6 step: 1188, loss is 0.25667786598205566\n",
      "epoch: 6 step: 1189, loss is 0.20538894832134247\n",
      "epoch: 6 step: 1190, loss is 0.13043998181819916\n",
      "epoch: 6 step: 1191, loss is 0.5458284020423889\n",
      "epoch: 6 step: 1192, loss is 0.49727752804756165\n",
      "epoch: 6 step: 1193, loss is 0.5009543299674988\n",
      "epoch: 6 step: 1194, loss is 0.45045384764671326\n",
      "epoch: 6 step: 1195, loss is 0.33558082580566406\n",
      "epoch: 6 step: 1196, loss is 0.2228962630033493\n",
      "epoch: 6 step: 1197, loss is 0.42052724957466125\n",
      "epoch: 6 step: 1198, loss is 0.11883789300918579\n",
      "epoch: 6 step: 1199, loss is 1.5413025617599487\n",
      "epoch: 6 step: 1200, loss is 0.17854836583137512\n",
      "epoch: 6 step: 1201, loss is 0.6859253644943237\n",
      "epoch: 6 step: 1202, loss is 0.38121137022972107\n",
      "epoch: 6 step: 1203, loss is 0.29462572932243347\n",
      "epoch: 6 step: 1204, loss is 0.33903759717941284\n",
      "epoch: 6 step: 1205, loss is 0.2091308981180191\n",
      "epoch: 6 step: 1206, loss is 0.047569289803504944\n",
      "epoch: 6 step: 1207, loss is 0.3753969073295593\n",
      "epoch: 6 step: 1208, loss is 0.331003874540329\n",
      "epoch: 6 step: 1209, loss is 0.1004415974020958\n",
      "epoch: 6 step: 1210, loss is 0.31988343596458435\n",
      "epoch: 6 step: 1211, loss is 0.25211524963378906\n",
      "epoch: 6 step: 1212, loss is 0.5107244253158569\n",
      "epoch: 6 step: 1213, loss is 0.1526251584291458\n",
      "epoch: 6 step: 1214, loss is 0.19645309448242188\n",
      "epoch: 6 step: 1215, loss is 0.6307417750358582\n",
      "epoch: 6 step: 1216, loss is 0.8628405332565308\n",
      "epoch: 6 step: 1217, loss is 0.6121875643730164\n",
      "epoch: 6 step: 1218, loss is 0.2787191569805145\n",
      "epoch: 6 step: 1219, loss is 0.12742002308368683\n",
      "epoch: 6 step: 1220, loss is 0.17234556376934052\n",
      "epoch: 6 step: 1221, loss is 0.5226226449012756\n",
      "epoch: 6 step: 1222, loss is 0.4100610613822937\n",
      "epoch: 6 step: 1223, loss is 0.15174058079719543\n",
      "epoch: 6 step: 1224, loss is 0.24914947152137756\n",
      "epoch: 6 step: 1225, loss is 0.3534516394138336\n",
      "epoch: 6 step: 1226, loss is 0.10164917260408401\n",
      "epoch: 6 step: 1227, loss is 0.256053626537323\n",
      "epoch: 6 step: 1228, loss is 0.18123559653759003\n",
      "epoch: 6 step: 1229, loss is 0.6108067035675049\n",
      "epoch: 6 step: 1230, loss is 0.5118070840835571\n",
      "epoch: 6 step: 1231, loss is 0.1813104897737503\n",
      "epoch: 6 step: 1232, loss is 0.20029141008853912\n",
      "epoch: 6 step: 1233, loss is 0.26324662566185\n",
      "epoch: 6 step: 1234, loss is 0.4439951479434967\n",
      "epoch: 6 step: 1235, loss is 0.310242623090744\n",
      "epoch: 6 step: 1236, loss is 0.15544311702251434\n",
      "epoch: 6 step: 1237, loss is 0.5476483702659607\n",
      "epoch: 6 step: 1238, loss is 0.25226542353630066\n",
      "epoch: 6 step: 1239, loss is 0.1402735412120819\n",
      "epoch: 6 step: 1240, loss is 0.44543078541755676\n",
      "epoch: 6 step: 1241, loss is 0.8295055031776428\n",
      "epoch: 6 step: 1242, loss is 0.6084010601043701\n",
      "epoch: 6 step: 1243, loss is 0.4023100435733795\n",
      "epoch: 6 step: 1244, loss is 0.6222760677337646\n",
      "epoch: 6 step: 1245, loss is 1.0561574697494507\n",
      "epoch: 6 step: 1246, loss is 0.40403619408607483\n",
      "epoch: 6 step: 1247, loss is 0.38317322731018066\n",
      "epoch: 6 step: 1248, loss is 0.5293847322463989\n",
      "epoch: 6 step: 1249, loss is 0.5089697241783142\n",
      "epoch: 6 step: 1250, loss is 0.3464266061782837\n",
      "epoch: 6 step: 1251, loss is 0.1778986006975174\n",
      "epoch: 6 step: 1252, loss is 0.5234845876693726\n",
      "epoch: 6 step: 1253, loss is 0.6240389943122864\n",
      "epoch: 6 step: 1254, loss is 0.3486986756324768\n",
      "epoch: 6 step: 1255, loss is 0.34316498041152954\n",
      "epoch: 6 step: 1256, loss is 0.39150291681289673\n",
      "epoch: 6 step: 1257, loss is 0.2425311803817749\n",
      "epoch: 6 step: 1258, loss is 0.19169792532920837\n",
      "epoch: 6 step: 1259, loss is 0.1314983069896698\n",
      "epoch: 6 step: 1260, loss is 0.6152507662773132\n",
      "epoch: 6 step: 1261, loss is 0.45006224513053894\n",
      "epoch: 6 step: 1262, loss is 0.5151572227478027\n",
      "epoch: 6 step: 1263, loss is 0.5026554465293884\n",
      "epoch: 6 step: 1264, loss is 0.21933875977993011\n",
      "epoch: 6 step: 1265, loss is 0.4788072109222412\n",
      "epoch: 6 step: 1266, loss is 0.1514895111322403\n",
      "epoch: 6 step: 1267, loss is 0.0957101508975029\n",
      "epoch: 6 step: 1268, loss is 0.4168861508369446\n",
      "epoch: 6 step: 1269, loss is 1.1066452264785767\n",
      "epoch: 6 step: 1270, loss is 0.31302446126937866\n",
      "epoch: 6 step: 1271, loss is 0.37677526473999023\n",
      "epoch: 6 step: 1272, loss is 0.37735849618911743\n",
      "epoch: 6 step: 1273, loss is 0.3647696077823639\n",
      "epoch: 6 step: 1274, loss is 0.14136242866516113\n",
      "epoch: 6 step: 1275, loss is 0.22430899739265442\n",
      "epoch: 6 step: 1276, loss is 0.333793044090271\n",
      "epoch: 6 step: 1277, loss is 0.3449874818325043\n",
      "epoch: 6 step: 1278, loss is 0.2082964926958084\n",
      "epoch: 6 step: 1279, loss is 0.5119972229003906\n",
      "epoch: 6 step: 1280, loss is 0.13526196777820587\n",
      "epoch: 6 step: 1281, loss is 0.37556177377700806\n",
      "epoch: 6 step: 1282, loss is 0.18597707152366638\n",
      "epoch: 6 step: 1283, loss is 0.3687053620815277\n",
      "epoch: 6 step: 1284, loss is 0.22324629127979279\n",
      "epoch: 6 step: 1285, loss is 0.32117894291877747\n",
      "epoch: 6 step: 1286, loss is 0.9222270846366882\n",
      "epoch: 6 step: 1287, loss is 0.237252339720726\n",
      "epoch: 6 step: 1288, loss is 0.1745935082435608\n",
      "epoch: 6 step: 1289, loss is 0.17752385139465332\n",
      "epoch: 6 step: 1290, loss is 0.2146580070257187\n",
      "epoch: 6 step: 1291, loss is 0.3563135266304016\n",
      "epoch: 6 step: 1292, loss is 0.3145463764667511\n",
      "epoch: 6 step: 1293, loss is 0.49645882844924927\n",
      "epoch: 6 step: 1294, loss is 0.2940094769001007\n",
      "epoch: 6 step: 1295, loss is 0.36337313055992126\n",
      "epoch: 6 step: 1296, loss is 0.5509783625602722\n",
      "epoch: 6 step: 1297, loss is 0.2737373113632202\n",
      "epoch: 6 step: 1298, loss is 0.7811044454574585\n",
      "epoch: 6 step: 1299, loss is 0.20984651148319244\n",
      "epoch: 6 step: 1300, loss is 0.5370564460754395\n",
      "epoch: 6 step: 1301, loss is 0.2615313231945038\n",
      "epoch: 6 step: 1302, loss is 0.6293991804122925\n",
      "epoch: 6 step: 1303, loss is 0.2960219085216522\n",
      "epoch: 6 step: 1304, loss is 0.3028079867362976\n",
      "epoch: 6 step: 1305, loss is 0.31235337257385254\n",
      "epoch: 6 step: 1306, loss is 0.23816102743148804\n",
      "epoch: 6 step: 1307, loss is 0.6148856282234192\n",
      "epoch: 6 step: 1308, loss is 0.23425579071044922\n",
      "epoch: 6 step: 1309, loss is 0.22815491259098053\n",
      "epoch: 6 step: 1310, loss is 0.3944368362426758\n",
      "epoch: 6 step: 1311, loss is 0.24001896381378174\n",
      "epoch: 6 step: 1312, loss is 0.6373105049133301\n",
      "epoch: 6 step: 1313, loss is 0.2961905598640442\n",
      "epoch: 6 step: 1314, loss is 0.2945552468299866\n",
      "epoch: 6 step: 1315, loss is 0.7359681129455566\n",
      "epoch: 6 step: 1316, loss is 0.4467661678791046\n",
      "epoch: 6 step: 1317, loss is 0.5155203342437744\n",
      "epoch: 6 step: 1318, loss is 0.08085263520479202\n",
      "epoch: 6 step: 1319, loss is 0.5911726355552673\n",
      "epoch: 6 step: 1320, loss is 0.4513069689273834\n",
      "epoch: 6 step: 1321, loss is 0.12002717703580856\n",
      "epoch: 6 step: 1322, loss is 0.3740268051624298\n",
      "epoch: 6 step: 1323, loss is 0.3423074185848236\n",
      "epoch: 6 step: 1324, loss is 0.3160820007324219\n",
      "epoch: 6 step: 1325, loss is 0.19601263105869293\n",
      "epoch: 6 step: 1326, loss is 0.45019951462745667\n",
      "epoch: 6 step: 1327, loss is 0.36642220616340637\n",
      "epoch: 6 step: 1328, loss is 0.3520331382751465\n",
      "epoch: 6 step: 1329, loss is 0.5306910872459412\n",
      "epoch: 6 step: 1330, loss is 0.8161494731903076\n",
      "epoch: 6 step: 1331, loss is 0.3946363627910614\n",
      "epoch: 6 step: 1332, loss is 0.1535111963748932\n",
      "epoch: 6 step: 1333, loss is 0.3866741359233856\n",
      "epoch: 6 step: 1334, loss is 0.48783078789711\n",
      "epoch: 6 step: 1335, loss is 0.1900874227285385\n",
      "epoch: 6 step: 1336, loss is 1.1908249855041504\n",
      "epoch: 6 step: 1337, loss is 0.7462583780288696\n",
      "epoch: 6 step: 1338, loss is 0.788320779800415\n",
      "epoch: 6 step: 1339, loss is 0.0724659189581871\n",
      "epoch: 6 step: 1340, loss is 0.601046085357666\n",
      "epoch: 6 step: 1341, loss is 0.2621922194957733\n",
      "epoch: 6 step: 1342, loss is 0.40364351868629456\n",
      "epoch: 6 step: 1343, loss is 0.20676174759864807\n",
      "epoch: 6 step: 1344, loss is 0.4151443839073181\n",
      "epoch: 6 step: 1345, loss is 0.675310492515564\n",
      "epoch: 6 step: 1346, loss is 0.4769730567932129\n",
      "epoch: 6 step: 1347, loss is 0.530565619468689\n",
      "epoch: 6 step: 1348, loss is 0.6400253176689148\n",
      "epoch: 6 step: 1349, loss is 0.6302710771560669\n",
      "epoch: 6 step: 1350, loss is 0.26940593123435974\n",
      "epoch: 6 step: 1351, loss is 0.4699327349662781\n",
      "epoch: 6 step: 1352, loss is 0.2809983193874359\n",
      "epoch: 6 step: 1353, loss is 0.45296669006347656\n",
      "epoch: 6 step: 1354, loss is 0.3249111473560333\n",
      "epoch: 6 step: 1355, loss is 0.2573433518409729\n",
      "epoch: 6 step: 1356, loss is 0.32088547945022583\n",
      "epoch: 6 step: 1357, loss is 0.31308427453041077\n",
      "epoch: 6 step: 1358, loss is 0.5137976408004761\n",
      "epoch: 6 step: 1359, loss is 0.3140368163585663\n",
      "epoch: 6 step: 1360, loss is 0.40708765387535095\n",
      "epoch: 6 step: 1361, loss is 0.44927868247032166\n",
      "epoch: 6 step: 1362, loss is 0.2050691545009613\n",
      "epoch: 6 step: 1363, loss is 0.5055878758430481\n",
      "epoch: 6 step: 1364, loss is 0.35810860991477966\n",
      "epoch: 6 step: 1365, loss is 0.20927694439888\n",
      "epoch: 6 step: 1366, loss is 0.5359470248222351\n",
      "epoch: 6 step: 1367, loss is 0.29542970657348633\n",
      "epoch: 6 step: 1368, loss is 0.23462484776973724\n",
      "epoch: 6 step: 1369, loss is 0.15819516777992249\n",
      "epoch: 6 step: 1370, loss is 0.13786537945270538\n",
      "epoch: 6 step: 1371, loss is 0.39854300022125244\n",
      "epoch: 6 step: 1372, loss is 0.23903387784957886\n",
      "epoch: 6 step: 1373, loss is 0.17343008518218994\n",
      "epoch: 6 step: 1374, loss is 0.6512829065322876\n",
      "epoch: 6 step: 1375, loss is 0.32896921038627625\n",
      "epoch: 6 step: 1376, loss is 1.055917501449585\n",
      "epoch: 6 step: 1377, loss is 0.15863299369812012\n",
      "epoch: 6 step: 1378, loss is 0.36628690361976624\n",
      "epoch: 6 step: 1379, loss is 0.34218940138816833\n",
      "epoch: 6 step: 1380, loss is 0.20602640509605408\n",
      "epoch: 6 step: 1381, loss is 0.6579521894454956\n",
      "epoch: 6 step: 1382, loss is 0.37261706590652466\n",
      "epoch: 6 step: 1383, loss is 0.4095103144645691\n",
      "epoch: 6 step: 1384, loss is 0.19395871460437775\n",
      "epoch: 6 step: 1385, loss is 0.1519004851579666\n",
      "epoch: 6 step: 1386, loss is 0.2694917321205139\n",
      "epoch: 6 step: 1387, loss is 0.3880895674228668\n",
      "epoch: 6 step: 1388, loss is 0.31259146332740784\n",
      "epoch: 6 step: 1389, loss is 0.3157108426094055\n",
      "epoch: 6 step: 1390, loss is 0.2805112302303314\n",
      "epoch: 6 step: 1391, loss is 0.23877659440040588\n",
      "epoch: 6 step: 1392, loss is 0.4439316987991333\n",
      "epoch: 6 step: 1393, loss is 0.17143581807613373\n",
      "epoch: 6 step: 1394, loss is 0.20740272104740143\n",
      "epoch: 6 step: 1395, loss is 0.4944602847099304\n",
      "epoch: 6 step: 1396, loss is 0.3465205430984497\n",
      "epoch: 6 step: 1397, loss is 0.3661525845527649\n",
      "epoch: 6 step: 1398, loss is 0.49799734354019165\n",
      "epoch: 6 step: 1399, loss is 0.32305794954299927\n",
      "epoch: 6 step: 1400, loss is 0.8194092512130737\n",
      "epoch: 6 step: 1401, loss is 0.6370283365249634\n",
      "epoch: 6 step: 1402, loss is 0.35936808586120605\n",
      "epoch: 6 step: 1403, loss is 0.2919490933418274\n",
      "epoch: 6 step: 1404, loss is 0.30692872405052185\n",
      "epoch: 6 step: 1405, loss is 0.4933338761329651\n",
      "epoch: 6 step: 1406, loss is 0.25654280185699463\n",
      "epoch: 6 step: 1407, loss is 0.26588934659957886\n",
      "epoch: 6 step: 1408, loss is 0.40519002079963684\n",
      "epoch: 6 step: 1409, loss is 0.2873939871788025\n",
      "epoch: 6 step: 1410, loss is 0.1984378546476364\n",
      "epoch: 6 step: 1411, loss is 0.6979411840438843\n",
      "epoch: 6 step: 1412, loss is 0.28274446725845337\n",
      "epoch: 6 step: 1413, loss is 0.5101296305656433\n",
      "epoch: 6 step: 1414, loss is 0.18067559599876404\n",
      "epoch: 6 step: 1415, loss is 0.4406043589115143\n",
      "epoch: 6 step: 1416, loss is 0.27303430438041687\n",
      "epoch: 6 step: 1417, loss is 0.2395545095205307\n",
      "epoch: 6 step: 1418, loss is 0.3937487304210663\n",
      "epoch: 6 step: 1419, loss is 0.5352097153663635\n",
      "epoch: 6 step: 1420, loss is 0.7492431402206421\n",
      "epoch: 6 step: 1421, loss is 0.6053427457809448\n",
      "epoch: 6 step: 1422, loss is 0.34353676438331604\n",
      "epoch: 6 step: 1423, loss is 0.37604284286499023\n",
      "epoch: 6 step: 1424, loss is 0.8403032422065735\n",
      "epoch: 6 step: 1425, loss is 0.742766797542572\n",
      "epoch: 6 step: 1426, loss is 0.33320045471191406\n",
      "epoch: 6 step: 1427, loss is 0.18764309585094452\n",
      "epoch: 6 step: 1428, loss is 0.3803265392780304\n",
      "epoch: 6 step: 1429, loss is 0.5565133690834045\n",
      "epoch: 6 step: 1430, loss is 0.39782029390335083\n",
      "epoch: 6 step: 1431, loss is 0.4141786992549896\n",
      "epoch: 6 step: 1432, loss is 0.4017486572265625\n",
      "epoch: 6 step: 1433, loss is 0.6434067487716675\n",
      "epoch: 6 step: 1434, loss is 0.4229040741920471\n",
      "epoch: 6 step: 1435, loss is 0.19941388070583344\n",
      "epoch: 6 step: 1436, loss is 0.33531859517097473\n",
      "epoch: 6 step: 1437, loss is 0.08714143931865692\n",
      "epoch: 6 step: 1438, loss is 0.21307888627052307\n",
      "epoch: 6 step: 1439, loss is 0.5083163380622864\n",
      "epoch: 6 step: 1440, loss is 0.16907238960266113\n",
      "epoch: 6 step: 1441, loss is 0.37216201424598694\n",
      "epoch: 6 step: 1442, loss is 0.17676500976085663\n",
      "epoch: 6 step: 1443, loss is 0.343995064496994\n",
      "epoch: 6 step: 1444, loss is 0.6198273301124573\n",
      "epoch: 6 step: 1445, loss is 0.2753462493419647\n",
      "epoch: 6 step: 1446, loss is 0.40272256731987\n",
      "epoch: 6 step: 1447, loss is 0.4344962239265442\n",
      "epoch: 6 step: 1448, loss is 0.29947978258132935\n",
      "epoch: 6 step: 1449, loss is 0.5504583120346069\n",
      "epoch: 6 step: 1450, loss is 0.2779075801372528\n",
      "epoch: 6 step: 1451, loss is 0.37650564312934875\n",
      "epoch: 6 step: 1452, loss is 0.3170580267906189\n",
      "epoch: 6 step: 1453, loss is 0.5964866280555725\n",
      "epoch: 6 step: 1454, loss is 0.5196884274482727\n",
      "epoch: 6 step: 1455, loss is 0.429986834526062\n",
      "epoch: 6 step: 1456, loss is 0.2747412323951721\n",
      "epoch: 6 step: 1457, loss is 0.47033652663230896\n",
      "epoch: 6 step: 1458, loss is 0.12105228006839752\n",
      "epoch: 6 step: 1459, loss is 0.43788933753967285\n",
      "epoch: 6 step: 1460, loss is 0.2989000678062439\n",
      "epoch: 6 step: 1461, loss is 0.4224921762943268\n",
      "epoch: 6 step: 1462, loss is 0.3188178241252899\n",
      "epoch: 6 step: 1463, loss is 0.39215925335884094\n",
      "epoch: 6 step: 1464, loss is 0.24308830499649048\n",
      "epoch: 6 step: 1465, loss is 0.8211002349853516\n",
      "epoch: 6 step: 1466, loss is 0.15571223199367523\n",
      "epoch: 6 step: 1467, loss is 0.07760400325059891\n",
      "epoch: 6 step: 1468, loss is 0.16186340153217316\n",
      "epoch: 6 step: 1469, loss is 0.1529913991689682\n",
      "epoch: 6 step: 1470, loss is 0.4355577826499939\n",
      "epoch: 6 step: 1471, loss is 0.2411365658044815\n",
      "epoch: 6 step: 1472, loss is 0.4235171377658844\n",
      "epoch: 6 step: 1473, loss is 0.4898836016654968\n",
      "epoch: 6 step: 1474, loss is 0.15521544218063354\n",
      "epoch: 6 step: 1475, loss is 0.6200524568557739\n",
      "epoch: 6 step: 1476, loss is 0.7034798264503479\n",
      "epoch: 6 step: 1477, loss is 0.3709352910518646\n",
      "epoch: 6 step: 1478, loss is 0.6130481958389282\n",
      "epoch: 6 step: 1479, loss is 0.11400170624256134\n",
      "epoch: 6 step: 1480, loss is 0.3480713367462158\n",
      "epoch: 6 step: 1481, loss is 0.35965967178344727\n",
      "epoch: 6 step: 1482, loss is 0.25860777497291565\n",
      "epoch: 6 step: 1483, loss is 0.21774959564208984\n",
      "epoch: 6 step: 1484, loss is 0.37139037251472473\n",
      "epoch: 6 step: 1485, loss is 0.27271753549575806\n",
      "epoch: 6 step: 1486, loss is 0.36350423097610474\n",
      "epoch: 6 step: 1487, loss is 0.03740886226296425\n",
      "epoch: 6 step: 1488, loss is 0.11393918097019196\n",
      "epoch: 6 step: 1489, loss is 0.20139408111572266\n",
      "epoch: 6 step: 1490, loss is 0.06543348729610443\n",
      "epoch: 6 step: 1491, loss is 0.15499134361743927\n",
      "epoch: 6 step: 1492, loss is 0.4612783193588257\n",
      "epoch: 6 step: 1493, loss is 0.2177523374557495\n",
      "epoch: 6 step: 1494, loss is 0.050480812788009644\n",
      "epoch: 6 step: 1495, loss is 0.07801340520381927\n",
      "epoch: 6 step: 1496, loss is 0.38162094354629517\n",
      "epoch: 6 step: 1497, loss is 0.725281834602356\n",
      "epoch: 6 step: 1498, loss is 1.0161359310150146\n",
      "epoch: 6 step: 1499, loss is 0.7592388987541199\n",
      "epoch: 6 step: 1500, loss is 0.3087422549724579\n",
      "epoch: 6 step: 1501, loss is 0.3771178424358368\n",
      "epoch: 6 step: 1502, loss is 0.7084364295005798\n",
      "epoch: 6 step: 1503, loss is 0.30186164379119873\n",
      "epoch: 6 step: 1504, loss is 0.4058357775211334\n",
      "epoch: 6 step: 1505, loss is 0.32926204800605774\n",
      "epoch: 6 step: 1506, loss is 0.22928555309772491\n",
      "epoch: 6 step: 1507, loss is 0.14703910052776337\n",
      "epoch: 6 step: 1508, loss is 0.39700618386268616\n",
      "epoch: 6 step: 1509, loss is 0.3186444044113159\n",
      "epoch: 6 step: 1510, loss is 0.49171867966651917\n",
      "epoch: 6 step: 1511, loss is 0.7496363520622253\n",
      "epoch: 6 step: 1512, loss is 0.23285558819770813\n",
      "epoch: 6 step: 1513, loss is 0.3439251184463501\n",
      "epoch: 6 step: 1514, loss is 0.5501843094825745\n",
      "epoch: 6 step: 1515, loss is 0.5682531595230103\n",
      "epoch: 6 step: 1516, loss is 0.5225329995155334\n",
      "epoch: 6 step: 1517, loss is 0.43053191900253296\n",
      "epoch: 6 step: 1518, loss is 0.19636976718902588\n",
      "epoch: 6 step: 1519, loss is 0.2579435706138611\n",
      "epoch: 6 step: 1520, loss is 0.5591155290603638\n",
      "epoch: 6 step: 1521, loss is 0.09385506063699722\n",
      "epoch: 6 step: 1522, loss is 0.15068688988685608\n",
      "epoch: 6 step: 1523, loss is 0.12221390753984451\n",
      "epoch: 6 step: 1524, loss is 0.2891177535057068\n",
      "epoch: 6 step: 1525, loss is 0.4538871645927429\n",
      "epoch: 6 step: 1526, loss is 0.16615639626979828\n",
      "epoch: 6 step: 1527, loss is 0.3541850447654724\n",
      "epoch: 6 step: 1528, loss is 0.6961403489112854\n",
      "epoch: 6 step: 1529, loss is 0.24081316590309143\n",
      "epoch: 6 step: 1530, loss is 0.24426594376564026\n",
      "epoch: 6 step: 1531, loss is 0.44446972012519836\n",
      "epoch: 6 step: 1532, loss is 0.2869734466075897\n",
      "epoch: 6 step: 1533, loss is 0.1872592270374298\n",
      "epoch: 6 step: 1534, loss is 0.5315032005310059\n",
      "epoch: 6 step: 1535, loss is 0.22508548200130463\n",
      "epoch: 6 step: 1536, loss is 0.4195488691329956\n",
      "epoch: 6 step: 1537, loss is 0.31315481662750244\n",
      "epoch: 6 step: 1538, loss is 0.5146328210830688\n",
      "epoch: 6 step: 1539, loss is 0.3905280828475952\n",
      "epoch: 6 step: 1540, loss is 0.37415623664855957\n",
      "epoch: 6 step: 1541, loss is 1.2179574966430664\n",
      "epoch: 6 step: 1542, loss is 0.7715042233467102\n",
      "epoch: 6 step: 1543, loss is 0.4001772105693817\n",
      "epoch: 6 step: 1544, loss is 0.3189904987812042\n",
      "epoch: 6 step: 1545, loss is 0.43908023834228516\n",
      "epoch: 6 step: 1546, loss is 0.590291440486908\n",
      "epoch: 6 step: 1547, loss is 0.13193662464618683\n",
      "epoch: 6 step: 1548, loss is 0.31341561675071716\n",
      "epoch: 6 step: 1549, loss is 0.5092516541481018\n",
      "epoch: 6 step: 1550, loss is 0.28754767775535583\n",
      "epoch: 6 step: 1551, loss is 0.37339064478874207\n",
      "epoch: 6 step: 1552, loss is 0.5152258276939392\n",
      "epoch: 6 step: 1553, loss is 0.7409495711326599\n",
      "epoch: 6 step: 1554, loss is 0.22973068058490753\n",
      "epoch: 6 step: 1555, loss is 0.27963680028915405\n",
      "epoch: 6 step: 1556, loss is 0.28238898515701294\n",
      "epoch: 6 step: 1557, loss is 0.6538353562355042\n",
      "epoch: 6 step: 1558, loss is 0.5907002091407776\n",
      "epoch: 6 step: 1559, loss is 0.2815711796283722\n",
      "epoch: 6 step: 1560, loss is 0.2012995034456253\n",
      "epoch: 6 step: 1561, loss is 0.37424662709236145\n",
      "epoch: 6 step: 1562, loss is 0.4660026729106903\n",
      "epoch: 6 step: 1563, loss is 0.33847713470458984\n",
      "epoch: 6 step: 1564, loss is 0.14145131409168243\n",
      "epoch: 6 step: 1565, loss is 0.38778743147850037\n",
      "epoch: 6 step: 1566, loss is 0.3525905907154083\n",
      "epoch: 6 step: 1567, loss is 0.5861269235610962\n",
      "epoch: 6 step: 1568, loss is 0.220663383603096\n",
      "epoch: 6 step: 1569, loss is 0.7056201696395874\n",
      "epoch: 6 step: 1570, loss is 0.1933596283197403\n",
      "epoch: 6 step: 1571, loss is 0.119743213057518\n",
      "epoch: 6 step: 1572, loss is 0.1541803479194641\n",
      "epoch: 6 step: 1573, loss is 0.18608585000038147\n",
      "epoch: 6 step: 1574, loss is 0.09061519056558609\n",
      "epoch: 6 step: 1575, loss is 0.3410680592060089\n",
      "epoch: 6 step: 1576, loss is 0.27612626552581787\n",
      "epoch: 6 step: 1577, loss is 0.46189725399017334\n",
      "epoch: 6 step: 1578, loss is 0.7184867858886719\n",
      "epoch: 6 step: 1579, loss is 0.26579201221466064\n",
      "epoch: 6 step: 1580, loss is 0.33685287833213806\n",
      "epoch: 6 step: 1581, loss is 0.5351991653442383\n",
      "epoch: 6 step: 1582, loss is 0.44024327397346497\n",
      "epoch: 6 step: 1583, loss is 0.4657006859779358\n",
      "epoch: 6 step: 1584, loss is 0.34793099761009216\n",
      "epoch: 6 step: 1585, loss is 0.10278121381998062\n",
      "epoch: 6 step: 1586, loss is 0.32364293932914734\n",
      "epoch: 6 step: 1587, loss is 0.20231564342975616\n",
      "epoch: 6 step: 1588, loss is 0.30210432410240173\n",
      "epoch: 6 step: 1589, loss is 0.21309208869934082\n",
      "epoch: 6 step: 1590, loss is 0.2505204379558563\n",
      "epoch: 6 step: 1591, loss is 0.1833176463842392\n",
      "epoch: 6 step: 1592, loss is 0.2396543174982071\n",
      "epoch: 6 step: 1593, loss is 0.12567780911922455\n",
      "epoch: 6 step: 1594, loss is 0.4086260497570038\n",
      "epoch: 6 step: 1595, loss is 0.6370233297348022\n",
      "epoch: 6 step: 1596, loss is 0.35455843806266785\n",
      "epoch: 6 step: 1597, loss is 0.19560544192790985\n",
      "epoch: 6 step: 1598, loss is 0.07623009383678436\n",
      "epoch: 6 step: 1599, loss is 0.10037374496459961\n",
      "epoch: 6 step: 1600, loss is 0.1505296528339386\n",
      "epoch: 6 step: 1601, loss is 0.6458919048309326\n",
      "epoch: 6 step: 1602, loss is 0.26133444905281067\n",
      "epoch: 6 step: 1603, loss is 0.48953720927238464\n",
      "epoch: 6 step: 1604, loss is 0.19697819650173187\n",
      "epoch: 6 step: 1605, loss is 0.585011899471283\n",
      "epoch: 6 step: 1606, loss is 0.2230144888162613\n",
      "epoch: 6 step: 1607, loss is 0.14903727173805237\n",
      "epoch: 6 step: 1608, loss is 0.23016497492790222\n",
      "epoch: 6 step: 1609, loss is 0.2676156759262085\n",
      "epoch: 6 step: 1610, loss is 0.17825980484485626\n",
      "epoch: 6 step: 1611, loss is 0.841588020324707\n",
      "epoch: 6 step: 1612, loss is 0.4084652364253998\n",
      "epoch: 6 step: 1613, loss is 0.3872545063495636\n",
      "epoch: 6 step: 1614, loss is 0.3627769649028778\n",
      "epoch: 6 step: 1615, loss is 0.7072257399559021\n",
      "epoch: 6 step: 1616, loss is 0.3713926374912262\n",
      "epoch: 6 step: 1617, loss is 0.3472282886505127\n",
      "epoch: 6 step: 1618, loss is 0.25113749504089355\n",
      "epoch: 6 step: 1619, loss is 0.3447215259075165\n",
      "epoch: 6 step: 1620, loss is 0.35736626386642456\n",
      "epoch: 6 step: 1621, loss is 0.2465101182460785\n",
      "epoch: 6 step: 1622, loss is 0.1607353240251541\n",
      "epoch: 6 step: 1623, loss is 0.17326754331588745\n",
      "epoch: 6 step: 1624, loss is 0.37415629625320435\n",
      "epoch: 6 step: 1625, loss is 0.299369752407074\n",
      "epoch: 6 step: 1626, loss is 0.1543276011943817\n",
      "epoch: 6 step: 1627, loss is 0.4189298748970032\n",
      "epoch: 6 step: 1628, loss is 0.3157521188259125\n",
      "epoch: 6 step: 1629, loss is 0.5920430421829224\n",
      "epoch: 6 step: 1630, loss is 0.1332443356513977\n",
      "epoch: 6 step: 1631, loss is 0.18892686069011688\n",
      "epoch: 6 step: 1632, loss is 0.11696670204401016\n",
      "epoch: 6 step: 1633, loss is 0.31303513050079346\n",
      "epoch: 6 step: 1634, loss is 0.16265743970870972\n",
      "epoch: 6 step: 1635, loss is 0.7154398560523987\n",
      "epoch: 6 step: 1636, loss is 0.36229556798934937\n",
      "epoch: 6 step: 1637, loss is 0.8663922548294067\n",
      "epoch: 6 step: 1638, loss is 0.2334236204624176\n",
      "epoch: 6 step: 1639, loss is 0.06187203899025917\n",
      "epoch: 6 step: 1640, loss is 0.11076310276985168\n",
      "epoch: 6 step: 1641, loss is 0.3029164969921112\n",
      "epoch: 6 step: 1642, loss is 0.7105738520622253\n",
      "epoch: 6 step: 1643, loss is 0.17352521419525146\n",
      "epoch: 6 step: 1644, loss is 0.2984653413295746\n",
      "epoch: 6 step: 1645, loss is 0.603877067565918\n",
      "epoch: 6 step: 1646, loss is 0.39079269766807556\n",
      "epoch: 6 step: 1647, loss is 0.15550066530704498\n",
      "epoch: 6 step: 1648, loss is 0.41040071845054626\n",
      "epoch: 6 step: 1649, loss is 0.11083424091339111\n",
      "epoch: 6 step: 1650, loss is 0.1400797963142395\n",
      "epoch: 6 step: 1651, loss is 0.6266603469848633\n",
      "epoch: 6 step: 1652, loss is 0.09590001404285431\n",
      "epoch: 6 step: 1653, loss is 0.3844929039478302\n",
      "epoch: 6 step: 1654, loss is 0.3413723409175873\n",
      "epoch: 6 step: 1655, loss is 0.21826361119747162\n",
      "epoch: 6 step: 1656, loss is 0.4580535888671875\n",
      "epoch: 6 step: 1657, loss is 0.2817095220088959\n",
      "epoch: 6 step: 1658, loss is 0.05252709984779358\n",
      "epoch: 6 step: 1659, loss is 0.08118735998868942\n",
      "epoch: 6 step: 1660, loss is 0.8639161586761475\n",
      "epoch: 6 step: 1661, loss is 0.16890999674797058\n",
      "epoch: 6 step: 1662, loss is 1.0571167469024658\n",
      "epoch: 6 step: 1663, loss is 0.43524202704429626\n",
      "epoch: 6 step: 1664, loss is 0.40327560901641846\n",
      "epoch: 6 step: 1665, loss is 0.3596591055393219\n",
      "epoch: 6 step: 1666, loss is 0.6478384733200073\n",
      "epoch: 6 step: 1667, loss is 0.1670341044664383\n",
      "epoch: 6 step: 1668, loss is 1.1390103101730347\n",
      "epoch: 6 step: 1669, loss is 0.16651953756809235\n",
      "epoch: 6 step: 1670, loss is 0.43306902050971985\n",
      "epoch: 6 step: 1671, loss is 0.44688576459884644\n",
      "epoch: 6 step: 1672, loss is 0.2968927323818207\n",
      "epoch: 6 step: 1673, loss is 0.2531180679798126\n",
      "epoch: 6 step: 1674, loss is 0.7227854132652283\n",
      "epoch: 6 step: 1675, loss is 0.1945810168981552\n",
      "epoch: 6 step: 1676, loss is 0.33590248227119446\n",
      "epoch: 6 step: 1677, loss is 0.20245996117591858\n",
      "epoch: 6 step: 1678, loss is 0.26215246319770813\n",
      "epoch: 6 step: 1679, loss is 0.26452168822288513\n",
      "epoch: 6 step: 1680, loss is 0.2623036503791809\n",
      "epoch: 6 step: 1681, loss is 0.7574721574783325\n",
      "epoch: 6 step: 1682, loss is 0.4924020767211914\n",
      "epoch: 6 step: 1683, loss is 0.5616005063056946\n",
      "epoch: 6 step: 1684, loss is 0.22698679566383362\n",
      "epoch: 6 step: 1685, loss is 0.2604382336139679\n",
      "epoch: 6 step: 1686, loss is 0.4614879786968231\n",
      "epoch: 6 step: 1687, loss is 0.48957526683807373\n",
      "epoch: 6 step: 1688, loss is 0.29389816522598267\n",
      "epoch: 6 step: 1689, loss is 0.2191837579011917\n",
      "epoch: 6 step: 1690, loss is 0.29228371381759644\n",
      "epoch: 6 step: 1691, loss is 0.5448492765426636\n",
      "epoch: 6 step: 1692, loss is 0.3534833788871765\n",
      "epoch: 6 step: 1693, loss is 0.5731348991394043\n",
      "epoch: 6 step: 1694, loss is 1.655237078666687\n",
      "epoch: 6 step: 1695, loss is 0.27742716670036316\n",
      "epoch: 6 step: 1696, loss is 0.3655271530151367\n",
      "epoch: 6 step: 1697, loss is 0.7896317839622498\n",
      "epoch: 6 step: 1698, loss is 0.6970384120941162\n",
      "epoch: 6 step: 1699, loss is 0.620802104473114\n",
      "epoch: 6 step: 1700, loss is 0.2323995977640152\n",
      "epoch: 6 step: 1701, loss is 0.5534472465515137\n",
      "epoch: 6 step: 1702, loss is 0.10525679588317871\n",
      "epoch: 6 step: 1703, loss is 0.4204966127872467\n",
      "epoch: 6 step: 1704, loss is 0.2797042727470398\n",
      "epoch: 6 step: 1705, loss is 0.3706541061401367\n",
      "epoch: 6 step: 1706, loss is 0.706487774848938\n",
      "epoch: 6 step: 1707, loss is 0.5477729439735413\n",
      "epoch: 6 step: 1708, loss is 0.39955005049705505\n",
      "epoch: 6 step: 1709, loss is 0.3884204626083374\n",
      "epoch: 6 step: 1710, loss is 0.20823538303375244\n",
      "epoch: 6 step: 1711, loss is 0.603125810623169\n",
      "epoch: 6 step: 1712, loss is 0.4323323965072632\n",
      "epoch: 6 step: 1713, loss is 1.0582834482192993\n",
      "epoch: 6 step: 1714, loss is 1.038810133934021\n",
      "epoch: 6 step: 1715, loss is 0.38273146748542786\n",
      "epoch: 6 step: 1716, loss is 0.2981793284416199\n",
      "epoch: 6 step: 1717, loss is 0.32107797265052795\n",
      "epoch: 6 step: 1718, loss is 0.5615503191947937\n",
      "epoch: 6 step: 1719, loss is 0.3172113001346588\n",
      "epoch: 6 step: 1720, loss is 0.19921444356441498\n",
      "epoch: 6 step: 1721, loss is 0.23791976273059845\n",
      "epoch: 6 step: 1722, loss is 0.30175504088401794\n",
      "epoch: 6 step: 1723, loss is 0.1904374212026596\n",
      "epoch: 6 step: 1724, loss is 0.27386680245399475\n",
      "epoch: 6 step: 1725, loss is 0.3102673292160034\n",
      "epoch: 6 step: 1726, loss is 0.458347886800766\n",
      "epoch: 6 step: 1727, loss is 0.4301297068595886\n",
      "epoch: 6 step: 1728, loss is 0.5389896035194397\n",
      "epoch: 6 step: 1729, loss is 0.7741892337799072\n",
      "epoch: 6 step: 1730, loss is 0.28540512919425964\n",
      "epoch: 6 step: 1731, loss is 0.22630338370800018\n",
      "epoch: 6 step: 1732, loss is 0.4059203267097473\n",
      "epoch: 6 step: 1733, loss is 0.3551759421825409\n",
      "epoch: 6 step: 1734, loss is 0.3241051733493805\n",
      "epoch: 6 step: 1735, loss is 0.5084784030914307\n",
      "epoch: 6 step: 1736, loss is 0.3262163996696472\n",
      "epoch: 6 step: 1737, loss is 0.1934165507555008\n",
      "epoch: 6 step: 1738, loss is 0.2749374508857727\n",
      "epoch: 6 step: 1739, loss is 0.23310965299606323\n",
      "epoch: 6 step: 1740, loss is 0.3739599287509918\n",
      "epoch: 6 step: 1741, loss is 0.13261128962039948\n",
      "epoch: 6 step: 1742, loss is 0.32645079493522644\n",
      "epoch: 6 step: 1743, loss is 0.227660670876503\n",
      "epoch: 6 step: 1744, loss is 0.16759508848190308\n",
      "epoch: 6 step: 1745, loss is 0.17973600327968597\n",
      "epoch: 6 step: 1746, loss is 0.17045558989048004\n",
      "epoch: 6 step: 1747, loss is 0.18414346873760223\n",
      "epoch: 6 step: 1748, loss is 0.286288857460022\n",
      "epoch: 6 step: 1749, loss is 0.20920400321483612\n",
      "epoch: 6 step: 1750, loss is 0.7357388138771057\n",
      "epoch: 6 step: 1751, loss is 0.35907697677612305\n",
      "epoch: 6 step: 1752, loss is 0.3587169051170349\n",
      "epoch: 6 step: 1753, loss is 0.17573407292366028\n",
      "epoch: 6 step: 1754, loss is 0.05890005826950073\n",
      "epoch: 6 step: 1755, loss is 0.5765539407730103\n",
      "epoch: 6 step: 1756, loss is 0.9146488904953003\n",
      "epoch: 6 step: 1757, loss is 0.22768621146678925\n",
      "epoch: 6 step: 1758, loss is 0.14327512681484222\n",
      "epoch: 6 step: 1759, loss is 0.3053990304470062\n",
      "epoch: 6 step: 1760, loss is 0.08200658857822418\n",
      "epoch: 6 step: 1761, loss is 0.3200249671936035\n",
      "epoch: 6 step: 1762, loss is 0.11769687384366989\n",
      "epoch: 6 step: 1763, loss is 0.1125938892364502\n",
      "epoch: 6 step: 1764, loss is 0.21030965447425842\n",
      "epoch: 6 step: 1765, loss is 0.5875976085662842\n",
      "epoch: 6 step: 1766, loss is 0.127852201461792\n",
      "epoch: 6 step: 1767, loss is 0.22234441339969635\n",
      "epoch: 6 step: 1768, loss is 0.5672356486320496\n",
      "epoch: 6 step: 1769, loss is 0.2041812390089035\n",
      "epoch: 6 step: 1770, loss is 0.25309574604034424\n",
      "epoch: 6 step: 1771, loss is 0.841677188873291\n",
      "epoch: 6 step: 1772, loss is 0.6062420010566711\n",
      "epoch: 6 step: 1773, loss is 0.8475633859634399\n",
      "epoch: 6 step: 1774, loss is 0.2341233342885971\n",
      "epoch: 6 step: 1775, loss is 0.4398232102394104\n",
      "epoch: 6 step: 1776, loss is 0.3413804769515991\n",
      "epoch: 6 step: 1777, loss is 0.08485712110996246\n",
      "epoch: 6 step: 1778, loss is 0.6261613368988037\n",
      "epoch: 6 step: 1779, loss is 0.1541203409433365\n",
      "epoch: 6 step: 1780, loss is 0.2898525893688202\n",
      "epoch: 6 step: 1781, loss is 0.3606261909008026\n",
      "epoch: 6 step: 1782, loss is 0.6553345918655396\n",
      "epoch: 6 step: 1783, loss is 0.28550755977630615\n",
      "epoch: 6 step: 1784, loss is 0.1023329347372055\n",
      "epoch: 6 step: 1785, loss is 0.31242239475250244\n",
      "epoch: 6 step: 1786, loss is 0.2670888602733612\n",
      "epoch: 6 step: 1787, loss is 0.531308650970459\n",
      "epoch: 6 step: 1788, loss is 0.4484667479991913\n",
      "epoch: 6 step: 1789, loss is 0.4261361360549927\n",
      "epoch: 6 step: 1790, loss is 0.376285582780838\n",
      "epoch: 6 step: 1791, loss is 0.10059039294719696\n",
      "epoch: 6 step: 1792, loss is 0.6603638529777527\n",
      "epoch: 6 step: 1793, loss is 0.4048047959804535\n",
      "epoch: 6 step: 1794, loss is 0.1776452362537384\n",
      "epoch: 6 step: 1795, loss is 0.11613982170820236\n",
      "epoch: 6 step: 1796, loss is 0.42911821603775024\n",
      "epoch: 6 step: 1797, loss is 1.0776145458221436\n",
      "epoch: 6 step: 1798, loss is 0.3391477167606354\n",
      "epoch: 6 step: 1799, loss is 0.11720439046621323\n",
      "epoch: 6 step: 1800, loss is 0.23067164421081543\n",
      "epoch: 6 step: 1801, loss is 0.209426611661911\n",
      "epoch: 6 step: 1802, loss is 0.4412662386894226\n",
      "epoch: 6 step: 1803, loss is 0.23499193787574768\n",
      "epoch: 6 step: 1804, loss is 0.18135201930999756\n",
      "epoch: 6 step: 1805, loss is 0.17678502202033997\n",
      "epoch: 6 step: 1806, loss is 0.1791967898607254\n",
      "epoch: 6 step: 1807, loss is 0.34588685631752014\n",
      "epoch: 6 step: 1808, loss is 0.3826608955860138\n",
      "epoch: 6 step: 1809, loss is 0.26702138781547546\n",
      "epoch: 6 step: 1810, loss is 0.5570534467697144\n",
      "epoch: 6 step: 1811, loss is 0.3531251549720764\n",
      "epoch: 6 step: 1812, loss is 0.2965066730976105\n",
      "epoch: 6 step: 1813, loss is 0.12765493988990784\n",
      "epoch: 6 step: 1814, loss is 0.29769366979599\n",
      "epoch: 6 step: 1815, loss is 0.07515989243984222\n",
      "epoch: 6 step: 1816, loss is 0.21958677470684052\n",
      "epoch: 6 step: 1817, loss is 0.45736825466156006\n",
      "epoch: 6 step: 1818, loss is 0.42296117544174194\n",
      "epoch: 6 step: 1819, loss is 0.29428550601005554\n",
      "epoch: 6 step: 1820, loss is 0.2408418357372284\n",
      "epoch: 6 step: 1821, loss is 0.2196262925863266\n",
      "epoch: 6 step: 1822, loss is 0.04579812288284302\n",
      "epoch: 6 step: 1823, loss is 0.11678222566843033\n",
      "epoch: 6 step: 1824, loss is 0.41745784878730774\n",
      "epoch: 6 step: 1825, loss is 0.890579342842102\n",
      "epoch: 6 step: 1826, loss is 0.16979575157165527\n",
      "epoch: 6 step: 1827, loss is 0.13920535147190094\n",
      "epoch: 6 step: 1828, loss is 0.21448597311973572\n",
      "epoch: 6 step: 1829, loss is 0.3245895802974701\n",
      "epoch: 6 step: 1830, loss is 0.21426784992218018\n",
      "epoch: 6 step: 1831, loss is 0.8575090765953064\n",
      "epoch: 6 step: 1832, loss is 0.15268577635288239\n",
      "epoch: 6 step: 1833, loss is 0.24824194610118866\n",
      "epoch: 6 step: 1834, loss is 0.5496675968170166\n",
      "epoch: 6 step: 1835, loss is 0.5839847326278687\n",
      "epoch: 6 step: 1836, loss is 0.8750540018081665\n",
      "epoch: 6 step: 1837, loss is 0.0790572315454483\n",
      "epoch: 6 step: 1838, loss is 0.38535547256469727\n",
      "epoch: 6 step: 1839, loss is 0.34744858741760254\n",
      "epoch: 6 step: 1840, loss is 0.3889997899532318\n",
      "epoch: 6 step: 1841, loss is 0.5120306611061096\n",
      "epoch: 6 step: 1842, loss is 0.28225722908973694\n",
      "epoch: 6 step: 1843, loss is 0.2921140491962433\n",
      "epoch: 6 step: 1844, loss is 0.1070311889052391\n",
      "epoch: 6 step: 1845, loss is 0.36667078733444214\n",
      "epoch: 6 step: 1846, loss is 0.2839590907096863\n",
      "epoch: 6 step: 1847, loss is 0.21244540810585022\n",
      "epoch: 6 step: 1848, loss is 0.2786498963832855\n",
      "epoch: 6 step: 1849, loss is 0.17094922065734863\n",
      "epoch: 6 step: 1850, loss is 0.4105781614780426\n",
      "epoch: 6 step: 1851, loss is 0.13131289184093475\n",
      "epoch: 6 step: 1852, loss is 0.5006740093231201\n",
      "epoch: 6 step: 1853, loss is 0.39345353841781616\n",
      "epoch: 6 step: 1854, loss is 0.4471980929374695\n",
      "epoch: 6 step: 1855, loss is 0.6156136393547058\n",
      "epoch: 6 step: 1856, loss is 0.203581303358078\n",
      "epoch: 6 step: 1857, loss is 0.4730052351951599\n",
      "epoch: 6 step: 1858, loss is 0.4002865254878998\n",
      "epoch: 6 step: 1859, loss is 0.4814130663871765\n",
      "epoch: 6 step: 1860, loss is 0.28789222240448\n",
      "epoch: 6 step: 1861, loss is 0.31242477893829346\n",
      "epoch: 6 step: 1862, loss is 0.08277789503335953\n",
      "epoch: 6 step: 1863, loss is 0.18694660067558289\n",
      "epoch: 6 step: 1864, loss is 0.63685142993927\n",
      "epoch: 6 step: 1865, loss is 0.20418232679367065\n",
      "epoch: 6 step: 1866, loss is 0.4223800301551819\n",
      "epoch: 6 step: 1867, loss is 0.786213219165802\n",
      "epoch: 6 step: 1868, loss is 0.25676611065864563\n",
      "epoch: 6 step: 1869, loss is 0.33850911259651184\n",
      "epoch: 6 step: 1870, loss is 0.41253426671028137\n",
      "epoch: 6 step: 1871, loss is 0.22266390919685364\n",
      "epoch: 6 step: 1872, loss is 0.18814589083194733\n",
      "epoch: 6 step: 1873, loss is 0.18568110466003418\n",
      "epoch: 6 step: 1874, loss is 0.21006934344768524\n",
      "epoch: 6 step: 1875, loss is 0.5302315354347229\n",
      "epoch: 6 step: 1876, loss is 0.14911629259586334\n",
      "epoch: 6 step: 1877, loss is 0.18756619095802307\n",
      "epoch: 6 step: 1878, loss is 0.18078704178333282\n",
      "epoch: 6 step: 1879, loss is 0.031114831566810608\n",
      "epoch: 6 step: 1880, loss is 0.4145142138004303\n",
      "epoch: 6 step: 1881, loss is 0.28802943229675293\n",
      "epoch: 6 step: 1882, loss is 0.3168720006942749\n",
      "epoch: 6 step: 1883, loss is 0.47250574827194214\n",
      "epoch: 6 step: 1884, loss is 0.4463721513748169\n",
      "epoch: 6 step: 1885, loss is 0.40963757038116455\n",
      "epoch: 6 step: 1886, loss is 0.2590026259422302\n",
      "epoch: 6 step: 1887, loss is 0.5905515551567078\n",
      "epoch: 6 step: 1888, loss is 0.10833090543746948\n",
      "epoch: 6 step: 1889, loss is 0.4436098337173462\n",
      "epoch: 6 step: 1890, loss is 0.3700316548347473\n",
      "epoch: 6 step: 1891, loss is 0.9019472599029541\n",
      "epoch: 6 step: 1892, loss is 0.3391529321670532\n",
      "epoch: 6 step: 1893, loss is 0.44775140285491943\n",
      "epoch: 6 step: 1894, loss is 0.24539175629615784\n",
      "epoch: 6 step: 1895, loss is 0.498637318611145\n",
      "epoch: 6 step: 1896, loss is 0.32941481471061707\n",
      "epoch: 6 step: 1897, loss is 0.05149397253990173\n",
      "epoch: 6 step: 1898, loss is 0.14634697139263153\n",
      "epoch: 6 step: 1899, loss is 0.15900783240795135\n",
      "epoch: 6 step: 1900, loss is 0.5327925682067871\n",
      "epoch: 6 step: 1901, loss is 0.6410793662071228\n",
      "epoch: 6 step: 1902, loss is 0.29022514820098877\n",
      "epoch: 6 step: 1903, loss is 0.4269018769264221\n",
      "epoch: 6 step: 1904, loss is 0.5364522337913513\n",
      "epoch: 6 step: 1905, loss is 0.36393094062805176\n",
      "epoch: 6 step: 1906, loss is 0.11325899511575699\n",
      "epoch: 6 step: 1907, loss is 0.2587987780570984\n",
      "epoch: 6 step: 1908, loss is 0.2605207860469818\n",
      "epoch: 6 step: 1909, loss is 0.1282544583082199\n",
      "epoch: 6 step: 1910, loss is 0.4663083851337433\n",
      "epoch: 6 step: 1911, loss is 0.1437990963459015\n",
      "epoch: 6 step: 1912, loss is 0.29180818796157837\n",
      "epoch: 6 step: 1913, loss is 0.038833729922771454\n",
      "epoch: 6 step: 1914, loss is 0.08712448924779892\n",
      "epoch: 6 step: 1915, loss is 0.5740248560905457\n",
      "epoch: 6 step: 1916, loss is 0.5001165866851807\n",
      "epoch: 6 step: 1917, loss is 0.17966723442077637\n",
      "epoch: 6 step: 1918, loss is 0.4837384819984436\n",
      "epoch: 6 step: 1919, loss is 0.11409662663936615\n",
      "epoch: 6 step: 1920, loss is 0.372265487909317\n",
      "epoch: 6 step: 1921, loss is 0.25313690304756165\n",
      "epoch: 6 step: 1922, loss is 0.5074483156204224\n",
      "epoch: 6 step: 1923, loss is 0.36974263191223145\n",
      "epoch: 6 step: 1924, loss is 0.39695504307746887\n",
      "epoch: 6 step: 1925, loss is 0.4181552827358246\n",
      "epoch: 6 step: 1926, loss is 0.2926350235939026\n",
      "epoch: 6 step: 1927, loss is 0.4970647990703583\n",
      "epoch: 6 step: 1928, loss is 0.3484419584274292\n",
      "epoch: 6 step: 1929, loss is 0.3858318030834198\n",
      "epoch: 6 step: 1930, loss is 0.4263553023338318\n",
      "epoch: 6 step: 1931, loss is 0.3337368965148926\n",
      "epoch: 6 step: 1932, loss is 0.4107381999492645\n",
      "epoch: 6 step: 1933, loss is 0.3724804222583771\n",
      "epoch: 6 step: 1934, loss is 0.2926015853881836\n",
      "epoch: 6 step: 1935, loss is 0.23518060147762299\n",
      "epoch: 6 step: 1936, loss is 0.40083786845207214\n",
      "epoch: 6 step: 1937, loss is 0.6598271727561951\n",
      "epoch: 6 step: 1938, loss is 0.7783923149108887\n",
      "epoch: 6 step: 1939, loss is 0.4609554708003998\n",
      "epoch: 6 step: 1940, loss is 0.22204823791980743\n",
      "epoch: 6 step: 1941, loss is 0.6839749217033386\n",
      "epoch: 6 step: 1942, loss is 0.4103431701660156\n",
      "epoch: 6 step: 1943, loss is 0.3509030342102051\n",
      "epoch: 6 step: 1944, loss is 0.43593937158584595\n",
      "epoch: 6 step: 1945, loss is 0.5942308306694031\n",
      "epoch: 6 step: 1946, loss is 0.34595829248428345\n",
      "epoch: 6 step: 1947, loss is 0.1799125224351883\n",
      "epoch: 6 step: 1948, loss is 0.2769625782966614\n",
      "epoch: 6 step: 1949, loss is 0.27935728430747986\n",
      "epoch: 6 step: 1950, loss is 0.24115295708179474\n",
      "epoch: 6 step: 1951, loss is 0.32789623737335205\n",
      "epoch: 6 step: 1952, loss is 0.186863973736763\n",
      "epoch: 6 step: 1953, loss is 0.2819341719150543\n",
      "epoch: 6 step: 1954, loss is 0.11267005652189255\n",
      "epoch: 6 step: 1955, loss is 0.418234646320343\n",
      "epoch: 6 step: 1956, loss is 0.3832906484603882\n",
      "epoch: 6 step: 1957, loss is 0.8710184097290039\n",
      "epoch: 6 step: 1958, loss is 0.08422993123531342\n",
      "epoch: 6 step: 1959, loss is 0.20376235246658325\n",
      "epoch: 6 step: 1960, loss is 0.2114098221063614\n",
      "epoch: 6 step: 1961, loss is 0.6394112706184387\n",
      "epoch: 6 step: 1962, loss is 0.16774475574493408\n",
      "epoch: 6 step: 1963, loss is 0.34497466683387756\n",
      "epoch: 6 step: 1964, loss is 0.2897748351097107\n",
      "epoch: 6 step: 1965, loss is 0.38187649846076965\n",
      "epoch: 6 step: 1966, loss is 0.24842801690101624\n",
      "epoch: 6 step: 1967, loss is 0.15432092547416687\n",
      "epoch: 6 step: 1968, loss is 0.7806576490402222\n",
      "epoch: 6 step: 1969, loss is 0.24069394171237946\n",
      "epoch: 6 step: 1970, loss is 0.20720957219600677\n",
      "epoch: 6 step: 1971, loss is 0.2401324212551117\n",
      "epoch: 6 step: 1972, loss is 0.20875006914138794\n",
      "epoch: 6 step: 1973, loss is 0.6985144019126892\n",
      "epoch: 6 step: 1974, loss is 0.16718943417072296\n",
      "epoch: 6 step: 1975, loss is 0.11083577573299408\n",
      "epoch: 6 step: 1976, loss is 0.27989062666893005\n",
      "epoch: 6 step: 1977, loss is 1.094828724861145\n",
      "epoch: 6 step: 1978, loss is 0.5159613490104675\n",
      "epoch: 6 step: 1979, loss is 0.11745469272136688\n",
      "epoch: 6 step: 1980, loss is 0.48525145649909973\n",
      "epoch: 6 step: 1981, loss is 0.05189038813114166\n",
      "epoch: 6 step: 1982, loss is 0.21012161672115326\n",
      "epoch: 6 step: 1983, loss is 0.5187060236930847\n",
      "epoch: 6 step: 1984, loss is 0.20153777301311493\n",
      "epoch: 6 step: 1985, loss is 0.35260671377182007\n",
      "epoch: 6 step: 1986, loss is 0.2919515073299408\n",
      "epoch: 6 step: 1987, loss is 0.44325369596481323\n",
      "epoch: 6 step: 1988, loss is 0.4605923891067505\n",
      "epoch: 6 step: 1989, loss is 0.6121141910552979\n",
      "epoch: 6 step: 1990, loss is 0.9039764404296875\n",
      "epoch: 6 step: 1991, loss is 0.3693021535873413\n",
      "epoch: 6 step: 1992, loss is 0.2013906091451645\n",
      "epoch: 6 step: 1993, loss is 0.1291048675775528\n",
      "epoch: 6 step: 1994, loss is 0.21849939227104187\n",
      "epoch: 6 step: 1995, loss is 0.19698531925678253\n",
      "epoch: 6 step: 1996, loss is 0.6986414194107056\n",
      "epoch: 6 step: 1997, loss is 0.12117193639278412\n",
      "epoch: 6 step: 1998, loss is 0.4107191264629364\n",
      "epoch: 6 step: 1999, loss is 0.13940255343914032\n",
      "epoch: 6 step: 2000, loss is 0.34835711121559143\n",
      "epoch: 6 step: 2001, loss is 0.2081567347049713\n",
      "epoch: 6 step: 2002, loss is 0.1325463354587555\n",
      "epoch: 6 step: 2003, loss is 0.11060861498117447\n",
      "epoch: 6 step: 2004, loss is 0.20178133249282837\n",
      "epoch: 6 step: 2005, loss is 0.06747625023126602\n",
      "epoch: 6 step: 2006, loss is 0.5749219655990601\n",
      "epoch: 6 step: 2007, loss is 0.3980254530906677\n",
      "epoch: 6 step: 2008, loss is 0.6278305649757385\n",
      "epoch: 6 step: 2009, loss is 0.1509256660938263\n",
      "epoch: 6 step: 2010, loss is 0.08220736682415009\n",
      "epoch: 6 step: 2011, loss is 0.12203002721071243\n",
      "epoch: 6 step: 2012, loss is 0.6477609872817993\n",
      "epoch: 6 step: 2013, loss is 0.2232140302658081\n",
      "epoch: 6 step: 2014, loss is 0.5484496355056763\n",
      "epoch: 6 step: 2015, loss is 0.48147138953208923\n",
      "epoch: 6 step: 2016, loss is 0.42498159408569336\n",
      "epoch: 6 step: 2017, loss is 0.2647753059864044\n",
      "epoch: 6 step: 2018, loss is 0.80769944190979\n",
      "epoch: 6 step: 2019, loss is 0.2398865669965744\n",
      "epoch: 6 step: 2020, loss is 0.22322934865951538\n",
      "epoch: 6 step: 2021, loss is 0.21465414762496948\n",
      "epoch: 6 step: 2022, loss is 0.36272814869880676\n",
      "epoch: 6 step: 2023, loss is 0.4159921109676361\n",
      "epoch: 6 step: 2024, loss is 0.2018650472164154\n",
      "epoch: 6 step: 2025, loss is 0.1996191293001175\n",
      "epoch: 6 step: 2026, loss is 0.42860275506973267\n",
      "epoch: 6 step: 2027, loss is 0.3631245493888855\n",
      "epoch: 6 step: 2028, loss is 0.20812800526618958\n",
      "epoch: 6 step: 2029, loss is 0.3292709290981293\n",
      "epoch: 6 step: 2030, loss is 0.730945348739624\n",
      "epoch: 6 step: 2031, loss is 0.36405640840530396\n",
      "epoch: 6 step: 2032, loss is 0.5346969366073608\n",
      "epoch: 6 step: 2033, loss is 0.5724110007286072\n",
      "epoch: 6 step: 2034, loss is 0.20483063161373138\n",
      "epoch: 6 step: 2035, loss is 0.229181170463562\n",
      "epoch: 6 step: 2036, loss is 0.06717772036790848\n",
      "epoch: 6 step: 2037, loss is 0.06881033629179001\n",
      "epoch: 6 step: 2038, loss is 0.11138849705457687\n",
      "epoch: 6 step: 2039, loss is 0.3031621277332306\n",
      "epoch: 6 step: 2040, loss is 0.20786947011947632\n",
      "epoch: 6 step: 2041, loss is 0.38972699642181396\n",
      "epoch: 6 step: 2042, loss is 0.476503849029541\n",
      "epoch: 6 step: 2043, loss is 0.31370583176612854\n",
      "epoch: 6 step: 2044, loss is 0.0796574279665947\n",
      "epoch: 6 step: 2045, loss is 0.6720886826515198\n",
      "epoch: 6 step: 2046, loss is 0.8208779692649841\n",
      "epoch: 6 step: 2047, loss is 0.8725053071975708\n",
      "epoch: 6 step: 2048, loss is 0.5375627279281616\n",
      "epoch: 6 step: 2049, loss is 0.06971846520900726\n",
      "epoch: 6 step: 2050, loss is 0.5912585258483887\n",
      "epoch: 6 step: 2051, loss is 0.3744913339614868\n",
      "epoch: 6 step: 2052, loss is 0.45206835865974426\n",
      "epoch: 6 step: 2053, loss is 0.45058688521385193\n",
      "epoch: 6 step: 2054, loss is 0.050684649497270584\n",
      "epoch: 6 step: 2055, loss is 0.11826043576002121\n",
      "epoch: 6 step: 2056, loss is 0.49302664399147034\n",
      "epoch: 6 step: 2057, loss is 0.2527438998222351\n",
      "epoch: 6 step: 2058, loss is 1.022306203842163\n",
      "epoch: 6 step: 2059, loss is 0.4940692186355591\n",
      "epoch: 6 step: 2060, loss is 0.2560955882072449\n",
      "epoch: 6 step: 2061, loss is 0.2177845984697342\n",
      "epoch: 6 step: 2062, loss is 0.2894643247127533\n",
      "epoch: 6 step: 2063, loss is 0.500446081161499\n",
      "epoch: 6 step: 2064, loss is 0.2282285839319229\n",
      "epoch: 6 step: 2065, loss is 0.5023216009140015\n",
      "epoch: 6 step: 2066, loss is 0.4419439136981964\n",
      "epoch: 6 step: 2067, loss is 0.5653121471405029\n",
      "epoch: 6 step: 2068, loss is 0.44180914759635925\n",
      "epoch: 6 step: 2069, loss is 0.41583096981048584\n",
      "epoch: 6 step: 2070, loss is 0.09924358129501343\n",
      "epoch: 6 step: 2071, loss is 0.30831557512283325\n",
      "epoch: 6 step: 2072, loss is 0.5048254728317261\n",
      "epoch: 6 step: 2073, loss is 0.39689987897872925\n",
      "epoch: 6 step: 2074, loss is 0.32321104407310486\n",
      "epoch: 6 step: 2075, loss is 0.7757939696311951\n",
      "epoch: 6 step: 2076, loss is 0.7086195945739746\n",
      "epoch: 6 step: 2077, loss is 0.2976057529449463\n",
      "epoch: 6 step: 2078, loss is 0.25110945105552673\n",
      "epoch: 6 step: 2079, loss is 0.2973567843437195\n",
      "epoch: 6 step: 2080, loss is 0.37645113468170166\n",
      "epoch: 6 step: 2081, loss is 0.30068689584732056\n",
      "epoch: 6 step: 2082, loss is 0.4362521469593048\n",
      "epoch: 6 step: 2083, loss is 0.3653044104576111\n",
      "epoch: 6 step: 2084, loss is 0.30222153663635254\n",
      "epoch: 6 step: 2085, loss is 0.45315343141555786\n",
      "epoch: 6 step: 2086, loss is 0.4629248082637787\n",
      "epoch: 6 step: 2087, loss is 0.2581360340118408\n",
      "epoch: 6 step: 2088, loss is 0.24392789602279663\n",
      "epoch: 6 step: 2089, loss is 0.44159719347953796\n",
      "epoch: 6 step: 2090, loss is 0.19095216691493988\n",
      "epoch: 6 step: 2091, loss is 0.5861688256263733\n",
      "epoch: 6 step: 2092, loss is 0.1238289475440979\n",
      "epoch: 6 step: 2093, loss is 0.5020431876182556\n",
      "epoch: 6 step: 2094, loss is 0.5986636877059937\n",
      "epoch: 6 step: 2095, loss is 0.20673152804374695\n",
      "epoch: 6 step: 2096, loss is 0.2890304625034332\n",
      "epoch: 6 step: 2097, loss is 0.1435597985982895\n",
      "epoch: 6 step: 2098, loss is 0.3289881646633148\n",
      "epoch: 6 step: 2099, loss is 0.8591163158416748\n",
      "epoch: 6 step: 2100, loss is 0.35296282172203064\n",
      "epoch: 6 step: 2101, loss is 0.4489893913269043\n",
      "epoch: 6 step: 2102, loss is 0.1540670245885849\n",
      "epoch: 6 step: 2103, loss is 0.19408012926578522\n",
      "epoch: 6 step: 2104, loss is 0.5769705772399902\n",
      "epoch: 6 step: 2105, loss is 0.3053898811340332\n",
      "epoch: 6 step: 2106, loss is 0.3961126208305359\n",
      "epoch: 6 step: 2107, loss is 0.15119591355323792\n",
      "epoch: 6 step: 2108, loss is 0.43870899081230164\n",
      "epoch: 6 step: 2109, loss is 0.5016074776649475\n",
      "epoch: 6 step: 2110, loss is 0.1232893317937851\n",
      "epoch: 6 step: 2111, loss is 0.5651772022247314\n",
      "epoch: 6 step: 2112, loss is 0.3395973742008209\n",
      "epoch: 6 step: 2113, loss is 0.3385820686817169\n",
      "epoch: 6 step: 2114, loss is 0.19075334072113037\n",
      "epoch: 6 step: 2115, loss is 0.3610544204711914\n",
      "epoch: 6 step: 2116, loss is 0.5580783486366272\n",
      "epoch: 6 step: 2117, loss is 0.367472767829895\n",
      "epoch: 6 step: 2118, loss is 0.19610360264778137\n",
      "epoch: 6 step: 2119, loss is 0.2482798993587494\n",
      "epoch: 6 step: 2120, loss is 0.07851594686508179\n",
      "epoch: 6 step: 2121, loss is 0.3093760311603546\n",
      "epoch: 6 step: 2122, loss is 0.14085052907466888\n",
      "epoch: 6 step: 2123, loss is 0.42644166946411133\n",
      "epoch: 6 step: 2124, loss is 0.38311588764190674\n",
      "epoch: 6 step: 2125, loss is 0.4412469267845154\n",
      "epoch: 6 step: 2126, loss is 0.22376561164855957\n",
      "epoch: 6 step: 2127, loss is 0.08399832993745804\n",
      "epoch: 6 step: 2128, loss is 0.37042635679244995\n",
      "epoch: 6 step: 2129, loss is 0.20775409042835236\n",
      "epoch: 6 step: 2130, loss is 0.43359339237213135\n",
      "epoch: 6 step: 2131, loss is 0.109549880027771\n",
      "epoch: 6 step: 2132, loss is 0.5229756236076355\n",
      "epoch: 6 step: 2133, loss is 0.9538611769676208\n",
      "epoch: 6 step: 2134, loss is 0.2389451414346695\n",
      "epoch: 6 step: 2135, loss is 0.15558476746082306\n",
      "epoch: 6 step: 2136, loss is 0.3874857425689697\n",
      "epoch: 6 step: 2137, loss is 0.3353724777698517\n",
      "epoch: 6 step: 2138, loss is 0.3770735263824463\n",
      "epoch: 6 step: 2139, loss is 0.3388351500034332\n",
      "epoch: 6 step: 2140, loss is 0.21987533569335938\n",
      "epoch: 6 step: 2141, loss is 0.9739084243774414\n",
      "epoch: 6 step: 2142, loss is 0.15100926160812378\n",
      "epoch: 6 step: 2143, loss is 0.24646110832691193\n",
      "epoch: 6 step: 2144, loss is 0.32418590784072876\n",
      "epoch: 6 step: 2145, loss is 0.8837465643882751\n",
      "epoch: 6 step: 2146, loss is 0.15019173920154572\n",
      "epoch: 6 step: 2147, loss is 0.3463001549243927\n",
      "epoch: 6 step: 2148, loss is 0.09857578575611115\n",
      "epoch: 6 step: 2149, loss is 0.3524220883846283\n",
      "epoch: 6 step: 2150, loss is 0.3012637495994568\n",
      "epoch: 6 step: 2151, loss is 0.5088029503822327\n",
      "epoch: 6 step: 2152, loss is 0.30997294187545776\n",
      "epoch: 6 step: 2153, loss is 0.08910798281431198\n",
      "epoch: 6 step: 2154, loss is 0.1377762258052826\n",
      "epoch: 6 step: 2155, loss is 0.24713678658008575\n",
      "epoch: 6 step: 2156, loss is 0.10875708609819412\n",
      "epoch: 6 step: 2157, loss is 0.08177084475755692\n",
      "epoch: 6 step: 2158, loss is 0.4401360750198364\n",
      "epoch: 6 step: 2159, loss is 0.7695689797401428\n",
      "epoch: 6 step: 2160, loss is 0.3315400779247284\n",
      "epoch: 6 step: 2161, loss is 0.1569410115480423\n",
      "epoch: 6 step: 2162, loss is 0.1269112527370453\n",
      "epoch: 6 step: 2163, loss is 0.2967233657836914\n",
      "epoch: 6 step: 2164, loss is 0.12114737182855606\n",
      "epoch: 6 step: 2165, loss is 0.15010449290275574\n",
      "epoch: 6 step: 2166, loss is 0.13001537322998047\n",
      "epoch: 6 step: 2167, loss is 1.0461490154266357\n",
      "epoch: 6 step: 2168, loss is 0.4445289373397827\n",
      "epoch: 6 step: 2169, loss is 0.47292712330818176\n",
      "epoch: 6 step: 2170, loss is 0.3261665999889374\n",
      "epoch: 6 step: 2171, loss is 0.3070017099380493\n",
      "epoch: 6 step: 2172, loss is 0.9577442407608032\n",
      "epoch: 6 step: 2173, loss is 0.3726890981197357\n",
      "epoch: 6 step: 2174, loss is 0.6898491978645325\n",
      "epoch: 6 step: 2175, loss is 0.3478170931339264\n",
      "epoch: 6 step: 2176, loss is 0.2610650062561035\n",
      "epoch: 6 step: 2177, loss is 0.23216262459754944\n",
      "epoch: 6 step: 2178, loss is 0.5531489849090576\n",
      "epoch: 6 step: 2179, loss is 0.4004109501838684\n",
      "epoch: 6 step: 2180, loss is 0.3650059998035431\n",
      "epoch: 6 step: 2181, loss is 0.8226574063301086\n",
      "epoch: 6 step: 2182, loss is 0.5447524785995483\n",
      "epoch: 6 step: 2183, loss is 0.33365878462791443\n",
      "epoch: 6 step: 2184, loss is 0.40823397040367126\n",
      "epoch: 6 step: 2185, loss is 0.22384734451770782\n",
      "epoch: 6 step: 2186, loss is 0.22721494734287262\n",
      "epoch: 6 step: 2187, loss is 0.426766037940979\n",
      "epoch: 6 step: 2188, loss is 0.4088488817214966\n",
      "epoch: 6 step: 2189, loss is 0.2715877294540405\n",
      "epoch: 6 step: 2190, loss is 0.24857468903064728\n",
      "epoch: 6 step: 2191, loss is 0.1556650698184967\n",
      "epoch: 6 step: 2192, loss is 0.2910594046115875\n",
      "epoch: 6 step: 2193, loss is 0.43473154306411743\n",
      "epoch: 6 step: 2194, loss is 0.12613803148269653\n",
      "epoch: 6 step: 2195, loss is 0.34142887592315674\n",
      "epoch: 6 step: 2196, loss is 0.6241493225097656\n",
      "epoch: 6 step: 2197, loss is 0.1393309235572815\n",
      "epoch: 6 step: 2198, loss is 0.2361585944890976\n",
      "epoch: 6 step: 2199, loss is 0.3123452067375183\n",
      "epoch: 6 step: 2200, loss is 0.2886312007904053\n",
      "epoch: 6 step: 2201, loss is 0.03231127932667732\n",
      "epoch: 6 step: 2202, loss is 0.20455890893936157\n",
      "epoch: 6 step: 2203, loss is 0.10864394903182983\n",
      "epoch: 6 step: 2204, loss is 0.2723921239376068\n",
      "epoch: 6 step: 2205, loss is 0.3869640827178955\n",
      "epoch: 6 step: 2206, loss is 0.18190747499465942\n",
      "epoch: 6 step: 2207, loss is 0.15098311007022858\n",
      "epoch: 6 step: 2208, loss is 0.40398314595222473\n",
      "epoch: 6 step: 2209, loss is 0.1338643729686737\n",
      "epoch: 6 step: 2210, loss is 0.27952226996421814\n",
      "epoch: 6 step: 2211, loss is 0.14190536737442017\n",
      "epoch: 6 step: 2212, loss is 0.20250682532787323\n",
      "epoch: 6 step: 2213, loss is 0.46565064787864685\n",
      "epoch: 6 step: 2214, loss is 0.6559584736824036\n",
      "epoch: 6 step: 2215, loss is 0.38568252325057983\n",
      "epoch: 6 step: 2216, loss is 0.5382338762283325\n",
      "epoch: 6 step: 2217, loss is 0.4540058374404907\n",
      "epoch: 6 step: 2218, loss is 0.27714821696281433\n",
      "epoch: 6 step: 2219, loss is 0.7433857321739197\n",
      "epoch: 6 step: 2220, loss is 0.26462802290916443\n",
      "epoch: 6 step: 2221, loss is 0.1799820363521576\n",
      "epoch: 6 step: 2222, loss is 0.24283665418624878\n",
      "epoch: 6 step: 2223, loss is 0.23794376850128174\n",
      "epoch: 6 step: 2224, loss is 0.2411503791809082\n",
      "epoch: 6 step: 2225, loss is 0.09195352345705032\n",
      "epoch: 6 step: 2226, loss is 0.1558038741350174\n",
      "epoch: 6 step: 2227, loss is 0.15934638679027557\n",
      "epoch: 6 step: 2228, loss is 0.13728167116641998\n",
      "epoch: 6 step: 2229, loss is 0.0990019291639328\n",
      "epoch: 6 step: 2230, loss is 0.17854753136634827\n",
      "epoch: 6 step: 2231, loss is 0.6393440365791321\n",
      "epoch: 6 step: 2232, loss is 0.0950201153755188\n",
      "epoch: 6 step: 2233, loss is 0.16444920003414154\n",
      "epoch: 6 step: 2234, loss is 0.36674609780311584\n",
      "epoch: 6 step: 2235, loss is 0.3700767755508423\n",
      "epoch: 6 step: 2236, loss is 0.04070403054356575\n",
      "epoch: 6 step: 2237, loss is 0.5635405778884888\n",
      "epoch: 6 step: 2238, loss is 0.6087613105773926\n",
      "epoch: 6 step: 2239, loss is 0.13025133311748505\n",
      "epoch: 6 step: 2240, loss is 0.8494749069213867\n",
      "epoch: 6 step: 2241, loss is 0.830915629863739\n",
      "epoch: 6 step: 2242, loss is 0.39731547236442566\n",
      "epoch: 6 step: 2243, loss is 0.1954711675643921\n",
      "epoch: 6 step: 2244, loss is 0.08456974476575851\n",
      "epoch: 6 step: 2245, loss is 0.29531049728393555\n",
      "epoch: 6 step: 2246, loss is 0.21444563567638397\n",
      "epoch: 6 step: 2247, loss is 0.35275059938430786\n",
      "epoch: 6 step: 2248, loss is 0.40540891885757446\n",
      "epoch: 6 step: 2249, loss is 0.2225947082042694\n",
      "epoch: 6 step: 2250, loss is 0.13094492256641388\n",
      "epoch: 6 step: 2251, loss is 0.2382609099149704\n",
      "epoch: 6 step: 2252, loss is 0.42848530411720276\n",
      "epoch: 6 step: 2253, loss is 0.28138670325279236\n",
      "epoch: 6 step: 2254, loss is 0.5183128118515015\n",
      "epoch: 6 step: 2255, loss is 0.11141853779554367\n",
      "epoch: 6 step: 2256, loss is 0.3273443877696991\n",
      "epoch: 6 step: 2257, loss is 0.5680152177810669\n",
      "epoch: 6 step: 2258, loss is 0.2931022644042969\n",
      "epoch: 6 step: 2259, loss is 0.2896375358104706\n",
      "epoch: 6 step: 2260, loss is 0.7219605445861816\n",
      "epoch: 6 step: 2261, loss is 0.27972355484962463\n",
      "epoch: 6 step: 2262, loss is 0.4510195255279541\n",
      "epoch: 6 step: 2263, loss is 0.5270497798919678\n",
      "epoch: 6 step: 2264, loss is 0.9499127864837646\n",
      "epoch: 6 step: 2265, loss is 0.22374626994132996\n",
      "epoch: 6 step: 2266, loss is 0.46223780512809753\n",
      "epoch: 6 step: 2267, loss is 0.4705179035663605\n",
      "epoch: 6 step: 2268, loss is 0.42796066403388977\n",
      "epoch: 6 step: 2269, loss is 0.1883329302072525\n",
      "epoch: 6 step: 2270, loss is 0.5552135705947876\n",
      "epoch: 6 step: 2271, loss is 0.24448393285274506\n",
      "epoch: 6 step: 2272, loss is 0.31297531723976135\n",
      "epoch: 6 step: 2273, loss is 0.20776379108428955\n",
      "epoch: 6 step: 2274, loss is 0.34013450145721436\n",
      "epoch: 6 step: 2275, loss is 0.1230117529630661\n",
      "epoch: 6 step: 2276, loss is 0.18946592509746552\n",
      "epoch: 6 step: 2277, loss is 0.31687021255493164\n",
      "epoch: 6 step: 2278, loss is 0.1248127669095993\n",
      "epoch: 6 step: 2279, loss is 0.6015148758888245\n",
      "epoch: 6 step: 2280, loss is 0.44691210985183716\n",
      "epoch: 6 step: 2281, loss is 0.15311767160892487\n",
      "epoch: 6 step: 2282, loss is 0.3116517663002014\n",
      "epoch: 6 step: 2283, loss is 0.33839693665504456\n",
      "epoch: 6 step: 2284, loss is 0.569547176361084\n",
      "epoch: 6 step: 2285, loss is 0.3687097430229187\n",
      "epoch: 6 step: 2286, loss is 0.3834078311920166\n",
      "epoch: 6 step: 2287, loss is 0.08018642663955688\n",
      "epoch: 6 step: 2288, loss is 0.5657604932785034\n",
      "epoch: 6 step: 2289, loss is 0.27964216470718384\n",
      "epoch: 6 step: 2290, loss is 0.3350583016872406\n",
      "epoch: 6 step: 2291, loss is 0.46849319338798523\n",
      "epoch: 6 step: 2292, loss is 0.1631849706172943\n",
      "epoch: 6 step: 2293, loss is 0.2922060787677765\n",
      "epoch: 6 step: 2294, loss is 0.19361867010593414\n",
      "epoch: 6 step: 2295, loss is 0.06276487559080124\n",
      "epoch: 6 step: 2296, loss is 0.2595461905002594\n",
      "epoch: 6 step: 2297, loss is 0.19932518899440765\n",
      "epoch: 6 step: 2298, loss is 0.2595530152320862\n",
      "epoch: 6 step: 2299, loss is 0.18252092599868774\n",
      "epoch: 6 step: 2300, loss is 0.15850283205509186\n",
      "epoch: 6 step: 2301, loss is 0.19758304953575134\n",
      "epoch: 6 step: 2302, loss is 0.2587282061576843\n",
      "epoch: 6 step: 2303, loss is 0.4628044068813324\n",
      "epoch: 6 step: 2304, loss is 0.11733478307723999\n",
      "epoch: 6 step: 2305, loss is 0.214601531624794\n",
      "epoch: 6 step: 2306, loss is 0.45742568373680115\n",
      "epoch: 6 step: 2307, loss is 0.07924225926399231\n",
      "epoch: 6 step: 2308, loss is 0.22985444962978363\n",
      "epoch: 6 step: 2309, loss is 0.4093904197216034\n",
      "epoch: 6 step: 2310, loss is 0.6544234156608582\n",
      "epoch: 6 step: 2311, loss is 0.05501103773713112\n",
      "epoch: 6 step: 2312, loss is 0.21694588661193848\n",
      "epoch: 6 step: 2313, loss is 0.2163165956735611\n",
      "epoch: 6 step: 2314, loss is 0.30244681239128113\n",
      "epoch: 6 step: 2315, loss is 0.443675696849823\n",
      "epoch: 6 step: 2316, loss is 0.5121812224388123\n",
      "epoch: 6 step: 2317, loss is 0.16266076266765594\n",
      "epoch: 6 step: 2318, loss is 0.4386301040649414\n",
      "epoch: 6 step: 2319, loss is 0.2295619398355484\n",
      "epoch: 6 step: 2320, loss is 0.7239668965339661\n",
      "epoch: 6 step: 2321, loss is 0.09522536396980286\n",
      "epoch: 6 step: 2322, loss is 0.1252324879169464\n",
      "epoch: 6 step: 2323, loss is 0.07416688650846481\n",
      "epoch: 6 step: 2324, loss is 0.25042015314102173\n",
      "epoch: 6 step: 2325, loss is 0.4356766641139984\n",
      "epoch: 6 step: 2326, loss is 0.16332319378852844\n",
      "epoch: 6 step: 2327, loss is 0.724839448928833\n",
      "epoch: 6 step: 2328, loss is 0.19872042536735535\n",
      "epoch: 6 step: 2329, loss is 0.43032464385032654\n",
      "epoch: 6 step: 2330, loss is 0.26197388768196106\n",
      "epoch: 6 step: 2331, loss is 0.03238902986049652\n",
      "epoch: 6 step: 2332, loss is 0.7165394425392151\n",
      "epoch: 6 step: 2333, loss is 0.18512201309204102\n",
      "epoch: 6 step: 2334, loss is 0.34476229548454285\n",
      "epoch: 6 step: 2335, loss is 0.8532393574714661\n",
      "epoch: 6 step: 2336, loss is 0.18862903118133545\n",
      "epoch: 6 step: 2337, loss is 0.3868750333786011\n",
      "epoch: 6 step: 2338, loss is 0.18111056089401245\n",
      "epoch: 6 step: 2339, loss is 0.547745406627655\n",
      "epoch: 6 step: 2340, loss is 0.14077220857143402\n",
      "epoch: 6 step: 2341, loss is 0.1651211678981781\n",
      "epoch: 6 step: 2342, loss is 0.11279167979955673\n",
      "epoch: 6 step: 2343, loss is 0.16013605892658234\n",
      "epoch: 6 step: 2344, loss is 0.5551849603652954\n",
      "epoch: 6 step: 2345, loss is 0.6485868692398071\n",
      "epoch: 6 step: 2346, loss is 0.3829162120819092\n",
      "epoch: 6 step: 2347, loss is 0.26426035165786743\n",
      "epoch: 6 step: 2348, loss is 0.18461443483829498\n",
      "epoch: 6 step: 2349, loss is 0.19017405807971954\n",
      "epoch: 6 step: 2350, loss is 0.6729268431663513\n",
      "epoch: 6 step: 2351, loss is 0.20905594527721405\n",
      "epoch: 6 step: 2352, loss is 0.22869649529457092\n",
      "epoch: 6 step: 2353, loss is 0.5326648950576782\n",
      "epoch: 6 step: 2354, loss is 0.25518298149108887\n",
      "epoch: 6 step: 2355, loss is 0.528887152671814\n",
      "epoch: 6 step: 2356, loss is 0.4268570840358734\n",
      "epoch: 6 step: 2357, loss is 0.4433998763561249\n",
      "epoch: 6 step: 2358, loss is 0.42063769698143005\n",
      "epoch: 6 step: 2359, loss is 0.21997538208961487\n",
      "epoch: 6 step: 2360, loss is 0.22115425765514374\n",
      "epoch: 6 step: 2361, loss is 0.28592613339424133\n",
      "epoch: 6 step: 2362, loss is 0.5841324925422668\n",
      "epoch: 6 step: 2363, loss is 0.4948003590106964\n",
      "epoch: 6 step: 2364, loss is 0.1322409212589264\n",
      "epoch: 6 step: 2365, loss is 0.8159841895103455\n",
      "epoch: 6 step: 2366, loss is 0.3717539310455322\n",
      "epoch: 6 step: 2367, loss is 0.5827621221542358\n",
      "epoch: 6 step: 2368, loss is 0.36460164189338684\n",
      "epoch: 6 step: 2369, loss is 0.48065245151519775\n",
      "epoch: 6 step: 2370, loss is 0.3008566200733185\n",
      "epoch: 6 step: 2371, loss is 0.3059075176715851\n",
      "epoch: 6 step: 2372, loss is 0.5115577578544617\n",
      "epoch: 6 step: 2373, loss is 0.3748307228088379\n",
      "epoch: 6 step: 2374, loss is 0.5780616998672485\n",
      "epoch: 6 step: 2375, loss is 0.23763670027256012\n",
      "epoch: 6 step: 2376, loss is 0.3746179938316345\n",
      "epoch: 6 step: 2377, loss is 0.1556057631969452\n",
      "epoch: 6 step: 2378, loss is 0.141165629029274\n",
      "epoch: 6 step: 2379, loss is 0.29800620675086975\n",
      "epoch: 6 step: 2380, loss is 0.14348720014095306\n",
      "epoch: 6 step: 2381, loss is 0.3455457091331482\n",
      "epoch: 6 step: 2382, loss is 0.29962143301963806\n",
      "epoch: 6 step: 2383, loss is 0.18203096091747284\n",
      "epoch: 6 step: 2384, loss is 0.24774572253227234\n",
      "epoch: 6 step: 2385, loss is 0.4085259437561035\n",
      "epoch: 6 step: 2386, loss is 0.5581175088882446\n",
      "epoch: 6 step: 2387, loss is 0.2783920466899872\n",
      "epoch: 6 step: 2388, loss is 0.3253147602081299\n",
      "epoch: 6 step: 2389, loss is 0.332950621843338\n",
      "epoch: 6 step: 2390, loss is 0.6237776279449463\n",
      "epoch: 6 step: 2391, loss is 0.51976478099823\n",
      "epoch: 6 step: 2392, loss is 0.2623513340950012\n",
      "epoch: 6 step: 2393, loss is 0.3970770537853241\n",
      "epoch: 6 step: 2394, loss is 0.6254796981811523\n",
      "epoch: 6 step: 2395, loss is 0.11186205595731735\n",
      "epoch: 6 step: 2396, loss is 0.39241355657577515\n",
      "epoch: 6 step: 2397, loss is 0.10692936182022095\n",
      "epoch: 6 step: 2398, loss is 0.12742991745471954\n",
      "epoch: 6 step: 2399, loss is 0.29315778613090515\n",
      "epoch: 6 step: 2400, loss is 0.8071537613868713\n",
      "epoch: 6 step: 2401, loss is 0.04831981286406517\n",
      "epoch: 6 step: 2402, loss is 0.290540874004364\n",
      "epoch: 6 step: 2403, loss is 0.4527601897716522\n",
      "epoch: 6 step: 2404, loss is 0.18785548210144043\n",
      "epoch: 6 step: 2405, loss is 0.27230697870254517\n",
      "epoch: 6 step: 2406, loss is 0.48927515745162964\n",
      "epoch: 6 step: 2407, loss is 0.64805006980896\n",
      "epoch: 6 step: 2408, loss is 0.14212636649608612\n",
      "epoch: 6 step: 2409, loss is 0.20165768265724182\n",
      "epoch: 6 step: 2410, loss is 0.9493046998977661\n",
      "epoch: 6 step: 2411, loss is 0.5383093953132629\n",
      "epoch: 6 step: 2412, loss is 0.4425992965698242\n",
      "epoch: 6 step: 2413, loss is 0.1487627625465393\n",
      "epoch: 6 step: 2414, loss is 0.29142946004867554\n",
      "epoch: 6 step: 2415, loss is 0.4095262885093689\n",
      "epoch: 6 step: 2416, loss is 0.3074861764907837\n",
      "epoch: 6 step: 2417, loss is 0.15125326812267303\n",
      "epoch: 6 step: 2418, loss is 0.4048560857772827\n",
      "epoch: 6 step: 2419, loss is 0.3896659016609192\n",
      "epoch: 6 step: 2420, loss is 0.5778493881225586\n",
      "epoch: 6 step: 2421, loss is 0.5695428252220154\n",
      "epoch: 6 step: 2422, loss is 0.38552573323249817\n",
      "epoch: 6 step: 2423, loss is 0.15309959650039673\n",
      "epoch: 6 step: 2424, loss is 0.26292362809181213\n",
      "epoch: 6 step: 2425, loss is 0.11562695354223251\n",
      "epoch: 6 step: 2426, loss is 0.07450485229492188\n",
      "epoch: 6 step: 2427, loss is 0.3198727071285248\n",
      "epoch: 6 step: 2428, loss is 0.07329583913087845\n",
      "epoch: 6 step: 2429, loss is 0.5657901763916016\n",
      "epoch: 6 step: 2430, loss is 0.8353808522224426\n",
      "epoch: 6 step: 2431, loss is 0.37272924184799194\n",
      "epoch: 6 step: 2432, loss is 0.10887149721384048\n",
      "epoch: 6 step: 2433, loss is 0.17832152545452118\n",
      "epoch: 6 step: 2434, loss is 0.3071468472480774\n",
      "epoch: 6 step: 2435, loss is 0.1690814346075058\n",
      "epoch: 6 step: 2436, loss is 0.6331753134727478\n",
      "epoch: 6 step: 2437, loss is 0.4855763018131256\n",
      "epoch: 6 step: 2438, loss is 0.09500372409820557\n",
      "epoch: 6 step: 2439, loss is 0.12066524475812912\n",
      "epoch: 6 step: 2440, loss is 0.2807842195034027\n",
      "epoch: 6 step: 2441, loss is 0.3112831711769104\n",
      "epoch: 6 step: 2442, loss is 0.2660030126571655\n",
      "epoch: 6 step: 2443, loss is 0.1992918699979782\n",
      "epoch: 6 step: 2444, loss is 0.5159990787506104\n",
      "epoch: 6 step: 2445, loss is 0.42161262035369873\n",
      "epoch: 6 step: 2446, loss is 0.24538776278495789\n",
      "epoch: 6 step: 2447, loss is 0.14509092271327972\n",
      "epoch: 6 step: 2448, loss is 0.17484262585639954\n",
      "epoch: 6 step: 2449, loss is 0.6544796228408813\n",
      "epoch: 6 step: 2450, loss is 0.34831663966178894\n",
      "epoch: 6 step: 2451, loss is 0.18584415316581726\n",
      "epoch: 6 step: 2452, loss is 0.5849574208259583\n",
      "epoch: 6 step: 2453, loss is 0.16694849729537964\n",
      "epoch: 6 step: 2454, loss is 0.24638631939888\n",
      "epoch: 6 step: 2455, loss is 0.4721366763114929\n",
      "epoch: 6 step: 2456, loss is 0.9071274399757385\n",
      "epoch: 6 step: 2457, loss is 0.22075201570987701\n",
      "epoch: 6 step: 2458, loss is 0.24411681294441223\n",
      "epoch: 6 step: 2459, loss is 0.4739333987236023\n",
      "epoch: 6 step: 2460, loss is 0.49238383769989014\n",
      "epoch: 6 step: 2461, loss is 0.3241647779941559\n",
      "epoch: 6 step: 2462, loss is 0.29201415181159973\n",
      "epoch: 6 step: 2463, loss is 0.3268711566925049\n",
      "epoch: 6 step: 2464, loss is 0.7570380568504333\n",
      "epoch: 6 step: 2465, loss is 0.340224951505661\n",
      "epoch: 6 step: 2466, loss is 0.09844335913658142\n",
      "epoch: 6 step: 2467, loss is 0.11877195537090302\n",
      "epoch: 6 step: 2468, loss is 0.5948017835617065\n",
      "epoch: 6 step: 2469, loss is 0.22342288494110107\n",
      "epoch: 6 step: 2470, loss is 0.052148763090372086\n",
      "epoch: 6 step: 2471, loss is 0.09318631142377853\n",
      "epoch: 6 step: 2472, loss is 0.3633873164653778\n",
      "epoch: 6 step: 2473, loss is 0.3321365416049957\n",
      "epoch: 6 step: 2474, loss is 0.1082020252943039\n",
      "epoch: 6 step: 2475, loss is 0.08380132168531418\n",
      "epoch: 6 step: 2476, loss is 0.030364716425538063\n",
      "epoch: 6 step: 2477, loss is 0.17956356704235077\n",
      "epoch: 6 step: 2478, loss is 0.21238447725772858\n",
      "epoch: 6 step: 2479, loss is 0.26096147298812866\n",
      "epoch: 6 step: 2480, loss is 0.10314450412988663\n",
      "epoch: 6 step: 2481, loss is 0.13034340739250183\n",
      "epoch: 6 step: 2482, loss is 0.1774636209011078\n",
      "epoch: 6 step: 2483, loss is 0.16255582869052887\n",
      "epoch: 6 step: 2484, loss is 0.07121364772319794\n",
      "epoch: 6 step: 2485, loss is 0.46831607818603516\n",
      "epoch: 6 step: 2486, loss is 0.3486301898956299\n",
      "epoch: 6 step: 2487, loss is 0.2597983777523041\n",
      "epoch: 6 step: 2488, loss is 0.07236175984144211\n",
      "epoch: 6 step: 2489, loss is 0.8792251944541931\n",
      "epoch: 6 step: 2490, loss is 0.2696084678173065\n",
      "epoch: 6 step: 2491, loss is 0.24743929505348206\n",
      "epoch: 6 step: 2492, loss is 0.05202680453658104\n",
      "epoch: 6 step: 2493, loss is 0.46401044726371765\n",
      "epoch: 6 step: 2494, loss is 0.6732333898544312\n",
      "epoch: 6 step: 2495, loss is 0.44754472374916077\n",
      "epoch: 6 step: 2496, loss is 0.27883443236351013\n",
      "epoch: 6 step: 2497, loss is 0.7858995795249939\n",
      "epoch: 6 step: 2498, loss is 0.06362339854240417\n",
      "epoch: 6 step: 2499, loss is 0.3814677894115448\n",
      "epoch: 6 step: 2500, loss is 0.4615011215209961\n",
      "epoch: 6 step: 2501, loss is 0.07780459523200989\n",
      "epoch: 6 step: 2502, loss is 0.12045261263847351\n",
      "epoch: 6 step: 2503, loss is 0.6298419237136841\n",
      "epoch: 6 step: 2504, loss is 0.23385393619537354\n",
      "epoch: 6 step: 2505, loss is 0.24299858510494232\n",
      "epoch: 6 step: 2506, loss is 0.37374284863471985\n",
      "epoch: 6 step: 2507, loss is 0.5627384781837463\n",
      "epoch: 6 step: 2508, loss is 0.1897444725036621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: unknown JFIF revision number 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 1, loss is 0.23267200589179993\n",
      "epoch: 7 step: 2, loss is 0.17417100071907043\n",
      "epoch: 7 step: 3, loss is 0.5129176378250122\n",
      "epoch: 7 step: 4, loss is 0.14134100079536438\n",
      "epoch: 7 step: 5, loss is 0.4039674401283264\n",
      "epoch: 7 step: 6, loss is 0.7248643636703491\n",
      "epoch: 7 step: 7, loss is 0.24529534578323364\n",
      "epoch: 7 step: 8, loss is 0.2589629590511322\n",
      "epoch: 7 step: 9, loss is 0.6925811171531677\n",
      "epoch: 7 step: 10, loss is 0.08429653942584991\n",
      "epoch: 7 step: 11, loss is 0.37940770387649536\n",
      "epoch: 7 step: 12, loss is 0.1323212832212448\n",
      "epoch: 7 step: 13, loss is 0.2803479731082916\n",
      "epoch: 7 step: 14, loss is 0.34223324060440063\n",
      "epoch: 7 step: 15, loss is 0.2244202345609665\n",
      "epoch: 7 step: 16, loss is 0.17576289176940918\n",
      "epoch: 7 step: 17, loss is 0.5389060974121094\n",
      "epoch: 7 step: 18, loss is 0.3325822055339813\n",
      "epoch: 7 step: 19, loss is 0.16016550362110138\n",
      "epoch: 7 step: 20, loss is 0.5231564044952393\n",
      "epoch: 7 step: 21, loss is 0.37519779801368713\n",
      "epoch: 7 step: 22, loss is 0.496823251247406\n",
      "epoch: 7 step: 23, loss is 0.684623658657074\n",
      "epoch: 7 step: 24, loss is 0.12382900714874268\n",
      "epoch: 7 step: 25, loss is 0.4874260723590851\n",
      "epoch: 7 step: 26, loss is 0.4592170715332031\n",
      "epoch: 7 step: 27, loss is 0.2674127519130707\n",
      "epoch: 7 step: 28, loss is 0.07166741043329239\n",
      "epoch: 7 step: 29, loss is 0.4128742814064026\n",
      "epoch: 7 step: 30, loss is 0.2136881947517395\n",
      "epoch: 7 step: 31, loss is 0.17795726656913757\n",
      "epoch: 7 step: 32, loss is 0.3699343204498291\n",
      "epoch: 7 step: 33, loss is 0.06822431087493896\n",
      "epoch: 7 step: 34, loss is 0.512454628944397\n",
      "epoch: 7 step: 35, loss is 0.24910877645015717\n",
      "epoch: 7 step: 36, loss is 0.17255261540412903\n",
      "epoch: 7 step: 37, loss is 0.3902527391910553\n",
      "epoch: 7 step: 38, loss is 0.3600970208644867\n",
      "epoch: 7 step: 39, loss is 0.5223293304443359\n",
      "epoch: 7 step: 40, loss is 0.5180928111076355\n",
      "epoch: 7 step: 41, loss is 0.23892350494861603\n",
      "epoch: 7 step: 42, loss is 0.5246753096580505\n",
      "epoch: 7 step: 43, loss is 0.20642037689685822\n",
      "epoch: 7 step: 44, loss is 0.5371578931808472\n",
      "epoch: 7 step: 45, loss is 0.1510905772447586\n",
      "epoch: 7 step: 46, loss is 0.047221921384334564\n",
      "epoch: 7 step: 47, loss is 0.0948779508471489\n",
      "epoch: 7 step: 48, loss is 0.3544997572898865\n",
      "epoch: 7 step: 49, loss is 0.5289987325668335\n",
      "epoch: 7 step: 50, loss is 0.09319192916154861\n",
      "epoch: 7 step: 51, loss is 0.12303312867879868\n",
      "epoch: 7 step: 52, loss is 0.5489287972450256\n",
      "epoch: 7 step: 53, loss is 0.10111439228057861\n",
      "epoch: 7 step: 54, loss is 0.2627717852592468\n",
      "epoch: 7 step: 55, loss is 0.23758399486541748\n",
      "epoch: 7 step: 56, loss is 0.15889321267604828\n",
      "epoch: 7 step: 57, loss is 0.3626205623149872\n",
      "epoch: 7 step: 58, loss is 0.09713853150606155\n",
      "epoch: 7 step: 59, loss is 0.2760443389415741\n",
      "epoch: 7 step: 60, loss is 0.681315541267395\n",
      "epoch: 7 step: 61, loss is 0.19171229004859924\n",
      "epoch: 7 step: 62, loss is 0.2550266683101654\n",
      "epoch: 7 step: 63, loss is 0.18378835916519165\n",
      "epoch: 7 step: 64, loss is 0.2438332438468933\n",
      "epoch: 7 step: 65, loss is 0.5156472325325012\n",
      "epoch: 7 step: 66, loss is 0.23196007311344147\n",
      "epoch: 7 step: 67, loss is 0.1728326976299286\n",
      "epoch: 7 step: 68, loss is 0.5542371273040771\n",
      "epoch: 7 step: 69, loss is 0.7143704891204834\n",
      "epoch: 7 step: 70, loss is 0.6886472105979919\n",
      "epoch: 7 step: 71, loss is 0.5390157103538513\n",
      "epoch: 7 step: 72, loss is 0.30389603972435\n",
      "epoch: 7 step: 73, loss is 0.25666630268096924\n",
      "epoch: 7 step: 74, loss is 0.3809581995010376\n",
      "epoch: 7 step: 75, loss is 0.053548794239759445\n",
      "epoch: 7 step: 76, loss is 0.22382068634033203\n",
      "epoch: 7 step: 77, loss is 0.1459479033946991\n",
      "epoch: 7 step: 78, loss is 0.4215259850025177\n",
      "epoch: 7 step: 79, loss is 0.23669663071632385\n",
      "epoch: 7 step: 80, loss is 0.11072442680597305\n",
      "epoch: 7 step: 81, loss is 0.5207646489143372\n",
      "epoch: 7 step: 82, loss is 0.08972778916358948\n",
      "epoch: 7 step: 83, loss is 0.07653352618217468\n",
      "epoch: 7 step: 84, loss is 0.25797000527381897\n",
      "epoch: 7 step: 85, loss is 0.16494007408618927\n",
      "epoch: 7 step: 86, loss is 0.8662996292114258\n",
      "epoch: 7 step: 87, loss is 0.19568495452404022\n",
      "epoch: 7 step: 88, loss is 0.3219463527202606\n",
      "epoch: 7 step: 89, loss is 0.472299188375473\n",
      "epoch: 7 step: 90, loss is 0.7513121366500854\n",
      "epoch: 7 step: 91, loss is 0.5868830680847168\n",
      "epoch: 7 step: 92, loss is 0.12278693169355392\n",
      "epoch: 7 step: 93, loss is 0.21962478756904602\n",
      "epoch: 7 step: 94, loss is 0.18224801123142242\n",
      "epoch: 7 step: 95, loss is 0.3522913455963135\n",
      "epoch: 7 step: 96, loss is 0.32669195532798767\n",
      "epoch: 7 step: 97, loss is 0.11080330610275269\n",
      "epoch: 7 step: 98, loss is 0.07295502722263336\n",
      "epoch: 7 step: 99, loss is 0.03150160610675812\n",
      "epoch: 7 step: 100, loss is 0.055903855711221695\n",
      "epoch: 7 step: 101, loss is 0.4610437750816345\n",
      "epoch: 7 step: 102, loss is 0.21190568804740906\n",
      "epoch: 7 step: 103, loss is 0.5133976340293884\n",
      "epoch: 7 step: 104, loss is 0.2658374607563019\n",
      "epoch: 7 step: 105, loss is 0.3551090955734253\n",
      "epoch: 7 step: 106, loss is 0.2295769304037094\n",
      "epoch: 7 step: 107, loss is 0.6919164061546326\n",
      "epoch: 7 step: 108, loss is 0.26250091195106506\n",
      "epoch: 7 step: 109, loss is 0.4490436017513275\n",
      "epoch: 7 step: 110, loss is 0.19597084820270538\n",
      "epoch: 7 step: 111, loss is 0.14891467988491058\n",
      "epoch: 7 step: 112, loss is 0.04656359925866127\n",
      "epoch: 7 step: 113, loss is 0.6611177325248718\n",
      "epoch: 7 step: 114, loss is 0.16192695498466492\n",
      "epoch: 7 step: 115, loss is 0.09779054671525955\n",
      "epoch: 7 step: 116, loss is 0.18508610129356384\n",
      "epoch: 7 step: 117, loss is 0.21256539225578308\n",
      "epoch: 7 step: 118, loss is 0.2198467254638672\n",
      "epoch: 7 step: 119, loss is 0.35082554817199707\n",
      "epoch: 7 step: 120, loss is 0.21184195578098297\n",
      "epoch: 7 step: 121, loss is 0.4452669024467468\n",
      "epoch: 7 step: 122, loss is 0.18821482360363007\n",
      "epoch: 7 step: 123, loss is 0.3064343333244324\n",
      "epoch: 7 step: 124, loss is 0.19854594767093658\n",
      "epoch: 7 step: 125, loss is 0.21687662601470947\n",
      "epoch: 7 step: 126, loss is 0.7946164011955261\n",
      "epoch: 7 step: 127, loss is 0.12249895930290222\n",
      "epoch: 7 step: 128, loss is 0.14228814840316772\n",
      "epoch: 7 step: 129, loss is 0.42775729298591614\n",
      "epoch: 7 step: 130, loss is 0.0522320382297039\n",
      "epoch: 7 step: 131, loss is 0.25486329197883606\n",
      "epoch: 7 step: 132, loss is 0.06970365345478058\n",
      "epoch: 7 step: 133, loss is 0.8928443789482117\n",
      "epoch: 7 step: 134, loss is 0.09919881820678711\n",
      "epoch: 7 step: 135, loss is 0.056293945759534836\n",
      "epoch: 7 step: 136, loss is 0.20996522903442383\n",
      "epoch: 7 step: 137, loss is 0.3272715210914612\n",
      "epoch: 7 step: 138, loss is 0.2433469444513321\n",
      "epoch: 7 step: 139, loss is 0.2805798649787903\n",
      "epoch: 7 step: 140, loss is 0.13795174658298492\n",
      "epoch: 7 step: 141, loss is 0.17434753477573395\n",
      "epoch: 7 step: 142, loss is 0.36647266149520874\n",
      "epoch: 7 step: 143, loss is 0.5296945571899414\n",
      "epoch: 7 step: 144, loss is 0.15277086198329926\n",
      "epoch: 7 step: 145, loss is 0.19030065834522247\n",
      "epoch: 7 step: 146, loss is 0.3033985495567322\n",
      "epoch: 7 step: 147, loss is 0.28117141127586365\n",
      "epoch: 7 step: 148, loss is 0.7146813869476318\n",
      "epoch: 7 step: 149, loss is 0.4964359700679779\n",
      "epoch: 7 step: 150, loss is 0.10513255000114441\n",
      "epoch: 7 step: 151, loss is 0.3261467218399048\n",
      "epoch: 7 step: 152, loss is 1.0554962158203125\n",
      "epoch: 7 step: 153, loss is 0.4363444745540619\n",
      "epoch: 7 step: 154, loss is 0.2637247145175934\n",
      "epoch: 7 step: 155, loss is 0.11415238678455353\n",
      "epoch: 7 step: 156, loss is 0.7681294679641724\n",
      "epoch: 7 step: 157, loss is 0.1495267003774643\n",
      "epoch: 7 step: 158, loss is 0.303948312997818\n",
      "epoch: 7 step: 159, loss is 0.8349263072013855\n",
      "epoch: 7 step: 160, loss is 0.38562628626823425\n",
      "epoch: 7 step: 161, loss is 0.07995759695768356\n",
      "epoch: 7 step: 162, loss is 0.34653791785240173\n",
      "epoch: 7 step: 163, loss is 0.17980249226093292\n",
      "epoch: 7 step: 164, loss is 0.36574655771255493\n",
      "epoch: 7 step: 165, loss is 0.07325375825166702\n",
      "epoch: 7 step: 166, loss is 0.37216585874557495\n",
      "epoch: 7 step: 167, loss is 0.15269072353839874\n",
      "epoch: 7 step: 168, loss is 0.3314822018146515\n",
      "epoch: 7 step: 169, loss is 0.7821394801139832\n",
      "epoch: 7 step: 170, loss is 0.26011893153190613\n",
      "epoch: 7 step: 171, loss is 0.37037691473960876\n",
      "epoch: 7 step: 172, loss is 0.3012547194957733\n",
      "epoch: 7 step: 173, loss is 0.35321807861328125\n",
      "epoch: 7 step: 174, loss is 0.6487647891044617\n",
      "epoch: 7 step: 175, loss is 0.2112399935722351\n",
      "epoch: 7 step: 176, loss is 0.42183154821395874\n",
      "epoch: 7 step: 177, loss is 0.3059699535369873\n",
      "epoch: 7 step: 178, loss is 0.10525237023830414\n",
      "epoch: 7 step: 179, loss is 0.31963130831718445\n",
      "epoch: 7 step: 180, loss is 0.5331059098243713\n",
      "epoch: 7 step: 181, loss is 0.4255406856536865\n",
      "epoch: 7 step: 182, loss is 0.18545548617839813\n",
      "epoch: 7 step: 183, loss is 0.24178026616573334\n",
      "epoch: 7 step: 184, loss is 0.26009663939476013\n",
      "epoch: 7 step: 185, loss is 0.3262053430080414\n",
      "epoch: 7 step: 186, loss is 0.15777531266212463\n",
      "epoch: 7 step: 187, loss is 0.11433612555265427\n",
      "epoch: 7 step: 188, loss is 0.10223636776208878\n",
      "epoch: 7 step: 189, loss is 0.116578608751297\n",
      "epoch: 7 step: 190, loss is 0.5894609093666077\n",
      "epoch: 7 step: 191, loss is 0.7050826549530029\n",
      "epoch: 7 step: 192, loss is 0.0904678925871849\n",
      "epoch: 7 step: 193, loss is 0.16696827113628387\n",
      "epoch: 7 step: 194, loss is 0.2843974530696869\n",
      "epoch: 7 step: 195, loss is 0.09815607964992523\n",
      "epoch: 7 step: 196, loss is 0.14816386997699738\n",
      "epoch: 7 step: 197, loss is 0.22788535058498383\n",
      "epoch: 7 step: 198, loss is 0.3664293885231018\n",
      "epoch: 7 step: 199, loss is 0.2549590468406677\n",
      "epoch: 7 step: 200, loss is 0.45034050941467285\n",
      "epoch: 7 step: 201, loss is 0.1089198887348175\n",
      "epoch: 7 step: 202, loss is 0.3778587877750397\n",
      "epoch: 7 step: 203, loss is 0.2997986376285553\n",
      "epoch: 7 step: 204, loss is 0.35054028034210205\n",
      "epoch: 7 step: 205, loss is 0.18038122355937958\n",
      "epoch: 7 step: 206, loss is 0.26815253496170044\n",
      "epoch: 7 step: 207, loss is 0.24640987813472748\n",
      "epoch: 7 step: 208, loss is 0.06020643934607506\n",
      "epoch: 7 step: 209, loss is 0.3050921559333801\n",
      "epoch: 7 step: 210, loss is 0.1980963945388794\n",
      "epoch: 7 step: 211, loss is 0.26290687918663025\n",
      "epoch: 7 step: 212, loss is 0.419940322637558\n",
      "epoch: 7 step: 213, loss is 0.18580560386180878\n",
      "epoch: 7 step: 214, loss is 0.533345639705658\n",
      "epoch: 7 step: 215, loss is 0.3113418221473694\n",
      "epoch: 7 step: 216, loss is 0.33198094367980957\n",
      "epoch: 7 step: 217, loss is 0.053853727877140045\n",
      "epoch: 7 step: 218, loss is 0.20641754567623138\n",
      "epoch: 7 step: 219, loss is 0.08828931301832199\n",
      "epoch: 7 step: 220, loss is 0.5749175548553467\n",
      "epoch: 7 step: 221, loss is 0.07490164041519165\n",
      "epoch: 7 step: 222, loss is 0.31765216588974\n",
      "epoch: 7 step: 223, loss is 0.06411296874284744\n",
      "epoch: 7 step: 224, loss is 0.3228229880332947\n",
      "epoch: 7 step: 225, loss is 0.15204443037509918\n",
      "epoch: 7 step: 226, loss is 0.4845464825630188\n",
      "epoch: 7 step: 227, loss is 0.4624876081943512\n",
      "epoch: 7 step: 228, loss is 0.17619821429252625\n",
      "epoch: 7 step: 229, loss is 0.06048240140080452\n",
      "epoch: 7 step: 230, loss is 0.15019357204437256\n",
      "epoch: 7 step: 231, loss is 0.6177188158035278\n",
      "epoch: 7 step: 232, loss is 0.12132589519023895\n",
      "epoch: 7 step: 233, loss is 0.25179728865623474\n",
      "epoch: 7 step: 234, loss is 0.07895103842020035\n",
      "epoch: 7 step: 235, loss is 0.31111952662467957\n",
      "epoch: 7 step: 236, loss is 0.07892446964979172\n",
      "epoch: 7 step: 237, loss is 0.5067592263221741\n",
      "epoch: 7 step: 238, loss is 0.45218905806541443\n",
      "epoch: 7 step: 239, loss is 0.6669068932533264\n",
      "epoch: 7 step: 240, loss is 1.0126488208770752\n",
      "epoch: 7 step: 241, loss is 0.169186070561409\n",
      "epoch: 7 step: 242, loss is 0.5656155347824097\n",
      "epoch: 7 step: 243, loss is 0.32929009199142456\n",
      "epoch: 7 step: 244, loss is 0.1372392773628235\n",
      "epoch: 7 step: 245, loss is 0.05769984424114227\n",
      "epoch: 7 step: 246, loss is 0.4021424949169159\n",
      "epoch: 7 step: 247, loss is 0.12808872759342194\n",
      "epoch: 7 step: 248, loss is 0.295218288898468\n",
      "epoch: 7 step: 249, loss is 0.35087892413139343\n",
      "epoch: 7 step: 250, loss is 0.3712620139122009\n",
      "epoch: 7 step: 251, loss is 0.10562115907669067\n",
      "epoch: 7 step: 252, loss is 0.09279990941286087\n",
      "epoch: 7 step: 253, loss is 0.41460973024368286\n",
      "epoch: 7 step: 254, loss is 0.0543685108423233\n",
      "epoch: 7 step: 255, loss is 0.5089948177337646\n",
      "epoch: 7 step: 256, loss is 0.2915000021457672\n",
      "epoch: 7 step: 257, loss is 0.09071968495845795\n",
      "epoch: 7 step: 258, loss is 0.3311466872692108\n",
      "epoch: 7 step: 259, loss is 0.22482748329639435\n",
      "epoch: 7 step: 260, loss is 0.22279426455497742\n",
      "epoch: 7 step: 261, loss is 0.3463612198829651\n",
      "epoch: 7 step: 262, loss is 0.25991469621658325\n",
      "epoch: 7 step: 263, loss is 0.3212936222553253\n",
      "epoch: 7 step: 264, loss is 0.15003004670143127\n",
      "epoch: 7 step: 265, loss is 0.4098759889602661\n",
      "epoch: 7 step: 266, loss is 0.1801327019929886\n",
      "epoch: 7 step: 267, loss is 0.4695165753364563\n",
      "epoch: 7 step: 268, loss is 0.7094734907150269\n",
      "epoch: 7 step: 269, loss is 0.473620742559433\n",
      "epoch: 7 step: 270, loss is 0.36658045649528503\n",
      "epoch: 7 step: 271, loss is 0.43237754702568054\n",
      "epoch: 7 step: 272, loss is 0.3651544749736786\n",
      "epoch: 7 step: 273, loss is 0.23565395176410675\n",
      "epoch: 7 step: 274, loss is 0.28114983439445496\n",
      "epoch: 7 step: 275, loss is 1.2631995677947998\n",
      "epoch: 7 step: 276, loss is 0.16792067885398865\n",
      "epoch: 7 step: 277, loss is 0.23063431680202484\n",
      "epoch: 7 step: 278, loss is 0.07483188062906265\n",
      "epoch: 7 step: 279, loss is 0.0971127599477768\n",
      "epoch: 7 step: 280, loss is 0.3394346237182617\n",
      "epoch: 7 step: 281, loss is 0.24091167747974396\n",
      "epoch: 7 step: 282, loss is 0.13643936812877655\n",
      "epoch: 7 step: 283, loss is 0.1597512662410736\n",
      "epoch: 7 step: 284, loss is 0.2012631893157959\n",
      "epoch: 7 step: 285, loss is 0.32975128293037415\n",
      "epoch: 7 step: 286, loss is 0.2356359362602234\n",
      "epoch: 7 step: 287, loss is 0.14470933377742767\n",
      "epoch: 7 step: 288, loss is 0.6429100632667542\n",
      "epoch: 7 step: 289, loss is 0.3932360112667084\n",
      "epoch: 7 step: 290, loss is 0.19266223907470703\n",
      "epoch: 7 step: 291, loss is 0.10598298907279968\n",
      "epoch: 7 step: 292, loss is 0.4882884621620178\n",
      "epoch: 7 step: 293, loss is 0.29638439416885376\n",
      "epoch: 7 step: 294, loss is 0.11628931760787964\n",
      "epoch: 7 step: 295, loss is 0.3385429084300995\n",
      "epoch: 7 step: 296, loss is 0.23122656345367432\n",
      "epoch: 7 step: 297, loss is 0.6596016883850098\n",
      "epoch: 7 step: 298, loss is 0.4014686942100525\n",
      "epoch: 7 step: 299, loss is 0.17820696532726288\n",
      "epoch: 7 step: 300, loss is 0.5018871426582336\n",
      "epoch: 7 step: 301, loss is 0.3589784502983093\n",
      "epoch: 7 step: 302, loss is 0.1439177691936493\n",
      "epoch: 7 step: 303, loss is 0.2195184826850891\n",
      "epoch: 7 step: 304, loss is 0.0922921821475029\n",
      "epoch: 7 step: 305, loss is 0.21736480295658112\n",
      "epoch: 7 step: 306, loss is 0.5144979357719421\n",
      "epoch: 7 step: 307, loss is 0.44039592146873474\n",
      "epoch: 7 step: 308, loss is 0.061448417603969574\n",
      "epoch: 7 step: 309, loss is 0.4046039283275604\n",
      "epoch: 7 step: 310, loss is 0.0500972680747509\n",
      "epoch: 7 step: 311, loss is 0.05317729711532593\n",
      "epoch: 7 step: 312, loss is 0.552139401435852\n",
      "epoch: 7 step: 313, loss is 0.3805259168148041\n",
      "epoch: 7 step: 314, loss is 0.1181679517030716\n",
      "epoch: 7 step: 315, loss is 0.09273871034383774\n",
      "epoch: 7 step: 316, loss is 0.06655299663543701\n",
      "epoch: 7 step: 317, loss is 0.1450752168893814\n",
      "epoch: 7 step: 318, loss is 0.05397504195570946\n",
      "epoch: 7 step: 319, loss is 0.40640369057655334\n",
      "epoch: 7 step: 320, loss is 0.08822362869977951\n",
      "epoch: 7 step: 321, loss is 0.12312214821577072\n",
      "epoch: 7 step: 322, loss is 0.1244630292057991\n",
      "epoch: 7 step: 323, loss is 0.2906121015548706\n",
      "epoch: 7 step: 324, loss is 0.728594958782196\n",
      "epoch: 7 step: 325, loss is 0.23253488540649414\n",
      "epoch: 7 step: 326, loss is 0.1439487338066101\n",
      "epoch: 7 step: 327, loss is 0.024207167327404022\n",
      "epoch: 7 step: 328, loss is 0.1607680767774582\n",
      "epoch: 7 step: 329, loss is 0.20231324434280396\n",
      "epoch: 7 step: 330, loss is 0.12530063092708588\n",
      "epoch: 7 step: 331, loss is 0.5755279064178467\n",
      "epoch: 7 step: 332, loss is 0.13143667578697205\n",
      "epoch: 7 step: 333, loss is 0.26452916860580444\n",
      "epoch: 7 step: 334, loss is 1.1965080499649048\n",
      "epoch: 7 step: 335, loss is 0.1509608030319214\n",
      "epoch: 7 step: 336, loss is 0.36910170316696167\n",
      "epoch: 7 step: 337, loss is 0.24408088624477386\n",
      "epoch: 7 step: 338, loss is 0.7139512896537781\n",
      "epoch: 7 step: 339, loss is 0.23503434658050537\n",
      "epoch: 7 step: 340, loss is 0.2381058633327484\n",
      "epoch: 7 step: 341, loss is 0.14007000625133514\n",
      "epoch: 7 step: 342, loss is 0.146528422832489\n",
      "epoch: 7 step: 343, loss is 0.20052021741867065\n",
      "epoch: 7 step: 344, loss is 0.3831563889980316\n",
      "epoch: 7 step: 345, loss is 0.14163275063037872\n",
      "epoch: 7 step: 346, loss is 0.2338125854730606\n",
      "epoch: 7 step: 347, loss is 1.2240275144577026\n",
      "epoch: 7 step: 348, loss is 0.6365078687667847\n",
      "epoch: 7 step: 349, loss is 0.02068016678094864\n",
      "epoch: 7 step: 350, loss is 0.5041282773017883\n",
      "epoch: 7 step: 351, loss is 0.35983511805534363\n",
      "epoch: 7 step: 352, loss is 0.716500997543335\n",
      "epoch: 7 step: 353, loss is 0.6262726187705994\n",
      "epoch: 7 step: 354, loss is 0.2078118622303009\n",
      "epoch: 7 step: 355, loss is 0.46405866742134094\n",
      "epoch: 7 step: 356, loss is 0.39683711528778076\n",
      "epoch: 7 step: 357, loss is 0.17142188549041748\n",
      "epoch: 7 step: 358, loss is 0.30413559079170227\n",
      "epoch: 7 step: 359, loss is 0.482544869184494\n",
      "epoch: 7 step: 360, loss is 0.15649494528770447\n",
      "epoch: 7 step: 361, loss is 0.21721932291984558\n",
      "epoch: 7 step: 362, loss is 0.3394755423069\n",
      "epoch: 7 step: 363, loss is 0.36089250445365906\n",
      "epoch: 7 step: 364, loss is 0.09526026248931885\n",
      "epoch: 7 step: 365, loss is 0.20017166435718536\n",
      "epoch: 7 step: 366, loss is 0.5121476054191589\n",
      "epoch: 7 step: 367, loss is 0.13393530249595642\n",
      "epoch: 7 step: 368, loss is 0.3233805000782013\n",
      "epoch: 7 step: 369, loss is 0.13341617584228516\n",
      "epoch: 7 step: 370, loss is 0.4828917980194092\n",
      "epoch: 7 step: 371, loss is 0.16321343183517456\n",
      "epoch: 7 step: 372, loss is 0.15132646262645721\n",
      "epoch: 7 step: 373, loss is 0.20090752840042114\n",
      "epoch: 7 step: 374, loss is 0.29108908772468567\n",
      "epoch: 7 step: 375, loss is 0.1849042773246765\n",
      "epoch: 7 step: 376, loss is 0.5225263833999634\n",
      "epoch: 7 step: 377, loss is 0.14090310037136078\n",
      "epoch: 7 step: 378, loss is 0.4005282521247864\n",
      "epoch: 7 step: 379, loss is 0.7722302079200745\n",
      "epoch: 7 step: 380, loss is 0.6082316040992737\n",
      "epoch: 7 step: 381, loss is 0.8399408459663391\n",
      "epoch: 7 step: 382, loss is 0.39292195439338684\n",
      "epoch: 7 step: 383, loss is 0.3130386173725128\n",
      "epoch: 7 step: 384, loss is 0.04405646026134491\n",
      "epoch: 7 step: 385, loss is 0.3465844690799713\n",
      "epoch: 7 step: 386, loss is 0.2686174511909485\n",
      "epoch: 7 step: 387, loss is 0.2381075769662857\n",
      "epoch: 7 step: 388, loss is 0.367156982421875\n",
      "epoch: 7 step: 389, loss is 0.042851079255342484\n",
      "epoch: 7 step: 390, loss is 0.6750076413154602\n",
      "epoch: 7 step: 391, loss is 0.46865323185920715\n",
      "epoch: 7 step: 392, loss is 0.19752153754234314\n",
      "epoch: 7 step: 393, loss is 0.48878213763237\n",
      "epoch: 7 step: 394, loss is 0.21179892122745514\n",
      "epoch: 7 step: 395, loss is 0.4196435511112213\n",
      "epoch: 7 step: 396, loss is 0.5225710868835449\n",
      "epoch: 7 step: 397, loss is 0.5459282398223877\n",
      "epoch: 7 step: 398, loss is 0.16852430999279022\n",
      "epoch: 7 step: 399, loss is 0.5732718706130981\n",
      "epoch: 7 step: 400, loss is 0.2976091504096985\n",
      "epoch: 7 step: 401, loss is 0.23400995135307312\n",
      "epoch: 7 step: 402, loss is 0.384070485830307\n",
      "epoch: 7 step: 403, loss is 0.4320661127567291\n",
      "epoch: 7 step: 404, loss is 0.16221818327903748\n",
      "epoch: 7 step: 405, loss is 0.5700070858001709\n",
      "epoch: 7 step: 406, loss is 0.40500929951667786\n",
      "epoch: 7 step: 407, loss is 0.46966293454170227\n",
      "epoch: 7 step: 408, loss is 0.3524772822856903\n",
      "epoch: 7 step: 409, loss is 0.7788915634155273\n",
      "epoch: 7 step: 410, loss is 0.09505012631416321\n",
      "epoch: 7 step: 411, loss is 0.22657907009124756\n",
      "epoch: 7 step: 412, loss is 0.6050856113433838\n",
      "epoch: 7 step: 413, loss is 0.38942986726760864\n",
      "epoch: 7 step: 414, loss is 0.17849723994731903\n",
      "epoch: 7 step: 415, loss is 0.5419262647628784\n",
      "epoch: 7 step: 416, loss is 0.2500293552875519\n",
      "epoch: 7 step: 417, loss is 0.2916370630264282\n",
      "epoch: 7 step: 418, loss is 0.2924917936325073\n",
      "epoch: 7 step: 419, loss is 0.2645755410194397\n",
      "epoch: 7 step: 420, loss is 0.14846950769424438\n",
      "epoch: 7 step: 421, loss is 0.1827915608882904\n",
      "epoch: 7 step: 422, loss is 0.2017602175474167\n",
      "epoch: 7 step: 423, loss is 0.30699029564857483\n",
      "epoch: 7 step: 424, loss is 0.5970200896263123\n",
      "epoch: 7 step: 425, loss is 0.38333621621131897\n",
      "epoch: 7 step: 426, loss is 0.045952219516038895\n",
      "epoch: 7 step: 427, loss is 0.1787337362766266\n",
      "epoch: 7 step: 428, loss is 0.5747144818305969\n",
      "epoch: 7 step: 429, loss is 0.2830147445201874\n",
      "epoch: 7 step: 430, loss is 0.4389258027076721\n",
      "epoch: 7 step: 431, loss is 0.5041965246200562\n",
      "epoch: 7 step: 432, loss is 0.3806208670139313\n",
      "epoch: 7 step: 433, loss is 0.19388830661773682\n",
      "epoch: 7 step: 434, loss is 0.546482264995575\n",
      "epoch: 7 step: 435, loss is 0.46157777309417725\n",
      "epoch: 7 step: 436, loss is 0.15359891951084137\n",
      "epoch: 7 step: 437, loss is 0.4986536502838135\n",
      "epoch: 7 step: 438, loss is 0.2267104536294937\n",
      "epoch: 7 step: 439, loss is 0.25437116622924805\n",
      "epoch: 7 step: 440, loss is 0.06445631384849548\n",
      "epoch: 7 step: 441, loss is 0.22881294786930084\n",
      "epoch: 7 step: 442, loss is 0.4117894470691681\n",
      "epoch: 7 step: 443, loss is 0.2657134532928467\n",
      "epoch: 7 step: 444, loss is 0.30856797099113464\n",
      "epoch: 7 step: 445, loss is 0.07897656410932541\n",
      "epoch: 7 step: 446, loss is 0.42454758286476135\n",
      "epoch: 7 step: 447, loss is 0.5467773079872131\n",
      "epoch: 7 step: 448, loss is 0.1672852784395218\n",
      "epoch: 7 step: 449, loss is 0.16910946369171143\n",
      "epoch: 7 step: 450, loss is 0.21816407144069672\n",
      "epoch: 7 step: 451, loss is 0.6440800428390503\n",
      "epoch: 7 step: 452, loss is 0.07504995167255402\n",
      "epoch: 7 step: 453, loss is 0.48729822039604187\n",
      "epoch: 7 step: 454, loss is 0.21224819123744965\n",
      "epoch: 7 step: 455, loss is 0.04238298162817955\n",
      "epoch: 7 step: 456, loss is 0.08508843183517456\n",
      "epoch: 7 step: 457, loss is 0.05788777023553848\n",
      "epoch: 7 step: 458, loss is 0.3650892376899719\n",
      "epoch: 7 step: 459, loss is 0.38870877027511597\n",
      "epoch: 7 step: 460, loss is 0.3352492153644562\n",
      "epoch: 7 step: 461, loss is 0.3002353608608246\n",
      "epoch: 7 step: 462, loss is 0.19353637099266052\n",
      "epoch: 7 step: 463, loss is 0.8289018869400024\n",
      "epoch: 7 step: 464, loss is 0.25663629174232483\n",
      "epoch: 7 step: 465, loss is 0.03846098855137825\n",
      "epoch: 7 step: 466, loss is 0.20580269396305084\n",
      "epoch: 7 step: 467, loss is 0.028069255873560905\n",
      "epoch: 7 step: 468, loss is 0.028349118307232857\n",
      "epoch: 7 step: 469, loss is 0.037044279277324677\n",
      "epoch: 7 step: 470, loss is 0.35153648257255554\n",
      "epoch: 7 step: 471, loss is 0.26927897334098816\n",
      "epoch: 7 step: 472, loss is 0.47014370560646057\n",
      "epoch: 7 step: 473, loss is 0.3216792643070221\n",
      "epoch: 7 step: 474, loss is 0.1454455554485321\n",
      "epoch: 7 step: 475, loss is 0.3071979284286499\n",
      "epoch: 7 step: 476, loss is 0.08084549009799957\n",
      "epoch: 7 step: 477, loss is 0.2588154673576355\n",
      "epoch: 7 step: 478, loss is 0.3276120722293854\n",
      "epoch: 7 step: 479, loss is 0.0489899180829525\n",
      "epoch: 7 step: 480, loss is 0.3271549344062805\n",
      "epoch: 7 step: 481, loss is 0.29043957591056824\n",
      "epoch: 7 step: 482, loss is 0.18316590785980225\n",
      "epoch: 7 step: 483, loss is 0.0673053115606308\n",
      "epoch: 7 step: 484, loss is 0.0706651359796524\n",
      "epoch: 7 step: 485, loss is 0.23374558985233307\n",
      "epoch: 7 step: 486, loss is 0.15904834866523743\n",
      "epoch: 7 step: 487, loss is 0.08481457829475403\n",
      "epoch: 7 step: 488, loss is 0.40636518597602844\n",
      "epoch: 7 step: 489, loss is 0.19566485285758972\n",
      "epoch: 7 step: 490, loss is 0.5397706627845764\n",
      "epoch: 7 step: 491, loss is 0.31836065649986267\n",
      "epoch: 7 step: 492, loss is 0.2024937868118286\n",
      "epoch: 7 step: 493, loss is 0.2453375905752182\n",
      "epoch: 7 step: 494, loss is 0.11254484951496124\n",
      "epoch: 7 step: 495, loss is 0.4147254526615143\n",
      "epoch: 7 step: 496, loss is 0.2506207823753357\n",
      "epoch: 7 step: 497, loss is 0.31849202513694763\n",
      "epoch: 7 step: 498, loss is 0.32586944103240967\n",
      "epoch: 7 step: 499, loss is 0.39019668102264404\n",
      "epoch: 7 step: 500, loss is 0.016840171068906784\n",
      "epoch: 7 step: 501, loss is 0.20313885807991028\n",
      "epoch: 7 step: 502, loss is 0.2641933858394623\n",
      "epoch: 7 step: 503, loss is 0.31078028678894043\n",
      "epoch: 7 step: 504, loss is 0.2518254518508911\n",
      "epoch: 7 step: 505, loss is 0.165009543299675\n",
      "epoch: 7 step: 506, loss is 0.07910242676734924\n",
      "epoch: 7 step: 507, loss is 0.1673600971698761\n",
      "epoch: 7 step: 508, loss is 0.405796617269516\n",
      "epoch: 7 step: 509, loss is 0.778947114944458\n",
      "epoch: 7 step: 510, loss is 0.487000435590744\n",
      "epoch: 7 step: 511, loss is 0.23688684403896332\n",
      "epoch: 7 step: 512, loss is 0.1738698035478592\n",
      "epoch: 7 step: 513, loss is 0.5970929861068726\n",
      "epoch: 7 step: 514, loss is 0.1569252759218216\n",
      "epoch: 7 step: 515, loss is 0.3194963335990906\n",
      "epoch: 7 step: 516, loss is 0.20561914145946503\n",
      "epoch: 7 step: 517, loss is 0.42366454005241394\n",
      "epoch: 7 step: 518, loss is 0.24658769369125366\n",
      "epoch: 7 step: 519, loss is 0.18226559460163116\n",
      "epoch: 7 step: 520, loss is 0.6960461139678955\n",
      "epoch: 7 step: 521, loss is 0.2840186655521393\n",
      "epoch: 7 step: 522, loss is 0.513278603553772\n",
      "epoch: 7 step: 523, loss is 0.6522302031517029\n",
      "epoch: 7 step: 524, loss is 0.10088955610990524\n",
      "epoch: 7 step: 525, loss is 0.3301593065261841\n",
      "epoch: 7 step: 526, loss is 0.48105645179748535\n",
      "epoch: 7 step: 527, loss is 0.3649972975254059\n",
      "epoch: 7 step: 528, loss is 0.2603628635406494\n",
      "epoch: 7 step: 529, loss is 0.4513004422187805\n",
      "epoch: 7 step: 530, loss is 0.4542042911052704\n",
      "epoch: 7 step: 531, loss is 0.256118506193161\n",
      "epoch: 7 step: 532, loss is 0.34061145782470703\n",
      "epoch: 7 step: 533, loss is 0.2350374162197113\n",
      "epoch: 7 step: 534, loss is 0.5258307456970215\n",
      "epoch: 7 step: 535, loss is 0.20718421041965485\n",
      "epoch: 7 step: 536, loss is 0.26434463262557983\n",
      "epoch: 7 step: 537, loss is 0.10459177941083908\n",
      "epoch: 7 step: 538, loss is 0.23778216540813446\n",
      "epoch: 7 step: 539, loss is 0.2849859297275543\n",
      "epoch: 7 step: 540, loss is 0.20990842580795288\n",
      "epoch: 7 step: 541, loss is 0.07266422361135483\n",
      "epoch: 7 step: 542, loss is 0.9137246012687683\n",
      "epoch: 7 step: 543, loss is 0.18147236108779907\n",
      "epoch: 7 step: 544, loss is 0.22830581665039062\n",
      "epoch: 7 step: 545, loss is 0.3257366120815277\n",
      "epoch: 7 step: 546, loss is 0.06492020189762115\n",
      "epoch: 7 step: 547, loss is 0.14957500994205475\n",
      "epoch: 7 step: 548, loss is 0.03812717646360397\n",
      "epoch: 7 step: 549, loss is 0.16945967078208923\n",
      "epoch: 7 step: 550, loss is 0.3073301315307617\n",
      "epoch: 7 step: 551, loss is 0.04878009855747223\n",
      "epoch: 7 step: 552, loss is 0.5238875150680542\n",
      "epoch: 7 step: 553, loss is 0.4129651188850403\n",
      "epoch: 7 step: 554, loss is 0.20211227238178253\n",
      "epoch: 7 step: 555, loss is 0.1261937916278839\n",
      "epoch: 7 step: 556, loss is 0.772949755191803\n",
      "epoch: 7 step: 557, loss is 0.6048792004585266\n",
      "epoch: 7 step: 558, loss is 0.49833905696868896\n",
      "epoch: 7 step: 559, loss is 0.22639477252960205\n",
      "epoch: 7 step: 560, loss is 0.11979500204324722\n",
      "epoch: 7 step: 561, loss is 0.09232756495475769\n",
      "epoch: 7 step: 562, loss is 0.3403116762638092\n",
      "epoch: 7 step: 563, loss is 0.07716091722249985\n",
      "epoch: 7 step: 564, loss is 0.13624531030654907\n",
      "epoch: 7 step: 565, loss is 0.37875041365623474\n",
      "epoch: 7 step: 566, loss is 0.5483378171920776\n",
      "epoch: 7 step: 567, loss is 0.11327680945396423\n",
      "epoch: 7 step: 568, loss is 0.3331407606601715\n",
      "epoch: 7 step: 569, loss is 0.07195045053958893\n",
      "epoch: 7 step: 570, loss is 0.0508933924138546\n",
      "epoch: 7 step: 571, loss is 0.09974933415651321\n",
      "epoch: 7 step: 572, loss is 0.6486033201217651\n",
      "epoch: 7 step: 573, loss is 0.32397520542144775\n",
      "epoch: 7 step: 574, loss is 0.6382477283477783\n",
      "epoch: 7 step: 575, loss is 0.6114711761474609\n",
      "epoch: 7 step: 576, loss is 0.25275033712387085\n",
      "epoch: 7 step: 577, loss is 0.7491611838340759\n",
      "epoch: 7 step: 578, loss is 0.49500617384910583\n",
      "epoch: 7 step: 579, loss is 0.5362035036087036\n",
      "epoch: 7 step: 580, loss is 0.21905198693275452\n",
      "epoch: 7 step: 581, loss is 0.08390635997056961\n",
      "epoch: 7 step: 582, loss is 0.2145259827375412\n",
      "epoch: 7 step: 583, loss is 0.19984175264835358\n",
      "epoch: 7 step: 584, loss is 0.5732503533363342\n",
      "epoch: 7 step: 585, loss is 0.7013037800788879\n",
      "epoch: 7 step: 586, loss is 0.4131563603878021\n",
      "epoch: 7 step: 587, loss is 0.3276553750038147\n",
      "epoch: 7 step: 588, loss is 0.13430865108966827\n",
      "epoch: 7 step: 589, loss is 0.2769971489906311\n",
      "epoch: 7 step: 590, loss is 0.30286315083503723\n",
      "epoch: 7 step: 591, loss is 0.5690529346466064\n",
      "epoch: 7 step: 592, loss is 0.5048959851264954\n",
      "epoch: 7 step: 593, loss is 0.26237252354621887\n",
      "epoch: 7 step: 594, loss is 0.4200754761695862\n",
      "epoch: 7 step: 595, loss is 0.24280305206775665\n",
      "epoch: 7 step: 596, loss is 0.23204603791236877\n",
      "epoch: 7 step: 597, loss is 0.0726894736289978\n",
      "epoch: 7 step: 598, loss is 0.2241073101758957\n",
      "epoch: 7 step: 599, loss is 0.2721504867076874\n",
      "epoch: 7 step: 600, loss is 0.2126573920249939\n",
      "epoch: 7 step: 601, loss is 0.22858189046382904\n",
      "epoch: 7 step: 602, loss is 0.6203562617301941\n",
      "epoch: 7 step: 603, loss is 0.28123441338539124\n",
      "epoch: 7 step: 604, loss is 0.26139596104621887\n",
      "epoch: 7 step: 605, loss is 0.15696826577186584\n",
      "epoch: 7 step: 606, loss is 0.17837277054786682\n",
      "epoch: 7 step: 607, loss is 0.027910936623811722\n",
      "epoch: 7 step: 608, loss is 0.08810745179653168\n",
      "epoch: 7 step: 609, loss is 0.08391569554805756\n",
      "epoch: 7 step: 610, loss is 0.16327853500843048\n",
      "epoch: 7 step: 611, loss is 0.19719433784484863\n",
      "epoch: 7 step: 612, loss is 0.07006382942199707\n",
      "epoch: 7 step: 613, loss is 0.2072315514087677\n",
      "epoch: 7 step: 614, loss is 0.11983152478933334\n",
      "epoch: 7 step: 615, loss is 0.7397922277450562\n",
      "epoch: 7 step: 616, loss is 0.08765895664691925\n",
      "epoch: 7 step: 617, loss is 0.3589714765548706\n",
      "epoch: 7 step: 618, loss is 0.1755305975675583\n",
      "epoch: 7 step: 619, loss is 0.3686703145503998\n",
      "epoch: 7 step: 620, loss is 0.5029531717300415\n",
      "epoch: 7 step: 621, loss is 0.42637088894844055\n",
      "epoch: 7 step: 622, loss is 0.030312899500131607\n",
      "epoch: 7 step: 623, loss is 0.35515308380126953\n",
      "epoch: 7 step: 624, loss is 0.07067446410655975\n",
      "epoch: 7 step: 625, loss is 0.7955985069274902\n",
      "epoch: 7 step: 626, loss is 0.40218570828437805\n",
      "epoch: 7 step: 627, loss is 0.30145788192749023\n",
      "epoch: 7 step: 628, loss is 0.3198992908000946\n",
      "epoch: 7 step: 629, loss is 0.2676948308944702\n",
      "epoch: 7 step: 630, loss is 0.10912282019853592\n",
      "epoch: 7 step: 631, loss is 0.2686377465724945\n",
      "epoch: 7 step: 632, loss is 0.6305258274078369\n",
      "epoch: 7 step: 633, loss is 0.5856729745864868\n",
      "epoch: 7 step: 634, loss is 0.225284144282341\n",
      "epoch: 7 step: 635, loss is 0.3070032596588135\n",
      "epoch: 7 step: 636, loss is 0.271005779504776\n",
      "epoch: 7 step: 637, loss is 0.0735102966427803\n",
      "epoch: 7 step: 638, loss is 0.5794304013252258\n",
      "epoch: 7 step: 639, loss is 0.08420514315366745\n",
      "epoch: 7 step: 640, loss is 0.2892138659954071\n",
      "epoch: 7 step: 641, loss is 0.3040913939476013\n",
      "epoch: 7 step: 642, loss is 0.34102535247802734\n",
      "epoch: 7 step: 643, loss is 0.09359004348516464\n",
      "epoch: 7 step: 644, loss is 0.2328214794397354\n",
      "epoch: 7 step: 645, loss is 0.6563073992729187\n",
      "epoch: 7 step: 646, loss is 0.2918010354042053\n",
      "epoch: 7 step: 647, loss is 0.3881244361400604\n",
      "epoch: 7 step: 648, loss is 0.22920067608356476\n",
      "epoch: 7 step: 649, loss is 0.12138788402080536\n",
      "epoch: 7 step: 650, loss is 0.3636820614337921\n",
      "epoch: 7 step: 651, loss is 0.38796618580818176\n",
      "epoch: 7 step: 652, loss is 0.03362022340297699\n",
      "epoch: 7 step: 653, loss is 0.5975943207740784\n",
      "epoch: 7 step: 654, loss is 0.1615772843360901\n",
      "epoch: 7 step: 655, loss is 0.17213109135627747\n",
      "epoch: 7 step: 656, loss is 0.08688203245401382\n",
      "epoch: 7 step: 657, loss is 0.07503798604011536\n",
      "epoch: 7 step: 658, loss is 0.17835567891597748\n",
      "epoch: 7 step: 659, loss is 0.39488402009010315\n",
      "epoch: 7 step: 660, loss is 0.41611340641975403\n",
      "epoch: 7 step: 661, loss is 0.09647338837385178\n",
      "epoch: 7 step: 662, loss is 0.26792487502098083\n",
      "epoch: 7 step: 663, loss is 1.4464802742004395\n",
      "epoch: 7 step: 664, loss is 0.07543189823627472\n",
      "epoch: 7 step: 665, loss is 0.4212305247783661\n",
      "epoch: 7 step: 666, loss is 0.03727633133530617\n",
      "epoch: 7 step: 667, loss is 0.560538649559021\n",
      "epoch: 7 step: 668, loss is 0.3372652530670166\n",
      "epoch: 7 step: 669, loss is 0.3209112286567688\n",
      "epoch: 7 step: 670, loss is 0.3327866196632385\n",
      "epoch: 7 step: 671, loss is 0.34163418412208557\n",
      "epoch: 7 step: 672, loss is 0.7090924382209778\n",
      "epoch: 7 step: 673, loss is 0.6648896932601929\n",
      "epoch: 7 step: 674, loss is 0.7385925650596619\n",
      "epoch: 7 step: 675, loss is 0.281904935836792\n",
      "epoch: 7 step: 676, loss is 0.44550153613090515\n",
      "epoch: 7 step: 677, loss is 0.5595155954360962\n",
      "epoch: 7 step: 678, loss is 0.6823222041130066\n",
      "epoch: 7 step: 679, loss is 0.16901960968971252\n",
      "epoch: 7 step: 680, loss is 0.2253129929304123\n",
      "epoch: 7 step: 681, loss is 0.44007590413093567\n",
      "epoch: 7 step: 682, loss is 0.3053824007511139\n",
      "epoch: 7 step: 683, loss is 0.2764500081539154\n",
      "epoch: 7 step: 684, loss is 0.6064338088035583\n",
      "epoch: 7 step: 685, loss is 0.23734089732170105\n",
      "epoch: 7 step: 686, loss is 0.0877351239323616\n",
      "epoch: 7 step: 687, loss is 0.17020660638809204\n",
      "epoch: 7 step: 688, loss is 0.25207018852233887\n",
      "epoch: 7 step: 689, loss is 0.29657045006752014\n",
      "epoch: 7 step: 690, loss is 0.5898352265357971\n",
      "epoch: 7 step: 691, loss is 0.3946255147457123\n",
      "epoch: 7 step: 692, loss is 0.4925388693809509\n",
      "epoch: 7 step: 693, loss is 0.45622268319129944\n",
      "epoch: 7 step: 694, loss is 0.518149733543396\n",
      "epoch: 7 step: 695, loss is 0.26079317927360535\n",
      "epoch: 7 step: 696, loss is 0.1652994155883789\n",
      "epoch: 7 step: 697, loss is 0.13163669407367706\n",
      "epoch: 7 step: 698, loss is 0.13969643414020538\n",
      "epoch: 7 step: 699, loss is 0.635049045085907\n",
      "epoch: 7 step: 700, loss is 0.4626419246196747\n",
      "epoch: 7 step: 701, loss is 0.3839600682258606\n",
      "epoch: 7 step: 702, loss is 0.16830280423164368\n",
      "epoch: 7 step: 703, loss is 0.5327203869819641\n",
      "epoch: 7 step: 704, loss is 0.33748355507850647\n",
      "epoch: 7 step: 705, loss is 0.19358551502227783\n",
      "epoch: 7 step: 706, loss is 0.1959291249513626\n",
      "epoch: 7 step: 707, loss is 0.30682703852653503\n",
      "epoch: 7 step: 708, loss is 0.13554316759109497\n",
      "epoch: 7 step: 709, loss is 0.5045769214630127\n",
      "epoch: 7 step: 710, loss is 0.15860655903816223\n",
      "epoch: 7 step: 711, loss is 0.08874029666185379\n",
      "epoch: 7 step: 712, loss is 0.6721570491790771\n",
      "epoch: 7 step: 713, loss is 0.2475132793188095\n",
      "epoch: 7 step: 714, loss is 0.18816030025482178\n",
      "epoch: 7 step: 715, loss is 0.06553136557340622\n",
      "epoch: 7 step: 716, loss is 0.17481791973114014\n",
      "epoch: 7 step: 717, loss is 0.1667681634426117\n",
      "epoch: 7 step: 718, loss is 0.9423561096191406\n",
      "epoch: 7 step: 719, loss is 0.18386347591876984\n",
      "epoch: 7 step: 720, loss is 0.37440136075019836\n",
      "epoch: 7 step: 721, loss is 0.4163946211338043\n",
      "epoch: 7 step: 722, loss is 0.13264955580234528\n",
      "epoch: 7 step: 723, loss is 0.2824273109436035\n",
      "epoch: 7 step: 724, loss is 0.215729221701622\n",
      "epoch: 7 step: 725, loss is 0.6957923173904419\n",
      "epoch: 7 step: 726, loss is 0.13257203996181488\n",
      "epoch: 7 step: 727, loss is 0.25095200538635254\n",
      "epoch: 7 step: 728, loss is 0.0973774790763855\n",
      "epoch: 7 step: 729, loss is 0.12300841510295868\n",
      "epoch: 7 step: 730, loss is 0.1301896572113037\n",
      "epoch: 7 step: 731, loss is 0.11557617038488388\n",
      "epoch: 7 step: 732, loss is 0.5271255970001221\n",
      "epoch: 7 step: 733, loss is 0.5673732757568359\n",
      "epoch: 7 step: 734, loss is 0.5105246901512146\n",
      "epoch: 7 step: 735, loss is 0.7166541814804077\n",
      "epoch: 7 step: 736, loss is 0.2608446776866913\n",
      "epoch: 7 step: 737, loss is 0.10253144800662994\n",
      "epoch: 7 step: 738, loss is 0.21695002913475037\n",
      "epoch: 7 step: 739, loss is 0.45303499698638916\n",
      "epoch: 7 step: 740, loss is 0.2093028426170349\n",
      "epoch: 7 step: 741, loss is 0.07064273208379745\n",
      "epoch: 7 step: 742, loss is 0.33727654814720154\n",
      "epoch: 7 step: 743, loss is 0.13373158872127533\n",
      "epoch: 7 step: 744, loss is 0.32342296838760376\n",
      "epoch: 7 step: 745, loss is 0.8974178433418274\n",
      "epoch: 7 step: 746, loss is 0.1663963943719864\n",
      "epoch: 7 step: 747, loss is 0.04650980606675148\n",
      "epoch: 7 step: 748, loss is 0.7064056396484375\n",
      "epoch: 7 step: 749, loss is 0.8542472720146179\n",
      "epoch: 7 step: 750, loss is 0.2575594484806061\n",
      "epoch: 7 step: 751, loss is 0.06000051274895668\n",
      "epoch: 7 step: 752, loss is 0.35027867555618286\n",
      "epoch: 7 step: 753, loss is 0.09757140278816223\n",
      "epoch: 7 step: 754, loss is 0.19744934141635895\n",
      "epoch: 7 step: 755, loss is 0.3170267641544342\n",
      "epoch: 7 step: 756, loss is 0.45129337906837463\n",
      "epoch: 7 step: 757, loss is 0.4713841378688812\n",
      "epoch: 7 step: 758, loss is 0.4911525547504425\n",
      "epoch: 7 step: 759, loss is 0.6198111772537231\n",
      "epoch: 7 step: 760, loss is 0.2250368744134903\n",
      "epoch: 7 step: 761, loss is 0.23191781342029572\n",
      "epoch: 7 step: 762, loss is 0.1870572417974472\n",
      "epoch: 7 step: 763, loss is 0.07411565631628036\n",
      "epoch: 7 step: 764, loss is 0.2910839319229126\n",
      "epoch: 7 step: 765, loss is 0.5243600010871887\n",
      "epoch: 7 step: 766, loss is 0.027820689603686333\n",
      "epoch: 7 step: 767, loss is 0.3964487612247467\n",
      "epoch: 7 step: 768, loss is 0.19918504357337952\n",
      "epoch: 7 step: 769, loss is 0.9264256358146667\n",
      "epoch: 7 step: 770, loss is 0.11914819478988647\n",
      "epoch: 7 step: 771, loss is 0.41479629278182983\n",
      "epoch: 7 step: 772, loss is 0.20917080342769623\n",
      "epoch: 7 step: 773, loss is 0.09374906867742538\n",
      "epoch: 7 step: 774, loss is 0.0811995267868042\n",
      "epoch: 7 step: 775, loss is 0.414100706577301\n",
      "epoch: 7 step: 776, loss is 0.26936256885528564\n",
      "epoch: 7 step: 777, loss is 0.14405623078346252\n",
      "epoch: 7 step: 778, loss is 0.48543381690979004\n",
      "epoch: 7 step: 779, loss is 0.14762382209300995\n",
      "epoch: 7 step: 780, loss is 0.7909114956855774\n",
      "epoch: 7 step: 781, loss is 0.8551933169364929\n",
      "epoch: 7 step: 782, loss is 0.3333952724933624\n",
      "epoch: 7 step: 783, loss is 0.15791043639183044\n",
      "epoch: 7 step: 784, loss is 0.25193464756011963\n",
      "epoch: 7 step: 785, loss is 0.2616829574108124\n",
      "epoch: 7 step: 786, loss is 0.7291422486305237\n",
      "epoch: 7 step: 787, loss is 0.14424489438533783\n",
      "epoch: 7 step: 788, loss is 1.0753225088119507\n",
      "epoch: 7 step: 789, loss is 0.11700725555419922\n",
      "epoch: 7 step: 790, loss is 0.15455079078674316\n",
      "epoch: 7 step: 791, loss is 0.38303500413894653\n",
      "epoch: 7 step: 792, loss is 0.3426622748374939\n",
      "epoch: 7 step: 793, loss is 0.3618351221084595\n",
      "epoch: 7 step: 794, loss is 0.5666043758392334\n",
      "epoch: 7 step: 795, loss is 0.27688902616500854\n",
      "epoch: 7 step: 796, loss is 0.31136319041252136\n",
      "epoch: 7 step: 797, loss is 0.3376498222351074\n",
      "epoch: 7 step: 798, loss is 0.22613337635993958\n",
      "epoch: 7 step: 799, loss is 0.03613405302166939\n",
      "epoch: 7 step: 800, loss is 0.18698036670684814\n",
      "epoch: 7 step: 801, loss is 0.5111424922943115\n",
      "epoch: 7 step: 802, loss is 0.7803186178207397\n",
      "epoch: 7 step: 803, loss is 0.1430761218070984\n",
      "epoch: 7 step: 804, loss is 0.36311864852905273\n",
      "epoch: 7 step: 805, loss is 0.4405076503753662\n",
      "epoch: 7 step: 806, loss is 0.6203831434249878\n",
      "epoch: 7 step: 807, loss is 0.17327255010604858\n",
      "epoch: 7 step: 808, loss is 0.15293550491333008\n",
      "epoch: 7 step: 809, loss is 0.35352930426597595\n",
      "epoch: 7 step: 810, loss is 0.18496152758598328\n",
      "epoch: 7 step: 811, loss is 0.06605467200279236\n",
      "epoch: 7 step: 812, loss is 0.14101168513298035\n",
      "epoch: 7 step: 813, loss is 0.4134026765823364\n",
      "epoch: 7 step: 814, loss is 0.1806926280260086\n",
      "epoch: 7 step: 815, loss is 0.5061631202697754\n",
      "epoch: 7 step: 816, loss is 0.44499704241752625\n",
      "epoch: 7 step: 817, loss is 0.4478689730167389\n",
      "epoch: 7 step: 818, loss is 0.21475258469581604\n",
      "epoch: 7 step: 819, loss is 0.5234817266464233\n",
      "epoch: 7 step: 820, loss is 0.7273511290550232\n",
      "epoch: 7 step: 821, loss is 0.3201740086078644\n",
      "epoch: 7 step: 822, loss is 0.5432190299034119\n",
      "epoch: 7 step: 823, loss is 0.32637640833854675\n",
      "epoch: 7 step: 824, loss is 0.11712529510259628\n",
      "epoch: 7 step: 825, loss is 0.2990098297595978\n",
      "epoch: 7 step: 826, loss is 0.11438123136758804\n",
      "epoch: 7 step: 827, loss is 0.23508815467357635\n",
      "epoch: 7 step: 828, loss is 0.38762372732162476\n",
      "epoch: 7 step: 829, loss is 0.10426533222198486\n",
      "epoch: 7 step: 830, loss is 0.14468468725681305\n",
      "epoch: 7 step: 831, loss is 0.1988646686077118\n",
      "epoch: 7 step: 832, loss is 0.15284666419029236\n",
      "epoch: 7 step: 833, loss is 0.16141092777252197\n",
      "epoch: 7 step: 834, loss is 0.19053466618061066\n",
      "epoch: 7 step: 835, loss is 0.14982692897319794\n",
      "epoch: 7 step: 836, loss is 0.1888028085231781\n",
      "epoch: 7 step: 837, loss is 0.5510076284408569\n",
      "epoch: 7 step: 838, loss is 0.08333580195903778\n",
      "epoch: 7 step: 839, loss is 0.3208963871002197\n",
      "epoch: 7 step: 840, loss is 0.08103925734758377\n",
      "epoch: 7 step: 841, loss is 0.3073147237300873\n",
      "epoch: 7 step: 842, loss is 0.0809517577290535\n",
      "epoch: 7 step: 843, loss is 0.06592341512441635\n",
      "epoch: 7 step: 844, loss is 0.06165504828095436\n",
      "epoch: 7 step: 845, loss is 0.22183652222156525\n",
      "epoch: 7 step: 846, loss is 0.4435948133468628\n",
      "epoch: 7 step: 847, loss is 0.3280423581600189\n",
      "epoch: 7 step: 848, loss is 0.18186543881893158\n",
      "epoch: 7 step: 849, loss is 0.32193079590797424\n",
      "epoch: 7 step: 850, loss is 0.7307074666023254\n",
      "epoch: 7 step: 851, loss is 0.09858084470033646\n",
      "epoch: 7 step: 852, loss is 0.24458493292331696\n",
      "epoch: 7 step: 853, loss is 0.08148708939552307\n",
      "epoch: 7 step: 854, loss is 0.4719371199607849\n",
      "epoch: 7 step: 855, loss is 0.09928831458091736\n",
      "epoch: 7 step: 856, loss is 0.055829327553510666\n",
      "epoch: 7 step: 857, loss is 0.515559732913971\n",
      "epoch: 7 step: 858, loss is 0.16870690882205963\n",
      "epoch: 7 step: 859, loss is 0.17268025875091553\n",
      "epoch: 7 step: 860, loss is 0.5473564267158508\n",
      "epoch: 7 step: 861, loss is 0.07249604910612106\n",
      "epoch: 7 step: 862, loss is 0.12804792821407318\n",
      "epoch: 7 step: 863, loss is 0.5465602874755859\n",
      "epoch: 7 step: 864, loss is 0.2158258706331253\n",
      "epoch: 7 step: 865, loss is 0.27543583512306213\n",
      "epoch: 7 step: 866, loss is 0.4616408050060272\n",
      "epoch: 7 step: 867, loss is 0.2001381367444992\n",
      "epoch: 7 step: 868, loss is 0.3623059093952179\n",
      "epoch: 7 step: 869, loss is 0.07833676040172577\n",
      "epoch: 7 step: 870, loss is 0.1940307766199112\n",
      "epoch: 7 step: 871, loss is 0.2573019862174988\n",
      "epoch: 7 step: 872, loss is 0.10729935765266418\n",
      "epoch: 7 step: 873, loss is 0.09310120344161987\n",
      "epoch: 7 step: 874, loss is 0.27854040265083313\n",
      "epoch: 7 step: 875, loss is 0.2036127895116806\n",
      "epoch: 7 step: 876, loss is 0.6887322068214417\n",
      "epoch: 7 step: 877, loss is 0.1024467721581459\n",
      "epoch: 7 step: 878, loss is 0.7670721411705017\n",
      "epoch: 7 step: 879, loss is 0.2252107560634613\n",
      "epoch: 7 step: 880, loss is 0.2730872929096222\n",
      "epoch: 7 step: 881, loss is 0.5885529518127441\n",
      "epoch: 7 step: 882, loss is 0.1578131765127182\n",
      "epoch: 7 step: 883, loss is 0.2849525213241577\n",
      "epoch: 7 step: 884, loss is 0.1746971607208252\n",
      "epoch: 7 step: 885, loss is 0.07025166600942612\n",
      "epoch: 7 step: 886, loss is 0.24939891695976257\n",
      "epoch: 7 step: 887, loss is 0.05860035493969917\n",
      "epoch: 7 step: 888, loss is 0.5055218935012817\n",
      "epoch: 7 step: 889, loss is 0.5281777381896973\n",
      "epoch: 7 step: 890, loss is 0.9700075387954712\n",
      "epoch: 7 step: 891, loss is 0.3281852900981903\n",
      "epoch: 7 step: 892, loss is 0.6087626218795776\n",
      "epoch: 7 step: 893, loss is 0.34488579630851746\n",
      "epoch: 7 step: 894, loss is 0.15308208763599396\n",
      "epoch: 7 step: 895, loss is 0.04863306134939194\n",
      "epoch: 7 step: 896, loss is 0.30836060643196106\n",
      "epoch: 7 step: 897, loss is 0.2151951640844345\n",
      "epoch: 7 step: 898, loss is 0.350948303937912\n",
      "epoch: 7 step: 899, loss is 0.23820431530475616\n",
      "epoch: 7 step: 900, loss is 0.2656945586204529\n",
      "epoch: 7 step: 901, loss is 0.08579185605049133\n",
      "epoch: 7 step: 902, loss is 0.39339470863342285\n",
      "epoch: 7 step: 903, loss is 0.431641548871994\n",
      "epoch: 7 step: 904, loss is 0.36655285954475403\n",
      "epoch: 7 step: 905, loss is 0.299899160861969\n",
      "epoch: 7 step: 906, loss is 0.6384032368659973\n",
      "epoch: 7 step: 907, loss is 0.0842449739575386\n",
      "epoch: 7 step: 908, loss is 0.7403495907783508\n",
      "epoch: 7 step: 909, loss is 0.31839489936828613\n",
      "epoch: 7 step: 910, loss is 0.5079365968704224\n",
      "epoch: 7 step: 911, loss is 0.13033366203308105\n",
      "epoch: 7 step: 912, loss is 0.2115286886692047\n",
      "epoch: 7 step: 913, loss is 0.18516245484352112\n",
      "epoch: 7 step: 914, loss is 0.42930084466934204\n",
      "epoch: 7 step: 915, loss is 0.2797025442123413\n",
      "epoch: 7 step: 916, loss is 0.1763918399810791\n",
      "epoch: 7 step: 917, loss is 0.2549463212490082\n",
      "epoch: 7 step: 918, loss is 0.28334537148475647\n",
      "epoch: 7 step: 919, loss is 0.6749507784843445\n",
      "epoch: 7 step: 920, loss is 0.11181050539016724\n",
      "epoch: 7 step: 921, loss is 0.3912971019744873\n",
      "epoch: 7 step: 922, loss is 0.10380350053310394\n",
      "epoch: 7 step: 923, loss is 0.39383453130722046\n",
      "epoch: 7 step: 924, loss is 0.588483989238739\n",
      "epoch: 7 step: 925, loss is 0.3093153238296509\n",
      "epoch: 7 step: 926, loss is 0.9808166027069092\n",
      "epoch: 7 step: 927, loss is 0.21195217967033386\n",
      "epoch: 7 step: 928, loss is 0.38826847076416016\n",
      "epoch: 7 step: 929, loss is 0.1615721583366394\n",
      "epoch: 7 step: 930, loss is 0.22820283472537994\n",
      "epoch: 7 step: 931, loss is 0.10605397075414658\n",
      "epoch: 7 step: 932, loss is 0.5923135280609131\n",
      "epoch: 7 step: 933, loss is 0.39095765352249146\n",
      "epoch: 7 step: 934, loss is 0.22045861184597015\n",
      "epoch: 7 step: 935, loss is 0.16441108286380768\n",
      "epoch: 7 step: 936, loss is 0.23557528853416443\n",
      "epoch: 7 step: 937, loss is 0.17913755774497986\n",
      "epoch: 7 step: 938, loss is 0.25609180331230164\n",
      "epoch: 7 step: 939, loss is 0.2968055009841919\n",
      "epoch: 7 step: 940, loss is 0.17285217344760895\n",
      "epoch: 7 step: 941, loss is 0.33830270171165466\n",
      "epoch: 7 step: 942, loss is 0.1734497845172882\n",
      "epoch: 7 step: 943, loss is 0.3624473810195923\n",
      "epoch: 7 step: 944, loss is 0.13589629530906677\n",
      "epoch: 7 step: 945, loss is 0.20420055091381073\n",
      "epoch: 7 step: 946, loss is 0.5311034917831421\n",
      "epoch: 7 step: 947, loss is 0.09317830204963684\n",
      "epoch: 7 step: 948, loss is 0.1979520469903946\n",
      "epoch: 7 step: 949, loss is 0.31831586360931396\n",
      "epoch: 7 step: 950, loss is 0.0686081200838089\n",
      "epoch: 7 step: 951, loss is 0.2183220237493515\n",
      "epoch: 7 step: 952, loss is 0.4829545021057129\n",
      "epoch: 7 step: 953, loss is 0.9057156443595886\n",
      "epoch: 7 step: 954, loss is 0.09639202058315277\n",
      "epoch: 7 step: 955, loss is 0.07062084972858429\n",
      "epoch: 7 step: 956, loss is 0.3047104477882385\n",
      "epoch: 7 step: 957, loss is 0.2643979489803314\n",
      "epoch: 7 step: 958, loss is 0.07156702131032944\n",
      "epoch: 7 step: 959, loss is 0.46817225217819214\n",
      "epoch: 7 step: 960, loss is 0.18574228882789612\n",
      "epoch: 7 step: 961, loss is 0.7601377367973328\n",
      "epoch: 7 step: 962, loss is 0.10471463203430176\n",
      "epoch: 7 step: 963, loss is 0.2689562141895294\n",
      "epoch: 7 step: 964, loss is 0.21088498830795288\n",
      "epoch: 7 step: 965, loss is 0.3654489517211914\n",
      "epoch: 7 step: 966, loss is 0.159001886844635\n",
      "epoch: 7 step: 967, loss is 0.3810376226902008\n",
      "epoch: 7 step: 968, loss is 0.3370581567287445\n",
      "epoch: 7 step: 969, loss is 0.6787442564964294\n",
      "epoch: 7 step: 970, loss is 0.26267603039741516\n",
      "epoch: 7 step: 971, loss is 0.07147055864334106\n",
      "epoch: 7 step: 972, loss is 0.37726807594299316\n",
      "epoch: 7 step: 973, loss is 0.2729991376399994\n",
      "epoch: 7 step: 974, loss is 0.4663490653038025\n",
      "epoch: 7 step: 975, loss is 0.1702788919210434\n",
      "epoch: 7 step: 976, loss is 0.2628069221973419\n",
      "epoch: 7 step: 977, loss is 0.413061261177063\n",
      "epoch: 7 step: 978, loss is 0.30842939019203186\n",
      "epoch: 7 step: 979, loss is 0.3185071647167206\n",
      "epoch: 7 step: 980, loss is 0.2817310392856598\n",
      "epoch: 7 step: 981, loss is 0.061073653399944305\n",
      "epoch: 7 step: 982, loss is 0.1751129925251007\n",
      "epoch: 7 step: 983, loss is 0.04501120746135712\n",
      "epoch: 7 step: 984, loss is 0.19755837321281433\n",
      "epoch: 7 step: 985, loss is 0.14881663024425507\n",
      "epoch: 7 step: 986, loss is 0.19589745998382568\n",
      "epoch: 7 step: 987, loss is 0.2155524492263794\n",
      "epoch: 7 step: 988, loss is 0.11902458965778351\n",
      "epoch: 7 step: 989, loss is 0.12034916877746582\n",
      "epoch: 7 step: 990, loss is 0.1670917123556137\n",
      "epoch: 7 step: 991, loss is 0.19381052255630493\n",
      "epoch: 7 step: 992, loss is 0.07110535353422165\n",
      "epoch: 7 step: 993, loss is 0.40203189849853516\n",
      "epoch: 7 step: 994, loss is 0.15226760506629944\n",
      "epoch: 7 step: 995, loss is 0.05834481865167618\n",
      "epoch: 7 step: 996, loss is 0.49102500081062317\n",
      "epoch: 7 step: 997, loss is 0.14312393963336945\n",
      "epoch: 7 step: 998, loss is 0.6965658068656921\n",
      "epoch: 7 step: 999, loss is 0.48774832487106323\n",
      "epoch: 7 step: 1000, loss is 0.06343918293714523\n",
      "epoch: 7 step: 1001, loss is 0.11147613823413849\n",
      "epoch: 7 step: 1002, loss is 0.21498450636863708\n",
      "epoch: 7 step: 1003, loss is 0.4693201184272766\n",
      "epoch: 7 step: 1004, loss is 0.14546696841716766\n",
      "epoch: 7 step: 1005, loss is 0.5262500643730164\n",
      "epoch: 7 step: 1006, loss is 0.12282298505306244\n",
      "epoch: 7 step: 1007, loss is 0.4376847445964813\n",
      "epoch: 7 step: 1008, loss is 0.3461010754108429\n",
      "epoch: 7 step: 1009, loss is 0.06854182481765747\n",
      "epoch: 7 step: 1010, loss is 1.13246488571167\n",
      "epoch: 7 step: 1011, loss is 0.42512550950050354\n",
      "epoch: 7 step: 1012, loss is 0.7208142280578613\n",
      "epoch: 7 step: 1013, loss is 0.20610222220420837\n",
      "epoch: 7 step: 1014, loss is 0.3766786754131317\n",
      "epoch: 7 step: 1015, loss is 0.543019711971283\n",
      "epoch: 7 step: 1016, loss is 0.540776252746582\n",
      "epoch: 7 step: 1017, loss is 0.3421661853790283\n",
      "epoch: 7 step: 1018, loss is 0.09616860002279282\n",
      "epoch: 7 step: 1019, loss is 0.40176063776016235\n",
      "epoch: 7 step: 1020, loss is 0.48085758090019226\n",
      "epoch: 7 step: 1021, loss is 0.9690428972244263\n",
      "epoch: 7 step: 1022, loss is 0.30482837557792664\n",
      "epoch: 7 step: 1023, loss is 0.5155412554740906\n",
      "epoch: 7 step: 1024, loss is 0.02819664031267166\n",
      "epoch: 7 step: 1025, loss is 0.5314409136772156\n",
      "epoch: 7 step: 1026, loss is 0.22277070581912994\n",
      "epoch: 7 step: 1027, loss is 0.32631340622901917\n",
      "epoch: 7 step: 1028, loss is 0.45959576964378357\n",
      "epoch: 7 step: 1029, loss is 0.4231038987636566\n",
      "epoch: 7 step: 1030, loss is 0.7386215925216675\n",
      "epoch: 7 step: 1031, loss is 0.4051353931427002\n",
      "epoch: 7 step: 1032, loss is 0.6041855812072754\n",
      "epoch: 7 step: 1033, loss is 0.4262717068195343\n",
      "epoch: 7 step: 1034, loss is 0.2922348380088806\n",
      "epoch: 7 step: 1035, loss is 0.227193683385849\n",
      "epoch: 7 step: 1036, loss is 0.2632067799568176\n",
      "epoch: 7 step: 1037, loss is 0.22715875506401062\n",
      "epoch: 7 step: 1038, loss is 0.3430224061012268\n",
      "epoch: 7 step: 1039, loss is 0.12426091730594635\n",
      "epoch: 7 step: 1040, loss is 0.19796790182590485\n",
      "epoch: 7 step: 1041, loss is 0.2208660989999771\n",
      "epoch: 7 step: 1042, loss is 0.28809604048728943\n",
      "epoch: 7 step: 1043, loss is 0.34333521127700806\n",
      "epoch: 7 step: 1044, loss is 0.384292334318161\n",
      "epoch: 7 step: 1045, loss is 0.26326534152030945\n",
      "epoch: 7 step: 1046, loss is 0.17069531977176666\n",
      "epoch: 7 step: 1047, loss is 0.17374861240386963\n",
      "epoch: 7 step: 1048, loss is 0.19393029808998108\n",
      "epoch: 7 step: 1049, loss is 0.42429089546203613\n",
      "epoch: 7 step: 1050, loss is 0.4002934694290161\n",
      "epoch: 7 step: 1051, loss is 0.06727664172649384\n",
      "epoch: 7 step: 1052, loss is 0.46014976501464844\n",
      "epoch: 7 step: 1053, loss is 0.37641245126724243\n",
      "epoch: 7 step: 1054, loss is 0.24789616465568542\n",
      "epoch: 7 step: 1055, loss is 0.3227464258670807\n",
      "epoch: 7 step: 1056, loss is 0.31699419021606445\n",
      "epoch: 7 step: 1057, loss is 0.05497552827000618\n",
      "epoch: 7 step: 1058, loss is 0.2924155294895172\n",
      "epoch: 7 step: 1059, loss is 0.16625292599201202\n",
      "epoch: 7 step: 1060, loss is 0.26571908593177795\n",
      "epoch: 7 step: 1061, loss is 0.6461917757987976\n",
      "epoch: 7 step: 1062, loss is 0.020424392074346542\n",
      "epoch: 7 step: 1063, loss is 0.10916837304830551\n",
      "epoch: 7 step: 1064, loss is 0.1244446337223053\n",
      "epoch: 7 step: 1065, loss is 0.3946038484573364\n",
      "epoch: 7 step: 1066, loss is 0.15242144465446472\n",
      "epoch: 7 step: 1067, loss is 0.6533699035644531\n",
      "epoch: 7 step: 1068, loss is 0.558426022529602\n",
      "epoch: 7 step: 1069, loss is 0.18865960836410522\n",
      "epoch: 7 step: 1070, loss is 0.07069282233715057\n",
      "epoch: 7 step: 1071, loss is 0.1926134079694748\n",
      "epoch: 7 step: 1072, loss is 0.576632559299469\n",
      "epoch: 7 step: 1073, loss is 0.07261491566896439\n",
      "epoch: 7 step: 1074, loss is 0.5784952640533447\n",
      "epoch: 7 step: 1075, loss is 0.19366677105426788\n",
      "epoch: 7 step: 1076, loss is 0.27791526913642883\n",
      "epoch: 7 step: 1077, loss is 0.21852655708789825\n",
      "epoch: 7 step: 1078, loss is 0.4547653794288635\n",
      "epoch: 7 step: 1079, loss is 0.12888333201408386\n",
      "epoch: 7 step: 1080, loss is 0.5054087042808533\n",
      "epoch: 7 step: 1081, loss is 0.03418596461415291\n",
      "epoch: 7 step: 1082, loss is 0.07896124571561813\n",
      "epoch: 7 step: 1083, loss is 0.09163416177034378\n",
      "epoch: 7 step: 1084, loss is 0.7232288122177124\n",
      "epoch: 7 step: 1085, loss is 0.38474538922309875\n",
      "epoch: 7 step: 1086, loss is 0.3552756905555725\n",
      "epoch: 7 step: 1087, loss is 0.12840628623962402\n",
      "epoch: 7 step: 1088, loss is 0.1640123426914215\n",
      "epoch: 7 step: 1089, loss is 0.21809883415699005\n",
      "epoch: 7 step: 1090, loss is 0.4762962758541107\n",
      "epoch: 7 step: 1091, loss is 0.07841657847166061\n",
      "epoch: 7 step: 1092, loss is 0.10634957253932953\n",
      "epoch: 7 step: 1093, loss is 0.16325116157531738\n",
      "epoch: 7 step: 1094, loss is 0.14943763613700867\n",
      "epoch: 7 step: 1095, loss is 0.04650005325675011\n",
      "epoch: 7 step: 1096, loss is 0.7889322638511658\n",
      "epoch: 7 step: 1097, loss is 0.16026516258716583\n",
      "epoch: 7 step: 1098, loss is 0.17705516517162323\n",
      "epoch: 7 step: 1099, loss is 0.0945998802781105\n",
      "epoch: 7 step: 1100, loss is 0.10733559727668762\n",
      "epoch: 7 step: 1101, loss is 0.09778820723295212\n",
      "epoch: 7 step: 1102, loss is 0.35150203108787537\n",
      "epoch: 7 step: 1103, loss is 0.10134725272655487\n",
      "epoch: 7 step: 1104, loss is 0.2619982063770294\n",
      "epoch: 7 step: 1105, loss is 0.4210636019706726\n",
      "epoch: 7 step: 1106, loss is 0.11845195293426514\n",
      "epoch: 7 step: 1107, loss is 0.5256767272949219\n",
      "epoch: 7 step: 1108, loss is 0.5631112456321716\n",
      "epoch: 7 step: 1109, loss is 0.3047844171524048\n",
      "epoch: 7 step: 1110, loss is 0.08304677158594131\n",
      "epoch: 7 step: 1111, loss is 0.08810964971780777\n",
      "epoch: 7 step: 1112, loss is 0.5673759579658508\n",
      "epoch: 7 step: 1113, loss is 0.18942153453826904\n",
      "epoch: 7 step: 1114, loss is 0.20861749351024628\n",
      "epoch: 7 step: 1115, loss is 0.5318849682807922\n",
      "epoch: 7 step: 1116, loss is 0.49648037552833557\n",
      "epoch: 7 step: 1117, loss is 0.5402653813362122\n",
      "epoch: 7 step: 1118, loss is 0.19002306461334229\n",
      "epoch: 7 step: 1119, loss is 0.07700187712907791\n",
      "epoch: 7 step: 1120, loss is 0.8759227991104126\n",
      "epoch: 7 step: 1121, loss is 0.34944948554039\n",
      "epoch: 7 step: 1122, loss is 0.443619042634964\n",
      "epoch: 7 step: 1123, loss is 0.16960115730762482\n",
      "epoch: 7 step: 1124, loss is 0.27553078532218933\n",
      "epoch: 7 step: 1125, loss is 0.29467979073524475\n",
      "epoch: 7 step: 1126, loss is 0.31703388690948486\n",
      "epoch: 7 step: 1127, loss is 0.22822624444961548\n",
      "epoch: 7 step: 1128, loss is 0.8915843963623047\n",
      "epoch: 7 step: 1129, loss is 0.7965788841247559\n",
      "epoch: 7 step: 1130, loss is 0.29700249433517456\n",
      "epoch: 7 step: 1131, loss is 0.49586430191993713\n",
      "epoch: 7 step: 1132, loss is 0.13075554370880127\n",
      "epoch: 7 step: 1133, loss is 0.26615890860557556\n",
      "epoch: 7 step: 1134, loss is 0.20910906791687012\n",
      "epoch: 7 step: 1135, loss is 0.39980658888816833\n",
      "epoch: 7 step: 1136, loss is 0.07918458431959152\n",
      "epoch: 7 step: 1137, loss is 0.570280909538269\n",
      "epoch: 7 step: 1138, loss is 0.48495742678642273\n",
      "epoch: 7 step: 1139, loss is 0.07519137114286423\n",
      "epoch: 7 step: 1140, loss is 0.3019750714302063\n",
      "epoch: 7 step: 1141, loss is 0.3421099781990051\n",
      "epoch: 7 step: 1142, loss is 0.20232059061527252\n",
      "epoch: 7 step: 1143, loss is 0.1210617870092392\n",
      "epoch: 7 step: 1144, loss is 0.4106842279434204\n",
      "epoch: 7 step: 1145, loss is 0.2190878540277481\n",
      "epoch: 7 step: 1146, loss is 0.21027572453022003\n",
      "epoch: 7 step: 1147, loss is 0.2674245238304138\n",
      "epoch: 7 step: 1148, loss is 0.2516516149044037\n",
      "epoch: 7 step: 1149, loss is 0.046476952731609344\n",
      "epoch: 7 step: 1150, loss is 0.15695948898792267\n",
      "epoch: 7 step: 1151, loss is 0.28817951679229736\n",
      "epoch: 7 step: 1152, loss is 0.5360113978385925\n",
      "epoch: 7 step: 1153, loss is 0.24594879150390625\n",
      "epoch: 7 step: 1154, loss is 0.44578543305397034\n",
      "epoch: 7 step: 1155, loss is 0.22085992991924286\n",
      "epoch: 7 step: 1156, loss is 0.6264728307723999\n",
      "epoch: 7 step: 1157, loss is 0.14768542349338531\n",
      "epoch: 7 step: 1158, loss is 0.2459389716386795\n",
      "epoch: 7 step: 1159, loss is 0.1373870074748993\n",
      "epoch: 7 step: 1160, loss is 0.8540273904800415\n",
      "epoch: 7 step: 1161, loss is 0.3815801739692688\n",
      "epoch: 7 step: 1162, loss is 0.21325130760669708\n",
      "epoch: 7 step: 1163, loss is 0.5720394253730774\n",
      "epoch: 7 step: 1164, loss is 0.07696934789419174\n",
      "epoch: 7 step: 1165, loss is 0.2624605596065521\n",
      "epoch: 7 step: 1166, loss is 0.15448933839797974\n",
      "epoch: 7 step: 1167, loss is 0.16530638933181763\n",
      "epoch: 7 step: 1168, loss is 0.1542469561100006\n",
      "epoch: 7 step: 1169, loss is 0.3318881690502167\n",
      "epoch: 7 step: 1170, loss is 0.40360087156295776\n",
      "epoch: 7 step: 1171, loss is 0.6927446722984314\n",
      "epoch: 7 step: 1172, loss is 0.2965298891067505\n",
      "epoch: 7 step: 1173, loss is 0.6607725620269775\n",
      "epoch: 7 step: 1174, loss is 0.3910847306251526\n",
      "epoch: 7 step: 1175, loss is 0.3702083230018616\n",
      "epoch: 7 step: 1176, loss is 0.4238899350166321\n",
      "epoch: 7 step: 1177, loss is 0.1718432903289795\n",
      "epoch: 7 step: 1178, loss is 0.7426483035087585\n",
      "epoch: 7 step: 1179, loss is 0.640981137752533\n",
      "epoch: 7 step: 1180, loss is 0.1076049730181694\n",
      "epoch: 7 step: 1181, loss is 0.13062258064746857\n",
      "epoch: 7 step: 1182, loss is 0.36202630400657654\n",
      "epoch: 7 step: 1183, loss is 0.06170353665947914\n",
      "epoch: 7 step: 1184, loss is 0.3947448134422302\n",
      "epoch: 7 step: 1185, loss is 0.12392991781234741\n",
      "epoch: 7 step: 1186, loss is 0.3962801396846771\n",
      "epoch: 7 step: 1187, loss is 0.3292953670024872\n",
      "epoch: 7 step: 1188, loss is 0.38341009616851807\n",
      "epoch: 7 step: 1189, loss is 0.22331872582435608\n",
      "epoch: 7 step: 1190, loss is 0.48988693952560425\n",
      "epoch: 7 step: 1191, loss is 0.2336866408586502\n",
      "epoch: 7 step: 1192, loss is 0.17671827971935272\n",
      "epoch: 7 step: 1193, loss is 0.38961198925971985\n",
      "epoch: 7 step: 1194, loss is 0.06080915778875351\n",
      "epoch: 7 step: 1195, loss is 0.06054019182920456\n",
      "epoch: 7 step: 1196, loss is 0.27078256011009216\n",
      "epoch: 7 step: 1197, loss is 0.12313296645879745\n",
      "epoch: 7 step: 1198, loss is 0.42494848370552063\n",
      "epoch: 7 step: 1199, loss is 0.3671473562717438\n",
      "epoch: 7 step: 1200, loss is 0.09336069226264954\n",
      "epoch: 7 step: 1201, loss is 0.200658917427063\n",
      "epoch: 7 step: 1202, loss is 0.195068821310997\n",
      "epoch: 7 step: 1203, loss is 0.24771453440189362\n",
      "epoch: 7 step: 1204, loss is 0.1113324761390686\n",
      "epoch: 7 step: 1205, loss is 0.3093998432159424\n",
      "epoch: 7 step: 1206, loss is 0.11605054140090942\n",
      "epoch: 7 step: 1207, loss is 0.5449002385139465\n",
      "epoch: 7 step: 1208, loss is 0.18233922123908997\n",
      "epoch: 7 step: 1209, loss is 0.14193195104599\n",
      "epoch: 7 step: 1210, loss is 0.4071655571460724\n",
      "epoch: 7 step: 1211, loss is 0.21824701130390167\n",
      "epoch: 7 step: 1212, loss is 0.3642730414867401\n",
      "epoch: 7 step: 1213, loss is 0.2346215844154358\n",
      "epoch: 7 step: 1214, loss is 0.06775804609060287\n",
      "epoch: 7 step: 1215, loss is 0.11307229846715927\n",
      "epoch: 7 step: 1216, loss is 0.17436392605304718\n",
      "epoch: 7 step: 1217, loss is 0.5513229966163635\n",
      "epoch: 7 step: 1218, loss is 0.40626126527786255\n",
      "epoch: 7 step: 1219, loss is 0.00974024273455143\n",
      "epoch: 7 step: 1220, loss is 0.24125681817531586\n",
      "epoch: 7 step: 1221, loss is 0.37649235129356384\n",
      "epoch: 7 step: 1222, loss is 0.7995350956916809\n",
      "epoch: 7 step: 1223, loss is 0.12739504873752594\n",
      "epoch: 7 step: 1224, loss is 0.08147316426038742\n",
      "epoch: 7 step: 1225, loss is 0.23858888447284698\n",
      "epoch: 7 step: 1226, loss is 0.14252933859825134\n",
      "epoch: 7 step: 1227, loss is 0.3964633047580719\n",
      "epoch: 7 step: 1228, loss is 0.2677728235721588\n",
      "epoch: 7 step: 1229, loss is 0.37193557620048523\n",
      "epoch: 7 step: 1230, loss is 0.13602803647518158\n",
      "epoch: 7 step: 1231, loss is 1.0173249244689941\n",
      "epoch: 7 step: 1232, loss is 0.5586678981781006\n",
      "epoch: 7 step: 1233, loss is 0.11191976815462112\n",
      "epoch: 7 step: 1234, loss is 0.46175190806388855\n",
      "epoch: 7 step: 1235, loss is 0.4802386164665222\n",
      "epoch: 7 step: 1236, loss is 0.4376993775367737\n",
      "epoch: 7 step: 1237, loss is 0.32407692074775696\n",
      "epoch: 7 step: 1238, loss is 0.27172866463661194\n",
      "epoch: 7 step: 1239, loss is 0.42356735467910767\n",
      "epoch: 7 step: 1240, loss is 0.32107651233673096\n",
      "epoch: 7 step: 1241, loss is 0.2920551896095276\n",
      "epoch: 7 step: 1242, loss is 0.3036996126174927\n",
      "epoch: 7 step: 1243, loss is 0.33735334873199463\n",
      "epoch: 7 step: 1244, loss is 0.22566349804401398\n",
      "epoch: 7 step: 1245, loss is 0.11930003017187119\n",
      "epoch: 7 step: 1246, loss is 0.40892335772514343\n",
      "epoch: 7 step: 1247, loss is 0.5407123565673828\n",
      "epoch: 7 step: 1248, loss is 0.42443594336509705\n",
      "epoch: 7 step: 1249, loss is 0.1605709344148636\n",
      "epoch: 7 step: 1250, loss is 0.1500098705291748\n",
      "epoch: 7 step: 1251, loss is 0.05831398442387581\n",
      "epoch: 7 step: 1252, loss is 0.218026801943779\n",
      "epoch: 7 step: 1253, loss is 0.09739295393228531\n",
      "epoch: 7 step: 1254, loss is 0.2793472409248352\n",
      "epoch: 7 step: 1255, loss is 0.48464879393577576\n",
      "epoch: 7 step: 1256, loss is 0.3553599715232849\n",
      "epoch: 7 step: 1257, loss is 0.1814202517271042\n",
      "epoch: 7 step: 1258, loss is 0.2427757829427719\n",
      "epoch: 7 step: 1259, loss is 0.4282037615776062\n",
      "epoch: 7 step: 1260, loss is 0.4387255907058716\n",
      "epoch: 7 step: 1261, loss is 0.5220209360122681\n",
      "epoch: 7 step: 1262, loss is 0.20521125197410583\n",
      "epoch: 7 step: 1263, loss is 0.34203484654426575\n",
      "epoch: 7 step: 1264, loss is 0.30304011702537537\n",
      "epoch: 7 step: 1265, loss is 0.45058393478393555\n",
      "epoch: 7 step: 1266, loss is 0.08673018217086792\n",
      "epoch: 7 step: 1267, loss is 0.21274995803833008\n",
      "epoch: 7 step: 1268, loss is 0.4131186008453369\n",
      "epoch: 7 step: 1269, loss is 0.11874416470527649\n",
      "epoch: 7 step: 1270, loss is 0.4005553126335144\n",
      "epoch: 7 step: 1271, loss is 0.09015922248363495\n",
      "epoch: 7 step: 1272, loss is 0.2452095001935959\n",
      "epoch: 7 step: 1273, loss is 0.06700707972049713\n",
      "epoch: 7 step: 1274, loss is 0.1860397309064865\n",
      "epoch: 7 step: 1275, loss is 0.5498656630516052\n",
      "epoch: 7 step: 1276, loss is 0.13965892791748047\n",
      "epoch: 7 step: 1277, loss is 0.26525822281837463\n",
      "epoch: 7 step: 1278, loss is 0.40448546409606934\n",
      "epoch: 7 step: 1279, loss is 0.1347821056842804\n",
      "epoch: 7 step: 1280, loss is 0.13098032772541046\n",
      "epoch: 7 step: 1281, loss is 0.07343344390392303\n",
      "epoch: 7 step: 1282, loss is 0.2017809897661209\n",
      "epoch: 7 step: 1283, loss is 0.14765207469463348\n",
      "epoch: 7 step: 1284, loss is 0.0601535364985466\n",
      "epoch: 7 step: 1285, loss is 0.5519025921821594\n",
      "epoch: 7 step: 1286, loss is 0.08667806535959244\n",
      "epoch: 7 step: 1287, loss is 0.34433624148368835\n",
      "epoch: 7 step: 1288, loss is 0.18758660554885864\n",
      "epoch: 7 step: 1289, loss is 0.494913786649704\n",
      "epoch: 7 step: 1290, loss is 0.3253896236419678\n",
      "epoch: 7 step: 1291, loss is 0.4890102446079254\n",
      "epoch: 7 step: 1292, loss is 0.02714054472744465\n",
      "epoch: 7 step: 1293, loss is 0.6866599321365356\n",
      "epoch: 7 step: 1294, loss is 0.20283067226409912\n",
      "epoch: 7 step: 1295, loss is 0.4404633343219757\n",
      "epoch: 7 step: 1296, loss is 0.2971785366535187\n",
      "epoch: 7 step: 1297, loss is 0.03943603113293648\n",
      "epoch: 7 step: 1298, loss is 0.6146486401557922\n",
      "epoch: 7 step: 1299, loss is 0.18059709668159485\n",
      "epoch: 7 step: 1300, loss is 0.02823929488658905\n",
      "epoch: 7 step: 1301, loss is 0.3027629256248474\n",
      "epoch: 7 step: 1302, loss is 0.07021523267030716\n",
      "epoch: 7 step: 1303, loss is 0.31706857681274414\n",
      "epoch: 7 step: 1304, loss is 0.3543089032173157\n",
      "epoch: 7 step: 1305, loss is 0.19930127263069153\n",
      "epoch: 7 step: 1306, loss is 0.10054756700992584\n",
      "epoch: 7 step: 1307, loss is 0.07754123955965042\n",
      "epoch: 7 step: 1308, loss is 0.06975696980953217\n",
      "epoch: 7 step: 1309, loss is 0.23996096849441528\n",
      "epoch: 7 step: 1310, loss is 0.03864094987511635\n",
      "epoch: 7 step: 1311, loss is 0.11774251610040665\n",
      "epoch: 7 step: 1312, loss is 0.10707643628120422\n",
      "epoch: 7 step: 1313, loss is 0.3854113817214966\n",
      "epoch: 7 step: 1314, loss is 0.07115718722343445\n",
      "epoch: 7 step: 1315, loss is 0.243368998169899\n",
      "epoch: 7 step: 1316, loss is 0.19932711124420166\n",
      "epoch: 7 step: 1317, loss is 0.08626445382833481\n",
      "epoch: 7 step: 1318, loss is 0.5228224396705627\n",
      "epoch: 7 step: 1319, loss is 0.14832469820976257\n",
      "epoch: 7 step: 1320, loss is 0.12853951752185822\n",
      "epoch: 7 step: 1321, loss is 0.26716047525405884\n",
      "epoch: 7 step: 1322, loss is 0.06106562539935112\n",
      "epoch: 7 step: 1323, loss is 0.05212849751114845\n",
      "epoch: 7 step: 1324, loss is 0.4497111737728119\n",
      "epoch: 7 step: 1325, loss is 0.056868910789489746\n",
      "epoch: 7 step: 1326, loss is 0.29805874824523926\n",
      "epoch: 7 step: 1327, loss is 0.0359853133559227\n",
      "epoch: 7 step: 1328, loss is 0.0556061752140522\n",
      "epoch: 7 step: 1329, loss is 0.24494335055351257\n",
      "epoch: 7 step: 1330, loss is 0.2811034917831421\n",
      "epoch: 7 step: 1331, loss is 0.3477210998535156\n",
      "epoch: 7 step: 1332, loss is 0.35263463854789734\n",
      "epoch: 7 step: 1333, loss is 0.05506134405732155\n",
      "epoch: 7 step: 1334, loss is 0.0901685580611229\n",
      "epoch: 7 step: 1335, loss is 0.42019596695899963\n",
      "epoch: 7 step: 1336, loss is 0.013931657187640667\n",
      "epoch: 7 step: 1337, loss is 0.6635310649871826\n",
      "epoch: 7 step: 1338, loss is 0.05810033902525902\n",
      "epoch: 7 step: 1339, loss is 0.23839563131332397\n",
      "epoch: 7 step: 1340, loss is 0.2403632253408432\n",
      "epoch: 7 step: 1341, loss is 0.17133621871471405\n",
      "epoch: 7 step: 1342, loss is 0.16848622262477875\n",
      "epoch: 7 step: 1343, loss is 0.05711778625845909\n",
      "epoch: 7 step: 1344, loss is 0.11136648058891296\n",
      "epoch: 7 step: 1345, loss is 0.374437540769577\n",
      "epoch: 7 step: 1346, loss is 0.07940003275871277\n",
      "epoch: 7 step: 1347, loss is 0.2838512063026428\n",
      "epoch: 7 step: 1348, loss is 0.09191983938217163\n",
      "epoch: 7 step: 1349, loss is 0.5334013104438782\n",
      "epoch: 7 step: 1350, loss is 0.14487510919570923\n",
      "epoch: 7 step: 1351, loss is 0.33272165060043335\n",
      "epoch: 7 step: 1352, loss is 0.12309791147708893\n",
      "epoch: 7 step: 1353, loss is 0.20041529834270477\n",
      "epoch: 7 step: 1354, loss is 0.5453928112983704\n",
      "epoch: 7 step: 1355, loss is 0.360339879989624\n",
      "epoch: 7 step: 1356, loss is 0.18087249994277954\n",
      "epoch: 7 step: 1357, loss is 0.48777341842651367\n",
      "epoch: 7 step: 1358, loss is 0.2131386250257492\n",
      "epoch: 7 step: 1359, loss is 0.40326833724975586\n",
      "epoch: 7 step: 1360, loss is 0.8028603792190552\n",
      "epoch: 7 step: 1361, loss is 0.14064417779445648\n",
      "epoch: 7 step: 1362, loss is 0.19281631708145142\n",
      "epoch: 7 step: 1363, loss is 0.22947758436203003\n",
      "epoch: 7 step: 1364, loss is 0.16314445436000824\n",
      "epoch: 7 step: 1365, loss is 0.13044826686382294\n",
      "epoch: 7 step: 1366, loss is 0.24456800520420074\n",
      "epoch: 7 step: 1367, loss is 0.2659025192260742\n",
      "epoch: 7 step: 1368, loss is 0.4715835452079773\n",
      "epoch: 7 step: 1369, loss is 0.36540910601615906\n",
      "epoch: 7 step: 1370, loss is 0.036674611270427704\n",
      "epoch: 7 step: 1371, loss is 0.7679657340049744\n",
      "epoch: 7 step: 1372, loss is 0.34404584765434265\n",
      "epoch: 7 step: 1373, loss is 0.7345691323280334\n",
      "epoch: 7 step: 1374, loss is 0.43677735328674316\n",
      "epoch: 7 step: 1375, loss is 0.11757796257734299\n",
      "epoch: 7 step: 1376, loss is 0.4083327054977417\n",
      "epoch: 7 step: 1377, loss is 0.422850102186203\n",
      "epoch: 7 step: 1378, loss is 0.12193646281957626\n",
      "epoch: 7 step: 1379, loss is 0.16558538377285004\n",
      "epoch: 7 step: 1380, loss is 0.28123125433921814\n",
      "epoch: 7 step: 1381, loss is 0.42324814200401306\n",
      "epoch: 7 step: 1382, loss is 0.4467836320400238\n",
      "epoch: 7 step: 1383, loss is 0.03482821583747864\n",
      "epoch: 7 step: 1384, loss is 0.011148270219564438\n",
      "epoch: 7 step: 1385, loss is 0.218478262424469\n",
      "epoch: 7 step: 1386, loss is 0.4996547996997833\n",
      "epoch: 7 step: 1387, loss is 0.2248990386724472\n",
      "epoch: 7 step: 1388, loss is 0.2602469027042389\n",
      "epoch: 7 step: 1389, loss is 0.19538098573684692\n",
      "epoch: 7 step: 1390, loss is 0.4449390470981598\n",
      "epoch: 7 step: 1391, loss is 0.36137455701828003\n",
      "epoch: 7 step: 1392, loss is 0.6918916702270508\n",
      "epoch: 7 step: 1393, loss is 0.595721960067749\n",
      "epoch: 7 step: 1394, loss is 0.2429211437702179\n",
      "epoch: 7 step: 1395, loss is 0.16148915886878967\n",
      "epoch: 7 step: 1396, loss is 0.5219073295593262\n",
      "epoch: 7 step: 1397, loss is 1.0807596445083618\n",
      "epoch: 7 step: 1398, loss is 0.2616809010505676\n",
      "epoch: 7 step: 1399, loss is 0.44392696022987366\n",
      "epoch: 7 step: 1400, loss is 0.3353346586227417\n",
      "epoch: 7 step: 1401, loss is 1.0558006763458252\n",
      "epoch: 7 step: 1402, loss is 0.2201814353466034\n",
      "epoch: 7 step: 1403, loss is 0.5003771185874939\n",
      "epoch: 7 step: 1404, loss is 0.12293371558189392\n",
      "epoch: 7 step: 1405, loss is 0.24820390343666077\n",
      "epoch: 7 step: 1406, loss is 0.5825777053833008\n",
      "epoch: 7 step: 1407, loss is 0.1856529414653778\n",
      "epoch: 7 step: 1408, loss is 0.3688949942588806\n",
      "epoch: 7 step: 1409, loss is 0.1881970316171646\n",
      "epoch: 7 step: 1410, loss is 0.5530717372894287\n",
      "epoch: 7 step: 1411, loss is 0.24631845951080322\n",
      "epoch: 7 step: 1412, loss is 0.18107639253139496\n",
      "epoch: 7 step: 1413, loss is 0.27695488929748535\n",
      "epoch: 7 step: 1414, loss is 0.46457815170288086\n",
      "epoch: 7 step: 1415, loss is 0.3953045606613159\n",
      "epoch: 7 step: 1416, loss is 0.44105440378189087\n",
      "epoch: 7 step: 1417, loss is 0.49147891998291016\n",
      "epoch: 7 step: 1418, loss is 0.3971059322357178\n",
      "epoch: 7 step: 1419, loss is 0.2684011459350586\n",
      "epoch: 7 step: 1420, loss is 0.09535777568817139\n",
      "epoch: 7 step: 1421, loss is 0.42470166087150574\n",
      "epoch: 7 step: 1422, loss is 0.3336141109466553\n",
      "epoch: 7 step: 1423, loss is 0.26709479093551636\n",
      "epoch: 7 step: 1424, loss is 0.16162604093551636\n",
      "epoch: 7 step: 1425, loss is 0.48965173959732056\n",
      "epoch: 7 step: 1426, loss is 0.3856881260871887\n",
      "epoch: 7 step: 1427, loss is 0.33186689019203186\n",
      "epoch: 7 step: 1428, loss is 0.2536482512950897\n",
      "epoch: 7 step: 1429, loss is 0.0620487704873085\n",
      "epoch: 7 step: 1430, loss is 0.3915364742279053\n",
      "epoch: 7 step: 1431, loss is 0.17505668103694916\n",
      "epoch: 7 step: 1432, loss is 0.12601712346076965\n",
      "epoch: 7 step: 1433, loss is 0.47494593262672424\n",
      "epoch: 7 step: 1434, loss is 0.5195398330688477\n",
      "epoch: 7 step: 1435, loss is 0.5503283143043518\n",
      "epoch: 7 step: 1436, loss is 0.3477998375892639\n",
      "epoch: 7 step: 1437, loss is 0.25558748841285706\n",
      "epoch: 7 step: 1438, loss is 0.3444107472896576\n",
      "epoch: 7 step: 1439, loss is 0.19459268450737\n",
      "epoch: 7 step: 1440, loss is 0.21614494919776917\n",
      "epoch: 7 step: 1441, loss is 0.0727805644273758\n",
      "epoch: 7 step: 1442, loss is 0.33252203464508057\n",
      "epoch: 7 step: 1443, loss is 0.12235774099826813\n",
      "epoch: 7 step: 1444, loss is 0.12304788082838058\n",
      "epoch: 7 step: 1445, loss is 0.22500456869602203\n",
      "epoch: 7 step: 1446, loss is 0.25736239552497864\n",
      "epoch: 7 step: 1447, loss is 0.08000493794679642\n",
      "epoch: 7 step: 1448, loss is 0.36490052938461304\n",
      "epoch: 7 step: 1449, loss is 0.2672206163406372\n",
      "epoch: 7 step: 1450, loss is 0.5040504336357117\n",
      "epoch: 7 step: 1451, loss is 0.22881683707237244\n",
      "epoch: 7 step: 1452, loss is 0.28218165040016174\n",
      "epoch: 7 step: 1453, loss is 0.7040546536445618\n",
      "epoch: 7 step: 1454, loss is 0.3374892473220825\n",
      "epoch: 7 step: 1455, loss is 0.6493241190910339\n",
      "epoch: 7 step: 1456, loss is 0.12535053491592407\n",
      "epoch: 7 step: 1457, loss is 0.19482089579105377\n",
      "epoch: 7 step: 1458, loss is 0.23104654252529144\n",
      "epoch: 7 step: 1459, loss is 0.08745060116052628\n",
      "epoch: 7 step: 1460, loss is 0.233390673995018\n",
      "epoch: 7 step: 1461, loss is 0.1566578596830368\n",
      "epoch: 7 step: 1462, loss is 0.10544557869434357\n",
      "epoch: 7 step: 1463, loss is 0.39210188388824463\n",
      "epoch: 7 step: 1464, loss is 0.0861891359090805\n",
      "epoch: 7 step: 1465, loss is 0.5545857548713684\n",
      "epoch: 7 step: 1466, loss is 0.3639581501483917\n",
      "epoch: 7 step: 1467, loss is 0.13160894811153412\n",
      "epoch: 7 step: 1468, loss is 0.19407469034194946\n",
      "epoch: 7 step: 1469, loss is 0.2734939754009247\n",
      "epoch: 7 step: 1470, loss is 0.16757923364639282\n",
      "epoch: 7 step: 1471, loss is 0.5152615904808044\n",
      "epoch: 7 step: 1472, loss is 0.06317798048257828\n",
      "epoch: 7 step: 1473, loss is 0.4647566080093384\n",
      "epoch: 7 step: 1474, loss is 0.11493375897407532\n",
      "epoch: 7 step: 1475, loss is 0.34508946537971497\n",
      "epoch: 7 step: 1476, loss is 0.3839706480503082\n",
      "epoch: 7 step: 1477, loss is 0.46440377831459045\n",
      "epoch: 7 step: 1478, loss is 0.22733688354492188\n",
      "epoch: 7 step: 1479, loss is 0.0607912540435791\n",
      "epoch: 7 step: 1480, loss is 0.3762185573577881\n",
      "epoch: 7 step: 1481, loss is 0.13920727372169495\n",
      "epoch: 7 step: 1482, loss is 0.11284401267766953\n",
      "epoch: 7 step: 1483, loss is 0.05554034933447838\n",
      "epoch: 7 step: 1484, loss is 0.08330266177654266\n",
      "epoch: 7 step: 1485, loss is 0.26983609795570374\n",
      "epoch: 7 step: 1486, loss is 0.4572507441043854\n",
      "epoch: 7 step: 1487, loss is 0.11621354520320892\n",
      "epoch: 7 step: 1488, loss is 0.15075969696044922\n",
      "epoch: 7 step: 1489, loss is 0.27037879824638367\n",
      "epoch: 7 step: 1490, loss is 0.2111998200416565\n",
      "epoch: 7 step: 1491, loss is 0.797144889831543\n",
      "epoch: 7 step: 1492, loss is 0.16864639520645142\n",
      "epoch: 7 step: 1493, loss is 0.04136184975504875\n",
      "epoch: 7 step: 1494, loss is 0.16531303524971008\n",
      "epoch: 7 step: 1495, loss is 0.14282001554965973\n",
      "epoch: 7 step: 1496, loss is 0.0467100627720356\n",
      "epoch: 7 step: 1497, loss is 0.3283747434616089\n",
      "epoch: 7 step: 1498, loss is 0.292312353849411\n",
      "epoch: 7 step: 1499, loss is 0.83463454246521\n",
      "epoch: 7 step: 1500, loss is 0.20582038164138794\n",
      "epoch: 7 step: 1501, loss is 0.060771916061639786\n",
      "epoch: 7 step: 1502, loss is 0.24064108729362488\n",
      "epoch: 7 step: 1503, loss is 0.1271955519914627\n",
      "epoch: 7 step: 1504, loss is 0.29454371333122253\n",
      "epoch: 7 step: 1505, loss is 0.6914357542991638\n",
      "epoch: 7 step: 1506, loss is 0.5575501322746277\n",
      "epoch: 7 step: 1507, loss is 0.8254740238189697\n",
      "epoch: 7 step: 1508, loss is 0.30164584517478943\n",
      "epoch: 7 step: 1509, loss is 0.4788985550403595\n",
      "epoch: 7 step: 1510, loss is 0.4125607907772064\n",
      "epoch: 7 step: 1511, loss is 0.35994359850883484\n",
      "epoch: 7 step: 1512, loss is 0.5891412496566772\n",
      "epoch: 7 step: 1513, loss is 0.1919388324022293\n",
      "epoch: 7 step: 1514, loss is 0.06694204360246658\n",
      "epoch: 7 step: 1515, loss is 0.12918329238891602\n",
      "epoch: 7 step: 1516, loss is 0.33494138717651367\n",
      "epoch: 7 step: 1517, loss is 0.2786981761455536\n",
      "epoch: 7 step: 1518, loss is 0.5006867051124573\n",
      "epoch: 7 step: 1519, loss is 0.7968350052833557\n",
      "epoch: 7 step: 1520, loss is 0.2691471576690674\n",
      "epoch: 7 step: 1521, loss is 0.19781306385993958\n",
      "epoch: 7 step: 1522, loss is 0.29411959648132324\n",
      "epoch: 7 step: 1523, loss is 0.14832471311092377\n",
      "epoch: 7 step: 1524, loss is 0.17369681596755981\n",
      "epoch: 7 step: 1525, loss is 0.1929238885641098\n",
      "epoch: 7 step: 1526, loss is 0.15020598471164703\n",
      "epoch: 7 step: 1527, loss is 0.4024224877357483\n",
      "epoch: 7 step: 1528, loss is 0.3345564007759094\n",
      "epoch: 7 step: 1529, loss is 0.29690101742744446\n",
      "epoch: 7 step: 1530, loss is 0.21651385724544525\n",
      "epoch: 7 step: 1531, loss is 0.2921016216278076\n",
      "epoch: 7 step: 1532, loss is 0.31850674748420715\n",
      "epoch: 7 step: 1533, loss is 0.18223148584365845\n",
      "epoch: 7 step: 1534, loss is 0.30327939987182617\n",
      "epoch: 7 step: 1535, loss is 0.07667608559131622\n",
      "epoch: 7 step: 1536, loss is 0.03895282372832298\n",
      "epoch: 7 step: 1537, loss is 0.2584247887134552\n",
      "epoch: 7 step: 1538, loss is 0.3601681888103485\n",
      "epoch: 7 step: 1539, loss is 0.3011731505393982\n",
      "epoch: 7 step: 1540, loss is 0.193597674369812\n",
      "epoch: 7 step: 1541, loss is 0.3785558044910431\n",
      "epoch: 7 step: 1542, loss is 0.5038204193115234\n",
      "epoch: 7 step: 1543, loss is 0.16616174578666687\n",
      "epoch: 7 step: 1544, loss is 0.15847273170948029\n",
      "epoch: 7 step: 1545, loss is 0.3643971085548401\n",
      "epoch: 7 step: 1546, loss is 0.4051559567451477\n",
      "epoch: 7 step: 1547, loss is 0.25671252608299255\n",
      "epoch: 7 step: 1548, loss is 0.06921592354774475\n",
      "epoch: 7 step: 1549, loss is 0.20022651553153992\n",
      "epoch: 7 step: 1550, loss is 0.5220075249671936\n",
      "epoch: 7 step: 1551, loss is 0.5214399099349976\n",
      "epoch: 7 step: 1552, loss is 0.044317375868558884\n",
      "epoch: 7 step: 1553, loss is 0.20417560636997223\n",
      "epoch: 7 step: 1554, loss is 0.4661113917827606\n",
      "epoch: 7 step: 1555, loss is 0.7121166586875916\n",
      "epoch: 7 step: 1556, loss is 0.20075169205665588\n",
      "epoch: 7 step: 1557, loss is 0.3750668168067932\n",
      "epoch: 7 step: 1558, loss is 0.10946518927812576\n",
      "epoch: 7 step: 1559, loss is 0.47075578570365906\n",
      "epoch: 7 step: 1560, loss is 0.2693985104560852\n",
      "epoch: 7 step: 1561, loss is 0.17011816799640656\n",
      "epoch: 7 step: 1562, loss is 0.24975454807281494\n",
      "epoch: 7 step: 1563, loss is 0.20126794278621674\n",
      "epoch: 7 step: 1564, loss is 0.2957570254802704\n",
      "epoch: 7 step: 1565, loss is 0.1483355015516281\n",
      "epoch: 7 step: 1566, loss is 0.07442173361778259\n",
      "epoch: 7 step: 1567, loss is 0.302646666765213\n",
      "epoch: 7 step: 1568, loss is 0.1638234555721283\n",
      "epoch: 7 step: 1569, loss is 0.12306869029998779\n",
      "epoch: 7 step: 1570, loss is 0.26989227533340454\n",
      "epoch: 7 step: 1571, loss is 0.15585389733314514\n",
      "epoch: 7 step: 1572, loss is 0.5342756509780884\n",
      "epoch: 7 step: 1573, loss is 0.16826562583446503\n",
      "epoch: 7 step: 1574, loss is 0.2846722900867462\n",
      "epoch: 7 step: 1575, loss is 0.1833944171667099\n",
      "epoch: 7 step: 1576, loss is 0.25963491201400757\n",
      "epoch: 7 step: 1577, loss is 0.23149201273918152\n",
      "epoch: 7 step: 1578, loss is 0.18622298538684845\n",
      "epoch: 7 step: 1579, loss is 0.4021274149417877\n",
      "epoch: 7 step: 1580, loss is 0.41398340463638306\n",
      "epoch: 7 step: 1581, loss is 0.230088010430336\n",
      "epoch: 7 step: 1582, loss is 0.34483957290649414\n",
      "epoch: 7 step: 1583, loss is 0.2824755907058716\n",
      "epoch: 7 step: 1584, loss is 0.2695937752723694\n",
      "epoch: 7 step: 1585, loss is 0.316497266292572\n",
      "epoch: 7 step: 1586, loss is 0.5678271651268005\n",
      "epoch: 7 step: 1587, loss is 0.20140181481838226\n",
      "epoch: 7 step: 1588, loss is 0.40590235590934753\n",
      "epoch: 7 step: 1589, loss is 0.28569498658180237\n",
      "epoch: 7 step: 1590, loss is 0.33789917826652527\n",
      "epoch: 7 step: 1591, loss is 0.6148850321769714\n",
      "epoch: 7 step: 1592, loss is 0.05614229291677475\n",
      "epoch: 7 step: 1593, loss is 0.024004928767681122\n",
      "epoch: 7 step: 1594, loss is 0.04977542906999588\n",
      "epoch: 7 step: 1595, loss is 0.5568336248397827\n",
      "epoch: 7 step: 1596, loss is 0.5248244404792786\n",
      "epoch: 7 step: 1597, loss is 0.23716433346271515\n",
      "epoch: 7 step: 1598, loss is 0.4757058024406433\n",
      "epoch: 7 step: 1599, loss is 0.34733399748802185\n",
      "epoch: 7 step: 1600, loss is 0.0652814507484436\n",
      "epoch: 7 step: 1601, loss is 0.421294629573822\n",
      "epoch: 7 step: 1602, loss is 0.4625607132911682\n",
      "epoch: 7 step: 1603, loss is 0.2891061305999756\n",
      "epoch: 7 step: 1604, loss is 0.4587494432926178\n",
      "epoch: 7 step: 1605, loss is 0.496150940656662\n",
      "epoch: 7 step: 1606, loss is 0.17457641661167145\n",
      "epoch: 7 step: 1607, loss is 0.030746322125196457\n",
      "epoch: 7 step: 1608, loss is 0.6276659369468689\n",
      "epoch: 7 step: 1609, loss is 0.3068045377731323\n",
      "epoch: 7 step: 1610, loss is 0.0842474177479744\n",
      "epoch: 7 step: 1611, loss is 0.18566305935382843\n",
      "epoch: 7 step: 1612, loss is 0.13871033489704132\n",
      "epoch: 7 step: 1613, loss is 0.23652109503746033\n",
      "epoch: 7 step: 1614, loss is 0.49946069717407227\n",
      "epoch: 7 step: 1615, loss is 0.5628595948219299\n",
      "epoch: 7 step: 1616, loss is 1.028470516204834\n",
      "epoch: 7 step: 1617, loss is 0.1758975088596344\n",
      "epoch: 7 step: 1618, loss is 0.2418614625930786\n",
      "epoch: 7 step: 1619, loss is 0.3163413107395172\n",
      "epoch: 7 step: 1620, loss is 0.7046172618865967\n",
      "epoch: 7 step: 1621, loss is 0.16391614079475403\n",
      "epoch: 7 step: 1622, loss is 0.22330768406391144\n",
      "epoch: 7 step: 1623, loss is 0.0792497843503952\n",
      "epoch: 7 step: 1624, loss is 0.66596919298172\n",
      "epoch: 7 step: 1625, loss is 0.6031945943832397\n",
      "epoch: 7 step: 1626, loss is 0.34257015585899353\n",
      "epoch: 7 step: 1627, loss is 0.14382943511009216\n",
      "epoch: 7 step: 1628, loss is 0.3003312647342682\n",
      "epoch: 7 step: 1629, loss is 0.19921503961086273\n",
      "epoch: 7 step: 1630, loss is 0.08415503799915314\n",
      "epoch: 7 step: 1631, loss is 0.4664744436740875\n",
      "epoch: 7 step: 1632, loss is 0.15095695853233337\n",
      "epoch: 7 step: 1633, loss is 0.44201675057411194\n",
      "epoch: 7 step: 1634, loss is 0.28722625970840454\n",
      "epoch: 7 step: 1635, loss is 0.1009572222828865\n",
      "epoch: 7 step: 1636, loss is 0.1172974705696106\n",
      "epoch: 7 step: 1637, loss is 0.13314852118492126\n",
      "epoch: 7 step: 1638, loss is 0.2254653126001358\n",
      "epoch: 7 step: 1639, loss is 0.22235102951526642\n",
      "epoch: 7 step: 1640, loss is 0.5421425104141235\n",
      "epoch: 7 step: 1641, loss is 0.14421787858009338\n",
      "epoch: 7 step: 1642, loss is 0.12973760068416595\n",
      "epoch: 7 step: 1643, loss is 0.152440145611763\n",
      "epoch: 7 step: 1644, loss is 0.32173189520835876\n",
      "epoch: 7 step: 1645, loss is 0.27812692523002625\n",
      "epoch: 7 step: 1646, loss is 0.6173080205917358\n",
      "epoch: 7 step: 1647, loss is 0.2831025719642639\n",
      "epoch: 7 step: 1648, loss is 0.1478402316570282\n",
      "epoch: 7 step: 1649, loss is 0.1971791535615921\n",
      "epoch: 7 step: 1650, loss is 0.4460722506046295\n",
      "epoch: 7 step: 1651, loss is 0.13125228881835938\n",
      "epoch: 7 step: 1652, loss is 0.42814549803733826\n",
      "epoch: 7 step: 1653, loss is 0.207696795463562\n",
      "epoch: 7 step: 1654, loss is 0.06144809350371361\n",
      "epoch: 7 step: 1655, loss is 0.15010392665863037\n",
      "epoch: 7 step: 1656, loss is 0.13482071459293365\n",
      "epoch: 7 step: 1657, loss is 0.05170081928372383\n",
      "epoch: 7 step: 1658, loss is 0.3002046048641205\n",
      "epoch: 7 step: 1659, loss is 0.07443126291036606\n",
      "epoch: 7 step: 1660, loss is 0.3101668655872345\n",
      "epoch: 7 step: 1661, loss is 0.19799409806728363\n",
      "epoch: 7 step: 1662, loss is 0.07279100269079208\n",
      "epoch: 7 step: 1663, loss is 0.19878774881362915\n",
      "epoch: 7 step: 1664, loss is 0.08821973949670792\n",
      "epoch: 7 step: 1665, loss is 0.3820604085922241\n",
      "epoch: 7 step: 1666, loss is 0.1508292853832245\n",
      "epoch: 7 step: 1667, loss is 0.1449701339006424\n",
      "epoch: 7 step: 1668, loss is 0.16429083049297333\n",
      "epoch: 7 step: 1669, loss is 0.19074353575706482\n",
      "epoch: 7 step: 1670, loss is 0.33817118406295776\n",
      "epoch: 7 step: 1671, loss is 0.08305756002664566\n",
      "epoch: 7 step: 1672, loss is 0.47427695989608765\n",
      "epoch: 7 step: 1673, loss is 0.3899115025997162\n",
      "epoch: 7 step: 1674, loss is 0.23356157541275024\n",
      "epoch: 7 step: 1675, loss is 0.14629527926445007\n",
      "epoch: 7 step: 1676, loss is 0.8845382332801819\n",
      "epoch: 7 step: 1677, loss is 0.2240980863571167\n",
      "epoch: 7 step: 1678, loss is 0.1868465691804886\n",
      "epoch: 7 step: 1679, loss is 0.4723235070705414\n",
      "epoch: 7 step: 1680, loss is 0.10532975941896439\n",
      "epoch: 7 step: 1681, loss is 0.09829936176538467\n",
      "epoch: 7 step: 1682, loss is 0.23715855181217194\n",
      "epoch: 7 step: 1683, loss is 0.8471293449401855\n",
      "epoch: 7 step: 1684, loss is 0.3288653790950775\n",
      "epoch: 7 step: 1685, loss is 0.19835726916790009\n",
      "epoch: 7 step: 1686, loss is 0.4141799807548523\n",
      "epoch: 7 step: 1687, loss is 0.2802965044975281\n",
      "epoch: 7 step: 1688, loss is 0.2537420392036438\n",
      "epoch: 7 step: 1689, loss is 0.18514348566532135\n",
      "epoch: 7 step: 1690, loss is 0.3574707806110382\n",
      "epoch: 7 step: 1691, loss is 0.2181926667690277\n",
      "epoch: 7 step: 1692, loss is 0.43840697407722473\n",
      "epoch: 7 step: 1693, loss is 0.501471996307373\n",
      "epoch: 7 step: 1694, loss is 0.14854104816913605\n",
      "epoch: 7 step: 1695, loss is 0.3060026466846466\n",
      "epoch: 7 step: 1696, loss is 0.7415258884429932\n",
      "epoch: 7 step: 1697, loss is 0.083298459649086\n",
      "epoch: 7 step: 1698, loss is 0.34004732966423035\n",
      "epoch: 7 step: 1699, loss is 0.18183928728103638\n",
      "epoch: 7 step: 1700, loss is 0.3993590176105499\n",
      "epoch: 7 step: 1701, loss is 0.2741624414920807\n",
      "epoch: 7 step: 1702, loss is 0.2736285626888275\n",
      "epoch: 7 step: 1703, loss is 0.10822833329439163\n",
      "epoch: 7 step: 1704, loss is 0.23833134770393372\n",
      "epoch: 7 step: 1705, loss is 0.4527507424354553\n",
      "epoch: 7 step: 1706, loss is 0.1135961040854454\n",
      "epoch: 7 step: 1707, loss is 0.30730411410331726\n",
      "epoch: 7 step: 1708, loss is 0.05294905602931976\n",
      "epoch: 7 step: 1709, loss is 0.4038163423538208\n",
      "epoch: 7 step: 1710, loss is 0.2683042287826538\n",
      "epoch: 7 step: 1711, loss is 0.6795721054077148\n",
      "epoch: 7 step: 1712, loss is 0.5455068349838257\n",
      "epoch: 7 step: 1713, loss is 0.25392505526542664\n",
      "epoch: 7 step: 1714, loss is 0.1832069456577301\n",
      "epoch: 7 step: 1715, loss is 0.18906767666339874\n",
      "epoch: 7 step: 1716, loss is 0.3460780084133148\n",
      "epoch: 7 step: 1717, loss is 0.40199604630470276\n",
      "epoch: 7 step: 1718, loss is 0.2019934505224228\n",
      "epoch: 7 step: 1719, loss is 0.02458173967897892\n",
      "epoch: 7 step: 1720, loss is 0.2863479256629944\n",
      "epoch: 7 step: 1721, loss is 0.11504185944795609\n",
      "epoch: 7 step: 1722, loss is 0.31398770213127136\n",
      "epoch: 7 step: 1723, loss is 0.7655692100524902\n",
      "epoch: 7 step: 1724, loss is 0.1331898272037506\n",
      "epoch: 7 step: 1725, loss is 0.18578468263149261\n",
      "epoch: 7 step: 1726, loss is 0.5612521171569824\n",
      "epoch: 7 step: 1727, loss is 0.30021193623542786\n",
      "epoch: 7 step: 1728, loss is 0.23400656878948212\n",
      "epoch: 7 step: 1729, loss is 0.0842033177614212\n",
      "epoch: 7 step: 1730, loss is 0.3931255042552948\n",
      "epoch: 7 step: 1731, loss is 0.13754256069660187\n",
      "epoch: 7 step: 1732, loss is 0.3162885308265686\n",
      "epoch: 7 step: 1733, loss is 0.08763609081506729\n",
      "epoch: 7 step: 1734, loss is 0.0715557113289833\n",
      "epoch: 7 step: 1735, loss is 0.400873064994812\n",
      "epoch: 7 step: 1736, loss is 0.09540364891290665\n",
      "epoch: 7 step: 1737, loss is 0.42888715863227844\n",
      "epoch: 7 step: 1738, loss is 0.4635922908782959\n",
      "epoch: 7 step: 1739, loss is 0.35472941398620605\n",
      "epoch: 7 step: 1740, loss is 0.5344545841217041\n",
      "epoch: 7 step: 1741, loss is 0.8159619569778442\n",
      "epoch: 7 step: 1742, loss is 0.621936023235321\n",
      "epoch: 7 step: 1743, loss is 0.739097535610199\n",
      "epoch: 7 step: 1744, loss is 0.20132149755954742\n",
      "epoch: 7 step: 1745, loss is 1.0306668281555176\n",
      "epoch: 7 step: 1746, loss is 0.8872583508491516\n",
      "epoch: 7 step: 1747, loss is 0.44552603363990784\n",
      "epoch: 7 step: 1748, loss is 0.3775886297225952\n",
      "epoch: 7 step: 1749, loss is 0.3910401463508606\n",
      "epoch: 7 step: 1750, loss is 0.15398313105106354\n",
      "epoch: 7 step: 1751, loss is 0.1638263612985611\n",
      "epoch: 7 step: 1752, loss is 0.1932828277349472\n",
      "epoch: 7 step: 1753, loss is 0.2471451759338379\n",
      "epoch: 7 step: 1754, loss is 0.5474692583084106\n",
      "epoch: 7 step: 1755, loss is 0.4081888496875763\n",
      "epoch: 7 step: 1756, loss is 0.20621967315673828\n",
      "epoch: 7 step: 1757, loss is 0.17046460509300232\n",
      "epoch: 7 step: 1758, loss is 0.2144632786512375\n",
      "epoch: 7 step: 1759, loss is 0.38380640745162964\n",
      "epoch: 7 step: 1760, loss is 0.5322505831718445\n",
      "epoch: 7 step: 1761, loss is 0.4151531159877777\n",
      "epoch: 7 step: 1762, loss is 0.2670324742794037\n",
      "epoch: 7 step: 1763, loss is 0.13475817441940308\n",
      "epoch: 7 step: 1764, loss is 0.3020159602165222\n",
      "epoch: 7 step: 1765, loss is 0.03926006332039833\n",
      "epoch: 7 step: 1766, loss is 0.4814488887786865\n",
      "epoch: 7 step: 1767, loss is 0.5041351318359375\n",
      "epoch: 7 step: 1768, loss is 0.34374329447746277\n",
      "epoch: 7 step: 1769, loss is 0.3300881087779999\n",
      "epoch: 7 step: 1770, loss is 0.11297278851270676\n",
      "epoch: 7 step: 1771, loss is 0.38056373596191406\n",
      "epoch: 7 step: 1772, loss is 0.11008410900831223\n",
      "epoch: 7 step: 1773, loss is 0.18435220420360565\n",
      "epoch: 7 step: 1774, loss is 0.1745724380016327\n",
      "epoch: 7 step: 1775, loss is 0.23871095478534698\n",
      "epoch: 7 step: 1776, loss is 0.30749213695526123\n",
      "epoch: 7 step: 1777, loss is 0.08855701982975006\n",
      "epoch: 7 step: 1778, loss is 0.2282561957836151\n",
      "epoch: 7 step: 1779, loss is 0.1660127192735672\n",
      "epoch: 7 step: 1780, loss is 0.07392647862434387\n",
      "epoch: 7 step: 1781, loss is 0.37136322259902954\n",
      "epoch: 7 step: 1782, loss is 0.1276862621307373\n",
      "epoch: 7 step: 1783, loss is 0.3104257583618164\n",
      "epoch: 7 step: 1784, loss is 0.37380585074424744\n",
      "epoch: 7 step: 1785, loss is 0.1832781732082367\n",
      "epoch: 7 step: 1786, loss is 0.37262600660324097\n",
      "epoch: 7 step: 1787, loss is 0.5372598171234131\n",
      "epoch: 7 step: 1788, loss is 0.18788689374923706\n",
      "epoch: 7 step: 1789, loss is 0.12149889767169952\n",
      "epoch: 7 step: 1790, loss is 0.3657054305076599\n",
      "epoch: 7 step: 1791, loss is 0.36328017711639404\n",
      "epoch: 7 step: 1792, loss is 0.30122387409210205\n",
      "epoch: 7 step: 1793, loss is 0.47047239542007446\n",
      "epoch: 7 step: 1794, loss is 0.140141561627388\n",
      "epoch: 7 step: 1795, loss is 0.6557267904281616\n",
      "epoch: 7 step: 1796, loss is 0.36900293827056885\n",
      "epoch: 7 step: 1797, loss is 0.04995105415582657\n",
      "epoch: 7 step: 1798, loss is 0.16446228325366974\n",
      "epoch: 7 step: 1799, loss is 0.309084415435791\n",
      "epoch: 7 step: 1800, loss is 0.3721160888671875\n",
      "epoch: 7 step: 1801, loss is 0.14309053122997284\n",
      "epoch: 7 step: 1802, loss is 0.4447181224822998\n",
      "epoch: 7 step: 1803, loss is 0.32683125138282776\n",
      "epoch: 7 step: 1804, loss is 0.2652716636657715\n",
      "epoch: 7 step: 1805, loss is 0.19351308047771454\n",
      "epoch: 7 step: 1806, loss is 0.3973501920700073\n",
      "epoch: 7 step: 1807, loss is 0.5340076088905334\n",
      "epoch: 7 step: 1808, loss is 0.23057596385478973\n",
      "epoch: 7 step: 1809, loss is 0.1722983419895172\n",
      "epoch: 7 step: 1810, loss is 0.31438323855400085\n",
      "epoch: 7 step: 1811, loss is 0.19389432668685913\n",
      "epoch: 7 step: 1812, loss is 0.3012029230594635\n",
      "epoch: 7 step: 1813, loss is 0.24276207387447357\n",
      "epoch: 7 step: 1814, loss is 0.5326668620109558\n",
      "epoch: 7 step: 1815, loss is 0.26206013560295105\n",
      "epoch: 7 step: 1816, loss is 0.47012531757354736\n",
      "epoch: 7 step: 1817, loss is 0.12306676059961319\n",
      "epoch: 7 step: 1818, loss is 0.623508095741272\n",
      "epoch: 7 step: 1819, loss is 0.20458368957042694\n",
      "epoch: 7 step: 1820, loss is 0.041359297931194305\n",
      "epoch: 7 step: 1821, loss is 0.1716868281364441\n",
      "epoch: 7 step: 1822, loss is 0.18139323592185974\n",
      "epoch: 7 step: 1823, loss is 0.2636573910713196\n",
      "epoch: 7 step: 1824, loss is 0.4447344243526459\n",
      "epoch: 7 step: 1825, loss is 0.11080201715230942\n",
      "epoch: 7 step: 1826, loss is 0.35831594467163086\n",
      "epoch: 7 step: 1827, loss is 0.4266532063484192\n",
      "epoch: 7 step: 1828, loss is 0.32875609397888184\n",
      "epoch: 7 step: 1829, loss is 0.1480700522661209\n",
      "epoch: 7 step: 1830, loss is 0.13108360767364502\n",
      "epoch: 7 step: 1831, loss is 0.10065057128667831\n",
      "epoch: 7 step: 1832, loss is 0.41336655616760254\n",
      "epoch: 7 step: 1833, loss is 0.8077212572097778\n",
      "epoch: 7 step: 1834, loss is 0.08907745778560638\n",
      "epoch: 7 step: 1835, loss is 0.3576434552669525\n",
      "epoch: 7 step: 1836, loss is 0.06377075612545013\n",
      "epoch: 7 step: 1837, loss is 0.18306639790534973\n",
      "epoch: 7 step: 1838, loss is 0.5272495746612549\n",
      "epoch: 7 step: 1839, loss is 0.24173139035701752\n",
      "epoch: 7 step: 1840, loss is 0.1570938229560852\n",
      "epoch: 7 step: 1841, loss is 0.11125435680150986\n",
      "epoch: 7 step: 1842, loss is 0.03967757150530815\n",
      "epoch: 7 step: 1843, loss is 0.4376542568206787\n",
      "epoch: 7 step: 1844, loss is 0.3340238332748413\n",
      "epoch: 7 step: 1845, loss is 0.3545774221420288\n",
      "epoch: 7 step: 1846, loss is 0.19383254647254944\n",
      "epoch: 7 step: 1847, loss is 0.07688415050506592\n",
      "epoch: 7 step: 1848, loss is 0.265424907207489\n",
      "epoch: 7 step: 1849, loss is 0.526943564414978\n",
      "epoch: 7 step: 1850, loss is 0.22871072590351105\n",
      "epoch: 7 step: 1851, loss is 0.3434995114803314\n",
      "epoch: 7 step: 1852, loss is 0.7123773097991943\n",
      "epoch: 7 step: 1853, loss is 0.33811330795288086\n",
      "epoch: 7 step: 1854, loss is 0.030018530786037445\n",
      "epoch: 7 step: 1855, loss is 0.35756340622901917\n",
      "epoch: 7 step: 1856, loss is 0.5069849491119385\n",
      "epoch: 7 step: 1857, loss is 0.2687024474143982\n",
      "epoch: 7 step: 1858, loss is 0.4704445004463196\n",
      "epoch: 7 step: 1859, loss is 0.28345781564712524\n",
      "epoch: 7 step: 1860, loss is 0.34911343455314636\n",
      "epoch: 7 step: 1861, loss is 0.606004536151886\n",
      "epoch: 7 step: 1862, loss is 0.058637142181396484\n",
      "epoch: 7 step: 1863, loss is 0.44319817423820496\n",
      "epoch: 7 step: 1864, loss is 0.4133242666721344\n",
      "epoch: 7 step: 1865, loss is 0.33516600728034973\n",
      "epoch: 7 step: 1866, loss is 0.09880711883306503\n",
      "epoch: 7 step: 1867, loss is 0.05120085924863815\n",
      "epoch: 7 step: 1868, loss is 0.16615329682826996\n",
      "epoch: 7 step: 1869, loss is 0.37739983201026917\n",
      "epoch: 7 step: 1870, loss is 0.33668163418769836\n",
      "epoch: 7 step: 1871, loss is 0.15505613386631012\n",
      "epoch: 7 step: 1872, loss is 0.2839551866054535\n",
      "epoch: 7 step: 1873, loss is 0.3469611704349518\n",
      "epoch: 7 step: 1874, loss is 0.22048541903495789\n",
      "epoch: 7 step: 1875, loss is 0.34118467569351196\n",
      "epoch: 7 step: 1876, loss is 0.1807088851928711\n",
      "epoch: 7 step: 1877, loss is 0.4246584475040436\n",
      "epoch: 7 step: 1878, loss is 0.23106421530246735\n",
      "epoch: 7 step: 1879, loss is 0.04304397106170654\n",
      "epoch: 7 step: 1880, loss is 0.14015105366706848\n",
      "epoch: 7 step: 1881, loss is 0.09893228113651276\n",
      "epoch: 7 step: 1882, loss is 0.8178645968437195\n",
      "epoch: 7 step: 1883, loss is 0.4129883944988251\n",
      "epoch: 7 step: 1884, loss is 0.06700915098190308\n",
      "epoch: 7 step: 1885, loss is 0.1333634853363037\n",
      "epoch: 7 step: 1886, loss is 0.365811288356781\n",
      "epoch: 7 step: 1887, loss is 0.06390756368637085\n",
      "epoch: 7 step: 1888, loss is 0.6583002209663391\n",
      "epoch: 7 step: 1889, loss is 0.02532581426203251\n",
      "epoch: 7 step: 1890, loss is 0.19180375337600708\n",
      "epoch: 7 step: 1891, loss is 0.05686764419078827\n",
      "epoch: 7 step: 1892, loss is 0.49219536781311035\n",
      "epoch: 7 step: 1893, loss is 0.496883362531662\n",
      "epoch: 7 step: 1894, loss is 0.5881914496421814\n",
      "epoch: 7 step: 1895, loss is 0.1911097764968872\n",
      "epoch: 7 step: 1896, loss is 0.034777458757162094\n",
      "epoch: 7 step: 1897, loss is 0.07016637176275253\n",
      "epoch: 7 step: 1898, loss is 0.21341387927532196\n",
      "epoch: 7 step: 1899, loss is 0.19633837044239044\n",
      "epoch: 7 step: 1900, loss is 0.3133004903793335\n",
      "epoch: 7 step: 1901, loss is 0.5689710378646851\n",
      "epoch: 7 step: 1902, loss is 0.20305296778678894\n",
      "epoch: 7 step: 1903, loss is 0.7760716676712036\n",
      "epoch: 7 step: 1904, loss is 0.036309074610471725\n",
      "epoch: 7 step: 1905, loss is 0.6301771402359009\n",
      "epoch: 7 step: 1906, loss is 0.36827030777931213\n",
      "epoch: 7 step: 1907, loss is 0.5338237285614014\n",
      "epoch: 7 step: 1908, loss is 0.12572692334651947\n",
      "epoch: 7 step: 1909, loss is 0.22349165380001068\n",
      "epoch: 7 step: 1910, loss is 0.25012630224227905\n",
      "epoch: 7 step: 1911, loss is 0.44989609718322754\n",
      "epoch: 7 step: 1912, loss is 0.1557622253894806\n",
      "epoch: 7 step: 1913, loss is 0.6249537467956543\n",
      "epoch: 7 step: 1914, loss is 0.5198167562484741\n",
      "epoch: 7 step: 1915, loss is 0.09285007417201996\n",
      "epoch: 7 step: 1916, loss is 0.07247234135866165\n",
      "epoch: 7 step: 1917, loss is 0.25367873907089233\n",
      "epoch: 7 step: 1918, loss is 0.03965368866920471\n",
      "epoch: 7 step: 1919, loss is 0.35849541425704956\n",
      "epoch: 7 step: 1920, loss is 1.7511720657348633\n",
      "epoch: 7 step: 1921, loss is 0.2532458007335663\n",
      "epoch: 7 step: 1922, loss is 0.49356669187545776\n",
      "epoch: 7 step: 1923, loss is 0.3741985261440277\n",
      "epoch: 7 step: 1924, loss is 0.44743111729621887\n",
      "epoch: 7 step: 1925, loss is 0.5355265140533447\n",
      "epoch: 7 step: 1926, loss is 0.7416142225265503\n",
      "epoch: 7 step: 1927, loss is 0.3647654056549072\n",
      "epoch: 7 step: 1928, loss is 0.5347964763641357\n",
      "epoch: 7 step: 1929, loss is 0.3153901696205139\n",
      "epoch: 7 step: 1930, loss is 0.23608772456645966\n",
      "epoch: 7 step: 1931, loss is 0.4193546175956726\n",
      "epoch: 7 step: 1932, loss is 0.23152723908424377\n",
      "epoch: 7 step: 1933, loss is 0.17726409435272217\n",
      "epoch: 7 step: 1934, loss is 0.8156483173370361\n",
      "epoch: 7 step: 1935, loss is 0.17971326410770416\n",
      "epoch: 7 step: 1936, loss is 0.3509943187236786\n",
      "epoch: 7 step: 1937, loss is 0.15045735239982605\n",
      "epoch: 7 step: 1938, loss is 0.4894627034664154\n",
      "epoch: 7 step: 1939, loss is 0.29633668065071106\n",
      "epoch: 7 step: 1940, loss is 0.14086802303791046\n",
      "epoch: 7 step: 1941, loss is 0.09810202568769455\n",
      "epoch: 7 step: 1942, loss is 0.3687402904033661\n",
      "epoch: 7 step: 1943, loss is 0.19912448525428772\n",
      "epoch: 7 step: 1944, loss is 0.3044871389865875\n",
      "epoch: 7 step: 1945, loss is 0.23561936616897583\n",
      "epoch: 7 step: 1946, loss is 0.1833271086215973\n",
      "epoch: 7 step: 1947, loss is 1.0529963970184326\n",
      "epoch: 7 step: 1948, loss is 0.19412153959274292\n",
      "epoch: 7 step: 1949, loss is 0.27894628047943115\n",
      "epoch: 7 step: 1950, loss is 0.3127121031284332\n",
      "epoch: 7 step: 1951, loss is 1.405636191368103\n",
      "epoch: 7 step: 1952, loss is 0.22738488018512726\n",
      "epoch: 7 step: 1953, loss is 0.30429381132125854\n",
      "epoch: 7 step: 1954, loss is 0.13353170454502106\n",
      "epoch: 7 step: 1955, loss is 0.18882882595062256\n",
      "epoch: 7 step: 1956, loss is 0.2517169713973999\n",
      "epoch: 7 step: 1957, loss is 0.23361468315124512\n",
      "epoch: 7 step: 1958, loss is 0.19620628654956818\n",
      "epoch: 7 step: 1959, loss is 0.1295892298221588\n",
      "epoch: 7 step: 1960, loss is 0.2800225615501404\n",
      "epoch: 7 step: 1961, loss is 0.5534505248069763\n",
      "epoch: 7 step: 1962, loss is 0.24370720982551575\n",
      "epoch: 7 step: 1963, loss is 0.3508260250091553\n",
      "epoch: 7 step: 1964, loss is 0.8510770201683044\n",
      "epoch: 7 step: 1965, loss is 0.28670504689216614\n",
      "epoch: 7 step: 1966, loss is 0.10365311056375504\n",
      "epoch: 7 step: 1967, loss is 0.4830891191959381\n",
      "epoch: 7 step: 1968, loss is 0.39940345287323\n",
      "epoch: 7 step: 1969, loss is 0.22597166895866394\n",
      "epoch: 7 step: 1970, loss is 0.14028896391391754\n",
      "epoch: 7 step: 1971, loss is 0.9250604510307312\n",
      "epoch: 7 step: 1972, loss is 0.4545551538467407\n",
      "epoch: 7 step: 1973, loss is 0.0628063976764679\n",
      "epoch: 7 step: 1974, loss is 0.1385190486907959\n",
      "epoch: 7 step: 1975, loss is 0.6857499480247498\n",
      "epoch: 7 step: 1976, loss is 0.021067725494503975\n",
      "epoch: 7 step: 1977, loss is 0.5159056186676025\n",
      "epoch: 7 step: 1978, loss is 0.11465918272733688\n",
      "epoch: 7 step: 1979, loss is 0.34099018573760986\n",
      "epoch: 7 step: 1980, loss is 0.09657513350248337\n",
      "epoch: 7 step: 1981, loss is 0.10424479097127914\n",
      "epoch: 7 step: 1982, loss is 0.549645721912384\n",
      "epoch: 7 step: 1983, loss is 0.32258790731430054\n",
      "epoch: 7 step: 1984, loss is 0.8617721199989319\n",
      "epoch: 7 step: 1985, loss is 0.4735037684440613\n",
      "epoch: 7 step: 1986, loss is 0.3540771007537842\n",
      "epoch: 7 step: 1987, loss is 0.234725221991539\n",
      "epoch: 7 step: 1988, loss is 0.2915845513343811\n",
      "epoch: 7 step: 1989, loss is 0.23019073903560638\n",
      "epoch: 7 step: 1990, loss is 0.07310696691274643\n",
      "epoch: 7 step: 1991, loss is 0.1635279804468155\n",
      "epoch: 7 step: 1992, loss is 0.3249487280845642\n",
      "epoch: 7 step: 1993, loss is 0.1607639044523239\n",
      "epoch: 7 step: 1994, loss is 0.2703620195388794\n",
      "epoch: 7 step: 1995, loss is 0.18560990691184998\n",
      "epoch: 7 step: 1996, loss is 0.18459393084049225\n",
      "epoch: 7 step: 1997, loss is 0.7835510969161987\n",
      "epoch: 7 step: 1998, loss is 0.11259771883487701\n",
      "epoch: 7 step: 1999, loss is 0.0786176323890686\n",
      "epoch: 7 step: 2000, loss is 0.05735832452774048\n",
      "epoch: 7 step: 2001, loss is 0.35541263222694397\n",
      "epoch: 7 step: 2002, loss is 0.13006389141082764\n",
      "epoch: 7 step: 2003, loss is 0.1094648689031601\n",
      "epoch: 7 step: 2004, loss is 0.3831377625465393\n",
      "epoch: 7 step: 2005, loss is 0.13187195360660553\n",
      "epoch: 7 step: 2006, loss is 1.542736291885376\n",
      "epoch: 7 step: 2007, loss is 0.08346249163150787\n",
      "epoch: 7 step: 2008, loss is 0.4103599488735199\n",
      "epoch: 7 step: 2009, loss is 0.13577353954315186\n",
      "epoch: 7 step: 2010, loss is 0.24929550290107727\n",
      "epoch: 7 step: 2011, loss is 0.07767759263515472\n",
      "epoch: 7 step: 2012, loss is 0.19027964770793915\n",
      "epoch: 7 step: 2013, loss is 0.7533217668533325\n",
      "epoch: 7 step: 2014, loss is 0.25706014037132263\n",
      "epoch: 7 step: 2015, loss is 0.49420416355133057\n",
      "epoch: 7 step: 2016, loss is 0.25237733125686646\n",
      "epoch: 7 step: 2017, loss is 0.2680702805519104\n",
      "epoch: 7 step: 2018, loss is 0.13402310013771057\n",
      "epoch: 7 step: 2019, loss is 0.06466791033744812\n",
      "epoch: 7 step: 2020, loss is 0.6456140875816345\n",
      "epoch: 7 step: 2021, loss is 0.21305693686008453\n",
      "epoch: 7 step: 2022, loss is 0.11248572915792465\n",
      "epoch: 7 step: 2023, loss is 0.07566876709461212\n",
      "epoch: 7 step: 2024, loss is 0.2680910527706146\n",
      "epoch: 7 step: 2025, loss is 0.22368468344211578\n",
      "epoch: 7 step: 2026, loss is 0.34964731335639954\n",
      "epoch: 7 step: 2027, loss is 0.1143404170870781\n",
      "epoch: 7 step: 2028, loss is 0.3282111585140228\n",
      "epoch: 7 step: 2029, loss is 0.07792194187641144\n",
      "epoch: 7 step: 2030, loss is 0.39366447925567627\n",
      "epoch: 7 step: 2031, loss is 0.03061145357787609\n",
      "epoch: 7 step: 2032, loss is 0.17281527817249298\n",
      "epoch: 7 step: 2033, loss is 0.12885069847106934\n",
      "epoch: 7 step: 2034, loss is 0.48374873399734497\n",
      "epoch: 7 step: 2035, loss is 0.05652020126581192\n",
      "epoch: 7 step: 2036, loss is 0.0955110713839531\n",
      "epoch: 7 step: 2037, loss is 0.047844428569078445\n",
      "epoch: 7 step: 2038, loss is 0.32415977120399475\n",
      "epoch: 7 step: 2039, loss is 0.7329784631729126\n",
      "epoch: 7 step: 2040, loss is 0.27971261739730835\n",
      "epoch: 7 step: 2041, loss is 0.7744521498680115\n",
      "epoch: 7 step: 2042, loss is 0.1874677687883377\n",
      "epoch: 7 step: 2043, loss is 0.1799321323633194\n",
      "epoch: 7 step: 2044, loss is 0.042237538844347\n",
      "epoch: 7 step: 2045, loss is 0.5729610919952393\n",
      "epoch: 7 step: 2046, loss is 0.22968347370624542\n",
      "epoch: 7 step: 2047, loss is 0.1955925077199936\n",
      "epoch: 7 step: 2048, loss is 0.4898928701877594\n",
      "epoch: 7 step: 2049, loss is 0.328369140625\n",
      "epoch: 7 step: 2050, loss is 0.12939243018627167\n",
      "epoch: 7 step: 2051, loss is 0.5261063575744629\n",
      "epoch: 7 step: 2052, loss is 0.5960422158241272\n",
      "epoch: 7 step: 2053, loss is 0.0777263268828392\n",
      "epoch: 7 step: 2054, loss is 0.37988606095314026\n",
      "epoch: 7 step: 2055, loss is 0.0930723175406456\n",
      "epoch: 7 step: 2056, loss is 0.20900139212608337\n",
      "epoch: 7 step: 2057, loss is 0.6992623805999756\n",
      "epoch: 7 step: 2058, loss is 0.46998339891433716\n",
      "epoch: 7 step: 2059, loss is 0.2594912648200989\n",
      "epoch: 7 step: 2060, loss is 0.2979792654514313\n",
      "epoch: 7 step: 2061, loss is 0.1769459843635559\n",
      "epoch: 7 step: 2062, loss is 0.15371587872505188\n",
      "epoch: 7 step: 2063, loss is 0.35940831899642944\n",
      "epoch: 7 step: 2064, loss is 0.3313846290111542\n",
      "epoch: 7 step: 2065, loss is 0.15356674790382385\n",
      "epoch: 7 step: 2066, loss is 0.11525770276784897\n",
      "epoch: 7 step: 2067, loss is 0.07767261564731598\n",
      "epoch: 7 step: 2068, loss is 0.340530663728714\n",
      "epoch: 7 step: 2069, loss is 0.3075539171695709\n",
      "epoch: 7 step: 2070, loss is 0.07531577348709106\n",
      "epoch: 7 step: 2071, loss is 0.17587922513484955\n",
      "epoch: 7 step: 2072, loss is 0.2923424243927002\n",
      "epoch: 7 step: 2073, loss is 0.48237740993499756\n",
      "epoch: 7 step: 2074, loss is 0.9995389580726624\n",
      "epoch: 7 step: 2075, loss is 0.555526852607727\n",
      "epoch: 7 step: 2076, loss is 0.80918949842453\n",
      "epoch: 7 step: 2077, loss is 0.0410057008266449\n",
      "epoch: 7 step: 2078, loss is 0.787507176399231\n",
      "epoch: 7 step: 2079, loss is 0.07983745634555817\n",
      "epoch: 7 step: 2080, loss is 0.08453869074583054\n",
      "epoch: 7 step: 2081, loss is 0.5242824554443359\n",
      "epoch: 7 step: 2082, loss is 0.2992662191390991\n",
      "epoch: 7 step: 2083, loss is 0.5261008739471436\n",
      "epoch: 7 step: 2084, loss is 0.24354027211666107\n",
      "epoch: 7 step: 2085, loss is 0.541591227054596\n",
      "epoch: 7 step: 2086, loss is 0.4932170510292053\n",
      "epoch: 7 step: 2087, loss is 0.1453549712896347\n",
      "epoch: 7 step: 2088, loss is 0.27891528606414795\n",
      "epoch: 7 step: 2089, loss is 0.05392209067940712\n",
      "epoch: 7 step: 2090, loss is 0.1432834416627884\n",
      "epoch: 7 step: 2091, loss is 0.07672442495822906\n",
      "epoch: 7 step: 2092, loss is 0.39345574378967285\n",
      "epoch: 7 step: 2093, loss is 0.2703690528869629\n",
      "epoch: 7 step: 2094, loss is 0.16477347910404205\n",
      "epoch: 7 step: 2095, loss is 0.3537416160106659\n",
      "epoch: 7 step: 2096, loss is 0.15105187892913818\n",
      "epoch: 7 step: 2097, loss is 0.3027391731739044\n",
      "epoch: 7 step: 2098, loss is 0.19063343107700348\n",
      "epoch: 7 step: 2099, loss is 0.3510417640209198\n",
      "epoch: 7 step: 2100, loss is 0.40959832072257996\n",
      "epoch: 7 step: 2101, loss is 0.3353305160999298\n",
      "epoch: 7 step: 2102, loss is 0.26144909858703613\n",
      "epoch: 7 step: 2103, loss is 0.4069260060787201\n",
      "epoch: 7 step: 2104, loss is 0.32637637853622437\n",
      "epoch: 7 step: 2105, loss is 0.38766607642173767\n",
      "epoch: 7 step: 2106, loss is 0.506309449672699\n",
      "epoch: 7 step: 2107, loss is 0.3143833875656128\n",
      "epoch: 7 step: 2108, loss is 0.43116140365600586\n",
      "epoch: 7 step: 2109, loss is 0.8478190898895264\n",
      "epoch: 7 step: 2110, loss is 0.16948164999485016\n",
      "epoch: 7 step: 2111, loss is 0.15908516943454742\n",
      "epoch: 7 step: 2112, loss is 0.2943459451198578\n",
      "epoch: 7 step: 2113, loss is 0.3042363226413727\n",
      "epoch: 7 step: 2114, loss is 0.20996490120887756\n",
      "epoch: 7 step: 2115, loss is 0.6375894546508789\n",
      "epoch: 7 step: 2116, loss is 0.31891125440597534\n",
      "epoch: 7 step: 2117, loss is 0.2407727837562561\n",
      "epoch: 7 step: 2118, loss is 0.2710856795310974\n",
      "epoch: 7 step: 2119, loss is 0.18054582178592682\n",
      "epoch: 7 step: 2120, loss is 0.4773198962211609\n",
      "epoch: 7 step: 2121, loss is 0.18760916590690613\n",
      "epoch: 7 step: 2122, loss is 0.5142711997032166\n",
      "epoch: 7 step: 2123, loss is 0.4140206575393677\n",
      "epoch: 7 step: 2124, loss is 0.43767449259757996\n",
      "epoch: 7 step: 2125, loss is 0.5710028409957886\n",
      "epoch: 7 step: 2126, loss is 0.2843163013458252\n",
      "epoch: 7 step: 2127, loss is 0.5122758150100708\n",
      "epoch: 7 step: 2128, loss is 0.19126467406749725\n",
      "epoch: 7 step: 2129, loss is 0.45123961567878723\n",
      "epoch: 7 step: 2130, loss is 0.2959490120410919\n",
      "epoch: 7 step: 2131, loss is 0.06610427796840668\n",
      "epoch: 7 step: 2132, loss is 0.33153262734413147\n",
      "epoch: 7 step: 2133, loss is 0.2394016683101654\n",
      "epoch: 7 step: 2134, loss is 0.17650583386421204\n",
      "epoch: 7 step: 2135, loss is 0.1578594446182251\n",
      "epoch: 7 step: 2136, loss is 0.12371709197759628\n",
      "epoch: 7 step: 2137, loss is 0.7689885497093201\n",
      "epoch: 7 step: 2138, loss is 0.16801337897777557\n",
      "epoch: 7 step: 2139, loss is 0.6988503932952881\n",
      "epoch: 7 step: 2140, loss is 0.05656609311699867\n",
      "epoch: 7 step: 2141, loss is 0.2758547365665436\n",
      "epoch: 7 step: 2142, loss is 0.33292871713638306\n",
      "epoch: 7 step: 2143, loss is 0.11362014710903168\n",
      "epoch: 7 step: 2144, loss is 0.22545713186264038\n",
      "epoch: 7 step: 2145, loss is 0.3787004351615906\n",
      "epoch: 7 step: 2146, loss is 0.3670817017555237\n",
      "epoch: 7 step: 2147, loss is 0.2356633096933365\n",
      "epoch: 7 step: 2148, loss is 0.418031245470047\n",
      "epoch: 7 step: 2149, loss is 0.38120099902153015\n",
      "epoch: 7 step: 2150, loss is 0.20677068829536438\n",
      "epoch: 7 step: 2151, loss is 0.5394323468208313\n",
      "epoch: 7 step: 2152, loss is 0.11229206621646881\n",
      "epoch: 7 step: 2153, loss is 0.09507610648870468\n",
      "epoch: 7 step: 2154, loss is 0.7124571800231934\n",
      "epoch: 7 step: 2155, loss is 0.27053385972976685\n",
      "epoch: 7 step: 2156, loss is 0.3719174265861511\n",
      "epoch: 7 step: 2157, loss is 0.5591943264007568\n",
      "epoch: 7 step: 2158, loss is 0.6770236492156982\n",
      "epoch: 7 step: 2159, loss is 0.4116343855857849\n",
      "epoch: 7 step: 2160, loss is 0.6190599203109741\n",
      "epoch: 7 step: 2161, loss is 0.10936136543750763\n",
      "epoch: 7 step: 2162, loss is 0.4393383860588074\n",
      "epoch: 7 step: 2163, loss is 0.22211632132530212\n",
      "epoch: 7 step: 2164, loss is 0.19488638639450073\n",
      "epoch: 7 step: 2165, loss is 0.38669076561927795\n",
      "epoch: 7 step: 2166, loss is 0.401967316865921\n",
      "epoch: 7 step: 2167, loss is 0.15526169538497925\n",
      "epoch: 7 step: 2168, loss is 0.22744698822498322\n",
      "epoch: 7 step: 2169, loss is 0.04429610073566437\n",
      "epoch: 7 step: 2170, loss is 0.2472306191921234\n",
      "epoch: 7 step: 2171, loss is 0.07586795836687088\n",
      "epoch: 7 step: 2172, loss is 0.24344509840011597\n",
      "epoch: 7 step: 2173, loss is 0.6670464277267456\n",
      "epoch: 7 step: 2174, loss is 0.37080883979797363\n",
      "epoch: 7 step: 2175, loss is 0.3400281071662903\n",
      "epoch: 7 step: 2176, loss is 0.21142974495887756\n",
      "epoch: 7 step: 2177, loss is 0.8147438168525696\n",
      "epoch: 7 step: 2178, loss is 0.3428896367549896\n",
      "epoch: 7 step: 2179, loss is 0.10514312237501144\n",
      "epoch: 7 step: 2180, loss is 0.1781674027442932\n",
      "epoch: 7 step: 2181, loss is 0.18317973613739014\n",
      "epoch: 7 step: 2182, loss is 0.3212018311023712\n",
      "epoch: 7 step: 2183, loss is 0.38920605182647705\n",
      "epoch: 7 step: 2184, loss is 0.5113820433616638\n",
      "epoch: 7 step: 2185, loss is 0.0733623132109642\n",
      "epoch: 7 step: 2186, loss is 0.121473528444767\n",
      "epoch: 7 step: 2187, loss is 0.2045198529958725\n",
      "epoch: 7 step: 2188, loss is 0.22173315286636353\n",
      "epoch: 7 step: 2189, loss is 0.05200287327170372\n",
      "epoch: 7 step: 2190, loss is 0.4130021333694458\n",
      "epoch: 7 step: 2191, loss is 0.11185115575790405\n",
      "epoch: 7 step: 2192, loss is 0.19328200817108154\n",
      "epoch: 7 step: 2193, loss is 0.062166839838027954\n",
      "epoch: 7 step: 2194, loss is 0.30117401480674744\n",
      "epoch: 7 step: 2195, loss is 0.5645981431007385\n",
      "epoch: 7 step: 2196, loss is 0.06015871465206146\n",
      "epoch: 7 step: 2197, loss is 0.24185213446617126\n",
      "epoch: 7 step: 2198, loss is 0.11922088265419006\n",
      "epoch: 7 step: 2199, loss is 0.23687970638275146\n",
      "epoch: 7 step: 2200, loss is 0.15656305849552155\n",
      "epoch: 7 step: 2201, loss is 0.48984310030937195\n",
      "epoch: 7 step: 2202, loss is 0.40570566058158875\n",
      "epoch: 7 step: 2203, loss is 0.31037160754203796\n",
      "epoch: 7 step: 2204, loss is 0.024481967091560364\n",
      "epoch: 7 step: 2205, loss is 0.4777422249317169\n",
      "epoch: 7 step: 2206, loss is 0.0978756695985794\n",
      "epoch: 7 step: 2207, loss is 0.07593795657157898\n",
      "epoch: 7 step: 2208, loss is 0.11307305097579956\n",
      "epoch: 7 step: 2209, loss is 0.4957500398159027\n",
      "epoch: 7 step: 2210, loss is 0.12705276906490326\n",
      "epoch: 7 step: 2211, loss is 0.2990877032279968\n",
      "epoch: 7 step: 2212, loss is 0.652765691280365\n",
      "epoch: 7 step: 2213, loss is 0.03820992261171341\n",
      "epoch: 7 step: 2214, loss is 0.28399667143821716\n",
      "epoch: 7 step: 2215, loss is 0.5427493453025818\n",
      "epoch: 7 step: 2216, loss is 0.07616464048624039\n",
      "epoch: 7 step: 2217, loss is 0.16893498599529266\n",
      "epoch: 7 step: 2218, loss is 0.4695182144641876\n",
      "epoch: 7 step: 2219, loss is 0.6218442320823669\n",
      "epoch: 7 step: 2220, loss is 0.05638708919286728\n",
      "epoch: 7 step: 2221, loss is 0.5738611221313477\n",
      "epoch: 7 step: 2222, loss is 0.40983399748802185\n",
      "epoch: 7 step: 2223, loss is 0.09954500943422318\n",
      "epoch: 7 step: 2224, loss is 0.19644573330879211\n",
      "epoch: 7 step: 2225, loss is 0.5842580199241638\n",
      "epoch: 7 step: 2226, loss is 0.238210067152977\n",
      "epoch: 7 step: 2227, loss is 0.08481447398662567\n",
      "epoch: 7 step: 2228, loss is 0.4147874712944031\n",
      "epoch: 7 step: 2229, loss is 0.38806334137916565\n",
      "epoch: 7 step: 2230, loss is 0.3511047065258026\n",
      "epoch: 7 step: 2231, loss is 0.14444918930530548\n",
      "epoch: 7 step: 2232, loss is 0.1318330615758896\n",
      "epoch: 7 step: 2233, loss is 0.38354262709617615\n",
      "epoch: 7 step: 2234, loss is 0.17580758035182953\n",
      "epoch: 7 step: 2235, loss is 0.3892078995704651\n",
      "epoch: 7 step: 2236, loss is 0.2853846251964569\n",
      "epoch: 7 step: 2237, loss is 0.0690590888261795\n",
      "epoch: 7 step: 2238, loss is 0.46314582228660583\n",
      "epoch: 7 step: 2239, loss is 0.6655230522155762\n",
      "epoch: 7 step: 2240, loss is 0.26577386260032654\n",
      "epoch: 7 step: 2241, loss is 0.12578265368938446\n",
      "epoch: 7 step: 2242, loss is 0.6330506801605225\n",
      "epoch: 7 step: 2243, loss is 0.44613152742385864\n",
      "epoch: 7 step: 2244, loss is 0.1668964922428131\n",
      "epoch: 7 step: 2245, loss is 0.4575878381729126\n",
      "epoch: 7 step: 2246, loss is 0.11482091248035431\n",
      "epoch: 7 step: 2247, loss is 0.4285764694213867\n",
      "epoch: 7 step: 2248, loss is 0.1321924924850464\n",
      "epoch: 7 step: 2249, loss is 0.28274795413017273\n",
      "epoch: 7 step: 2250, loss is 0.6960306763648987\n",
      "epoch: 7 step: 2251, loss is 0.41649511456489563\n",
      "epoch: 7 step: 2252, loss is 0.2863345444202423\n",
      "epoch: 7 step: 2253, loss is 0.16769537329673767\n",
      "epoch: 7 step: 2254, loss is 0.197793647646904\n",
      "epoch: 7 step: 2255, loss is 0.5912302136421204\n",
      "epoch: 7 step: 2256, loss is 0.3332977592945099\n",
      "epoch: 7 step: 2257, loss is 0.3977608382701874\n",
      "epoch: 7 step: 2258, loss is 0.1600622981786728\n",
      "epoch: 7 step: 2259, loss is 0.4994604289531708\n",
      "epoch: 7 step: 2260, loss is 0.473402202129364\n",
      "epoch: 7 step: 2261, loss is 0.06633962690830231\n",
      "epoch: 7 step: 2262, loss is 0.10952755063772202\n",
      "epoch: 7 step: 2263, loss is 0.3601829409599304\n",
      "epoch: 7 step: 2264, loss is 0.393584668636322\n",
      "epoch: 7 step: 2265, loss is 0.2745201587677002\n",
      "epoch: 7 step: 2266, loss is 0.4686358571052551\n",
      "epoch: 7 step: 2267, loss is 0.15235581994056702\n",
      "epoch: 7 step: 2268, loss is 0.08880701661109924\n",
      "epoch: 7 step: 2269, loss is 0.18676161766052246\n",
      "epoch: 7 step: 2270, loss is 0.4831479787826538\n",
      "epoch: 7 step: 2271, loss is 0.23308248817920685\n",
      "epoch: 7 step: 2272, loss is 0.07294145971536636\n",
      "epoch: 7 step: 2273, loss is 0.3681921362876892\n",
      "epoch: 7 step: 2274, loss is 0.15358181297779083\n",
      "epoch: 7 step: 2275, loss is 0.1736012101173401\n",
      "epoch: 7 step: 2276, loss is 0.22905941307544708\n",
      "epoch: 7 step: 2277, loss is 0.09247330576181412\n",
      "epoch: 7 step: 2278, loss is 0.8759711980819702\n",
      "epoch: 7 step: 2279, loss is 0.21149609982967377\n",
      "epoch: 7 step: 2280, loss is 0.5898725986480713\n",
      "epoch: 7 step: 2281, loss is 0.16906000673770905\n",
      "epoch: 7 step: 2282, loss is 0.03884812816977501\n",
      "epoch: 7 step: 2283, loss is 0.6144332885742188\n",
      "epoch: 7 step: 2284, loss is 0.2955228388309479\n",
      "epoch: 7 step: 2285, loss is 0.1957472264766693\n",
      "epoch: 7 step: 2286, loss is 0.4077534079551697\n",
      "epoch: 7 step: 2287, loss is 0.1581270843744278\n",
      "epoch: 7 step: 2288, loss is 0.0423763282597065\n",
      "epoch: 7 step: 2289, loss is 0.1859005242586136\n",
      "epoch: 7 step: 2290, loss is 0.21420273184776306\n",
      "epoch: 7 step: 2291, loss is 0.35587024688720703\n",
      "epoch: 7 step: 2292, loss is 0.2930606007575989\n",
      "epoch: 7 step: 2293, loss is 0.08936676383018494\n",
      "epoch: 7 step: 2294, loss is 0.521737277507782\n",
      "epoch: 7 step: 2295, loss is 0.33220866322517395\n",
      "epoch: 7 step: 2296, loss is 0.19046033918857574\n",
      "epoch: 7 step: 2297, loss is 0.07710724323987961\n",
      "epoch: 7 step: 2298, loss is 0.1824260801076889\n",
      "epoch: 7 step: 2299, loss is 0.518034040927887\n",
      "epoch: 7 step: 2300, loss is 0.1107783168554306\n",
      "epoch: 7 step: 2301, loss is 0.39641207456588745\n",
      "epoch: 7 step: 2302, loss is 0.03101474419236183\n",
      "epoch: 7 step: 2303, loss is 0.2200152426958084\n",
      "epoch: 7 step: 2304, loss is 0.08967636525630951\n",
      "epoch: 7 step: 2305, loss is 0.3447014093399048\n",
      "epoch: 7 step: 2306, loss is 0.3661222755908966\n",
      "epoch: 7 step: 2307, loss is 0.15446873009204865\n",
      "epoch: 7 step: 2308, loss is 0.14209547638893127\n",
      "epoch: 7 step: 2309, loss is 0.23821330070495605\n",
      "epoch: 7 step: 2310, loss is 0.1399441659450531\n",
      "epoch: 7 step: 2311, loss is 0.25967830419540405\n",
      "epoch: 7 step: 2312, loss is 0.08400790393352509\n",
      "epoch: 7 step: 2313, loss is 0.11959050595760345\n",
      "epoch: 7 step: 2314, loss is 0.18630537390708923\n",
      "epoch: 7 step: 2315, loss is 0.1822904497385025\n",
      "epoch: 7 step: 2316, loss is 0.027305161580443382\n",
      "epoch: 7 step: 2317, loss is 0.2660983204841614\n",
      "epoch: 7 step: 2318, loss is 0.7349252104759216\n",
      "epoch: 7 step: 2319, loss is 0.7827234268188477\n",
      "epoch: 7 step: 2320, loss is 0.6923285722732544\n",
      "epoch: 7 step: 2321, loss is 0.38850095868110657\n",
      "epoch: 7 step: 2322, loss is 0.26418083906173706\n",
      "epoch: 7 step: 2323, loss is 0.23213349282741547\n",
      "epoch: 7 step: 2324, loss is 0.18703289330005646\n",
      "epoch: 7 step: 2325, loss is 0.32646989822387695\n",
      "epoch: 7 step: 2326, loss is 0.45782262086868286\n",
      "epoch: 7 step: 2327, loss is 0.4379923939704895\n",
      "epoch: 7 step: 2328, loss is 0.08254150301218033\n",
      "epoch: 7 step: 2329, loss is 0.29508551955223083\n",
      "epoch: 7 step: 2330, loss is 0.09011249989271164\n",
      "epoch: 7 step: 2331, loss is 0.057449303567409515\n",
      "epoch: 7 step: 2332, loss is 0.158619225025177\n",
      "epoch: 7 step: 2333, loss is 0.07604210823774338\n",
      "epoch: 7 step: 2334, loss is 0.22586411237716675\n",
      "epoch: 7 step: 2335, loss is 0.13817724585533142\n",
      "epoch: 7 step: 2336, loss is 0.15978538990020752\n",
      "epoch: 7 step: 2337, loss is 0.053919631987810135\n",
      "epoch: 7 step: 2338, loss is 0.33362695574760437\n",
      "epoch: 7 step: 2339, loss is 0.1552397608757019\n",
      "epoch: 7 step: 2340, loss is 0.12612465023994446\n",
      "epoch: 7 step: 2341, loss is 0.24548593163490295\n",
      "epoch: 7 step: 2342, loss is 0.3496268093585968\n",
      "epoch: 7 step: 2343, loss is 0.18856266140937805\n",
      "epoch: 7 step: 2344, loss is 0.1920565515756607\n",
      "epoch: 7 step: 2345, loss is 0.13757306337356567\n",
      "epoch: 7 step: 2346, loss is 0.5274090766906738\n",
      "epoch: 7 step: 2347, loss is 0.15943267941474915\n",
      "epoch: 7 step: 2348, loss is 0.04725710675120354\n",
      "epoch: 7 step: 2349, loss is 0.13419118523597717\n",
      "epoch: 7 step: 2350, loss is 0.33706367015838623\n",
      "epoch: 7 step: 2351, loss is 0.14444731175899506\n",
      "epoch: 7 step: 2352, loss is 0.01490178145468235\n",
      "epoch: 7 step: 2353, loss is 0.5274285674095154\n",
      "epoch: 7 step: 2354, loss is 0.34505319595336914\n",
      "epoch: 7 step: 2355, loss is 0.07930880784988403\n",
      "epoch: 7 step: 2356, loss is 0.10762485861778259\n",
      "epoch: 7 step: 2357, loss is 0.22898156940937042\n",
      "epoch: 7 step: 2358, loss is 0.467219740152359\n",
      "epoch: 7 step: 2359, loss is 0.12998567521572113\n",
      "epoch: 7 step: 2360, loss is 0.10930629819631577\n",
      "epoch: 7 step: 2361, loss is 0.3705596923828125\n",
      "epoch: 7 step: 2362, loss is 0.05098634958267212\n",
      "epoch: 7 step: 2363, loss is 0.15635621547698975\n",
      "epoch: 7 step: 2364, loss is 0.025786779820919037\n",
      "epoch: 7 step: 2365, loss is 0.057479292154312134\n",
      "epoch: 7 step: 2366, loss is 0.07367920130491257\n",
      "epoch: 7 step: 2367, loss is 0.37216535210609436\n",
      "epoch: 7 step: 2368, loss is 0.1612953245639801\n",
      "epoch: 7 step: 2369, loss is 0.26993152499198914\n",
      "epoch: 7 step: 2370, loss is 0.4259865880012512\n",
      "epoch: 7 step: 2371, loss is 0.07968054711818695\n",
      "epoch: 7 step: 2372, loss is 0.4115782678127289\n",
      "epoch: 7 step: 2373, loss is 0.16648918390274048\n",
      "epoch: 7 step: 2374, loss is 0.10722695291042328\n",
      "epoch: 7 step: 2375, loss is 0.12775075435638428\n",
      "epoch: 7 step: 2376, loss is 0.2531035840511322\n",
      "epoch: 7 step: 2377, loss is 0.17030222713947296\n",
      "epoch: 7 step: 2378, loss is 0.03310342878103256\n",
      "epoch: 7 step: 2379, loss is 0.415311336517334\n",
      "epoch: 7 step: 2380, loss is 0.26090821623802185\n",
      "epoch: 7 step: 2381, loss is 0.2617127001285553\n",
      "epoch: 7 step: 2382, loss is 0.5942424535751343\n",
      "epoch: 7 step: 2383, loss is 0.12819738686084747\n",
      "epoch: 7 step: 2384, loss is 0.15043939650058746\n",
      "epoch: 7 step: 2385, loss is 0.04862787574529648\n",
      "epoch: 7 step: 2386, loss is 0.3032567799091339\n",
      "epoch: 7 step: 2387, loss is 0.09074932336807251\n",
      "epoch: 7 step: 2388, loss is 0.23597313463687897\n",
      "epoch: 7 step: 2389, loss is 0.3643762469291687\n",
      "epoch: 7 step: 2390, loss is 0.4667045474052429\n",
      "epoch: 7 step: 2391, loss is 0.41296136379241943\n",
      "epoch: 7 step: 2392, loss is 0.926632285118103\n",
      "epoch: 7 step: 2393, loss is 0.08969385176897049\n",
      "epoch: 7 step: 2394, loss is 0.19820179045200348\n",
      "epoch: 7 step: 2395, loss is 0.22503919899463654\n",
      "epoch: 7 step: 2396, loss is 0.5418913960456848\n",
      "epoch: 7 step: 2397, loss is 0.38637155294418335\n",
      "epoch: 7 step: 2398, loss is 0.3679302930831909\n",
      "epoch: 7 step: 2399, loss is 0.26083871722221375\n",
      "epoch: 7 step: 2400, loss is 0.04450370371341705\n",
      "epoch: 7 step: 2401, loss is 0.38868167996406555\n",
      "epoch: 7 step: 2402, loss is 0.08421830832958221\n",
      "epoch: 7 step: 2403, loss is 0.09104444086551666\n",
      "epoch: 7 step: 2404, loss is 0.24109824001789093\n",
      "epoch: 7 step: 2405, loss is 0.4954145550727844\n",
      "epoch: 7 step: 2406, loss is 0.1896204948425293\n",
      "epoch: 7 step: 2407, loss is 0.14619284868240356\n",
      "epoch: 7 step: 2408, loss is 0.11193190515041351\n",
      "epoch: 7 step: 2409, loss is 0.3171837329864502\n",
      "epoch: 7 step: 2410, loss is 0.25069475173950195\n",
      "epoch: 7 step: 2411, loss is 0.464907169342041\n",
      "epoch: 7 step: 2412, loss is 0.31706497073173523\n",
      "epoch: 7 step: 2413, loss is 0.5710878372192383\n",
      "epoch: 7 step: 2414, loss is 0.29350242018699646\n",
      "epoch: 7 step: 2415, loss is 0.5957077741622925\n",
      "epoch: 7 step: 2416, loss is 0.40799543261528015\n",
      "epoch: 7 step: 2417, loss is 0.6635546684265137\n",
      "epoch: 7 step: 2418, loss is 0.26122698187828064\n",
      "epoch: 7 step: 2419, loss is 0.545628011226654\n",
      "epoch: 7 step: 2420, loss is 0.215648353099823\n",
      "epoch: 7 step: 2421, loss is 0.11622296273708344\n",
      "epoch: 7 step: 2422, loss is 0.10326363146305084\n",
      "epoch: 7 step: 2423, loss is 0.1313263475894928\n",
      "epoch: 7 step: 2424, loss is 0.2766582667827606\n",
      "epoch: 7 step: 2425, loss is 0.5359758734703064\n",
      "epoch: 7 step: 2426, loss is 0.1071409061551094\n",
      "epoch: 7 step: 2427, loss is 0.23036642372608185\n",
      "epoch: 7 step: 2428, loss is 0.5137763619422913\n",
      "epoch: 7 step: 2429, loss is 0.48959270119667053\n",
      "epoch: 7 step: 2430, loss is 0.2278737872838974\n",
      "epoch: 7 step: 2431, loss is 0.14893381297588348\n",
      "epoch: 7 step: 2432, loss is 0.2787170112133026\n",
      "epoch: 7 step: 2433, loss is 0.3016662895679474\n",
      "epoch: 7 step: 2434, loss is 0.2241736203432083\n",
      "epoch: 7 step: 2435, loss is 1.0230129957199097\n",
      "epoch: 7 step: 2436, loss is 0.046901635825634\n",
      "epoch: 7 step: 2437, loss is 0.21450857818126678\n",
      "epoch: 7 step: 2438, loss is 0.33191725611686707\n",
      "epoch: 7 step: 2439, loss is 0.22818441689014435\n",
      "epoch: 7 step: 2440, loss is 0.28403154015541077\n",
      "epoch: 7 step: 2441, loss is 0.17347855865955353\n",
      "epoch: 7 step: 2442, loss is 0.4844018220901489\n",
      "epoch: 7 step: 2443, loss is 0.8036410212516785\n",
      "epoch: 7 step: 2444, loss is 0.3821714520454407\n",
      "epoch: 7 step: 2445, loss is 0.47631770372390747\n",
      "epoch: 7 step: 2446, loss is 0.39773979783058167\n",
      "epoch: 7 step: 2447, loss is 0.46575671434402466\n",
      "epoch: 7 step: 2448, loss is 0.5496946573257446\n",
      "epoch: 7 step: 2449, loss is 0.1554194986820221\n",
      "epoch: 7 step: 2450, loss is 0.23964159190654755\n",
      "epoch: 7 step: 2451, loss is 0.17720477283000946\n",
      "epoch: 7 step: 2452, loss is 0.40784165263175964\n",
      "epoch: 7 step: 2453, loss is 0.09107240289449692\n",
      "epoch: 7 step: 2454, loss is 0.21756282448768616\n",
      "epoch: 7 step: 2455, loss is 0.5072312951087952\n",
      "epoch: 7 step: 2456, loss is 0.22421608865261078\n",
      "epoch: 7 step: 2457, loss is 0.15112102031707764\n",
      "epoch: 7 step: 2458, loss is 0.36194950342178345\n",
      "epoch: 7 step: 2459, loss is 0.07652311772108078\n",
      "epoch: 7 step: 2460, loss is 0.17107918858528137\n",
      "epoch: 7 step: 2461, loss is 0.15597181022167206\n",
      "epoch: 7 step: 2462, loss is 0.22127972543239594\n",
      "epoch: 7 step: 2463, loss is 0.2762952446937561\n",
      "epoch: 7 step: 2464, loss is 0.43559128046035767\n",
      "epoch: 7 step: 2465, loss is 0.22364014387130737\n",
      "epoch: 7 step: 2466, loss is 0.3510144352912903\n",
      "epoch: 7 step: 2467, loss is 0.25116434693336487\n",
      "epoch: 7 step: 2468, loss is 0.15421448647975922\n",
      "epoch: 7 step: 2469, loss is 0.3139556348323822\n",
      "epoch: 7 step: 2470, loss is 0.14383988082408905\n",
      "epoch: 7 step: 2471, loss is 0.2940104305744171\n",
      "epoch: 7 step: 2472, loss is 0.09800893813371658\n",
      "epoch: 7 step: 2473, loss is 0.39337337017059326\n",
      "epoch: 7 step: 2474, loss is 0.2232266068458557\n",
      "epoch: 7 step: 2475, loss is 0.4013957381248474\n",
      "epoch: 7 step: 2476, loss is 0.20545490086078644\n",
      "epoch: 7 step: 2477, loss is 0.13069798052310944\n",
      "epoch: 7 step: 2478, loss is 0.06472975760698318\n",
      "epoch: 7 step: 2479, loss is 0.14143610000610352\n",
      "epoch: 7 step: 2480, loss is 0.1696186065673828\n",
      "epoch: 7 step: 2481, loss is 0.10785481333732605\n",
      "epoch: 7 step: 2482, loss is 0.5870537161827087\n",
      "epoch: 7 step: 2483, loss is 0.20737746357917786\n",
      "epoch: 7 step: 2484, loss is 0.030972011387348175\n",
      "epoch: 7 step: 2485, loss is 0.7369357347488403\n",
      "epoch: 7 step: 2486, loss is 0.6385900378227234\n",
      "epoch: 7 step: 2487, loss is 0.14278829097747803\n",
      "epoch: 7 step: 2488, loss is 0.26070818305015564\n",
      "epoch: 7 step: 2489, loss is 0.38298219442367554\n",
      "epoch: 7 step: 2490, loss is 0.17341552674770355\n",
      "epoch: 7 step: 2491, loss is 0.22785955667495728\n",
      "epoch: 7 step: 2492, loss is 0.18723881244659424\n",
      "epoch: 7 step: 2493, loss is 0.2429465353488922\n",
      "epoch: 7 step: 2494, loss is 0.5304438471794128\n",
      "epoch: 7 step: 2495, loss is 0.21894235908985138\n",
      "epoch: 7 step: 2496, loss is 0.06383310258388519\n",
      "epoch: 7 step: 2497, loss is 0.3749089539051056\n",
      "epoch: 7 step: 2498, loss is 0.24222131073474884\n",
      "epoch: 7 step: 2499, loss is 0.4958639442920685\n",
      "epoch: 7 step: 2500, loss is 0.4176725149154663\n",
      "epoch: 7 step: 2501, loss is 0.17563089728355408\n",
      "epoch: 7 step: 2502, loss is 0.21702441573143005\n",
      "epoch: 7 step: 2503, loss is 0.10512077808380127\n",
      "epoch: 7 step: 2504, loss is 0.11975578218698502\n",
      "epoch: 7 step: 2505, loss is 0.07319016754627228\n",
      "epoch: 7 step: 2506, loss is 0.054311055690050125\n",
      "epoch: 7 step: 2507, loss is 0.24997416138648987\n",
      "epoch: 7 step: 2508, loss is 0.9671915769577026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: unknown JFIF revision number 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 8 step: 1, loss is 0.20588819682598114\n",
      "epoch: 8 step: 2, loss is 0.1065378189086914\n",
      "epoch: 8 step: 3, loss is 0.18660204112529755\n",
      "epoch: 8 step: 4, loss is 0.26820358633995056\n",
      "epoch: 8 step: 5, loss is 0.21716108918190002\n",
      "epoch: 8 step: 6, loss is 0.2191799134016037\n",
      "epoch: 8 step: 7, loss is 0.31219449639320374\n",
      "epoch: 8 step: 8, loss is 0.09910470992326736\n",
      "epoch: 8 step: 9, loss is 0.05361131578683853\n",
      "epoch: 8 step: 10, loss is 0.1383945196866989\n",
      "epoch: 8 step: 11, loss is 0.05303046852350235\n",
      "epoch: 8 step: 12, loss is 0.40637341141700745\n",
      "epoch: 8 step: 13, loss is 0.4043847620487213\n",
      "epoch: 8 step: 14, loss is 0.09819456934928894\n",
      "epoch: 8 step: 15, loss is 0.30940601229667664\n",
      "epoch: 8 step: 16, loss is 0.4853968620300293\n",
      "epoch: 8 step: 17, loss is 0.3347378969192505\n",
      "epoch: 8 step: 18, loss is 0.05933932960033417\n",
      "epoch: 8 step: 19, loss is 0.09843938052654266\n",
      "epoch: 8 step: 20, loss is 0.5284938812255859\n",
      "epoch: 8 step: 21, loss is 0.20718331634998322\n",
      "epoch: 8 step: 22, loss is 0.07819917798042297\n",
      "epoch: 8 step: 23, loss is 0.14101949334144592\n",
      "epoch: 8 step: 24, loss is 0.05481363460421562\n",
      "epoch: 8 step: 25, loss is 0.15183426439762115\n",
      "epoch: 8 step: 26, loss is 0.30202651023864746\n",
      "epoch: 8 step: 27, loss is 0.322333425283432\n",
      "epoch: 8 step: 28, loss is 0.1814611256122589\n",
      "epoch: 8 step: 29, loss is 0.2694382071495056\n",
      "epoch: 8 step: 30, loss is 0.23741042613983154\n",
      "epoch: 8 step: 31, loss is 0.0819787085056305\n",
      "epoch: 8 step: 32, loss is 0.22706365585327148\n",
      "epoch: 8 step: 33, loss is 0.23314936459064484\n",
      "epoch: 8 step: 34, loss is 0.15623627603054047\n",
      "epoch: 8 step: 35, loss is 0.3806019425392151\n",
      "epoch: 8 step: 36, loss is 0.48780468106269836\n",
      "epoch: 8 step: 37, loss is 0.6340173482894897\n",
      "epoch: 8 step: 38, loss is 0.15426860749721527\n",
      "epoch: 8 step: 39, loss is 0.05190802365541458\n",
      "epoch: 8 step: 40, loss is 0.05132665857672691\n",
      "epoch: 8 step: 41, loss is 0.05276759713888168\n",
      "epoch: 8 step: 42, loss is 0.3197847604751587\n",
      "epoch: 8 step: 43, loss is 0.17499133944511414\n",
      "epoch: 8 step: 44, loss is 0.38598406314849854\n",
      "epoch: 8 step: 45, loss is 0.2422206997871399\n",
      "epoch: 8 step: 46, loss is 0.5510298609733582\n",
      "epoch: 8 step: 47, loss is 0.2456643283367157\n",
      "epoch: 8 step: 48, loss is 0.15751732885837555\n",
      "epoch: 8 step: 49, loss is 0.7796670198440552\n",
      "epoch: 8 step: 50, loss is 0.17794238030910492\n",
      "epoch: 8 step: 51, loss is 0.06996314972639084\n",
      "epoch: 8 step: 52, loss is 0.3032219707965851\n",
      "epoch: 8 step: 53, loss is 0.23154492676258087\n",
      "epoch: 8 step: 54, loss is 0.24852871894836426\n",
      "epoch: 8 step: 55, loss is 0.5236972570419312\n",
      "epoch: 8 step: 56, loss is 0.1325080841779709\n",
      "epoch: 8 step: 57, loss is 0.09818942844867706\n",
      "epoch: 8 step: 58, loss is 0.5890817046165466\n",
      "epoch: 8 step: 59, loss is 0.1236710175871849\n",
      "epoch: 8 step: 60, loss is 0.3666110634803772\n",
      "epoch: 8 step: 61, loss is 0.043959058821201324\n",
      "epoch: 8 step: 62, loss is 0.418320894241333\n",
      "epoch: 8 step: 63, loss is 1.1209805011749268\n",
      "epoch: 8 step: 64, loss is 0.49188870191574097\n",
      "epoch: 8 step: 65, loss is 0.07398047298192978\n",
      "epoch: 8 step: 66, loss is 0.11077295243740082\n",
      "epoch: 8 step: 67, loss is 0.20152759552001953\n",
      "epoch: 8 step: 68, loss is 0.3574923276901245\n",
      "epoch: 8 step: 69, loss is 0.05795037001371384\n",
      "epoch: 8 step: 70, loss is 0.23844937980175018\n",
      "epoch: 8 step: 71, loss is 0.13721157610416412\n",
      "epoch: 8 step: 72, loss is 0.09386013448238373\n",
      "epoch: 8 step: 73, loss is 0.4687008559703827\n",
      "epoch: 8 step: 74, loss is 0.20623737573623657\n",
      "epoch: 8 step: 75, loss is 0.07602597773075104\n",
      "epoch: 8 step: 76, loss is 0.6680116653442383\n",
      "epoch: 8 step: 77, loss is 0.16788730025291443\n",
      "epoch: 8 step: 78, loss is 0.12072233855724335\n",
      "epoch: 8 step: 79, loss is 0.4969051480293274\n",
      "epoch: 8 step: 80, loss is 0.13514350354671478\n",
      "epoch: 8 step: 81, loss is 0.1232842430472374\n",
      "epoch: 8 step: 82, loss is 0.6455628275871277\n",
      "epoch: 8 step: 83, loss is 0.054091718047857285\n",
      "epoch: 8 step: 84, loss is 0.46183204650878906\n",
      "epoch: 8 step: 85, loss is 0.370126336812973\n",
      "epoch: 8 step: 86, loss is 0.31779035925865173\n",
      "epoch: 8 step: 87, loss is 0.26790285110473633\n",
      "epoch: 8 step: 88, loss is 0.34228944778442383\n",
      "epoch: 8 step: 89, loss is 0.46410512924194336\n",
      "epoch: 8 step: 90, loss is 0.12319928407669067\n",
      "epoch: 8 step: 91, loss is 0.4343705475330353\n",
      "epoch: 8 step: 92, loss is 0.1099420040845871\n",
      "epoch: 8 step: 93, loss is 0.0893997848033905\n",
      "epoch: 8 step: 94, loss is 0.16277581453323364\n",
      "epoch: 8 step: 95, loss is 0.29131320118904114\n",
      "epoch: 8 step: 96, loss is 0.11554837971925735\n",
      "epoch: 8 step: 97, loss is 0.16744865477085114\n",
      "epoch: 8 step: 98, loss is 0.11299629509449005\n",
      "epoch: 8 step: 99, loss is 0.2235235571861267\n",
      "epoch: 8 step: 100, loss is 0.026370184496045113\n",
      "epoch: 8 step: 101, loss is 0.10558202117681503\n",
      "epoch: 8 step: 102, loss is 0.09627304971218109\n",
      "epoch: 8 step: 103, loss is 0.11827436089515686\n",
      "epoch: 8 step: 104, loss is 0.17646940052509308\n",
      "epoch: 8 step: 105, loss is 0.7548185586929321\n",
      "epoch: 8 step: 106, loss is 0.4468289017677307\n",
      "epoch: 8 step: 107, loss is 0.14211250841617584\n",
      "epoch: 8 step: 108, loss is 0.19111375510692596\n",
      "epoch: 8 step: 109, loss is 0.126744344830513\n",
      "epoch: 8 step: 110, loss is 0.11169277876615524\n",
      "epoch: 8 step: 111, loss is 0.39994996786117554\n",
      "epoch: 8 step: 112, loss is 0.37244778871536255\n",
      "epoch: 8 step: 113, loss is 0.0670156329870224\n",
      "epoch: 8 step: 114, loss is 0.15791942179203033\n",
      "epoch: 8 step: 115, loss is 0.18341565132141113\n",
      "epoch: 8 step: 116, loss is 0.2950321137905121\n",
      "epoch: 8 step: 117, loss is 0.03991454839706421\n",
      "epoch: 8 step: 118, loss is 0.339555025100708\n",
      "epoch: 8 step: 119, loss is 0.18425999581813812\n",
      "epoch: 8 step: 120, loss is 0.0806199461221695\n",
      "epoch: 8 step: 121, loss is 0.21569357812404633\n",
      "epoch: 8 step: 122, loss is 0.4497927129268646\n",
      "epoch: 8 step: 123, loss is 0.7191630601882935\n",
      "epoch: 8 step: 124, loss is 0.17209820449352264\n",
      "epoch: 8 step: 125, loss is 1.4121233224868774\n",
      "epoch: 8 step: 126, loss is 0.07843644171953201\n",
      "epoch: 8 step: 127, loss is 0.3790777027606964\n",
      "epoch: 8 step: 128, loss is 0.2530655562877655\n",
      "epoch: 8 step: 129, loss is 0.6920689940452576\n",
      "epoch: 8 step: 130, loss is 0.7655329704284668\n",
      "epoch: 8 step: 131, loss is 0.37822648882865906\n",
      "epoch: 8 step: 132, loss is 0.21657730638980865\n",
      "epoch: 8 step: 133, loss is 0.27544036507606506\n",
      "epoch: 8 step: 134, loss is 0.17211276292800903\n",
      "epoch: 8 step: 135, loss is 0.40096133947372437\n",
      "epoch: 8 step: 136, loss is 0.30203351378440857\n",
      "epoch: 8 step: 137, loss is 0.10918233543634415\n",
      "epoch: 8 step: 138, loss is 0.5843678116798401\n",
      "epoch: 8 step: 139, loss is 0.17270301282405853\n",
      "epoch: 8 step: 140, loss is 0.42658326029777527\n",
      "epoch: 8 step: 141, loss is 0.3067117929458618\n",
      "epoch: 8 step: 142, loss is 0.261524498462677\n",
      "epoch: 8 step: 143, loss is 0.18436966836452484\n",
      "epoch: 8 step: 144, loss is 0.26214802265167236\n",
      "epoch: 8 step: 145, loss is 0.48370495438575745\n",
      "epoch: 8 step: 146, loss is 1.051703929901123\n",
      "epoch: 8 step: 147, loss is 0.067012719810009\n",
      "epoch: 8 step: 148, loss is 0.30707255005836487\n",
      "epoch: 8 step: 149, loss is 0.1295376867055893\n",
      "epoch: 8 step: 150, loss is 0.20911161601543427\n",
      "epoch: 8 step: 151, loss is 0.44491299986839294\n",
      "epoch: 8 step: 152, loss is 0.47725939750671387\n",
      "epoch: 8 step: 153, loss is 0.5302504897117615\n",
      "epoch: 8 step: 154, loss is 0.07163950055837631\n",
      "epoch: 8 step: 155, loss is 0.22471235692501068\n",
      "epoch: 8 step: 156, loss is 0.22328664362430573\n",
      "epoch: 8 step: 157, loss is 0.392404705286026\n",
      "epoch: 8 step: 158, loss is 0.5514975786209106\n",
      "epoch: 8 step: 159, loss is 0.11659905314445496\n",
      "epoch: 8 step: 160, loss is 0.17030693590641022\n",
      "epoch: 8 step: 161, loss is 0.11873335391283035\n",
      "epoch: 8 step: 162, loss is 0.7850252985954285\n",
      "epoch: 8 step: 163, loss is 0.0871155634522438\n",
      "epoch: 8 step: 164, loss is 0.11506225913763046\n",
      "epoch: 8 step: 165, loss is 0.173776313662529\n",
      "epoch: 8 step: 166, loss is 0.09560201317071915\n",
      "epoch: 8 step: 167, loss is 1.402369737625122\n",
      "epoch: 8 step: 168, loss is 0.38515985012054443\n",
      "epoch: 8 step: 169, loss is 0.1687583029270172\n",
      "epoch: 8 step: 170, loss is 0.25214070081710815\n",
      "epoch: 8 step: 171, loss is 0.30827832221984863\n",
      "epoch: 8 step: 172, loss is 0.19654998183250427\n",
      "epoch: 8 step: 173, loss is 0.23245340585708618\n",
      "epoch: 8 step: 174, loss is 0.04090048372745514\n",
      "epoch: 8 step: 175, loss is 0.3787963390350342\n",
      "epoch: 8 step: 176, loss is 0.21700479090213776\n",
      "epoch: 8 step: 177, loss is 0.291959673166275\n",
      "epoch: 8 step: 178, loss is 0.19631095230579376\n",
      "epoch: 8 step: 179, loss is 0.22124025225639343\n",
      "epoch: 8 step: 180, loss is 0.05349317938089371\n",
      "epoch: 8 step: 181, loss is 0.1432158350944519\n",
      "epoch: 8 step: 182, loss is 0.10698124021291733\n",
      "epoch: 8 step: 183, loss is 0.24856553971767426\n",
      "epoch: 8 step: 184, loss is 0.09648782759904861\n",
      "epoch: 8 step: 185, loss is 0.10522079467773438\n",
      "epoch: 8 step: 186, loss is 0.1673848181962967\n",
      "epoch: 8 step: 187, loss is 0.16789516806602478\n",
      "epoch: 8 step: 188, loss is 0.155846506357193\n",
      "epoch: 8 step: 189, loss is 0.41622453927993774\n",
      "epoch: 8 step: 190, loss is 0.11976761370897293\n",
      "epoch: 8 step: 191, loss is 0.10289384424686432\n",
      "epoch: 8 step: 192, loss is 0.5076990127563477\n",
      "epoch: 8 step: 193, loss is 0.1626228392124176\n",
      "epoch: 8 step: 194, loss is 0.09471925348043442\n",
      "epoch: 8 step: 195, loss is 0.40508654713630676\n",
      "epoch: 8 step: 196, loss is 0.3213098645210266\n",
      "epoch: 8 step: 197, loss is 0.9155105948448181\n",
      "epoch: 8 step: 198, loss is 0.40099185705184937\n",
      "epoch: 8 step: 199, loss is 0.5392822623252869\n",
      "epoch: 8 step: 200, loss is 0.17822852730751038\n",
      "epoch: 8 step: 201, loss is 0.12305806577205658\n",
      "epoch: 8 step: 202, loss is 0.10538114607334137\n",
      "epoch: 8 step: 203, loss is 0.0876893699169159\n",
      "epoch: 8 step: 204, loss is 0.3619052767753601\n",
      "epoch: 8 step: 205, loss is 0.3554237484931946\n",
      "epoch: 8 step: 206, loss is 0.09615927934646606\n",
      "epoch: 8 step: 207, loss is 0.2672293186187744\n",
      "epoch: 8 step: 208, loss is 0.041292574256658554\n",
      "epoch: 8 step: 209, loss is 0.47071635723114014\n",
      "epoch: 8 step: 210, loss is 0.2958483099937439\n",
      "epoch: 8 step: 211, loss is 0.22184745967388153\n",
      "epoch: 8 step: 212, loss is 0.35390257835388184\n",
      "epoch: 8 step: 213, loss is 0.08761807531118393\n",
      "epoch: 8 step: 214, loss is 0.06925386190414429\n",
      "epoch: 8 step: 215, loss is 0.14406540989875793\n",
      "epoch: 8 step: 216, loss is 0.6050117015838623\n",
      "epoch: 8 step: 217, loss is 0.43780606985092163\n",
      "epoch: 8 step: 218, loss is 0.18241173028945923\n",
      "epoch: 8 step: 219, loss is 0.16586893796920776\n",
      "epoch: 8 step: 220, loss is 0.2856892943382263\n",
      "epoch: 8 step: 221, loss is 0.822077214717865\n",
      "epoch: 8 step: 222, loss is 0.13332636654376984\n",
      "epoch: 8 step: 223, loss is 0.05455447733402252\n",
      "epoch: 8 step: 224, loss is 0.056492019444704056\n",
      "epoch: 8 step: 225, loss is 0.11810440570116043\n",
      "epoch: 8 step: 226, loss is 0.3581227958202362\n",
      "epoch: 8 step: 227, loss is 0.5067793726921082\n",
      "epoch: 8 step: 228, loss is 0.29454824328422546\n",
      "epoch: 8 step: 229, loss is 0.0618547759950161\n",
      "epoch: 8 step: 230, loss is 0.25165626406669617\n",
      "epoch: 8 step: 231, loss is 0.24667596817016602\n",
      "epoch: 8 step: 232, loss is 0.6430537104606628\n",
      "epoch: 8 step: 233, loss is 0.1587793081998825\n",
      "epoch: 8 step: 234, loss is 0.6886537075042725\n",
      "epoch: 8 step: 235, loss is 0.31919151544570923\n",
      "epoch: 8 step: 236, loss is 0.16243408620357513\n",
      "epoch: 8 step: 237, loss is 0.052218854427337646\n",
      "epoch: 8 step: 238, loss is 0.7148966193199158\n",
      "epoch: 8 step: 239, loss is 0.16607415676116943\n",
      "epoch: 8 step: 240, loss is 0.0550287663936615\n",
      "epoch: 8 step: 241, loss is 0.25935620069503784\n",
      "epoch: 8 step: 242, loss is 0.207588791847229\n",
      "epoch: 8 step: 243, loss is 0.18976856768131256\n",
      "epoch: 8 step: 244, loss is 0.3003706932067871\n",
      "epoch: 8 step: 245, loss is 0.3920551538467407\n",
      "epoch: 8 step: 246, loss is 0.1332365870475769\n",
      "epoch: 8 step: 247, loss is 0.16358117759227753\n",
      "epoch: 8 step: 248, loss is 0.3686322271823883\n",
      "epoch: 8 step: 249, loss is 0.21137075126171112\n",
      "epoch: 8 step: 250, loss is 0.8823250532150269\n",
      "epoch: 8 step: 251, loss is 0.05595389008522034\n",
      "epoch: 8 step: 252, loss is 0.5408743619918823\n",
      "epoch: 8 step: 253, loss is 0.18326446413993835\n",
      "epoch: 8 step: 254, loss is 0.0887439176440239\n",
      "epoch: 8 step: 255, loss is 0.057586412876844406\n",
      "epoch: 8 step: 256, loss is 0.14044822752475739\n",
      "epoch: 8 step: 257, loss is 0.1236276626586914\n",
      "epoch: 8 step: 258, loss is 0.17849425971508026\n",
      "epoch: 8 step: 259, loss is 0.24740786850452423\n",
      "epoch: 8 step: 260, loss is 0.49824023246765137\n",
      "epoch: 8 step: 261, loss is 0.5754098296165466\n",
      "epoch: 8 step: 262, loss is 0.24165776371955872\n",
      "epoch: 8 step: 263, loss is 0.13145579397678375\n",
      "epoch: 8 step: 264, loss is 0.06412309408187866\n",
      "epoch: 8 step: 265, loss is 0.024700429290533066\n",
      "epoch: 8 step: 266, loss is 0.10277185589075089\n",
      "epoch: 8 step: 267, loss is 0.4519546329975128\n",
      "epoch: 8 step: 268, loss is 0.08976224809885025\n",
      "epoch: 8 step: 269, loss is 0.7001073956489563\n",
      "epoch: 8 step: 270, loss is 0.2820424735546112\n",
      "epoch: 8 step: 271, loss is 0.06446757167577744\n",
      "epoch: 8 step: 272, loss is 0.4105660021305084\n",
      "epoch: 8 step: 273, loss is 0.2475549429655075\n",
      "epoch: 8 step: 274, loss is 0.05412354692816734\n",
      "epoch: 8 step: 275, loss is 0.031857725232839584\n",
      "epoch: 8 step: 276, loss is 0.10962923616170883\n",
      "epoch: 8 step: 277, loss is 0.08384387195110321\n",
      "epoch: 8 step: 278, loss is 0.3304867446422577\n",
      "epoch: 8 step: 279, loss is 0.08814109861850739\n",
      "epoch: 8 step: 280, loss is 0.43755972385406494\n",
      "epoch: 8 step: 281, loss is 0.12986281514167786\n",
      "epoch: 8 step: 282, loss is 0.9067888259887695\n",
      "epoch: 8 step: 283, loss is 0.13454170525074005\n",
      "epoch: 8 step: 284, loss is 0.3875057101249695\n",
      "epoch: 8 step: 285, loss is 0.14976930618286133\n",
      "epoch: 8 step: 286, loss is 0.14865738153457642\n",
      "epoch: 8 step: 287, loss is 0.3748110830783844\n",
      "epoch: 8 step: 288, loss is 0.7017434239387512\n",
      "epoch: 8 step: 289, loss is 0.21081142127513885\n",
      "epoch: 8 step: 290, loss is 0.3254350423812866\n",
      "epoch: 8 step: 291, loss is 0.18828734755516052\n",
      "epoch: 8 step: 292, loss is 0.38817253708839417\n",
      "epoch: 8 step: 293, loss is 0.04238373413681984\n",
      "epoch: 8 step: 294, loss is 0.6516574621200562\n",
      "epoch: 8 step: 295, loss is 0.09485025703907013\n",
      "epoch: 8 step: 296, loss is 0.1423047035932541\n",
      "epoch: 8 step: 297, loss is 0.23313848674297333\n",
      "epoch: 8 step: 298, loss is 0.27604031562805176\n",
      "epoch: 8 step: 299, loss is 0.06380084902048111\n",
      "epoch: 8 step: 300, loss is 0.11908651888370514\n",
      "epoch: 8 step: 301, loss is 0.33390292525291443\n",
      "epoch: 8 step: 302, loss is 0.22728025913238525\n",
      "epoch: 8 step: 303, loss is 0.15417246520519257\n",
      "epoch: 8 step: 304, loss is 0.3782327473163605\n",
      "epoch: 8 step: 305, loss is 0.17127485573291779\n",
      "epoch: 8 step: 306, loss is 0.22849707305431366\n",
      "epoch: 8 step: 307, loss is 0.0518839992582798\n",
      "epoch: 8 step: 308, loss is 0.4948976933956146\n",
      "epoch: 8 step: 309, loss is 0.472153902053833\n",
      "epoch: 8 step: 310, loss is 0.15978838503360748\n",
      "epoch: 8 step: 311, loss is 0.43044087290763855\n",
      "epoch: 8 step: 312, loss is 0.6746407747268677\n",
      "epoch: 8 step: 313, loss is 0.1728190779685974\n",
      "epoch: 8 step: 314, loss is 0.19326092302799225\n",
      "epoch: 8 step: 315, loss is 0.9248334169387817\n",
      "epoch: 8 step: 316, loss is 0.26048988103866577\n",
      "epoch: 8 step: 317, loss is 0.18104729056358337\n",
      "epoch: 8 step: 318, loss is 0.45748981833457947\n",
      "epoch: 8 step: 319, loss is 0.5202745199203491\n",
      "epoch: 8 step: 320, loss is 0.5453125834465027\n",
      "epoch: 8 step: 321, loss is 0.28929999470710754\n",
      "epoch: 8 step: 322, loss is 0.15780600905418396\n",
      "epoch: 8 step: 323, loss is 0.07439818233251572\n",
      "epoch: 8 step: 324, loss is 0.08057977259159088\n",
      "epoch: 8 step: 325, loss is 0.09180156141519547\n",
      "epoch: 8 step: 326, loss is 0.3583655059337616\n",
      "epoch: 8 step: 327, loss is 0.0993654653429985\n",
      "epoch: 8 step: 328, loss is 0.10834556072950363\n",
      "epoch: 8 step: 329, loss is 0.14454905688762665\n",
      "epoch: 8 step: 330, loss is 0.8907221555709839\n",
      "epoch: 8 step: 331, loss is 0.38589078187942505\n",
      "epoch: 8 step: 332, loss is 0.7157029509544373\n",
      "epoch: 8 step: 333, loss is 0.3203856348991394\n",
      "epoch: 8 step: 334, loss is 0.1589639037847519\n",
      "epoch: 8 step: 335, loss is 0.10333850234746933\n",
      "epoch: 8 step: 336, loss is 0.3763207495212555\n",
      "epoch: 8 step: 337, loss is 0.07600894570350647\n",
      "epoch: 8 step: 338, loss is 0.10589727014303207\n",
      "epoch: 8 step: 339, loss is 0.19465307891368866\n",
      "epoch: 8 step: 340, loss is 0.32570168375968933\n",
      "epoch: 8 step: 341, loss is 0.11195458471775055\n",
      "epoch: 8 step: 342, loss is 0.15702034533023834\n",
      "epoch: 8 step: 343, loss is 0.14144058525562286\n",
      "epoch: 8 step: 344, loss is 0.20181024074554443\n",
      "epoch: 8 step: 345, loss is 0.194350928068161\n",
      "epoch: 8 step: 346, loss is 0.3081147074699402\n",
      "epoch: 8 step: 347, loss is 0.2397918403148651\n",
      "epoch: 8 step: 348, loss is 0.3449310064315796\n",
      "epoch: 8 step: 349, loss is 0.13942889869213104\n",
      "epoch: 8 step: 350, loss is 0.13413572311401367\n",
      "epoch: 8 step: 351, loss is 0.29454776644706726\n",
      "epoch: 8 step: 352, loss is 0.17516794800758362\n",
      "epoch: 8 step: 353, loss is 0.5761675238609314\n",
      "epoch: 8 step: 354, loss is 0.3040882647037506\n",
      "epoch: 8 step: 355, loss is 0.3253180980682373\n",
      "epoch: 8 step: 356, loss is 0.09919456392526627\n",
      "epoch: 8 step: 357, loss is 0.06630965322256088\n",
      "epoch: 8 step: 358, loss is 0.26172685623168945\n",
      "epoch: 8 step: 359, loss is 0.048359062522649765\n",
      "epoch: 8 step: 360, loss is 0.3620746433734894\n",
      "epoch: 8 step: 361, loss is 0.10811233520507812\n",
      "epoch: 8 step: 362, loss is 0.16032767295837402\n",
      "epoch: 8 step: 363, loss is 0.1977161467075348\n",
      "epoch: 8 step: 364, loss is 0.13026845455169678\n",
      "epoch: 8 step: 365, loss is 0.2800125479698181\n",
      "epoch: 8 step: 366, loss is 0.06971946358680725\n",
      "epoch: 8 step: 367, loss is 0.040949974209070206\n",
      "epoch: 8 step: 368, loss is 0.1291515976190567\n",
      "epoch: 8 step: 369, loss is 0.3570873439311981\n",
      "epoch: 8 step: 370, loss is 0.20573702454566956\n",
      "epoch: 8 step: 371, loss is 0.342337042093277\n",
      "epoch: 8 step: 372, loss is 0.15532095730304718\n",
      "epoch: 8 step: 373, loss is 0.20295128226280212\n",
      "epoch: 8 step: 374, loss is 0.03399493545293808\n",
      "epoch: 8 step: 375, loss is 0.4444335103034973\n",
      "epoch: 8 step: 376, loss is 0.20661410689353943\n",
      "epoch: 8 step: 377, loss is 0.23241759836673737\n",
      "epoch: 8 step: 378, loss is 0.151868537068367\n",
      "epoch: 8 step: 379, loss is 0.559845507144928\n",
      "epoch: 8 step: 380, loss is 0.20736974477767944\n",
      "epoch: 8 step: 381, loss is 0.6005260944366455\n",
      "epoch: 8 step: 382, loss is 0.11866217851638794\n",
      "epoch: 8 step: 383, loss is 0.15904362499713898\n",
      "epoch: 8 step: 384, loss is 0.3319774270057678\n",
      "epoch: 8 step: 385, loss is 0.23719820380210876\n",
      "epoch: 8 step: 386, loss is 0.18563705682754517\n",
      "epoch: 8 step: 387, loss is 0.2589317262172699\n",
      "epoch: 8 step: 388, loss is 0.6280125379562378\n",
      "epoch: 8 step: 389, loss is 0.3853638172149658\n",
      "epoch: 8 step: 390, loss is 0.10656926035881042\n",
      "epoch: 8 step: 391, loss is 0.43983128666877747\n",
      "epoch: 8 step: 392, loss is 0.6095978021621704\n",
      "epoch: 8 step: 393, loss is 0.41330814361572266\n",
      "epoch: 8 step: 394, loss is 0.2289579212665558\n",
      "epoch: 8 step: 395, loss is 0.326236754655838\n",
      "epoch: 8 step: 396, loss is 0.3513006269931793\n",
      "epoch: 8 step: 397, loss is 0.2951784133911133\n",
      "epoch: 8 step: 398, loss is 0.2572418451309204\n",
      "epoch: 8 step: 399, loss is 0.6316720843315125\n",
      "epoch: 8 step: 400, loss is 0.15806861221790314\n",
      "epoch: 8 step: 401, loss is 0.3363199234008789\n",
      "epoch: 8 step: 402, loss is 0.12466370314359665\n",
      "epoch: 8 step: 403, loss is 0.329440176486969\n",
      "epoch: 8 step: 404, loss is 0.11192021518945694\n",
      "epoch: 8 step: 405, loss is 0.34848901629447937\n",
      "epoch: 8 step: 406, loss is 0.131364643573761\n",
      "epoch: 8 step: 407, loss is 0.13015790283679962\n",
      "epoch: 8 step: 408, loss is 0.4049873352050781\n",
      "epoch: 8 step: 409, loss is 0.4803692400455475\n",
      "epoch: 8 step: 410, loss is 0.6647225022315979\n",
      "epoch: 8 step: 411, loss is 0.14615945518016815\n",
      "epoch: 8 step: 412, loss is 0.4539119303226471\n",
      "epoch: 8 step: 413, loss is 0.6326960325241089\n",
      "epoch: 8 step: 414, loss is 0.2376309186220169\n",
      "epoch: 8 step: 415, loss is 0.586179256439209\n",
      "epoch: 8 step: 416, loss is 0.10672954469919205\n",
      "epoch: 8 step: 417, loss is 0.3388507664203644\n",
      "epoch: 8 step: 418, loss is 0.11660578101873398\n",
      "epoch: 8 step: 419, loss is 0.4712669849395752\n",
      "epoch: 8 step: 420, loss is 0.2556849718093872\n",
      "epoch: 8 step: 421, loss is 0.4028705060482025\n",
      "epoch: 8 step: 422, loss is 0.15190356969833374\n",
      "epoch: 8 step: 423, loss is 0.1730450987815857\n",
      "epoch: 8 step: 424, loss is 0.09408047050237656\n",
      "epoch: 8 step: 425, loss is 0.36449697613716125\n",
      "epoch: 8 step: 426, loss is 0.43744391202926636\n",
      "epoch: 8 step: 427, loss is 0.42182624340057373\n",
      "epoch: 8 step: 428, loss is 0.1759340614080429\n",
      "epoch: 8 step: 429, loss is 0.14414989948272705\n",
      "epoch: 8 step: 430, loss is 0.32164615392684937\n",
      "epoch: 8 step: 431, loss is 0.19425855576992035\n",
      "epoch: 8 step: 432, loss is 0.07930777221918106\n",
      "epoch: 8 step: 433, loss is 0.32171934843063354\n",
      "epoch: 8 step: 434, loss is 0.34877336025238037\n",
      "epoch: 8 step: 435, loss is 0.38116323947906494\n",
      "epoch: 8 step: 436, loss is 0.31494462490081787\n",
      "epoch: 8 step: 437, loss is 0.0880323275923729\n",
      "epoch: 8 step: 438, loss is 0.13975973427295685\n",
      "epoch: 8 step: 439, loss is 0.22736820578575134\n",
      "epoch: 8 step: 440, loss is 0.420357882976532\n",
      "epoch: 8 step: 441, loss is 0.356477826833725\n",
      "epoch: 8 step: 442, loss is 0.5452027916908264\n",
      "epoch: 8 step: 443, loss is 0.5701727867126465\n",
      "epoch: 8 step: 444, loss is 0.21348489820957184\n",
      "epoch: 8 step: 445, loss is 0.15606410801410675\n",
      "epoch: 8 step: 446, loss is 0.07013477385044098\n",
      "epoch: 8 step: 447, loss is 0.6537091135978699\n",
      "epoch: 8 step: 448, loss is 0.1078493595123291\n",
      "epoch: 8 step: 449, loss is 0.052088413387537\n",
      "epoch: 8 step: 450, loss is 0.1858394593000412\n",
      "epoch: 8 step: 451, loss is 0.2834426164627075\n",
      "epoch: 8 step: 452, loss is 0.09937209635972977\n",
      "epoch: 8 step: 453, loss is 0.19422347843647003\n",
      "epoch: 8 step: 454, loss is 0.20848575234413147\n",
      "epoch: 8 step: 455, loss is 0.1698143035173416\n",
      "epoch: 8 step: 456, loss is 0.8504272103309631\n",
      "epoch: 8 step: 457, loss is 0.34664207696914673\n",
      "epoch: 8 step: 458, loss is 0.02519669383764267\n",
      "epoch: 8 step: 459, loss is 0.0726066529750824\n",
      "epoch: 8 step: 460, loss is 0.0949910432100296\n",
      "epoch: 8 step: 461, loss is 0.24961142241954803\n",
      "epoch: 8 step: 462, loss is 0.1377723664045334\n",
      "epoch: 8 step: 463, loss is 0.05951697379350662\n",
      "epoch: 8 step: 464, loss is 0.1121906042098999\n",
      "epoch: 8 step: 465, loss is 0.27689841389656067\n",
      "epoch: 8 step: 466, loss is 0.22184593975543976\n",
      "epoch: 8 step: 467, loss is 0.1261366605758667\n",
      "epoch: 8 step: 468, loss is 0.4930807650089264\n",
      "epoch: 8 step: 469, loss is 0.2230304628610611\n",
      "epoch: 8 step: 470, loss is 0.14243361353874207\n",
      "epoch: 8 step: 471, loss is 0.17043405771255493\n",
      "epoch: 8 step: 472, loss is 0.12472888827323914\n",
      "epoch: 8 step: 473, loss is 0.3375239372253418\n",
      "epoch: 8 step: 474, loss is 0.05573766306042671\n",
      "epoch: 8 step: 475, loss is 0.04372502863407135\n",
      "epoch: 8 step: 476, loss is 0.12810562551021576\n",
      "epoch: 8 step: 477, loss is 0.18788743019104004\n",
      "epoch: 8 step: 478, loss is 0.06066330894827843\n",
      "epoch: 8 step: 479, loss is 0.12169863283634186\n",
      "epoch: 8 step: 480, loss is 0.5058402419090271\n",
      "epoch: 8 step: 481, loss is 0.19184987246990204\n",
      "epoch: 8 step: 482, loss is 0.8330724835395813\n",
      "epoch: 8 step: 483, loss is 0.05539353936910629\n",
      "epoch: 8 step: 484, loss is 0.13851693272590637\n",
      "epoch: 8 step: 485, loss is 0.13159509003162384\n",
      "epoch: 8 step: 486, loss is 0.035167496651411057\n",
      "epoch: 8 step: 487, loss is 0.14974531531333923\n",
      "epoch: 8 step: 488, loss is 0.11479650437831879\n",
      "epoch: 8 step: 489, loss is 0.5595555901527405\n",
      "epoch: 8 step: 490, loss is 0.24490274488925934\n",
      "epoch: 8 step: 491, loss is 0.18366242945194244\n",
      "epoch: 8 step: 492, loss is 0.5748636722564697\n",
      "epoch: 8 step: 493, loss is 0.2789548635482788\n",
      "epoch: 8 step: 494, loss is 0.011804651468992233\n",
      "epoch: 8 step: 495, loss is 0.3338592052459717\n",
      "epoch: 8 step: 496, loss is 0.42854195833206177\n",
      "epoch: 8 step: 497, loss is 0.4015481173992157\n",
      "epoch: 8 step: 498, loss is 0.2378040850162506\n",
      "epoch: 8 step: 499, loss is 0.07688341289758682\n",
      "epoch: 8 step: 500, loss is 0.16131722927093506\n",
      "epoch: 8 step: 501, loss is 0.05572650209069252\n",
      "epoch: 8 step: 502, loss is 0.0866454690694809\n",
      "epoch: 8 step: 503, loss is 0.12706421315670013\n",
      "epoch: 8 step: 504, loss is 0.5420112609863281\n",
      "epoch: 8 step: 505, loss is 0.26009732484817505\n",
      "epoch: 8 step: 506, loss is 0.17425668239593506\n",
      "epoch: 8 step: 507, loss is 0.1598273068666458\n",
      "epoch: 8 step: 508, loss is 0.4338511526584625\n",
      "epoch: 8 step: 509, loss is 0.16145962476730347\n",
      "epoch: 8 step: 510, loss is 0.25219783186912537\n",
      "epoch: 8 step: 511, loss is 0.5129936933517456\n",
      "epoch: 8 step: 512, loss is 0.18793204426765442\n",
      "epoch: 8 step: 513, loss is 0.11241057515144348\n",
      "epoch: 8 step: 514, loss is 0.18212385475635529\n",
      "epoch: 8 step: 515, loss is 0.10069216787815094\n",
      "epoch: 8 step: 516, loss is 0.2751525044441223\n",
      "epoch: 8 step: 517, loss is 0.4033600986003876\n",
      "epoch: 8 step: 518, loss is 0.6378849148750305\n",
      "epoch: 8 step: 519, loss is 0.7072678208351135\n",
      "epoch: 8 step: 520, loss is 0.47149914503097534\n",
      "epoch: 8 step: 521, loss is 0.3214327394962311\n",
      "epoch: 8 step: 522, loss is 0.22870635986328125\n",
      "epoch: 8 step: 523, loss is 0.7580797076225281\n",
      "epoch: 8 step: 524, loss is 0.13048698008060455\n",
      "epoch: 8 step: 525, loss is 0.14324843883514404\n",
      "epoch: 8 step: 526, loss is 0.3276936113834381\n",
      "epoch: 8 step: 527, loss is 0.14876827597618103\n",
      "epoch: 8 step: 528, loss is 0.05040460079908371\n",
      "epoch: 8 step: 529, loss is 0.28805679082870483\n",
      "epoch: 8 step: 530, loss is 0.4143890142440796\n",
      "epoch: 8 step: 531, loss is 0.03211602196097374\n",
      "epoch: 8 step: 532, loss is 0.012139344587922096\n",
      "epoch: 8 step: 533, loss is 0.1911567598581314\n",
      "epoch: 8 step: 534, loss is 0.16746988892555237\n",
      "epoch: 8 step: 535, loss is 0.11784599721431732\n",
      "epoch: 8 step: 536, loss is 0.525562047958374\n",
      "epoch: 8 step: 537, loss is 0.10166211426258087\n",
      "epoch: 8 step: 538, loss is 0.22473929822444916\n",
      "epoch: 8 step: 539, loss is 0.3434956669807434\n",
      "epoch: 8 step: 540, loss is 0.1745990663766861\n",
      "epoch: 8 step: 541, loss is 0.3112183213233948\n",
      "epoch: 8 step: 542, loss is 0.12436982244253159\n",
      "epoch: 8 step: 543, loss is 0.3565816283226013\n",
      "epoch: 8 step: 544, loss is 0.48042792081832886\n",
      "epoch: 8 step: 545, loss is 0.5046294331550598\n",
      "epoch: 8 step: 546, loss is 0.3455275893211365\n",
      "epoch: 8 step: 547, loss is 0.4082057774066925\n",
      "epoch: 8 step: 548, loss is 0.23357133567333221\n",
      "epoch: 8 step: 549, loss is 0.0721340924501419\n",
      "epoch: 8 step: 550, loss is 0.09012310951948166\n",
      "epoch: 8 step: 551, loss is 0.47821906208992004\n",
      "epoch: 8 step: 552, loss is 0.2191237360239029\n",
      "epoch: 8 step: 553, loss is 0.14803263545036316\n",
      "epoch: 8 step: 554, loss is 0.6547775268554688\n",
      "epoch: 8 step: 555, loss is 0.14921265840530396\n",
      "epoch: 8 step: 556, loss is 0.478583961725235\n",
      "epoch: 8 step: 557, loss is 0.33966776728630066\n",
      "epoch: 8 step: 558, loss is 0.08323661237955093\n",
      "epoch: 8 step: 559, loss is 0.21704335510730743\n",
      "epoch: 8 step: 560, loss is 0.11909753084182739\n",
      "epoch: 8 step: 561, loss is 0.43503233790397644\n",
      "epoch: 8 step: 562, loss is 0.27828913927078247\n",
      "epoch: 8 step: 563, loss is 0.6527697443962097\n",
      "epoch: 8 step: 564, loss is 0.8846084475517273\n",
      "epoch: 8 step: 565, loss is 0.6926801800727844\n",
      "epoch: 8 step: 566, loss is 0.6420408487319946\n",
      "epoch: 8 step: 567, loss is 0.3392188549041748\n",
      "epoch: 8 step: 568, loss is 0.05441571772098541\n",
      "epoch: 8 step: 569, loss is 0.18111172318458557\n",
      "epoch: 8 step: 570, loss is 0.13130129873752594\n",
      "epoch: 8 step: 571, loss is 0.28465187549591064\n",
      "epoch: 8 step: 572, loss is 0.29808279871940613\n",
      "epoch: 8 step: 573, loss is 0.19898387789726257\n",
      "epoch: 8 step: 574, loss is 0.655462920665741\n",
      "epoch: 8 step: 575, loss is 0.18932680785655975\n",
      "epoch: 8 step: 576, loss is 0.11129449307918549\n",
      "epoch: 8 step: 577, loss is 0.21854087710380554\n",
      "epoch: 8 step: 578, loss is 0.02997332625091076\n",
      "epoch: 8 step: 579, loss is 0.1960352063179016\n",
      "epoch: 8 step: 580, loss is 0.3347320556640625\n",
      "epoch: 8 step: 581, loss is 0.10202401131391525\n",
      "epoch: 8 step: 582, loss is 0.10303190350532532\n",
      "epoch: 8 step: 583, loss is 0.3973953425884247\n",
      "epoch: 8 step: 584, loss is 0.0899166464805603\n",
      "epoch: 8 step: 585, loss is 1.0640003681182861\n",
      "epoch: 8 step: 586, loss is 0.1455378532409668\n",
      "epoch: 8 step: 587, loss is 0.2773744761943817\n",
      "epoch: 8 step: 588, loss is 0.3152428865432739\n",
      "epoch: 8 step: 589, loss is 0.6185033917427063\n",
      "epoch: 8 step: 590, loss is 0.16484121978282928\n",
      "epoch: 8 step: 591, loss is 0.3131849765777588\n",
      "epoch: 8 step: 592, loss is 0.22150522470474243\n",
      "epoch: 8 step: 593, loss is 0.16921423375606537\n",
      "epoch: 8 step: 594, loss is 0.12355176359415054\n",
      "epoch: 8 step: 595, loss is 0.21407735347747803\n",
      "epoch: 8 step: 596, loss is 0.49902859330177307\n",
      "epoch: 8 step: 597, loss is 0.17568783462047577\n",
      "epoch: 8 step: 598, loss is 0.07892186939716339\n",
      "epoch: 8 step: 599, loss is 0.22602654993534088\n",
      "epoch: 8 step: 600, loss is 0.1657279133796692\n",
      "epoch: 8 step: 601, loss is 0.11691448092460632\n",
      "epoch: 8 step: 602, loss is 0.15328910946846008\n",
      "epoch: 8 step: 603, loss is 0.19570328295230865\n",
      "epoch: 8 step: 604, loss is 0.23769018054008484\n",
      "epoch: 8 step: 605, loss is 0.19855573773384094\n",
      "epoch: 8 step: 606, loss is 0.1041392907500267\n",
      "epoch: 8 step: 607, loss is 0.13199011981487274\n",
      "epoch: 8 step: 608, loss is 0.16685959696769714\n",
      "epoch: 8 step: 609, loss is 0.37153759598731995\n",
      "epoch: 8 step: 610, loss is 0.040189359337091446\n",
      "epoch: 8 step: 611, loss is 0.8026599287986755\n",
      "epoch: 8 step: 612, loss is 0.1097782552242279\n",
      "epoch: 8 step: 613, loss is 0.1426950842142105\n",
      "epoch: 8 step: 614, loss is 0.19308769702911377\n",
      "epoch: 8 step: 615, loss is 0.024423930794000626\n",
      "epoch: 8 step: 616, loss is 0.3924357295036316\n",
      "epoch: 8 step: 617, loss is 0.26874974370002747\n",
      "epoch: 8 step: 618, loss is 0.2771936357021332\n",
      "epoch: 8 step: 619, loss is 0.25320956110954285\n",
      "epoch: 8 step: 620, loss is 0.08907869458198547\n",
      "epoch: 8 step: 621, loss is 0.743495762348175\n",
      "epoch: 8 step: 622, loss is 0.3636479675769806\n",
      "epoch: 8 step: 623, loss is 0.9687350392341614\n",
      "epoch: 8 step: 624, loss is 0.15231341123580933\n",
      "epoch: 8 step: 625, loss is 0.23554813861846924\n",
      "epoch: 8 step: 626, loss is 0.2778536379337311\n",
      "epoch: 8 step: 627, loss is 0.3073931038379669\n",
      "epoch: 8 step: 628, loss is 0.3842257857322693\n",
      "epoch: 8 step: 629, loss is 0.38161054253578186\n",
      "epoch: 8 step: 630, loss is 0.4435048997402191\n",
      "epoch: 8 step: 631, loss is 0.239222452044487\n",
      "epoch: 8 step: 632, loss is 0.09737648814916611\n",
      "epoch: 8 step: 633, loss is 0.45149946212768555\n",
      "epoch: 8 step: 634, loss is 0.017503749579191208\n",
      "epoch: 8 step: 635, loss is 0.10674288123846054\n",
      "epoch: 8 step: 636, loss is 0.15726016461849213\n",
      "epoch: 8 step: 637, loss is 0.09497367590665817\n",
      "epoch: 8 step: 638, loss is 0.20886069536209106\n",
      "epoch: 8 step: 639, loss is 0.2759554386138916\n",
      "epoch: 8 step: 640, loss is 0.2748493254184723\n",
      "epoch: 8 step: 641, loss is 0.13798215985298157\n",
      "epoch: 8 step: 642, loss is 0.14142607152462006\n",
      "epoch: 8 step: 643, loss is 0.17636781930923462\n",
      "epoch: 8 step: 644, loss is 0.456815630197525\n",
      "epoch: 8 step: 645, loss is 0.3424931764602661\n",
      "epoch: 8 step: 646, loss is 0.4836966395378113\n",
      "epoch: 8 step: 647, loss is 0.15576761960983276\n",
      "epoch: 8 step: 648, loss is 0.05220216140151024\n",
      "epoch: 8 step: 649, loss is 0.08648757636547089\n",
      "epoch: 8 step: 650, loss is 0.23602065443992615\n",
      "epoch: 8 step: 651, loss is 0.145494282245636\n",
      "epoch: 8 step: 652, loss is 0.13109856843948364\n",
      "epoch: 8 step: 653, loss is 0.17209897935390472\n",
      "epoch: 8 step: 654, loss is 0.21946482360363007\n",
      "epoch: 8 step: 655, loss is 0.04023953899741173\n",
      "epoch: 8 step: 656, loss is 0.16654907166957855\n",
      "epoch: 8 step: 657, loss is 0.06623964756727219\n",
      "epoch: 8 step: 658, loss is 0.5576431751251221\n",
      "epoch: 8 step: 659, loss is 0.20643900334835052\n",
      "epoch: 8 step: 660, loss is 0.01646491326391697\n",
      "epoch: 8 step: 661, loss is 0.2751753628253937\n",
      "epoch: 8 step: 662, loss is 0.30772775411605835\n",
      "epoch: 8 step: 663, loss is 0.07784440368413925\n",
      "epoch: 8 step: 664, loss is 0.15139465034008026\n",
      "epoch: 8 step: 665, loss is 0.5931609869003296\n",
      "epoch: 8 step: 666, loss is 0.3221680223941803\n",
      "epoch: 8 step: 667, loss is 0.19782818853855133\n",
      "epoch: 8 step: 668, loss is 0.7875807285308838\n",
      "epoch: 8 step: 669, loss is 0.06231909245252609\n",
      "epoch: 8 step: 670, loss is 0.048551399260759354\n",
      "epoch: 8 step: 671, loss is 0.4052557945251465\n",
      "epoch: 8 step: 672, loss is 0.2914629578590393\n",
      "epoch: 8 step: 673, loss is 0.04644189029932022\n",
      "epoch: 8 step: 674, loss is 0.14357249438762665\n",
      "epoch: 8 step: 675, loss is 0.169206902384758\n",
      "epoch: 8 step: 676, loss is 0.2783718407154083\n",
      "epoch: 8 step: 677, loss is 0.02850477397441864\n",
      "epoch: 8 step: 678, loss is 0.3344816267490387\n",
      "epoch: 8 step: 679, loss is 0.2159341275691986\n",
      "epoch: 8 step: 680, loss is 0.5999698042869568\n",
      "epoch: 8 step: 681, loss is 0.17406369745731354\n",
      "epoch: 8 step: 682, loss is 0.466558575630188\n",
      "epoch: 8 step: 683, loss is 0.3994029760360718\n",
      "epoch: 8 step: 684, loss is 0.1690331995487213\n",
      "epoch: 8 step: 685, loss is 0.1614942103624344\n",
      "epoch: 8 step: 686, loss is 0.6198112368583679\n",
      "epoch: 8 step: 687, loss is 0.13490523397922516\n",
      "epoch: 8 step: 688, loss is 0.007141826208680868\n",
      "epoch: 8 step: 689, loss is 0.11909447610378265\n",
      "epoch: 8 step: 690, loss is 0.2813063859939575\n",
      "epoch: 8 step: 691, loss is 0.17778922617435455\n",
      "epoch: 8 step: 692, loss is 0.19786477088928223\n",
      "epoch: 8 step: 693, loss is 0.1400490403175354\n",
      "epoch: 8 step: 694, loss is 0.2469545602798462\n",
      "epoch: 8 step: 695, loss is 0.2899938225746155\n",
      "epoch: 8 step: 696, loss is 0.2765711843967438\n",
      "epoch: 8 step: 697, loss is 0.15029041469097137\n",
      "epoch: 8 step: 698, loss is 0.2329258918762207\n",
      "epoch: 8 step: 699, loss is 0.7389494180679321\n",
      "epoch: 8 step: 700, loss is 0.10338228940963745\n",
      "epoch: 8 step: 701, loss is 0.23090118169784546\n",
      "epoch: 8 step: 702, loss is 0.2952244281768799\n",
      "epoch: 8 step: 703, loss is 0.14267656207084656\n",
      "epoch: 8 step: 704, loss is 0.1472337692975998\n",
      "epoch: 8 step: 705, loss is 0.013985395431518555\n",
      "epoch: 8 step: 706, loss is 0.14595647156238556\n",
      "epoch: 8 step: 707, loss is 0.43095895648002625\n",
      "epoch: 8 step: 708, loss is 0.22788120806217194\n",
      "epoch: 8 step: 709, loss is 0.2038150578737259\n",
      "epoch: 8 step: 710, loss is 0.14484313130378723\n",
      "epoch: 8 step: 711, loss is 0.08714073151350021\n",
      "epoch: 8 step: 712, loss is 0.13374453783035278\n",
      "epoch: 8 step: 713, loss is 0.1006915271282196\n",
      "epoch: 8 step: 714, loss is 0.98647540807724\n",
      "epoch: 8 step: 715, loss is 0.2515712082386017\n",
      "epoch: 8 step: 716, loss is 0.19346843659877777\n",
      "epoch: 8 step: 717, loss is 0.3089572489261627\n",
      "epoch: 8 step: 718, loss is 0.06601914763450623\n",
      "epoch: 8 step: 719, loss is 0.07621163129806519\n",
      "epoch: 8 step: 720, loss is 0.1666272133588791\n",
      "epoch: 8 step: 721, loss is 0.03847432881593704\n",
      "epoch: 8 step: 722, loss is 0.0603589303791523\n",
      "epoch: 8 step: 723, loss is 0.352542519569397\n",
      "epoch: 8 step: 724, loss is 0.24877339601516724\n",
      "epoch: 8 step: 725, loss is 0.24195829033851624\n",
      "epoch: 8 step: 726, loss is 0.19842678308486938\n",
      "epoch: 8 step: 727, loss is 0.02342471480369568\n",
      "epoch: 8 step: 728, loss is 0.8141543865203857\n",
      "epoch: 8 step: 729, loss is 0.056914255023002625\n",
      "epoch: 8 step: 730, loss is 0.3155536353588104\n",
      "epoch: 8 step: 731, loss is 0.18333043158054352\n",
      "epoch: 8 step: 732, loss is 0.02933087758719921\n",
      "epoch: 8 step: 733, loss is 0.1898004710674286\n",
      "epoch: 8 step: 734, loss is 0.20093141496181488\n",
      "epoch: 8 step: 735, loss is 0.03625069186091423\n",
      "epoch: 8 step: 736, loss is 0.17042119801044464\n",
      "epoch: 8 step: 737, loss is 0.11455032974481583\n",
      "epoch: 8 step: 738, loss is 0.5609310865402222\n",
      "epoch: 8 step: 739, loss is 0.2761644721031189\n",
      "epoch: 8 step: 740, loss is 0.27234771847724915\n",
      "epoch: 8 step: 741, loss is 0.13019335269927979\n",
      "epoch: 8 step: 742, loss is 0.054728295654058456\n",
      "epoch: 8 step: 743, loss is 0.33393576741218567\n",
      "epoch: 8 step: 744, loss is 0.10337834060192108\n",
      "epoch: 8 step: 745, loss is 0.057713281363248825\n",
      "epoch: 8 step: 746, loss is 0.23449549078941345\n",
      "epoch: 8 step: 747, loss is 0.8615900874137878\n",
      "epoch: 8 step: 748, loss is 0.4510466456413269\n",
      "epoch: 8 step: 749, loss is 0.3514091968536377\n",
      "epoch: 8 step: 750, loss is 0.1538311243057251\n",
      "epoch: 8 step: 751, loss is 0.06971926987171173\n",
      "epoch: 8 step: 752, loss is 0.1338544338941574\n",
      "epoch: 8 step: 753, loss is 0.23097778856754303\n",
      "epoch: 8 step: 754, loss is 0.038227278739213943\n",
      "epoch: 8 step: 755, loss is 0.2373029887676239\n",
      "epoch: 8 step: 756, loss is 0.2418755441904068\n",
      "epoch: 8 step: 757, loss is 0.12421807646751404\n",
      "epoch: 8 step: 758, loss is 0.08634353429079056\n",
      "epoch: 8 step: 759, loss is 0.2580969035625458\n",
      "epoch: 8 step: 760, loss is 0.18537695705890656\n",
      "epoch: 8 step: 761, loss is 0.23412063717842102\n",
      "epoch: 8 step: 762, loss is 0.23324738442897797\n",
      "epoch: 8 step: 763, loss is 0.21660944819450378\n",
      "epoch: 8 step: 764, loss is 0.258762001991272\n",
      "epoch: 8 step: 765, loss is 0.027053574100136757\n",
      "epoch: 8 step: 766, loss is 0.45855942368507385\n",
      "epoch: 8 step: 767, loss is 0.03972305357456207\n",
      "epoch: 8 step: 768, loss is 0.6471973061561584\n",
      "epoch: 8 step: 769, loss is 0.07311460375785828\n",
      "epoch: 8 step: 770, loss is 0.4311899244785309\n",
      "epoch: 8 step: 771, loss is 0.050226930528879166\n",
      "epoch: 8 step: 772, loss is 0.07335684448480606\n",
      "epoch: 8 step: 773, loss is 0.21363668143749237\n",
      "epoch: 8 step: 774, loss is 0.3117513358592987\n",
      "epoch: 8 step: 775, loss is 0.16670265793800354\n",
      "epoch: 8 step: 776, loss is 0.60366290807724\n",
      "epoch: 8 step: 777, loss is 0.09496790915727615\n",
      "epoch: 8 step: 778, loss is 0.39291977882385254\n",
      "epoch: 8 step: 779, loss is 0.10764665901660919\n",
      "epoch: 8 step: 780, loss is 0.07894328981637955\n",
      "epoch: 8 step: 781, loss is 0.39753252267837524\n",
      "epoch: 8 step: 782, loss is 0.4636670649051666\n",
      "epoch: 8 step: 783, loss is 0.17292557656764984\n",
      "epoch: 8 step: 784, loss is 0.021385598927736282\n",
      "epoch: 8 step: 785, loss is 0.26763975620269775\n",
      "epoch: 8 step: 786, loss is 0.2747246325016022\n",
      "epoch: 8 step: 787, loss is 0.11412495374679565\n",
      "epoch: 8 step: 788, loss is 0.08866910636425018\n",
      "epoch: 8 step: 789, loss is 0.8791893124580383\n",
      "epoch: 8 step: 790, loss is 0.22666141390800476\n",
      "epoch: 8 step: 791, loss is 0.29530757665634155\n",
      "epoch: 8 step: 792, loss is 0.16095289587974548\n",
      "epoch: 8 step: 793, loss is 0.5408133864402771\n",
      "epoch: 8 step: 794, loss is 0.16496451199054718\n",
      "epoch: 8 step: 795, loss is 0.15736493468284607\n",
      "epoch: 8 step: 796, loss is 0.28661030530929565\n",
      "epoch: 8 step: 797, loss is 0.03704839572310448\n",
      "epoch: 8 step: 798, loss is 0.3789648115634918\n",
      "epoch: 8 step: 799, loss is 0.16291867196559906\n",
      "epoch: 8 step: 800, loss is 0.13028773665428162\n",
      "epoch: 8 step: 801, loss is 0.1738838106393814\n",
      "epoch: 8 step: 802, loss is 0.2737659513950348\n",
      "epoch: 8 step: 803, loss is 0.24987201392650604\n",
      "epoch: 8 step: 804, loss is 0.25169965624809265\n",
      "epoch: 8 step: 805, loss is 0.5677398443222046\n",
      "epoch: 8 step: 806, loss is 0.38353443145751953\n",
      "epoch: 8 step: 807, loss is 0.18840163946151733\n",
      "epoch: 8 step: 808, loss is 0.12362822890281677\n",
      "epoch: 8 step: 809, loss is 0.10470980405807495\n",
      "epoch: 8 step: 810, loss is 0.5546175837516785\n",
      "epoch: 8 step: 811, loss is 0.009487677365541458\n",
      "epoch: 8 step: 812, loss is 0.22882355749607086\n",
      "epoch: 8 step: 813, loss is 0.44458332657814026\n",
      "epoch: 8 step: 814, loss is 0.5167111158370972\n",
      "epoch: 8 step: 815, loss is 0.1635214388370514\n",
      "epoch: 8 step: 816, loss is 0.055352166295051575\n",
      "epoch: 8 step: 817, loss is 0.31433284282684326\n",
      "epoch: 8 step: 818, loss is 0.5055084824562073\n",
      "epoch: 8 step: 819, loss is 0.08762356638908386\n",
      "epoch: 8 step: 820, loss is 0.14362047612667084\n",
      "epoch: 8 step: 821, loss is 0.33930471539497375\n",
      "epoch: 8 step: 822, loss is 0.23008951544761658\n",
      "epoch: 8 step: 823, loss is 0.08437894284725189\n",
      "epoch: 8 step: 824, loss is 0.03279104828834534\n",
      "epoch: 8 step: 825, loss is 0.13436833024024963\n",
      "epoch: 8 step: 826, loss is 0.27439576387405396\n",
      "epoch: 8 step: 827, loss is 0.2762572169303894\n",
      "epoch: 8 step: 828, loss is 0.08770126849412918\n",
      "epoch: 8 step: 829, loss is 0.17405350506305695\n",
      "epoch: 8 step: 830, loss is 0.8772751092910767\n",
      "epoch: 8 step: 831, loss is 0.2629597783088684\n",
      "epoch: 8 step: 832, loss is 0.18635693192481995\n",
      "epoch: 8 step: 833, loss is 0.06651350110769272\n",
      "epoch: 8 step: 834, loss is 0.0633610412478447\n",
      "epoch: 8 step: 835, loss is 0.15407754480838776\n",
      "epoch: 8 step: 836, loss is 0.17026090621948242\n",
      "epoch: 8 step: 837, loss is 0.7213163375854492\n",
      "epoch: 8 step: 838, loss is 0.1102624237537384\n",
      "epoch: 8 step: 839, loss is 0.20477545261383057\n",
      "epoch: 8 step: 840, loss is 0.6796555519104004\n",
      "epoch: 8 step: 841, loss is 0.054936882108449936\n",
      "epoch: 8 step: 842, loss is 0.48226961493492126\n",
      "epoch: 8 step: 843, loss is 0.48300665616989136\n",
      "epoch: 8 step: 844, loss is 0.2668854892253876\n",
      "epoch: 8 step: 845, loss is 0.06471709907054901\n",
      "epoch: 8 step: 846, loss is 0.2772420346736908\n",
      "epoch: 8 step: 847, loss is 0.24500568211078644\n",
      "epoch: 8 step: 848, loss is 0.22758565843105316\n",
      "epoch: 8 step: 849, loss is 0.08377708494663239\n",
      "epoch: 8 step: 850, loss is 0.1557190865278244\n",
      "epoch: 8 step: 851, loss is 0.1942126601934433\n",
      "epoch: 8 step: 852, loss is 0.5223164558410645\n",
      "epoch: 8 step: 853, loss is 0.126042440533638\n",
      "epoch: 8 step: 854, loss is 0.3411313593387604\n",
      "epoch: 8 step: 855, loss is 0.30643776059150696\n",
      "epoch: 8 step: 856, loss is 0.29436182975769043\n",
      "epoch: 8 step: 857, loss is 0.06879305094480515\n",
      "epoch: 8 step: 858, loss is 0.6638931632041931\n",
      "epoch: 8 step: 859, loss is 0.37968504428863525\n",
      "epoch: 8 step: 860, loss is 0.04780968278646469\n",
      "epoch: 8 step: 861, loss is 0.32207992672920227\n",
      "epoch: 8 step: 862, loss is 0.1108335480093956\n",
      "epoch: 8 step: 863, loss is 0.45311760902404785\n",
      "epoch: 8 step: 864, loss is 0.14849792420864105\n",
      "epoch: 8 step: 865, loss is 0.06524144113063812\n",
      "epoch: 8 step: 866, loss is 0.37736019492149353\n",
      "epoch: 8 step: 867, loss is 0.21185745298862457\n",
      "epoch: 8 step: 868, loss is 0.4088916778564453\n",
      "epoch: 8 step: 869, loss is 0.2925092875957489\n",
      "epoch: 8 step: 870, loss is 0.5305236577987671\n",
      "epoch: 8 step: 871, loss is 0.21456149220466614\n",
      "epoch: 8 step: 872, loss is 0.3926742970943451\n",
      "epoch: 8 step: 873, loss is 0.038843803107738495\n",
      "epoch: 8 step: 874, loss is 0.37109673023223877\n",
      "epoch: 8 step: 875, loss is 0.3509761095046997\n",
      "epoch: 8 step: 876, loss is 0.2599759101867676\n",
      "epoch: 8 step: 877, loss is 0.09489148110151291\n",
      "epoch: 8 step: 878, loss is 0.252146452665329\n",
      "epoch: 8 step: 879, loss is 0.5064312219619751\n",
      "epoch: 8 step: 880, loss is 0.22702215611934662\n",
      "epoch: 8 step: 881, loss is 0.36669018864631653\n",
      "epoch: 8 step: 882, loss is 0.10509617626667023\n",
      "epoch: 8 step: 883, loss is 0.5282493233680725\n",
      "epoch: 8 step: 884, loss is 0.10796641558408737\n",
      "epoch: 8 step: 885, loss is 0.05056925490498543\n",
      "epoch: 8 step: 886, loss is 0.5630112886428833\n",
      "epoch: 8 step: 887, loss is 0.08783650398254395\n",
      "epoch: 8 step: 888, loss is 0.27748987078666687\n",
      "epoch: 8 step: 889, loss is 0.3341144621372223\n",
      "epoch: 8 step: 890, loss is 0.11158638447523117\n",
      "epoch: 8 step: 891, loss is 0.3619424104690552\n",
      "epoch: 8 step: 892, loss is 0.32772234082221985\n",
      "epoch: 8 step: 893, loss is 0.3021586239337921\n",
      "epoch: 8 step: 894, loss is 0.06434477865695953\n",
      "epoch: 8 step: 895, loss is 1.0059369802474976\n",
      "epoch: 8 step: 896, loss is 0.031088460236787796\n",
      "epoch: 8 step: 897, loss is 0.12674665451049805\n",
      "epoch: 8 step: 898, loss is 0.5651321411132812\n",
      "epoch: 8 step: 899, loss is 0.5440011024475098\n",
      "epoch: 8 step: 900, loss is 0.29066261649131775\n",
      "epoch: 8 step: 901, loss is 0.18308517336845398\n",
      "epoch: 8 step: 902, loss is 0.29716163873672485\n",
      "epoch: 8 step: 903, loss is 0.08227983862161636\n",
      "epoch: 8 step: 904, loss is 0.10806838423013687\n",
      "epoch: 8 step: 905, loss is 0.06759480386972427\n",
      "epoch: 8 step: 906, loss is 0.16064861416816711\n",
      "epoch: 8 step: 907, loss is 0.087820865213871\n",
      "epoch: 8 step: 908, loss is 0.3678303062915802\n",
      "epoch: 8 step: 909, loss is 0.29215124249458313\n",
      "epoch: 8 step: 910, loss is 0.29081130027770996\n",
      "epoch: 8 step: 911, loss is 0.397765189409256\n",
      "epoch: 8 step: 912, loss is 0.48537224531173706\n",
      "epoch: 8 step: 913, loss is 0.07977356016635895\n",
      "epoch: 8 step: 914, loss is 0.39361336827278137\n",
      "epoch: 8 step: 915, loss is 0.8805984258651733\n",
      "epoch: 8 step: 916, loss is 0.36236968636512756\n",
      "epoch: 8 step: 917, loss is 0.1415809690952301\n",
      "epoch: 8 step: 918, loss is 0.1774882674217224\n",
      "epoch: 8 step: 919, loss is 0.15756694972515106\n",
      "epoch: 8 step: 920, loss is 0.42962825298309326\n",
      "epoch: 8 step: 921, loss is 0.5730562806129456\n",
      "epoch: 8 step: 922, loss is 0.26471206545829773\n",
      "epoch: 8 step: 923, loss is 0.5169067978858948\n",
      "epoch: 8 step: 924, loss is 0.5473963618278503\n",
      "epoch: 8 step: 925, loss is 0.4040122628211975\n",
      "epoch: 8 step: 926, loss is 0.2499057501554489\n",
      "epoch: 8 step: 927, loss is 0.25342199206352234\n",
      "epoch: 8 step: 928, loss is 0.1238318383693695\n",
      "epoch: 8 step: 929, loss is 0.10768832266330719\n",
      "epoch: 8 step: 930, loss is 0.39892199635505676\n",
      "epoch: 8 step: 931, loss is 0.427295982837677\n",
      "epoch: 8 step: 932, loss is 0.12510645389556885\n",
      "epoch: 8 step: 933, loss is 0.1741214096546173\n",
      "epoch: 8 step: 934, loss is 0.14165160059928894\n",
      "epoch: 8 step: 935, loss is 0.20219409465789795\n",
      "epoch: 8 step: 936, loss is 0.08560751378536224\n",
      "epoch: 8 step: 937, loss is 0.28936195373535156\n",
      "epoch: 8 step: 938, loss is 0.24859678745269775\n",
      "epoch: 8 step: 939, loss is 0.41423138976097107\n",
      "epoch: 8 step: 940, loss is 0.6888522505760193\n",
      "epoch: 8 step: 941, loss is 0.249689981341362\n",
      "epoch: 8 step: 942, loss is 0.1308279037475586\n",
      "epoch: 8 step: 943, loss is 0.6561750769615173\n",
      "epoch: 8 step: 944, loss is 0.10736379772424698\n",
      "epoch: 8 step: 945, loss is 0.11729224026203156\n",
      "epoch: 8 step: 946, loss is 0.19456487894058228\n",
      "epoch: 8 step: 947, loss is 0.4178459644317627\n",
      "epoch: 8 step: 948, loss is 0.42522647976875305\n",
      "epoch: 8 step: 949, loss is 0.714966356754303\n",
      "epoch: 8 step: 950, loss is 0.534736692905426\n",
      "epoch: 8 step: 951, loss is 0.03648679330945015\n",
      "epoch: 8 step: 952, loss is 0.25763705372810364\n",
      "epoch: 8 step: 953, loss is 0.12337896972894669\n",
      "epoch: 8 step: 954, loss is 0.06019297614693642\n",
      "epoch: 8 step: 955, loss is 0.24365954101085663\n",
      "epoch: 8 step: 956, loss is 0.32043755054473877\n",
      "epoch: 8 step: 957, loss is 0.25117334723472595\n",
      "epoch: 8 step: 958, loss is 0.19765160977840424\n",
      "epoch: 8 step: 959, loss is 0.23533424735069275\n",
      "epoch: 8 step: 960, loss is 0.7378032803535461\n",
      "epoch: 8 step: 961, loss is 0.15060655772686005\n",
      "epoch: 8 step: 962, loss is 0.29947924613952637\n",
      "epoch: 8 step: 963, loss is 0.299559623003006\n",
      "epoch: 8 step: 964, loss is 0.1498003602027893\n",
      "epoch: 8 step: 965, loss is 0.34187665581703186\n",
      "epoch: 8 step: 966, loss is 0.050286665558815\n",
      "epoch: 8 step: 967, loss is 0.31312790513038635\n",
      "epoch: 8 step: 968, loss is 0.039989084005355835\n",
      "epoch: 8 step: 969, loss is 0.4546782970428467\n",
      "epoch: 8 step: 970, loss is 0.22632254660129547\n",
      "epoch: 8 step: 971, loss is 0.12912403047084808\n",
      "epoch: 8 step: 972, loss is 0.24465881288051605\n",
      "epoch: 8 step: 973, loss is 0.21720729768276215\n",
      "epoch: 8 step: 974, loss is 0.07036329805850983\n",
      "epoch: 8 step: 975, loss is 0.29613223671913147\n",
      "epoch: 8 step: 976, loss is 0.4159167408943176\n",
      "epoch: 8 step: 977, loss is 0.28589245676994324\n",
      "epoch: 8 step: 978, loss is 0.19015070796012878\n",
      "epoch: 8 step: 979, loss is 0.21924744546413422\n",
      "epoch: 8 step: 980, loss is 0.40546712279319763\n",
      "epoch: 8 step: 981, loss is 0.32889097929000854\n",
      "epoch: 8 step: 982, loss is 0.06593193858861923\n",
      "epoch: 8 step: 983, loss is 0.2766283452510834\n",
      "epoch: 8 step: 984, loss is 0.27749356627464294\n",
      "epoch: 8 step: 985, loss is 0.19581744074821472\n",
      "epoch: 8 step: 986, loss is 0.19474415481090546\n",
      "epoch: 8 step: 987, loss is 0.32165130972862244\n",
      "epoch: 8 step: 988, loss is 0.3274581730365753\n",
      "epoch: 8 step: 989, loss is 0.5000937581062317\n",
      "epoch: 8 step: 990, loss is 0.15434199571609497\n",
      "epoch: 8 step: 991, loss is 0.35280612111091614\n",
      "epoch: 8 step: 992, loss is 0.16533048450946808\n",
      "epoch: 8 step: 993, loss is 0.21039626002311707\n",
      "epoch: 8 step: 994, loss is 0.058361779898405075\n",
      "epoch: 8 step: 995, loss is 0.5881439447402954\n",
      "epoch: 8 step: 996, loss is 0.05490289255976677\n",
      "epoch: 8 step: 997, loss is 0.09624920040369034\n",
      "epoch: 8 step: 998, loss is 0.384721577167511\n",
      "epoch: 8 step: 999, loss is 0.4913295805454254\n",
      "epoch: 8 step: 1000, loss is 0.4546011984348297\n",
      "epoch: 8 step: 1001, loss is 0.035863813012838364\n",
      "epoch: 8 step: 1002, loss is 0.06639201939105988\n",
      "epoch: 8 step: 1003, loss is 1.074407935142517\n",
      "epoch: 8 step: 1004, loss is 0.2503187954425812\n",
      "epoch: 8 step: 1005, loss is 0.11449731141328812\n",
      "epoch: 8 step: 1006, loss is 0.22700729966163635\n",
      "epoch: 8 step: 1007, loss is 0.6747497916221619\n",
      "epoch: 8 step: 1008, loss is 0.40455707907676697\n",
      "epoch: 8 step: 1009, loss is 0.3652630150318146\n",
      "epoch: 8 step: 1010, loss is 0.100506491959095\n",
      "epoch: 8 step: 1011, loss is 0.17394262552261353\n",
      "epoch: 8 step: 1012, loss is 0.1814853996038437\n",
      "epoch: 8 step: 1013, loss is 0.16804693639278412\n",
      "epoch: 8 step: 1014, loss is 0.01966293342411518\n",
      "epoch: 8 step: 1015, loss is 0.28929460048675537\n",
      "epoch: 8 step: 1016, loss is 0.39580851793289185\n",
      "epoch: 8 step: 1017, loss is 0.11316456645727158\n",
      "epoch: 8 step: 1018, loss is 0.14808101952075958\n",
      "epoch: 8 step: 1019, loss is 0.18820403516292572\n",
      "epoch: 8 step: 1020, loss is 0.254054456949234\n",
      "epoch: 8 step: 1021, loss is 0.33240175247192383\n",
      "epoch: 8 step: 1022, loss is 0.40072375535964966\n",
      "epoch: 8 step: 1023, loss is 0.34923791885375977\n",
      "epoch: 8 step: 1024, loss is 0.2300681173801422\n",
      "epoch: 8 step: 1025, loss is 0.4390769898891449\n",
      "epoch: 8 step: 1026, loss is 0.20316393673419952\n",
      "epoch: 8 step: 1027, loss is 0.2822456359863281\n",
      "epoch: 8 step: 1028, loss is 0.37408822774887085\n",
      "epoch: 8 step: 1029, loss is 0.10328199714422226\n",
      "epoch: 8 step: 1030, loss is 0.2719074785709381\n",
      "epoch: 8 step: 1031, loss is 0.6147136688232422\n",
      "epoch: 8 step: 1032, loss is 0.050048861652612686\n",
      "epoch: 8 step: 1033, loss is 0.3372133672237396\n",
      "epoch: 8 step: 1034, loss is 0.3373105525970459\n",
      "epoch: 8 step: 1035, loss is 0.31975406408309937\n",
      "epoch: 8 step: 1036, loss is 0.26626211404800415\n",
      "epoch: 8 step: 1037, loss is 0.4531340003013611\n",
      "epoch: 8 step: 1038, loss is 0.22023850679397583\n",
      "epoch: 8 step: 1039, loss is 0.21546465158462524\n",
      "epoch: 8 step: 1040, loss is 0.24480031430721283\n",
      "epoch: 8 step: 1041, loss is 0.27037370204925537\n",
      "epoch: 8 step: 1042, loss is 0.3680221140384674\n",
      "epoch: 8 step: 1043, loss is 0.1635851263999939\n",
      "epoch: 8 step: 1044, loss is 0.2184668481349945\n",
      "epoch: 8 step: 1045, loss is 0.25124356150627136\n",
      "epoch: 8 step: 1046, loss is 0.322219580411911\n",
      "epoch: 8 step: 1047, loss is 0.19240044057369232\n",
      "epoch: 8 step: 1048, loss is 0.0527532622218132\n",
      "epoch: 8 step: 1049, loss is 0.2168884426355362\n",
      "epoch: 8 step: 1050, loss is 0.5987033247947693\n",
      "epoch: 8 step: 1051, loss is 0.1561506688594818\n",
      "epoch: 8 step: 1052, loss is 0.1259423792362213\n",
      "epoch: 8 step: 1053, loss is 0.7339017391204834\n",
      "epoch: 8 step: 1054, loss is 0.1559702605009079\n",
      "epoch: 8 step: 1055, loss is 0.3918588161468506\n",
      "epoch: 8 step: 1056, loss is 0.3202023506164551\n",
      "epoch: 8 step: 1057, loss is 0.6923341751098633\n",
      "epoch: 8 step: 1058, loss is 0.6342542171478271\n",
      "epoch: 8 step: 1059, loss is 0.14713580906391144\n",
      "epoch: 8 step: 1060, loss is 0.2718137502670288\n",
      "epoch: 8 step: 1061, loss is 0.6086453795433044\n",
      "epoch: 8 step: 1062, loss is 0.3839898109436035\n",
      "epoch: 8 step: 1063, loss is 0.31959980726242065\n",
      "epoch: 8 step: 1064, loss is 0.07944095134735107\n",
      "epoch: 8 step: 1065, loss is 0.17421534657478333\n",
      "epoch: 8 step: 1066, loss is 0.1817285567522049\n",
      "epoch: 8 step: 1067, loss is 0.25864213705062866\n",
      "epoch: 8 step: 1068, loss is 0.09236045926809311\n",
      "epoch: 8 step: 1069, loss is 0.28732308745384216\n",
      "epoch: 8 step: 1070, loss is 0.439410537481308\n",
      "epoch: 8 step: 1071, loss is 0.15759636461734772\n",
      "epoch: 8 step: 1072, loss is 0.43809404969215393\n",
      "epoch: 8 step: 1073, loss is 0.3910304605960846\n",
      "epoch: 8 step: 1074, loss is 0.3502384424209595\n",
      "epoch: 8 step: 1075, loss is 0.32230448722839355\n",
      "epoch: 8 step: 1076, loss is 0.035103682428598404\n",
      "epoch: 8 step: 1077, loss is 0.13799814879894257\n",
      "epoch: 8 step: 1078, loss is 0.32486623525619507\n",
      "epoch: 8 step: 1079, loss is 0.42502301931381226\n",
      "epoch: 8 step: 1080, loss is 0.2078848034143448\n",
      "epoch: 8 step: 1081, loss is 0.1291331648826599\n",
      "epoch: 8 step: 1082, loss is 0.10185881704092026\n",
      "epoch: 8 step: 1083, loss is 0.10472095757722855\n",
      "epoch: 8 step: 1084, loss is 0.12994307279586792\n",
      "epoch: 8 step: 1085, loss is 0.37856873869895935\n",
      "epoch: 8 step: 1086, loss is 0.16081169247627258\n",
      "epoch: 8 step: 1087, loss is 0.7150734663009644\n",
      "epoch: 8 step: 1088, loss is 0.16940972208976746\n",
      "epoch: 8 step: 1089, loss is 0.1614893674850464\n",
      "epoch: 8 step: 1090, loss is 0.6540356874465942\n",
      "epoch: 8 step: 1091, loss is 0.1815372109413147\n",
      "epoch: 8 step: 1092, loss is 0.20228509604930878\n",
      "epoch: 8 step: 1093, loss is 0.031215090304613113\n",
      "epoch: 8 step: 1094, loss is 0.5011957287788391\n",
      "epoch: 8 step: 1095, loss is 0.09927759319543839\n",
      "epoch: 8 step: 1096, loss is 0.35151407122612\n",
      "epoch: 8 step: 1097, loss is 0.029647523537278175\n",
      "epoch: 8 step: 1098, loss is 0.16470648348331451\n",
      "epoch: 8 step: 1099, loss is 0.3165169060230255\n",
      "epoch: 8 step: 1100, loss is 0.3638063669204712\n",
      "epoch: 8 step: 1101, loss is 0.10389567911624908\n",
      "epoch: 8 step: 1102, loss is 1.0898947715759277\n",
      "epoch: 8 step: 1103, loss is 0.09211928397417068\n",
      "epoch: 8 step: 1104, loss is 0.47355639934539795\n",
      "epoch: 8 step: 1105, loss is 0.09118413925170898\n",
      "epoch: 8 step: 1106, loss is 0.4864809215068817\n",
      "epoch: 8 step: 1107, loss is 0.5823346972465515\n",
      "epoch: 8 step: 1108, loss is 0.24002686142921448\n",
      "epoch: 8 step: 1109, loss is 0.5912694931030273\n",
      "epoch: 8 step: 1110, loss is 0.7150201201438904\n",
      "epoch: 8 step: 1111, loss is 0.3057519197463989\n",
      "epoch: 8 step: 1112, loss is 0.15067459642887115\n",
      "epoch: 8 step: 1113, loss is 0.2627025842666626\n",
      "epoch: 8 step: 1114, loss is 0.13767576217651367\n",
      "epoch: 8 step: 1115, loss is 0.10066962987184525\n",
      "epoch: 8 step: 1116, loss is 0.404086709022522\n",
      "epoch: 8 step: 1117, loss is 0.48058652877807617\n",
      "epoch: 8 step: 1118, loss is 0.22432841360569\n",
      "epoch: 8 step: 1119, loss is 0.27087900042533875\n",
      "epoch: 8 step: 1120, loss is 0.10175786912441254\n",
      "epoch: 8 step: 1121, loss is 0.02340574562549591\n",
      "epoch: 8 step: 1122, loss is 0.6032723188400269\n",
      "epoch: 8 step: 1123, loss is 0.2905663549900055\n",
      "epoch: 8 step: 1124, loss is 0.6906004548072815\n",
      "epoch: 8 step: 1125, loss is 0.5204302668571472\n",
      "epoch: 8 step: 1126, loss is 0.18565993010997772\n",
      "epoch: 8 step: 1127, loss is 0.4360819458961487\n",
      "epoch: 8 step: 1128, loss is 0.20182296633720398\n",
      "epoch: 8 step: 1129, loss is 0.22095264494419098\n",
      "epoch: 8 step: 1130, loss is 0.20177559554576874\n",
      "epoch: 8 step: 1131, loss is 0.452301025390625\n",
      "epoch: 8 step: 1132, loss is 0.4718766212463379\n",
      "epoch: 8 step: 1133, loss is 0.6253382563591003\n",
      "epoch: 8 step: 1134, loss is 0.1020103171467781\n",
      "epoch: 8 step: 1135, loss is 0.19470608234405518\n",
      "epoch: 8 step: 1136, loss is 0.40237459540367126\n",
      "epoch: 8 step: 1137, loss is 0.07104197144508362\n",
      "epoch: 8 step: 1138, loss is 0.16727685928344727\n",
      "epoch: 8 step: 1139, loss is 0.21120873093605042\n",
      "epoch: 8 step: 1140, loss is 0.14077873528003693\n",
      "epoch: 8 step: 1141, loss is 0.29484307765960693\n",
      "epoch: 8 step: 1142, loss is 0.5467677116394043\n",
      "epoch: 8 step: 1143, loss is 0.3837142288684845\n",
      "epoch: 8 step: 1144, loss is 0.08931640535593033\n",
      "epoch: 8 step: 1145, loss is 0.07418646663427353\n",
      "epoch: 8 step: 1146, loss is 0.06832630932331085\n",
      "epoch: 8 step: 1147, loss is 0.05578063800930977\n",
      "epoch: 8 step: 1148, loss is 0.22058965265750885\n",
      "epoch: 8 step: 1149, loss is 0.5088306069374084\n",
      "epoch: 8 step: 1150, loss is 0.3947398066520691\n",
      "epoch: 8 step: 1151, loss is 0.13229182362556458\n",
      "epoch: 8 step: 1152, loss is 0.09275603294372559\n",
      "epoch: 8 step: 1153, loss is 0.18740613758563995\n",
      "epoch: 8 step: 1154, loss is 0.25006213784217834\n",
      "epoch: 8 step: 1155, loss is 0.4820660650730133\n",
      "epoch: 8 step: 1156, loss is 0.1321936696767807\n",
      "epoch: 8 step: 1157, loss is 0.12188001722097397\n",
      "epoch: 8 step: 1158, loss is 0.3242746889591217\n",
      "epoch: 8 step: 1159, loss is 0.14147792756557465\n",
      "epoch: 8 step: 1160, loss is 0.21842950582504272\n",
      "epoch: 8 step: 1161, loss is 0.5592482089996338\n",
      "epoch: 8 step: 1162, loss is 0.407530277967453\n",
      "epoch: 8 step: 1163, loss is 0.2480870485305786\n",
      "epoch: 8 step: 1164, loss is 0.30898457765579224\n",
      "epoch: 8 step: 1165, loss is 0.020776808261871338\n",
      "epoch: 8 step: 1166, loss is 0.2005932629108429\n",
      "epoch: 8 step: 1167, loss is 0.5025428533554077\n",
      "epoch: 8 step: 1168, loss is 0.1847718507051468\n",
      "epoch: 8 step: 1169, loss is 0.422695517539978\n",
      "epoch: 8 step: 1170, loss is 0.8296027779579163\n",
      "epoch: 8 step: 1171, loss is 0.2525292932987213\n",
      "epoch: 8 step: 1172, loss is 0.16807737946510315\n",
      "epoch: 8 step: 1173, loss is 0.14996403455734253\n",
      "epoch: 8 step: 1174, loss is 0.49147307872772217\n",
      "epoch: 8 step: 1175, loss is 0.6881117224693298\n",
      "epoch: 8 step: 1176, loss is 0.6527899503707886\n",
      "epoch: 8 step: 1177, loss is 0.05960191786289215\n",
      "epoch: 8 step: 1178, loss is 0.4176428020000458\n",
      "epoch: 8 step: 1179, loss is 0.14691971242427826\n",
      "epoch: 8 step: 1180, loss is 0.04596943035721779\n",
      "epoch: 8 step: 1181, loss is 0.4599398076534271\n",
      "epoch: 8 step: 1182, loss is 0.16627170145511627\n",
      "epoch: 8 step: 1183, loss is 0.2764919102191925\n",
      "epoch: 8 step: 1184, loss is 0.13148802518844604\n",
      "epoch: 8 step: 1185, loss is 0.1245524138212204\n",
      "epoch: 8 step: 1186, loss is 0.0894128829240799\n",
      "epoch: 8 step: 1187, loss is 0.23087471723556519\n",
      "epoch: 8 step: 1188, loss is 0.3547728359699249\n",
      "epoch: 8 step: 1189, loss is 0.22501525282859802\n",
      "epoch: 8 step: 1190, loss is 0.29274773597717285\n",
      "epoch: 8 step: 1191, loss is 0.1425505429506302\n",
      "epoch: 8 step: 1192, loss is 0.088084876537323\n",
      "epoch: 8 step: 1193, loss is 0.33916372060775757\n",
      "epoch: 8 step: 1194, loss is 0.35934770107269287\n",
      "epoch: 8 step: 1195, loss is 0.21798574924468994\n",
      "epoch: 8 step: 1196, loss is 0.12707243859767914\n",
      "epoch: 8 step: 1197, loss is 0.5676805973052979\n",
      "epoch: 8 step: 1198, loss is 0.5251234173774719\n",
      "epoch: 8 step: 1199, loss is 0.4084954559803009\n",
      "epoch: 8 step: 1200, loss is 0.1618514060974121\n",
      "epoch: 8 step: 1201, loss is 0.18971917033195496\n",
      "epoch: 8 step: 1202, loss is 0.2649880349636078\n",
      "epoch: 8 step: 1203, loss is 0.2288164347410202\n",
      "epoch: 8 step: 1204, loss is 0.3169342577457428\n",
      "epoch: 8 step: 1205, loss is 0.2320931851863861\n",
      "epoch: 8 step: 1206, loss is 0.1874861717224121\n",
      "epoch: 8 step: 1207, loss is 0.37104910612106323\n",
      "epoch: 8 step: 1208, loss is 0.3064902722835541\n",
      "epoch: 8 step: 1209, loss is 0.19465813040733337\n",
      "epoch: 8 step: 1210, loss is 0.27929455041885376\n",
      "epoch: 8 step: 1211, loss is 0.11529000103473663\n",
      "epoch: 8 step: 1212, loss is 0.04001803696155548\n",
      "epoch: 8 step: 1213, loss is 0.4278671145439148\n",
      "epoch: 8 step: 1214, loss is 0.34651193022727966\n",
      "epoch: 8 step: 1215, loss is 0.1732514649629593\n",
      "epoch: 8 step: 1216, loss is 0.5751106142997742\n",
      "epoch: 8 step: 1217, loss is 0.39554962515830994\n",
      "epoch: 8 step: 1218, loss is 0.46897679567337036\n",
      "epoch: 8 step: 1219, loss is 0.45588478446006775\n",
      "epoch: 8 step: 1220, loss is 0.349354088306427\n",
      "epoch: 8 step: 1221, loss is 0.029184537008404732\n",
      "epoch: 8 step: 1222, loss is 0.07837086170911789\n",
      "epoch: 8 step: 1223, loss is 0.22946369647979736\n",
      "epoch: 8 step: 1224, loss is 0.27106204628944397\n",
      "epoch: 8 step: 1225, loss is 0.1381414532661438\n",
      "epoch: 8 step: 1226, loss is 0.23633870482444763\n",
      "epoch: 8 step: 1227, loss is 0.12968122959136963\n",
      "epoch: 8 step: 1228, loss is 0.12657468020915985\n",
      "epoch: 8 step: 1229, loss is 0.03793315216898918\n",
      "epoch: 8 step: 1230, loss is 0.22447942197322845\n",
      "epoch: 8 step: 1231, loss is 0.2732110023498535\n",
      "epoch: 8 step: 1232, loss is 0.1179126426577568\n",
      "epoch: 8 step: 1233, loss is 0.05101681500673294\n",
      "epoch: 8 step: 1234, loss is 0.29852530360221863\n",
      "epoch: 8 step: 1235, loss is 0.03817040100693703\n",
      "epoch: 8 step: 1236, loss is 0.0566205233335495\n",
      "epoch: 8 step: 1237, loss is 0.009158799424767494\n",
      "epoch: 8 step: 1238, loss is 0.03200794756412506\n",
      "epoch: 8 step: 1239, loss is 0.3909248113632202\n",
      "epoch: 8 step: 1240, loss is 0.04597723111510277\n",
      "epoch: 8 step: 1241, loss is 0.6425551772117615\n",
      "epoch: 8 step: 1242, loss is 0.7649592757225037\n",
      "epoch: 8 step: 1243, loss is 0.5541598796844482\n",
      "epoch: 8 step: 1244, loss is 0.3429240584373474\n",
      "epoch: 8 step: 1245, loss is 0.4388978183269501\n",
      "epoch: 8 step: 1246, loss is 0.3660503625869751\n",
      "epoch: 8 step: 1247, loss is 0.08101718127727509\n",
      "epoch: 8 step: 1248, loss is 0.5519110560417175\n",
      "epoch: 8 step: 1249, loss is 0.24209310114383698\n",
      "epoch: 8 step: 1250, loss is 0.3808409869670868\n",
      "epoch: 8 step: 1251, loss is 0.11727169901132584\n",
      "epoch: 8 step: 1252, loss is 0.5939101576805115\n",
      "epoch: 8 step: 1253, loss is 0.03788890317082405\n",
      "epoch: 8 step: 1254, loss is 0.10032389312982559\n",
      "epoch: 8 step: 1255, loss is 0.4718392789363861\n",
      "epoch: 8 step: 1256, loss is 0.4958023726940155\n",
      "epoch: 8 step: 1257, loss is 0.3434838652610779\n",
      "epoch: 8 step: 1258, loss is 0.6273732781410217\n",
      "epoch: 8 step: 1259, loss is 0.12216191738843918\n",
      "epoch: 8 step: 1260, loss is 0.05663655698299408\n",
      "epoch: 8 step: 1261, loss is 0.26600387692451477\n",
      "epoch: 8 step: 1262, loss is 0.04390355572104454\n",
      "epoch: 8 step: 1263, loss is 0.6254193782806396\n",
      "epoch: 8 step: 1264, loss is 0.14745843410491943\n",
      "epoch: 8 step: 1265, loss is 0.08982647955417633\n",
      "epoch: 8 step: 1266, loss is 0.22533980011940002\n",
      "epoch: 8 step: 1267, loss is 0.44829627871513367\n",
      "epoch: 8 step: 1268, loss is 0.057527609169483185\n",
      "epoch: 8 step: 1269, loss is 0.3245759606361389\n",
      "epoch: 8 step: 1270, loss is 0.15674322843551636\n",
      "epoch: 8 step: 1271, loss is 0.10440658777952194\n",
      "epoch: 8 step: 1272, loss is 0.3495042324066162\n",
      "epoch: 8 step: 1273, loss is 0.07661045342683792\n",
      "epoch: 8 step: 1274, loss is 0.3542954623699188\n",
      "epoch: 8 step: 1275, loss is 0.7740216851234436\n",
      "epoch: 8 step: 1276, loss is 0.13615457713603973\n",
      "epoch: 8 step: 1277, loss is 0.18293169140815735\n",
      "epoch: 8 step: 1278, loss is 0.07663553208112717\n",
      "epoch: 8 step: 1279, loss is 0.1292331963777542\n",
      "epoch: 8 step: 1280, loss is 0.12015270441770554\n",
      "epoch: 8 step: 1281, loss is 0.12286640703678131\n",
      "epoch: 8 step: 1282, loss is 0.10389277338981628\n",
      "epoch: 8 step: 1283, loss is 0.18428750336170197\n",
      "epoch: 8 step: 1284, loss is 0.3022315204143524\n",
      "epoch: 8 step: 1285, loss is 0.1141144260764122\n",
      "epoch: 8 step: 1286, loss is 0.13622143864631653\n",
      "epoch: 8 step: 1287, loss is 0.2965746223926544\n",
      "epoch: 8 step: 1288, loss is 0.2221771776676178\n",
      "epoch: 8 step: 1289, loss is 0.34008175134658813\n",
      "epoch: 8 step: 1290, loss is 0.0926646962761879\n",
      "epoch: 8 step: 1291, loss is 0.048514172434806824\n",
      "epoch: 8 step: 1292, loss is 0.09571428596973419\n",
      "epoch: 8 step: 1293, loss is 0.19144807755947113\n",
      "epoch: 8 step: 1294, loss is 0.3686610758304596\n",
      "epoch: 8 step: 1295, loss is 0.24039794504642487\n",
      "epoch: 8 step: 1296, loss is 0.26313671469688416\n",
      "epoch: 8 step: 1297, loss is 0.1300152987241745\n",
      "epoch: 8 step: 1298, loss is 0.02386082336306572\n",
      "epoch: 8 step: 1299, loss is 0.40586549043655396\n",
      "epoch: 8 step: 1300, loss is 0.327863484621048\n",
      "epoch: 8 step: 1301, loss is 0.6421477794647217\n",
      "epoch: 8 step: 1302, loss is 0.13019433617591858\n",
      "epoch: 8 step: 1303, loss is 0.01831020601093769\n",
      "epoch: 8 step: 1304, loss is 0.4242323637008667\n",
      "epoch: 8 step: 1305, loss is 0.4333915114402771\n",
      "epoch: 8 step: 1306, loss is 0.32849669456481934\n",
      "epoch: 8 step: 1307, loss is 0.22497887909412384\n",
      "epoch: 8 step: 1308, loss is 0.24052640795707703\n",
      "epoch: 8 step: 1309, loss is 0.1723044216632843\n",
      "epoch: 8 step: 1310, loss is 0.1046600192785263\n",
      "epoch: 8 step: 1311, loss is 0.24067354202270508\n",
      "epoch: 8 step: 1312, loss is 0.14583194255828857\n",
      "epoch: 8 step: 1313, loss is 0.1390138417482376\n",
      "epoch: 8 step: 1314, loss is 0.21407994627952576\n",
      "epoch: 8 step: 1315, loss is 0.17199824750423431\n",
      "epoch: 8 step: 1316, loss is 0.2975587248802185\n",
      "epoch: 8 step: 1317, loss is 0.15779121220111847\n",
      "epoch: 8 step: 1318, loss is 0.5528815388679504\n",
      "epoch: 8 step: 1319, loss is 0.3035908639431\n",
      "epoch: 8 step: 1320, loss is 0.2500830888748169\n",
      "epoch: 8 step: 1321, loss is 0.2037115842103958\n",
      "epoch: 8 step: 1322, loss is 0.06854606419801712\n",
      "epoch: 8 step: 1323, loss is 0.5955581068992615\n",
      "epoch: 8 step: 1324, loss is 0.1266423910856247\n",
      "epoch: 8 step: 1325, loss is 0.010383760556578636\n",
      "epoch: 8 step: 1326, loss is 0.07780095934867859\n",
      "epoch: 8 step: 1327, loss is 0.139082670211792\n",
      "epoch: 8 step: 1328, loss is 0.1952265053987503\n",
      "epoch: 8 step: 1329, loss is 0.017582302913069725\n",
      "epoch: 8 step: 1330, loss is 0.4742700755596161\n",
      "epoch: 8 step: 1331, loss is 0.41208985447883606\n",
      "epoch: 8 step: 1332, loss is 0.08941866457462311\n",
      "epoch: 8 step: 1333, loss is 0.13122601807117462\n",
      "epoch: 8 step: 1334, loss is 0.08699338138103485\n",
      "epoch: 8 step: 1335, loss is 0.03112952969968319\n",
      "epoch: 8 step: 1336, loss is 0.13031913340091705\n",
      "epoch: 8 step: 1337, loss is 0.1408172845840454\n",
      "epoch: 8 step: 1338, loss is 0.2996048033237457\n",
      "epoch: 8 step: 1339, loss is 1.0946568250656128\n",
      "epoch: 8 step: 1340, loss is 0.16268396377563477\n",
      "epoch: 8 step: 1341, loss is 0.12642671167850494\n",
      "epoch: 8 step: 1342, loss is 0.10345279425382614\n",
      "epoch: 8 step: 1343, loss is 0.13598082959651947\n",
      "epoch: 8 step: 1344, loss is 0.8951932787895203\n",
      "epoch: 8 step: 1345, loss is 0.17717470228672028\n",
      "epoch: 8 step: 1346, loss is 0.2970704734325409\n",
      "epoch: 8 step: 1347, loss is 0.08667976409196854\n",
      "epoch: 8 step: 1348, loss is 0.683019757270813\n",
      "epoch: 8 step: 1349, loss is 0.1743922382593155\n",
      "epoch: 8 step: 1350, loss is 0.17315886914730072\n",
      "epoch: 8 step: 1351, loss is 0.1697072684764862\n",
      "epoch: 8 step: 1352, loss is 0.44065818190574646\n",
      "epoch: 8 step: 1353, loss is 0.05300405994057655\n",
      "epoch: 8 step: 1354, loss is 0.5412760972976685\n",
      "epoch: 8 step: 1355, loss is 0.25126296281814575\n",
      "epoch: 8 step: 1356, loss is 0.28573325276374817\n",
      "epoch: 8 step: 1357, loss is 0.21452972292900085\n",
      "epoch: 8 step: 1358, loss is 0.16819068789482117\n",
      "epoch: 8 step: 1359, loss is 0.40972986817359924\n",
      "epoch: 8 step: 1360, loss is 0.1463276445865631\n",
      "epoch: 8 step: 1361, loss is 0.02073843963444233\n",
      "epoch: 8 step: 1362, loss is 0.19651095569133759\n",
      "epoch: 8 step: 1363, loss is 0.09671848267316818\n",
      "epoch: 8 step: 1364, loss is 0.33829712867736816\n",
      "epoch: 8 step: 1365, loss is 0.16281132400035858\n",
      "epoch: 8 step: 1366, loss is 0.09236586093902588\n",
      "epoch: 8 step: 1367, loss is 0.22838108241558075\n",
      "epoch: 8 step: 1368, loss is 0.3027893304824829\n",
      "epoch: 8 step: 1369, loss is 0.0707569494843483\n",
      "epoch: 8 step: 1370, loss is 0.3733351528644562\n",
      "epoch: 8 step: 1371, loss is 0.16986437141895294\n",
      "epoch: 8 step: 1372, loss is 0.033742789179086685\n",
      "epoch: 8 step: 1373, loss is 0.14734598994255066\n",
      "epoch: 8 step: 1374, loss is 0.3488630950450897\n",
      "epoch: 8 step: 1375, loss is 0.4251002073287964\n",
      "epoch: 8 step: 1376, loss is 0.020358283072710037\n",
      "epoch: 8 step: 1377, loss is 0.3877038061618805\n",
      "epoch: 8 step: 1378, loss is 0.49677684903144836\n",
      "epoch: 8 step: 1379, loss is 0.07400703430175781\n",
      "epoch: 8 step: 1380, loss is 0.020705295726656914\n",
      "epoch: 8 step: 1381, loss is 0.06746791303157806\n",
      "epoch: 8 step: 1382, loss is 0.29605719447135925\n",
      "epoch: 8 step: 1383, loss is 0.13119295239448547\n",
      "epoch: 8 step: 1384, loss is 0.1326190084218979\n",
      "epoch: 8 step: 1385, loss is 0.11831893771886826\n",
      "epoch: 8 step: 1386, loss is 0.08164414763450623\n",
      "epoch: 8 step: 1387, loss is 0.7296987175941467\n",
      "epoch: 8 step: 1388, loss is 0.10151718556880951\n",
      "epoch: 8 step: 1389, loss is 0.12853799760341644\n",
      "epoch: 8 step: 1390, loss is 0.09512366354465485\n",
      "epoch: 8 step: 1391, loss is 0.13961860537528992\n",
      "epoch: 8 step: 1392, loss is 0.11321324855089188\n",
      "epoch: 8 step: 1393, loss is 0.057994768023490906\n",
      "epoch: 8 step: 1394, loss is 0.08993206918239594\n",
      "epoch: 8 step: 1395, loss is 0.06522765755653381\n",
      "epoch: 8 step: 1396, loss is 0.3599391281604767\n",
      "epoch: 8 step: 1397, loss is 0.4717268645763397\n",
      "epoch: 8 step: 1398, loss is 0.24749936163425446\n",
      "epoch: 8 step: 1399, loss is 0.43148505687713623\n",
      "epoch: 8 step: 1400, loss is 0.18136747181415558\n",
      "epoch: 8 step: 1401, loss is 0.26359835267066956\n",
      "epoch: 8 step: 1402, loss is 0.31519386172294617\n",
      "epoch: 8 step: 1403, loss is 0.11736509948968887\n",
      "epoch: 8 step: 1404, loss is 0.435104638338089\n",
      "epoch: 8 step: 1405, loss is 0.38088664412498474\n",
      "epoch: 8 step: 1406, loss is 0.12072674185037613\n",
      "epoch: 8 step: 1407, loss is 0.056425414979457855\n",
      "epoch: 8 step: 1408, loss is 0.1583058089017868\n",
      "epoch: 8 step: 1409, loss is 0.06215227395296097\n",
      "epoch: 8 step: 1410, loss is 0.1388709843158722\n",
      "epoch: 8 step: 1411, loss is 0.4621078670024872\n",
      "epoch: 8 step: 1412, loss is 0.21565620601177216\n",
      "epoch: 8 step: 1413, loss is 0.10015437006950378\n",
      "epoch: 8 step: 1414, loss is 0.316609650850296\n",
      "epoch: 8 step: 1415, loss is 0.4700828194618225\n",
      "epoch: 8 step: 1416, loss is 0.370766282081604\n",
      "epoch: 8 step: 1417, loss is 0.2941170930862427\n",
      "epoch: 8 step: 1418, loss is 0.5933663249015808\n",
      "epoch: 8 step: 1419, loss is 0.23247355222702026\n",
      "epoch: 8 step: 1420, loss is 0.2132451981306076\n",
      "epoch: 8 step: 1421, loss is 0.49372217059135437\n",
      "epoch: 8 step: 1422, loss is 0.5581833720207214\n",
      "epoch: 8 step: 1423, loss is 0.19924311339855194\n",
      "epoch: 8 step: 1424, loss is 0.056884750723838806\n",
      "epoch: 8 step: 1425, loss is 0.09129124879837036\n",
      "epoch: 8 step: 1426, loss is 0.1298387497663498\n",
      "epoch: 8 step: 1427, loss is 0.08834090083837509\n",
      "epoch: 8 step: 1428, loss is 0.19960878789424896\n",
      "epoch: 8 step: 1429, loss is 0.11052405089139938\n",
      "epoch: 8 step: 1430, loss is 0.04473333805799484\n",
      "epoch: 8 step: 1431, loss is 0.04683993011713028\n",
      "epoch: 8 step: 1432, loss is 0.03057289868593216\n",
      "epoch: 8 step: 1433, loss is 0.148833766579628\n",
      "epoch: 8 step: 1434, loss is 0.8162585496902466\n",
      "epoch: 8 step: 1435, loss is 0.17764124274253845\n",
      "epoch: 8 step: 1436, loss is 0.09093589335680008\n",
      "epoch: 8 step: 1437, loss is 0.09984024614095688\n",
      "epoch: 8 step: 1438, loss is 0.3842363655567169\n",
      "epoch: 8 step: 1439, loss is 0.33026188611984253\n",
      "epoch: 8 step: 1440, loss is 0.0979575663805008\n",
      "epoch: 8 step: 1441, loss is 0.39507320523262024\n",
      "epoch: 8 step: 1442, loss is 0.47325369715690613\n",
      "epoch: 8 step: 1443, loss is 0.41932040452957153\n",
      "epoch: 8 step: 1444, loss is 0.021909428760409355\n",
      "epoch: 8 step: 1445, loss is 0.14053112268447876\n",
      "epoch: 8 step: 1446, loss is 0.05026259273290634\n",
      "epoch: 8 step: 1447, loss is 0.16530312597751617\n",
      "epoch: 8 step: 1448, loss is 0.1622302085161209\n",
      "epoch: 8 step: 1449, loss is 0.3910738229751587\n",
      "epoch: 8 step: 1450, loss is 0.3178977072238922\n",
      "epoch: 8 step: 1451, loss is 0.18874259293079376\n",
      "epoch: 8 step: 1452, loss is 0.09745928645133972\n",
      "epoch: 8 step: 1453, loss is 0.31825724244117737\n",
      "epoch: 8 step: 1454, loss is 0.411803662776947\n",
      "epoch: 8 step: 1455, loss is 0.30075421929359436\n",
      "epoch: 8 step: 1456, loss is 0.15103217959403992\n",
      "epoch: 8 step: 1457, loss is 0.1925532966852188\n",
      "epoch: 8 step: 1458, loss is 0.03196430206298828\n",
      "epoch: 8 step: 1459, loss is 0.4088265299797058\n",
      "epoch: 8 step: 1460, loss is 0.2098851352930069\n",
      "epoch: 8 step: 1461, loss is 0.15573328733444214\n",
      "epoch: 8 step: 1462, loss is 0.29431071877479553\n",
      "epoch: 8 step: 1463, loss is 0.10991456359624863\n",
      "epoch: 8 step: 1464, loss is 0.3807443082332611\n",
      "epoch: 8 step: 1465, loss is 0.016405971720814705\n",
      "epoch: 8 step: 1466, loss is 0.6521944403648376\n",
      "epoch: 8 step: 1467, loss is 0.370132714509964\n",
      "epoch: 8 step: 1468, loss is 0.38538044691085815\n",
      "epoch: 8 step: 1469, loss is 0.1514022946357727\n",
      "epoch: 8 step: 1470, loss is 0.02565249241888523\n",
      "epoch: 8 step: 1471, loss is 0.18005578219890594\n",
      "epoch: 8 step: 1472, loss is 0.3226509094238281\n",
      "epoch: 8 step: 1473, loss is 0.9454416632652283\n",
      "epoch: 8 step: 1474, loss is 0.08510543406009674\n",
      "epoch: 8 step: 1475, loss is 0.32158413529396057\n",
      "epoch: 8 step: 1476, loss is 0.061868973076343536\n",
      "epoch: 8 step: 1477, loss is 0.15940527617931366\n",
      "epoch: 8 step: 1478, loss is 0.11401035636663437\n",
      "epoch: 8 step: 1479, loss is 0.28050118684768677\n",
      "epoch: 8 step: 1480, loss is 0.22592774033546448\n",
      "epoch: 8 step: 1481, loss is 0.09798888862133026\n",
      "epoch: 8 step: 1482, loss is 0.1355898380279541\n",
      "epoch: 8 step: 1483, loss is 0.16832928359508514\n",
      "epoch: 8 step: 1484, loss is 0.17461180686950684\n",
      "epoch: 8 step: 1485, loss is 0.4472510814666748\n",
      "epoch: 8 step: 1486, loss is 0.15528060495853424\n",
      "epoch: 8 step: 1487, loss is 0.22284454107284546\n",
      "epoch: 8 step: 1488, loss is 0.5864377617835999\n",
      "epoch: 8 step: 1489, loss is 0.19382807612419128\n",
      "epoch: 8 step: 1490, loss is 0.09588905423879623\n",
      "epoch: 8 step: 1491, loss is 0.08553420007228851\n",
      "epoch: 8 step: 1492, loss is 0.46366366744041443\n",
      "epoch: 8 step: 1493, loss is 0.23542800545692444\n",
      "epoch: 8 step: 1494, loss is 0.2420935183763504\n",
      "epoch: 8 step: 1495, loss is 0.34761735796928406\n",
      "epoch: 8 step: 1496, loss is 0.058145672082901\n",
      "epoch: 8 step: 1497, loss is 0.21599698066711426\n",
      "epoch: 8 step: 1498, loss is 0.15008945763111115\n",
      "epoch: 8 step: 1499, loss is 0.19313354790210724\n",
      "epoch: 8 step: 1500, loss is 0.23411151766777039\n",
      "epoch: 8 step: 1501, loss is 0.3562627136707306\n",
      "epoch: 8 step: 1502, loss is 0.1191195696592331\n",
      "epoch: 8 step: 1503, loss is 0.17878833413124084\n",
      "epoch: 8 step: 1504, loss is 0.06835957616567612\n",
      "epoch: 8 step: 1505, loss is 0.060630351305007935\n",
      "epoch: 8 step: 1506, loss is 0.06894983351230621\n",
      "epoch: 8 step: 1507, loss is 0.05132443085312843\n",
      "epoch: 8 step: 1508, loss is 0.15232664346694946\n",
      "epoch: 8 step: 1509, loss is 1.0880264043807983\n",
      "epoch: 8 step: 1510, loss is 0.1362631916999817\n",
      "epoch: 8 step: 1511, loss is 0.6099132299423218\n",
      "epoch: 8 step: 1512, loss is 0.11019842326641083\n",
      "epoch: 8 step: 1513, loss is 0.23436681926250458\n",
      "epoch: 8 step: 1514, loss is 0.21890653669834137\n",
      "epoch: 8 step: 1515, loss is 0.10962483286857605\n",
      "epoch: 8 step: 1516, loss is 0.19481974840164185\n",
      "epoch: 8 step: 1517, loss is 0.034873079508543015\n",
      "epoch: 8 step: 1518, loss is 0.032192934304475784\n",
      "epoch: 8 step: 1519, loss is 0.0046372502110898495\n",
      "epoch: 8 step: 1520, loss is 0.031344879418611526\n",
      "epoch: 8 step: 1521, loss is 0.05222806707024574\n",
      "epoch: 8 step: 1522, loss is 0.4995390772819519\n",
      "epoch: 8 step: 1523, loss is 0.3057451546192169\n",
      "epoch: 8 step: 1524, loss is 0.15264569222927094\n",
      "epoch: 8 step: 1525, loss is 0.39627066254615784\n",
      "epoch: 8 step: 1526, loss is 0.5034139156341553\n",
      "epoch: 8 step: 1527, loss is 0.30023887753486633\n",
      "epoch: 8 step: 1528, loss is 0.336398720741272\n",
      "epoch: 8 step: 1529, loss is 0.2097667008638382\n",
      "epoch: 8 step: 1530, loss is 0.39862340688705444\n",
      "epoch: 8 step: 1531, loss is 0.2550286650657654\n",
      "epoch: 8 step: 1532, loss is 0.21984942257404327\n",
      "epoch: 8 step: 1533, loss is 0.04934527352452278\n",
      "epoch: 8 step: 1534, loss is 0.9186621904373169\n",
      "epoch: 8 step: 1535, loss is 0.5476477146148682\n",
      "epoch: 8 step: 1536, loss is 0.06179533898830414\n",
      "epoch: 8 step: 1537, loss is 0.7581575512886047\n",
      "epoch: 8 step: 1538, loss is 0.21173453330993652\n",
      "epoch: 8 step: 1539, loss is 0.4520326554775238\n",
      "epoch: 8 step: 1540, loss is 0.03977375477552414\n",
      "epoch: 8 step: 1541, loss is 0.09956210106611252\n",
      "epoch: 8 step: 1542, loss is 0.4692944586277008\n",
      "epoch: 8 step: 1543, loss is 0.4128453731536865\n",
      "epoch: 8 step: 1544, loss is 0.38524720072746277\n",
      "epoch: 8 step: 1545, loss is 0.4105120897293091\n",
      "epoch: 8 step: 1546, loss is 0.8657759428024292\n",
      "epoch: 8 step: 1547, loss is 0.2874031960964203\n",
      "epoch: 8 step: 1548, loss is 0.2677183151245117\n",
      "epoch: 8 step: 1549, loss is 0.21685588359832764\n",
      "epoch: 8 step: 1550, loss is 0.34841427206993103\n",
      "epoch: 8 step: 1551, loss is 0.15983504056930542\n",
      "epoch: 8 step: 1552, loss is 0.23818035423755646\n",
      "epoch: 8 step: 1553, loss is 0.1426372528076172\n",
      "epoch: 8 step: 1554, loss is 0.26205047965049744\n",
      "epoch: 8 step: 1555, loss is 0.3812442421913147\n",
      "epoch: 8 step: 1556, loss is 0.22357094287872314\n",
      "epoch: 8 step: 1557, loss is 0.04625631868839264\n",
      "epoch: 8 step: 1558, loss is 0.1935781091451645\n",
      "epoch: 8 step: 1559, loss is 0.26783594489097595\n",
      "epoch: 8 step: 1560, loss is 0.1969001740217209\n",
      "epoch: 8 step: 1561, loss is 0.2002849578857422\n",
      "epoch: 8 step: 1562, loss is 0.223673477768898\n",
      "epoch: 8 step: 1563, loss is 0.06700654327869415\n",
      "epoch: 8 step: 1564, loss is 0.2539098262786865\n",
      "epoch: 8 step: 1565, loss is 0.21643167734146118\n",
      "epoch: 8 step: 1566, loss is 0.11595550179481506\n",
      "epoch: 8 step: 1567, loss is 0.47792571783065796\n",
      "epoch: 8 step: 1568, loss is 0.4822719395160675\n",
      "epoch: 8 step: 1569, loss is 0.15663737058639526\n",
      "epoch: 8 step: 1570, loss is 0.2552993595600128\n",
      "epoch: 8 step: 1571, loss is 0.11767487227916718\n",
      "epoch: 8 step: 1572, loss is 0.06320027261972427\n",
      "epoch: 8 step: 1573, loss is 0.6724190711975098\n",
      "epoch: 8 step: 1574, loss is 0.06435538083314896\n",
      "epoch: 8 step: 1575, loss is 0.4255081117153168\n",
      "epoch: 8 step: 1576, loss is 0.11936811357736588\n",
      "epoch: 8 step: 1577, loss is 0.17967212200164795\n",
      "epoch: 8 step: 1578, loss is 0.07211695611476898\n",
      "epoch: 8 step: 1579, loss is 0.055978842079639435\n",
      "epoch: 8 step: 1580, loss is 0.18503734469413757\n",
      "epoch: 8 step: 1581, loss is 0.3526235818862915\n",
      "epoch: 8 step: 1582, loss is 0.7403020262718201\n",
      "epoch: 8 step: 1583, loss is 0.10814037173986435\n",
      "epoch: 8 step: 1584, loss is 0.29335343837738037\n",
      "epoch: 8 step: 1585, loss is 0.4674820303916931\n",
      "epoch: 8 step: 1586, loss is 0.31313860416412354\n",
      "epoch: 8 step: 1587, loss is 0.26278623938560486\n",
      "epoch: 8 step: 1588, loss is 0.46629875898361206\n",
      "epoch: 8 step: 1589, loss is 0.9212785959243774\n",
      "epoch: 8 step: 1590, loss is 0.1801646500825882\n",
      "epoch: 8 step: 1591, loss is 0.17219145596027374\n",
      "epoch: 8 step: 1592, loss is 0.31319084763526917\n",
      "epoch: 8 step: 1593, loss is 0.5764592289924622\n",
      "epoch: 8 step: 1594, loss is 0.08563783764839172\n",
      "epoch: 8 step: 1595, loss is 0.16734279692173004\n",
      "epoch: 8 step: 1596, loss is 0.28758788108825684\n",
      "epoch: 8 step: 1597, loss is 0.4024842083454132\n",
      "epoch: 8 step: 1598, loss is 0.2644873261451721\n",
      "epoch: 8 step: 1599, loss is 0.053541868925094604\n",
      "epoch: 8 step: 1600, loss is 0.49585819244384766\n",
      "epoch: 8 step: 1601, loss is 0.2031347155570984\n",
      "epoch: 8 step: 1602, loss is 0.5090420246124268\n",
      "epoch: 8 step: 1603, loss is 0.08326567709445953\n",
      "epoch: 8 step: 1604, loss is 0.120934396982193\n",
      "epoch: 8 step: 1605, loss is 0.3879167437553406\n",
      "epoch: 8 step: 1606, loss is 0.32953137159347534\n",
      "epoch: 8 step: 1607, loss is 0.4443032443523407\n",
      "epoch: 8 step: 1608, loss is 0.1665131151676178\n",
      "epoch: 8 step: 1609, loss is 0.6906632781028748\n",
      "epoch: 8 step: 1610, loss is 0.22584649920463562\n",
      "epoch: 8 step: 1611, loss is 0.25288844108581543\n",
      "epoch: 8 step: 1612, loss is 0.05007905885577202\n",
      "epoch: 8 step: 1613, loss is 0.18000958859920502\n",
      "epoch: 8 step: 1614, loss is 0.2638848125934601\n",
      "epoch: 8 step: 1615, loss is 0.2009337693452835\n",
      "epoch: 8 step: 1616, loss is 0.5009269118309021\n",
      "epoch: 8 step: 1617, loss is 0.05836652219295502\n",
      "epoch: 8 step: 1618, loss is 0.4056408405303955\n",
      "epoch: 8 step: 1619, loss is 0.2813529074192047\n",
      "epoch: 8 step: 1620, loss is 0.18912464380264282\n",
      "epoch: 8 step: 1621, loss is 0.5332808494567871\n",
      "epoch: 8 step: 1622, loss is 1.050557255744934\n",
      "epoch: 8 step: 1623, loss is 0.5165849924087524\n",
      "epoch: 8 step: 1624, loss is 0.12334100902080536\n",
      "epoch: 8 step: 1625, loss is 0.21206103265285492\n",
      "epoch: 8 step: 1626, loss is 0.22993908822536469\n",
      "epoch: 8 step: 1627, loss is 0.2817879617214203\n",
      "epoch: 8 step: 1628, loss is 0.16493412852287292\n",
      "epoch: 8 step: 1629, loss is 0.2034023106098175\n",
      "epoch: 8 step: 1630, loss is 0.22907057404518127\n",
      "epoch: 8 step: 1631, loss is 0.2763703167438507\n",
      "epoch: 8 step: 1632, loss is 0.24018798768520355\n",
      "epoch: 8 step: 1633, loss is 0.34596583247184753\n",
      "epoch: 8 step: 1634, loss is 0.3780711889266968\n",
      "epoch: 8 step: 1635, loss is 0.22340603172779083\n",
      "epoch: 8 step: 1636, loss is 0.22861211001873016\n",
      "epoch: 8 step: 1637, loss is 0.039492323994636536\n",
      "epoch: 8 step: 1638, loss is 0.24891144037246704\n",
      "epoch: 8 step: 1639, loss is 0.07664549350738525\n",
      "epoch: 8 step: 1640, loss is 0.3007056415081024\n",
      "epoch: 8 step: 1641, loss is 0.3316664397716522\n",
      "epoch: 8 step: 1642, loss is 0.18350321054458618\n",
      "epoch: 8 step: 1643, loss is 0.21894389390945435\n",
      "epoch: 8 step: 1644, loss is 0.552544355392456\n",
      "epoch: 8 step: 1645, loss is 0.21795035898685455\n",
      "epoch: 8 step: 1646, loss is 0.05447142571210861\n",
      "epoch: 8 step: 1647, loss is 0.6969630718231201\n",
      "epoch: 8 step: 1648, loss is 0.6643556952476501\n",
      "epoch: 8 step: 1649, loss is 0.25014764070510864\n",
      "epoch: 8 step: 1650, loss is 0.28155046701431274\n",
      "epoch: 8 step: 1651, loss is 0.17786309123039246\n",
      "epoch: 8 step: 1652, loss is 0.28556516766548157\n",
      "epoch: 8 step: 1653, loss is 0.33339226245880127\n",
      "epoch: 8 step: 1654, loss is 0.17172026634216309\n",
      "epoch: 8 step: 1655, loss is 0.1737930029630661\n",
      "epoch: 8 step: 1656, loss is 0.1832970678806305\n",
      "epoch: 8 step: 1657, loss is 0.21015769243240356\n",
      "epoch: 8 step: 1658, loss is 0.09651688486337662\n",
      "epoch: 8 step: 1659, loss is 0.06813202798366547\n",
      "epoch: 8 step: 1660, loss is 0.17434196174144745\n",
      "epoch: 8 step: 1661, loss is 0.0956210121512413\n",
      "epoch: 8 step: 1662, loss is 0.06333450973033905\n",
      "epoch: 8 step: 1663, loss is 0.17272022366523743\n",
      "epoch: 8 step: 1664, loss is 0.08290300518274307\n",
      "epoch: 8 step: 1665, loss is 0.09181156754493713\n",
      "epoch: 8 step: 1666, loss is 0.01620692014694214\n",
      "epoch: 8 step: 1667, loss is 0.24200186133384705\n",
      "epoch: 8 step: 1668, loss is 0.18724778294563293\n",
      "epoch: 8 step: 1669, loss is 0.1989390105009079\n",
      "epoch: 8 step: 1670, loss is 0.08671143651008606\n",
      "epoch: 8 step: 1671, loss is 0.5045841336250305\n",
      "epoch: 8 step: 1672, loss is 0.10953190177679062\n",
      "epoch: 8 step: 1673, loss is 0.12895707786083221\n",
      "epoch: 8 step: 1674, loss is 0.5313019752502441\n",
      "epoch: 8 step: 1675, loss is 0.5038619041442871\n",
      "epoch: 8 step: 1676, loss is 0.137155681848526\n",
      "epoch: 8 step: 1677, loss is 0.26655837893486023\n",
      "epoch: 8 step: 1678, loss is 0.37241053581237793\n",
      "epoch: 8 step: 1679, loss is 0.03514498099684715\n",
      "epoch: 8 step: 1680, loss is 0.15751856565475464\n",
      "epoch: 8 step: 1681, loss is 0.45564761757850647\n",
      "epoch: 8 step: 1682, loss is 0.16865094006061554\n",
      "epoch: 8 step: 1683, loss is 0.2599429786205292\n",
      "epoch: 8 step: 1684, loss is 0.11898341774940491\n",
      "epoch: 8 step: 1685, loss is 0.14375971257686615\n",
      "epoch: 8 step: 1686, loss is 0.27879637479782104\n",
      "epoch: 8 step: 1687, loss is 0.34727802872657776\n",
      "epoch: 8 step: 1688, loss is 0.20851247012615204\n",
      "epoch: 8 step: 1689, loss is 0.04759404435753822\n",
      "epoch: 8 step: 1690, loss is 0.436149924993515\n",
      "epoch: 8 step: 1691, loss is 0.27159208059310913\n",
      "epoch: 8 step: 1692, loss is 0.20922762155532837\n",
      "epoch: 8 step: 1693, loss is 0.3146544098854065\n",
      "epoch: 8 step: 1694, loss is 0.03700420632958412\n",
      "epoch: 8 step: 1695, loss is 0.15734855830669403\n",
      "epoch: 8 step: 1696, loss is 0.1061667650938034\n",
      "epoch: 8 step: 1697, loss is 0.04456499591469765\n",
      "epoch: 8 step: 1698, loss is 0.12761810421943665\n",
      "epoch: 8 step: 1699, loss is 0.03692483529448509\n",
      "epoch: 8 step: 1700, loss is 0.35088083148002625\n",
      "epoch: 8 step: 1701, loss is 0.035609059035778046\n",
      "epoch: 8 step: 1702, loss is 0.2159581482410431\n",
      "epoch: 8 step: 1703, loss is 0.08518393337726593\n",
      "epoch: 8 step: 1704, loss is 0.09421439468860626\n",
      "epoch: 8 step: 1705, loss is 0.29419076442718506\n",
      "epoch: 8 step: 1706, loss is 0.052083928138017654\n",
      "epoch: 8 step: 1707, loss is 0.18350087106227875\n",
      "epoch: 8 step: 1708, loss is 0.04154712334275246\n",
      "epoch: 8 step: 1709, loss is 0.17788058519363403\n",
      "epoch: 8 step: 1710, loss is 0.035459086298942566\n",
      "epoch: 8 step: 1711, loss is 0.017041970044374466\n",
      "epoch: 8 step: 1712, loss is 0.4195432960987091\n",
      "epoch: 8 step: 1713, loss is 0.46339985728263855\n",
      "epoch: 8 step: 1714, loss is 0.09010697156190872\n",
      "epoch: 8 step: 1715, loss is 0.016937538981437683\n",
      "epoch: 8 step: 1716, loss is 0.008015196770429611\n",
      "epoch: 8 step: 1717, loss is 0.10163631290197372\n",
      "epoch: 8 step: 1718, loss is 0.6498131155967712\n",
      "epoch: 8 step: 1719, loss is 0.38501039147377014\n",
      "epoch: 8 step: 1720, loss is 0.15502913296222687\n",
      "epoch: 8 step: 1721, loss is 0.14731280505657196\n",
      "epoch: 8 step: 1722, loss is 0.300660103559494\n",
      "epoch: 8 step: 1723, loss is 0.06385679543018341\n",
      "epoch: 8 step: 1724, loss is 0.08816207200288773\n",
      "epoch: 8 step: 1725, loss is 0.09341689944267273\n",
      "epoch: 8 step: 1726, loss is 0.048405811190605164\n",
      "epoch: 8 step: 1727, loss is 0.34841352701187134\n",
      "epoch: 8 step: 1728, loss is 0.5444682240486145\n",
      "epoch: 8 step: 1729, loss is 0.3464747369289398\n",
      "epoch: 8 step: 1730, loss is 0.49107810854911804\n",
      "epoch: 8 step: 1731, loss is 0.4503791034221649\n",
      "epoch: 8 step: 1732, loss is 0.4074432849884033\n",
      "epoch: 8 step: 1733, loss is 0.5633614659309387\n",
      "epoch: 8 step: 1734, loss is 0.03864385932683945\n",
      "epoch: 8 step: 1735, loss is 0.759541392326355\n",
      "epoch: 8 step: 1736, loss is 0.15902802348136902\n",
      "epoch: 8 step: 1737, loss is 0.13807177543640137\n",
      "epoch: 8 step: 1738, loss is 0.19459299743175507\n",
      "epoch: 8 step: 1739, loss is 0.0991896316409111\n",
      "epoch: 8 step: 1740, loss is 0.2628997266292572\n",
      "epoch: 8 step: 1741, loss is 0.22854673862457275\n",
      "epoch: 8 step: 1742, loss is 0.18357697129249573\n",
      "epoch: 8 step: 1743, loss is 0.19310709834098816\n",
      "epoch: 8 step: 1744, loss is 0.1067371666431427\n",
      "epoch: 8 step: 1745, loss is 0.06701622158288956\n",
      "epoch: 8 step: 1746, loss is 0.18068335950374603\n",
      "epoch: 8 step: 1747, loss is 0.22238321602344513\n",
      "epoch: 8 step: 1748, loss is 0.2580426335334778\n",
      "epoch: 8 step: 1749, loss is 0.2745637893676758\n",
      "epoch: 8 step: 1750, loss is 0.07739481329917908\n",
      "epoch: 8 step: 1751, loss is 0.23512251675128937\n",
      "epoch: 8 step: 1752, loss is 0.0973844826221466\n",
      "epoch: 8 step: 1753, loss is 0.13154691457748413\n",
      "epoch: 8 step: 1754, loss is 0.04600285738706589\n",
      "epoch: 8 step: 1755, loss is 0.374571293592453\n",
      "epoch: 8 step: 1756, loss is 0.04146706312894821\n",
      "epoch: 8 step: 1757, loss is 0.2592223286628723\n",
      "epoch: 8 step: 1758, loss is 0.3780578076839447\n",
      "epoch: 8 step: 1759, loss is 0.3643341064453125\n",
      "epoch: 8 step: 1760, loss is 0.39722615480422974\n",
      "epoch: 8 step: 1761, loss is 0.04801829904317856\n",
      "epoch: 8 step: 1762, loss is 0.14557501673698425\n",
      "epoch: 8 step: 1763, loss is 0.3320756256580353\n",
      "epoch: 8 step: 1764, loss is 0.11394091695547104\n",
      "epoch: 8 step: 1765, loss is 0.20840969681739807\n",
      "epoch: 8 step: 1766, loss is 0.4123498201370239\n",
      "epoch: 8 step: 1767, loss is 0.15431185066699982\n",
      "epoch: 8 step: 1768, loss is 0.24033187329769135\n",
      "epoch: 8 step: 1769, loss is 0.6459053754806519\n",
      "epoch: 8 step: 1770, loss is 0.21145139634609222\n",
      "epoch: 8 step: 1771, loss is 0.13437849283218384\n",
      "epoch: 8 step: 1772, loss is 0.29934951663017273\n",
      "epoch: 8 step: 1773, loss is 0.14644713699817657\n",
      "epoch: 8 step: 1774, loss is 0.12925848364830017\n",
      "epoch: 8 step: 1775, loss is 0.11771088093519211\n",
      "epoch: 8 step: 1776, loss is 0.07913649827241898\n",
      "epoch: 8 step: 1777, loss is 0.23170742392539978\n",
      "epoch: 8 step: 1778, loss is 0.39401480555534363\n",
      "epoch: 8 step: 1779, loss is 0.06806036829948425\n",
      "epoch: 8 step: 1780, loss is 0.0286167673766613\n",
      "epoch: 8 step: 1781, loss is 0.06252291798591614\n",
      "epoch: 8 step: 1782, loss is 0.10131698101758957\n",
      "epoch: 8 step: 1783, loss is 0.15131773054599762\n",
      "epoch: 8 step: 1784, loss is 0.19647997617721558\n",
      "epoch: 8 step: 1785, loss is 0.1980435997247696\n",
      "epoch: 8 step: 1786, loss is 0.06483441591262817\n",
      "epoch: 8 step: 1787, loss is 0.1554516702890396\n",
      "epoch: 8 step: 1788, loss is 0.2523009181022644\n",
      "epoch: 8 step: 1789, loss is 0.07503020763397217\n",
      "epoch: 8 step: 1790, loss is 0.2527880072593689\n",
      "epoch: 8 step: 1791, loss is 0.09113337844610214\n",
      "epoch: 8 step: 1792, loss is 0.03758861497044563\n",
      "epoch: 8 step: 1793, loss is 0.02256910689175129\n",
      "epoch: 8 step: 1794, loss is 0.2568237781524658\n",
      "epoch: 8 step: 1795, loss is 0.08970779180526733\n",
      "epoch: 8 step: 1796, loss is 0.6751510500907898\n",
      "epoch: 8 step: 1797, loss is 0.11377105116844177\n",
      "epoch: 8 step: 1798, loss is 0.07494290918111801\n",
      "epoch: 8 step: 1799, loss is 0.05465158447623253\n",
      "epoch: 8 step: 1800, loss is 0.12116548418998718\n",
      "epoch: 8 step: 1801, loss is 0.6080315113067627\n",
      "epoch: 8 step: 1802, loss is 0.13157470524311066\n",
      "epoch: 8 step: 1803, loss is 0.007731777150183916\n",
      "epoch: 8 step: 1804, loss is 0.2835414409637451\n",
      "epoch: 8 step: 1805, loss is 0.11386688798666\n",
      "epoch: 8 step: 1806, loss is 0.32925960421562195\n",
      "epoch: 8 step: 1807, loss is 0.11443672329187393\n",
      "epoch: 8 step: 1808, loss is 0.04122146591544151\n",
      "epoch: 8 step: 1809, loss is 0.08069165796041489\n",
      "epoch: 8 step: 1810, loss is 0.2058548778295517\n",
      "epoch: 8 step: 1811, loss is 0.7973822355270386\n",
      "epoch: 8 step: 1812, loss is 0.23828375339508057\n",
      "epoch: 8 step: 1813, loss is 0.01900528185069561\n",
      "epoch: 8 step: 1814, loss is 0.05606275424361229\n",
      "epoch: 8 step: 1815, loss is 0.2007199227809906\n",
      "epoch: 8 step: 1816, loss is 0.9216651916503906\n",
      "epoch: 8 step: 1817, loss is 0.033635616302490234\n",
      "epoch: 8 step: 1818, loss is 0.3164556920528412\n",
      "epoch: 8 step: 1819, loss is 0.46132031083106995\n",
      "epoch: 8 step: 1820, loss is 0.09292850643396378\n",
      "epoch: 8 step: 1821, loss is 0.1589062660932541\n",
      "epoch: 8 step: 1822, loss is 0.1634993702173233\n",
      "epoch: 8 step: 1823, loss is 0.3531021475791931\n",
      "epoch: 8 step: 1824, loss is 0.15820063650608063\n",
      "epoch: 8 step: 1825, loss is 0.19804984331130981\n",
      "epoch: 8 step: 1826, loss is 0.2885928153991699\n",
      "epoch: 8 step: 1827, loss is 0.1746540367603302\n",
      "epoch: 8 step: 1828, loss is 0.2005864530801773\n",
      "epoch: 8 step: 1829, loss is 0.7137581706047058\n",
      "epoch: 8 step: 1830, loss is 0.23074236512184143\n",
      "epoch: 8 step: 1831, loss is 0.5462283492088318\n",
      "epoch: 8 step: 1832, loss is 0.27761635184288025\n",
      "epoch: 8 step: 1833, loss is 0.5469182133674622\n",
      "epoch: 8 step: 1834, loss is 0.13861502707004547\n",
      "epoch: 8 step: 1835, loss is 0.12804082036018372\n",
      "epoch: 8 step: 1836, loss is 0.2522505521774292\n",
      "epoch: 8 step: 1837, loss is 0.1082020252943039\n",
      "epoch: 8 step: 1838, loss is 0.17914250493049622\n",
      "epoch: 8 step: 1839, loss is 0.07574782520532608\n",
      "epoch: 8 step: 1840, loss is 0.29924193024635315\n",
      "epoch: 8 step: 1841, loss is 0.19152121245861053\n",
      "epoch: 8 step: 1842, loss is 0.320596843957901\n",
      "epoch: 8 step: 1843, loss is 0.21743611991405487\n",
      "epoch: 8 step: 1844, loss is 0.8293477296829224\n",
      "epoch: 8 step: 1845, loss is 0.24745739996433258\n",
      "epoch: 8 step: 1846, loss is 0.21177655458450317\n",
      "epoch: 8 step: 1847, loss is 0.1578795164823532\n",
      "epoch: 8 step: 1848, loss is 0.11894554644823074\n",
      "epoch: 8 step: 1849, loss is 0.7137055397033691\n",
      "epoch: 8 step: 1850, loss is 0.47477492690086365\n",
      "epoch: 8 step: 1851, loss is 0.27235183119773865\n",
      "epoch: 8 step: 1852, loss is 0.12065721303224564\n",
      "epoch: 8 step: 1853, loss is 0.19917480647563934\n",
      "epoch: 8 step: 1854, loss is 0.24845443665981293\n",
      "epoch: 8 step: 1855, loss is 0.14295899868011475\n",
      "epoch: 8 step: 1856, loss is 0.7776796817779541\n",
      "epoch: 8 step: 1857, loss is 0.27152466773986816\n",
      "epoch: 8 step: 1858, loss is 0.17153441905975342\n",
      "epoch: 8 step: 1859, loss is 0.25597789883613586\n",
      "epoch: 8 step: 1860, loss is 0.16338308155536652\n",
      "epoch: 8 step: 1861, loss is 0.35331669449806213\n",
      "epoch: 8 step: 1862, loss is 0.2501896619796753\n",
      "epoch: 8 step: 1863, loss is 0.046550143510103226\n",
      "epoch: 8 step: 1864, loss is 0.2340407520532608\n",
      "epoch: 8 step: 1865, loss is 0.152016744017601\n",
      "epoch: 8 step: 1866, loss is 0.055993277579545975\n",
      "epoch: 8 step: 1867, loss is 0.3204719126224518\n",
      "epoch: 8 step: 1868, loss is 0.14415332674980164\n",
      "epoch: 8 step: 1869, loss is 0.10980509966611862\n",
      "epoch: 8 step: 1870, loss is 0.4726381003856659\n",
      "epoch: 8 step: 1871, loss is 0.0945868268609047\n",
      "epoch: 8 step: 1872, loss is 0.760644257068634\n",
      "epoch: 8 step: 1873, loss is 0.12750157713890076\n",
      "epoch: 8 step: 1874, loss is 0.04904478043317795\n",
      "epoch: 8 step: 1875, loss is 0.05642351135611534\n",
      "epoch: 8 step: 1876, loss is 0.15962927043437958\n",
      "epoch: 8 step: 1877, loss is 0.4438854157924652\n",
      "epoch: 8 step: 1878, loss is 0.2917977273464203\n",
      "epoch: 8 step: 1879, loss is 0.26956063508987427\n",
      "epoch: 8 step: 1880, loss is 0.1704670488834381\n",
      "epoch: 8 step: 1881, loss is 0.15707863867282867\n",
      "epoch: 8 step: 1882, loss is 0.2726513743400574\n",
      "epoch: 8 step: 1883, loss is 0.32576414942741394\n",
      "epoch: 8 step: 1884, loss is 0.19778820872306824\n",
      "epoch: 8 step: 1885, loss is 0.04839843511581421\n",
      "epoch: 8 step: 1886, loss is 0.19442367553710938\n",
      "epoch: 8 step: 1887, loss is 0.25361329317092896\n",
      "epoch: 8 step: 1888, loss is 0.1363174319267273\n",
      "epoch: 8 step: 1889, loss is 0.0853274017572403\n",
      "epoch: 8 step: 1890, loss is 0.055120907723903656\n",
      "epoch: 8 step: 1891, loss is 0.0964767336845398\n",
      "epoch: 8 step: 1892, loss is 0.5590494275093079\n",
      "epoch: 8 step: 1893, loss is 0.2742806375026703\n",
      "epoch: 8 step: 1894, loss is 0.05352119356393814\n",
      "epoch: 8 step: 1895, loss is 0.3871375322341919\n",
      "epoch: 8 step: 1896, loss is 0.30142152309417725\n",
      "epoch: 8 step: 1897, loss is 0.18060095608234406\n",
      "epoch: 8 step: 1898, loss is 0.21008029580116272\n",
      "epoch: 8 step: 1899, loss is 0.06716088205575943\n",
      "epoch: 8 step: 1900, loss is 0.19821971654891968\n",
      "epoch: 8 step: 1901, loss is 0.4652920067310333\n",
      "epoch: 8 step: 1902, loss is 0.09042105078697205\n",
      "epoch: 8 step: 1903, loss is 0.08630985021591187\n",
      "epoch: 8 step: 1904, loss is 0.46251216530799866\n",
      "epoch: 8 step: 1905, loss is 0.7078969478607178\n",
      "epoch: 8 step: 1906, loss is 0.16956542432308197\n",
      "epoch: 8 step: 1907, loss is 0.03322120010852814\n",
      "epoch: 8 step: 1908, loss is 0.05828654766082764\n",
      "epoch: 8 step: 1909, loss is 0.2248009741306305\n",
      "epoch: 8 step: 1910, loss is 0.762504518032074\n",
      "epoch: 8 step: 1911, loss is 0.04769428074359894\n",
      "epoch: 8 step: 1912, loss is 0.07410751283168793\n",
      "epoch: 8 step: 1913, loss is 0.328489750623703\n",
      "epoch: 8 step: 1914, loss is 0.17772872745990753\n",
      "epoch: 8 step: 1915, loss is 0.3793514370918274\n",
      "epoch: 8 step: 1916, loss is 0.5718116760253906\n",
      "epoch: 8 step: 1917, loss is 0.15196679532527924\n",
      "epoch: 8 step: 1918, loss is 0.6620033979415894\n",
      "epoch: 8 step: 1919, loss is 0.4252687394618988\n",
      "epoch: 8 step: 1920, loss is 0.09808186441659927\n",
      "epoch: 8 step: 1921, loss is 0.299468994140625\n",
      "epoch: 8 step: 1922, loss is 0.3221854269504547\n",
      "epoch: 8 step: 1923, loss is 0.05174892395734787\n",
      "epoch: 8 step: 1924, loss is 0.25888094305992126\n",
      "epoch: 8 step: 1925, loss is 0.40294548869132996\n",
      "epoch: 8 step: 1926, loss is 0.7342603206634521\n",
      "epoch: 8 step: 1927, loss is 0.20584486424922943\n",
      "epoch: 8 step: 1928, loss is 0.2952994406223297\n",
      "epoch: 8 step: 1929, loss is 0.28361693024635315\n",
      "epoch: 8 step: 1930, loss is 0.6397050023078918\n",
      "epoch: 8 step: 1931, loss is 0.18604137003421783\n",
      "epoch: 8 step: 1932, loss is 0.0739758312702179\n",
      "epoch: 8 step: 1933, loss is 0.1416405886411667\n",
      "epoch: 8 step: 1934, loss is 0.2528754770755768\n",
      "epoch: 8 step: 1935, loss is 0.22706398367881775\n",
      "epoch: 8 step: 1936, loss is 0.26739469170570374\n",
      "epoch: 8 step: 1937, loss is 0.10825328528881073\n",
      "epoch: 8 step: 1938, loss is 0.0854908898472786\n",
      "epoch: 8 step: 1939, loss is 0.31706395745277405\n",
      "epoch: 8 step: 1940, loss is 0.6067098379135132\n",
      "epoch: 8 step: 1941, loss is 0.3370587229728699\n",
      "epoch: 8 step: 1942, loss is 0.15401548147201538\n",
      "epoch: 8 step: 1943, loss is 0.06125761196017265\n",
      "epoch: 8 step: 1944, loss is 0.0687248557806015\n",
      "epoch: 8 step: 1945, loss is 0.12180877476930618\n",
      "epoch: 8 step: 1946, loss is 0.1391536146402359\n",
      "epoch: 8 step: 1947, loss is 0.30092936754226685\n",
      "epoch: 8 step: 1948, loss is 0.25249776244163513\n",
      "epoch: 8 step: 1949, loss is 0.36572349071502686\n",
      "epoch: 8 step: 1950, loss is 0.2478989064693451\n",
      "epoch: 8 step: 1951, loss is 0.04483885318040848\n",
      "epoch: 8 step: 1952, loss is 0.15032193064689636\n",
      "epoch: 8 step: 1953, loss is 0.2251836508512497\n",
      "epoch: 8 step: 1954, loss is 0.49468836188316345\n",
      "epoch: 8 step: 1955, loss is 0.5483970046043396\n",
      "epoch: 8 step: 1956, loss is 0.1921529620885849\n",
      "epoch: 8 step: 1957, loss is 0.5572428703308105\n",
      "epoch: 8 step: 1958, loss is 0.6460483074188232\n",
      "epoch: 8 step: 1959, loss is 0.09320970624685287\n",
      "epoch: 8 step: 1960, loss is 0.05741659179329872\n",
      "epoch: 8 step: 1961, loss is 0.20501551032066345\n",
      "epoch: 8 step: 1962, loss is 0.19819888472557068\n",
      "epoch: 8 step: 1963, loss is 0.2782460153102875\n",
      "epoch: 8 step: 1964, loss is 0.17239594459533691\n",
      "epoch: 8 step: 1965, loss is 0.19853587448596954\n",
      "epoch: 8 step: 1966, loss is 0.09687262773513794\n",
      "epoch: 8 step: 1967, loss is 0.40926894545555115\n",
      "epoch: 8 step: 1968, loss is 0.04802841320633888\n",
      "epoch: 8 step: 1969, loss is 0.05986066907644272\n",
      "epoch: 8 step: 1970, loss is 0.02486584708094597\n",
      "epoch: 8 step: 1971, loss is 0.40298330783843994\n",
      "epoch: 8 step: 1972, loss is 0.11800466477870941\n",
      "epoch: 8 step: 1973, loss is 0.18859347701072693\n",
      "epoch: 8 step: 1974, loss is 0.2188311666250229\n",
      "epoch: 8 step: 1975, loss is 0.8804515600204468\n",
      "epoch: 8 step: 1976, loss is 0.14634770154953003\n",
      "epoch: 8 step: 1977, loss is 0.37718701362609863\n",
      "epoch: 8 step: 1978, loss is 0.04216786101460457\n",
      "epoch: 8 step: 1979, loss is 0.3880259692668915\n",
      "epoch: 8 step: 1980, loss is 0.13190576434135437\n",
      "epoch: 8 step: 1981, loss is 0.030152658000588417\n",
      "epoch: 8 step: 1982, loss is 0.018044155091047287\n",
      "epoch: 8 step: 1983, loss is 0.6952747106552124\n",
      "epoch: 8 step: 1984, loss is 0.12563970685005188\n",
      "epoch: 8 step: 1985, loss is 0.7549735903739929\n",
      "epoch: 8 step: 1986, loss is 0.07207594811916351\n",
      "epoch: 8 step: 1987, loss is 0.019840043038129807\n",
      "epoch: 8 step: 1988, loss is 0.3397333323955536\n",
      "epoch: 8 step: 1989, loss is 0.12000808864831924\n",
      "epoch: 8 step: 1990, loss is 0.07655477523803711\n",
      "epoch: 8 step: 1991, loss is 0.16082999110221863\n",
      "epoch: 8 step: 1992, loss is 0.49999144673347473\n",
      "epoch: 8 step: 1993, loss is 0.2791425287723541\n",
      "epoch: 8 step: 1994, loss is 0.1586097925901413\n",
      "epoch: 8 step: 1995, loss is 0.09424560517072678\n",
      "epoch: 8 step: 1996, loss is 0.08400390297174454\n",
      "epoch: 8 step: 1997, loss is 0.13985958695411682\n",
      "epoch: 8 step: 1998, loss is 0.34665870666503906\n",
      "epoch: 8 step: 1999, loss is 0.5646929740905762\n",
      "epoch: 8 step: 2000, loss is 0.25329673290252686\n",
      "epoch: 8 step: 2001, loss is 0.04323255643248558\n",
      "epoch: 8 step: 2002, loss is 0.20115269720554352\n",
      "epoch: 8 step: 2003, loss is 0.23396721482276917\n",
      "epoch: 8 step: 2004, loss is 0.2686336934566498\n",
      "epoch: 8 step: 2005, loss is 0.26984962821006775\n",
      "epoch: 8 step: 2006, loss is 0.17228169739246368\n",
      "epoch: 8 step: 2007, loss is 0.28020361065864563\n",
      "epoch: 8 step: 2008, loss is 0.13437874615192413\n",
      "epoch: 8 step: 2009, loss is 0.0877659022808075\n",
      "epoch: 8 step: 2010, loss is 0.2125205248594284\n",
      "epoch: 8 step: 2011, loss is 0.16233505308628082\n",
      "epoch: 8 step: 2012, loss is 0.290065735578537\n",
      "epoch: 8 step: 2013, loss is 0.019344110041856766\n",
      "epoch: 8 step: 2014, loss is 0.598278284072876\n",
      "epoch: 8 step: 2015, loss is 0.0027818947564810514\n",
      "epoch: 8 step: 2016, loss is 0.11639256030321121\n",
      "epoch: 8 step: 2017, loss is 0.16726769506931305\n",
      "epoch: 8 step: 2018, loss is 0.043056976050138474\n",
      "epoch: 8 step: 2019, loss is 0.5707038044929504\n",
      "epoch: 8 step: 2020, loss is 0.11199252307415009\n",
      "epoch: 8 step: 2021, loss is 0.2068237066268921\n",
      "epoch: 8 step: 2022, loss is 0.0962703600525856\n",
      "epoch: 8 step: 2023, loss is 0.4425836205482483\n",
      "epoch: 8 step: 2024, loss is 0.3196468949317932\n",
      "epoch: 8 step: 2025, loss is 0.035720497369766235\n",
      "epoch: 8 step: 2026, loss is 0.15946675837039948\n",
      "epoch: 8 step: 2027, loss is 0.183628112077713\n",
      "epoch: 8 step: 2028, loss is 0.08217766135931015\n",
      "epoch: 8 step: 2029, loss is 0.26313233375549316\n",
      "epoch: 8 step: 2030, loss is 0.3828465938568115\n",
      "epoch: 8 step: 2031, loss is 0.3801257312297821\n",
      "epoch: 8 step: 2032, loss is 0.8586375713348389\n",
      "epoch: 8 step: 2033, loss is 0.6880427002906799\n",
      "epoch: 8 step: 2034, loss is 0.061100542545318604\n",
      "epoch: 8 step: 2035, loss is 0.07479716092348099\n",
      "epoch: 8 step: 2036, loss is 0.44463711977005005\n",
      "epoch: 8 step: 2037, loss is 0.5952551960945129\n",
      "epoch: 8 step: 2038, loss is 0.3698328733444214\n",
      "epoch: 8 step: 2039, loss is 0.20114679634571075\n",
      "epoch: 8 step: 2040, loss is 0.29447630047798157\n",
      "epoch: 8 step: 2041, loss is 0.17742769420146942\n",
      "epoch: 8 step: 2042, loss is 0.07327082753181458\n",
      "epoch: 8 step: 2043, loss is 0.063151516020298\n",
      "epoch: 8 step: 2044, loss is 0.39192819595336914\n",
      "epoch: 8 step: 2045, loss is 0.5294544696807861\n",
      "epoch: 8 step: 2046, loss is 0.09184964746236801\n",
      "epoch: 8 step: 2047, loss is 0.04380936175584793\n",
      "epoch: 8 step: 2048, loss is 0.3306473195552826\n",
      "epoch: 8 step: 2049, loss is 0.19971691071987152\n",
      "epoch: 8 step: 2050, loss is 0.2978382408618927\n",
      "epoch: 8 step: 2051, loss is 0.8391289710998535\n",
      "epoch: 8 step: 2052, loss is 0.19625204801559448\n",
      "epoch: 8 step: 2053, loss is 0.43937912583351135\n",
      "epoch: 8 step: 2054, loss is 0.414241760969162\n",
      "epoch: 8 step: 2055, loss is 0.1300450563430786\n",
      "epoch: 8 step: 2056, loss is 0.40081045031547546\n",
      "epoch: 8 step: 2057, loss is 0.10737894475460052\n",
      "epoch: 8 step: 2058, loss is 0.2862156629562378\n",
      "epoch: 8 step: 2059, loss is 0.11283126473426819\n",
      "epoch: 8 step: 2060, loss is 0.08768030256032944\n",
      "epoch: 8 step: 2061, loss is 0.16516651213169098\n",
      "epoch: 8 step: 2062, loss is 0.22975264489650726\n",
      "epoch: 8 step: 2063, loss is 0.31686267256736755\n",
      "epoch: 8 step: 2064, loss is 0.7016755938529968\n",
      "epoch: 8 step: 2065, loss is 0.18293660879135132\n",
      "epoch: 8 step: 2066, loss is 0.2047625035047531\n",
      "epoch: 8 step: 2067, loss is 0.19019873440265656\n",
      "epoch: 8 step: 2068, loss is 0.14273087680339813\n",
      "epoch: 8 step: 2069, loss is 0.10050874203443527\n",
      "epoch: 8 step: 2070, loss is 0.311488538980484\n",
      "epoch: 8 step: 2071, loss is 0.10152441263198853\n",
      "epoch: 8 step: 2072, loss is 0.05706778168678284\n",
      "epoch: 8 step: 2073, loss is 0.17312532663345337\n",
      "epoch: 8 step: 2074, loss is 0.033512018620967865\n",
      "epoch: 8 step: 2075, loss is 0.2681099474430084\n",
      "epoch: 8 step: 2076, loss is 0.4242531359195709\n",
      "epoch: 8 step: 2077, loss is 0.11303318291902542\n",
      "epoch: 8 step: 2078, loss is 0.13984525203704834\n",
      "epoch: 8 step: 2079, loss is 1.0914400815963745\n",
      "epoch: 8 step: 2080, loss is 0.24718137085437775\n",
      "epoch: 8 step: 2081, loss is 0.27184250950813293\n",
      "epoch: 8 step: 2082, loss is 0.10507500916719437\n",
      "epoch: 8 step: 2083, loss is 0.030381793156266212\n",
      "epoch: 8 step: 2084, loss is 1.1502563953399658\n",
      "epoch: 8 step: 2085, loss is 0.09117623418569565\n",
      "epoch: 8 step: 2086, loss is 0.3133459687232971\n",
      "epoch: 8 step: 2087, loss is 0.3225797414779663\n",
      "epoch: 8 step: 2088, loss is 0.05481670796871185\n",
      "epoch: 8 step: 2089, loss is 0.26180499792099\n",
      "epoch: 8 step: 2090, loss is 0.2953965365886688\n",
      "epoch: 8 step: 2091, loss is 0.1906624436378479\n",
      "epoch: 8 step: 2092, loss is 0.13815872371196747\n",
      "epoch: 8 step: 2093, loss is 0.08953269571065903\n",
      "epoch: 8 step: 2094, loss is 0.1614503562450409\n",
      "epoch: 8 step: 2095, loss is 0.324604332447052\n",
      "epoch: 8 step: 2096, loss is 0.08089397847652435\n",
      "epoch: 8 step: 2097, loss is 0.18207885324954987\n",
      "epoch: 8 step: 2098, loss is 0.3368907570838928\n",
      "epoch: 8 step: 2099, loss is 0.23209889233112335\n",
      "epoch: 8 step: 2100, loss is 0.2556927502155304\n",
      "epoch: 8 step: 2101, loss is 0.2142019420862198\n",
      "epoch: 8 step: 2102, loss is 0.2995019257068634\n",
      "epoch: 8 step: 2103, loss is 0.36622458696365356\n",
      "epoch: 8 step: 2104, loss is 0.09534371644258499\n",
      "epoch: 8 step: 2105, loss is 0.35721713304519653\n",
      "epoch: 8 step: 2106, loss is 0.5352018475532532\n",
      "epoch: 8 step: 2107, loss is 0.06736646592617035\n",
      "epoch: 8 step: 2108, loss is 1.032144546508789\n",
      "epoch: 8 step: 2109, loss is 0.4510158598423004\n",
      "epoch: 8 step: 2110, loss is 0.22010359168052673\n",
      "epoch: 8 step: 2111, loss is 0.27553173899650574\n",
      "epoch: 8 step: 2112, loss is 0.024401972070336342\n",
      "epoch: 8 step: 2113, loss is 0.3205101191997528\n",
      "epoch: 8 step: 2114, loss is 0.28060221672058105\n",
      "epoch: 8 step: 2115, loss is 0.7537229061126709\n",
      "epoch: 8 step: 2116, loss is 0.060553763061761856\n",
      "epoch: 8 step: 2117, loss is 0.15987326204776764\n",
      "epoch: 8 step: 2118, loss is 0.24966537952423096\n",
      "epoch: 8 step: 2119, loss is 0.05740739777684212\n",
      "epoch: 8 step: 2120, loss is 0.5416120290756226\n",
      "epoch: 8 step: 2121, loss is 0.10242150723934174\n",
      "epoch: 8 step: 2122, loss is 0.2456296980381012\n",
      "epoch: 8 step: 2123, loss is 0.10903363674879074\n",
      "epoch: 8 step: 2124, loss is 0.3411027491092682\n",
      "epoch: 8 step: 2125, loss is 0.14155171811580658\n",
      "epoch: 8 step: 2126, loss is 0.3989182710647583\n",
      "epoch: 8 step: 2127, loss is 0.5694375038146973\n",
      "epoch: 8 step: 2128, loss is 0.3263968527317047\n",
      "epoch: 8 step: 2129, loss is 0.07466956973075867\n",
      "epoch: 8 step: 2130, loss is 0.10095789283514023\n",
      "epoch: 8 step: 2131, loss is 0.4630548357963562\n",
      "epoch: 8 step: 2132, loss is 0.10475508868694305\n",
      "epoch: 8 step: 2133, loss is 0.2003764808177948\n",
      "epoch: 8 step: 2134, loss is 0.35443517565727234\n",
      "epoch: 8 step: 2135, loss is 0.10729041695594788\n",
      "epoch: 8 step: 2136, loss is 0.4427504241466522\n",
      "epoch: 8 step: 2137, loss is 0.45886489748954773\n",
      "epoch: 8 step: 2138, loss is 0.2905347943305969\n",
      "epoch: 8 step: 2139, loss is 0.033421773463487625\n",
      "epoch: 8 step: 2140, loss is 0.19014890491962433\n",
      "epoch: 8 step: 2141, loss is 0.20840291678905487\n",
      "epoch: 8 step: 2142, loss is 0.1406702995300293\n",
      "epoch: 8 step: 2143, loss is 0.48173534870147705\n",
      "epoch: 8 step: 2144, loss is 0.23414923250675201\n",
      "epoch: 8 step: 2145, loss is 0.16042591631412506\n",
      "epoch: 8 step: 2146, loss is 0.2037682980298996\n",
      "epoch: 8 step: 2147, loss is 0.49006277322769165\n",
      "epoch: 8 step: 2148, loss is 0.4438968002796173\n",
      "epoch: 8 step: 2149, loss is 0.2561686933040619\n",
      "epoch: 8 step: 2150, loss is 0.6162089705467224\n",
      "epoch: 8 step: 2151, loss is 1.1148549318313599\n",
      "epoch: 8 step: 2152, loss is 0.3720448613166809\n",
      "epoch: 8 step: 2153, loss is 0.1778905987739563\n",
      "epoch: 8 step: 2154, loss is 0.32181742787361145\n",
      "epoch: 8 step: 2155, loss is 0.12189818918704987\n",
      "epoch: 8 step: 2156, loss is 0.26956892013549805\n",
      "epoch: 8 step: 2157, loss is 0.2656310796737671\n",
      "epoch: 8 step: 2158, loss is 0.05777301266789436\n",
      "epoch: 8 step: 2159, loss is 0.04406645894050598\n",
      "epoch: 8 step: 2160, loss is 0.17913870513439178\n",
      "epoch: 8 step: 2161, loss is 0.223388209939003\n",
      "epoch: 8 step: 2162, loss is 0.10232382267713547\n",
      "epoch: 8 step: 2163, loss is 0.4130186438560486\n",
      "epoch: 8 step: 2164, loss is 0.5483232140541077\n",
      "epoch: 8 step: 2165, loss is 0.19307079911231995\n",
      "epoch: 8 step: 2166, loss is 0.5334664583206177\n",
      "epoch: 8 step: 2167, loss is 0.07652411609888077\n",
      "epoch: 8 step: 2168, loss is 0.23582503199577332\n",
      "epoch: 8 step: 2169, loss is 0.6248486638069153\n",
      "epoch: 8 step: 2170, loss is 0.31194010376930237\n",
      "epoch: 8 step: 2171, loss is 0.12483421713113785\n",
      "epoch: 8 step: 2172, loss is 0.2079431116580963\n",
      "epoch: 8 step: 2173, loss is 0.24771691858768463\n",
      "epoch: 8 step: 2174, loss is 0.19053176045417786\n",
      "epoch: 8 step: 2175, loss is 0.4732078015804291\n",
      "epoch: 8 step: 2176, loss is 0.10045668482780457\n",
      "epoch: 8 step: 2177, loss is 0.23339985311031342\n",
      "epoch: 8 step: 2178, loss is 0.04738011956214905\n",
      "epoch: 8 step: 2179, loss is 0.0783044770359993\n",
      "epoch: 8 step: 2180, loss is 0.5370908379554749\n",
      "epoch: 8 step: 2181, loss is 0.24492013454437256\n",
      "epoch: 8 step: 2182, loss is 0.07008267194032669\n",
      "epoch: 8 step: 2183, loss is 0.2896735966205597\n",
      "epoch: 8 step: 2184, loss is 0.22204065322875977\n",
      "epoch: 8 step: 2185, loss is 0.24188101291656494\n",
      "epoch: 8 step: 2186, loss is 0.04641197994351387\n",
      "epoch: 8 step: 2187, loss is 0.1714026778936386\n",
      "epoch: 8 step: 2188, loss is 0.1455422192811966\n",
      "epoch: 8 step: 2189, loss is 0.046383269131183624\n",
      "epoch: 8 step: 2190, loss is 0.15367522835731506\n",
      "epoch: 8 step: 2191, loss is 0.09663300961256027\n",
      "epoch: 8 step: 2192, loss is 0.054146137088537216\n",
      "epoch: 8 step: 2193, loss is 0.1565363109111786\n",
      "epoch: 8 step: 2194, loss is 0.18828268349170685\n",
      "epoch: 8 step: 2195, loss is 0.031370267271995544\n",
      "epoch: 8 step: 2196, loss is 0.04734780266880989\n",
      "epoch: 8 step: 2197, loss is 0.08183273673057556\n",
      "epoch: 8 step: 2198, loss is 0.016592929139733315\n",
      "epoch: 8 step: 2199, loss is 0.19162890315055847\n",
      "epoch: 8 step: 2200, loss is 0.34263139963150024\n",
      "epoch: 8 step: 2201, loss is 0.1949671506881714\n",
      "epoch: 8 step: 2202, loss is 0.06901475787162781\n",
      "epoch: 8 step: 2203, loss is 0.828627347946167\n",
      "epoch: 8 step: 2204, loss is 0.5639861226081848\n",
      "epoch: 8 step: 2205, loss is 0.4316334128379822\n",
      "epoch: 8 step: 2206, loss is 0.05224677547812462\n",
      "epoch: 8 step: 2207, loss is 0.08110951632261276\n",
      "epoch: 8 step: 2208, loss is 0.05699102580547333\n",
      "epoch: 8 step: 2209, loss is 0.14265678822994232\n",
      "epoch: 8 step: 2210, loss is 0.4040675163269043\n",
      "epoch: 8 step: 2211, loss is 0.833224892616272\n",
      "epoch: 8 step: 2212, loss is 0.03870198130607605\n",
      "epoch: 8 step: 2213, loss is 0.03973456099629402\n",
      "epoch: 8 step: 2214, loss is 0.27798882126808167\n",
      "epoch: 8 step: 2215, loss is 0.7123773694038391\n",
      "epoch: 8 step: 2216, loss is 0.14524030685424805\n",
      "epoch: 8 step: 2217, loss is 0.2681680917739868\n",
      "epoch: 8 step: 2218, loss is 0.26895004510879517\n",
      "epoch: 8 step: 2219, loss is 0.047279734164476395\n",
      "epoch: 8 step: 2220, loss is 0.3807927966117859\n",
      "epoch: 8 step: 2221, loss is 0.048625774681568146\n",
      "epoch: 8 step: 2222, loss is 0.18486963212490082\n",
      "epoch: 8 step: 2223, loss is 0.11333245038986206\n",
      "epoch: 8 step: 2224, loss is 0.18070779740810394\n",
      "epoch: 8 step: 2225, loss is 0.21096271276474\n",
      "epoch: 8 step: 2226, loss is 0.13360102474689484\n",
      "epoch: 8 step: 2227, loss is 0.39550790190696716\n",
      "epoch: 8 step: 2228, loss is 0.38850176334381104\n",
      "epoch: 8 step: 2229, loss is 0.04531529173254967\n",
      "epoch: 8 step: 2230, loss is 0.19947806000709534\n",
      "epoch: 8 step: 2231, loss is 0.3187348246574402\n",
      "epoch: 8 step: 2232, loss is 0.24157117307186127\n",
      "epoch: 8 step: 2233, loss is 0.18827670812606812\n",
      "epoch: 8 step: 2234, loss is 0.060240961611270905\n",
      "epoch: 8 step: 2235, loss is 0.18408739566802979\n",
      "epoch: 8 step: 2236, loss is 0.18247157335281372\n",
      "epoch: 8 step: 2237, loss is 0.7226858735084534\n",
      "epoch: 8 step: 2238, loss is 0.42719054222106934\n",
      "epoch: 8 step: 2239, loss is 0.08961701393127441\n",
      "epoch: 8 step: 2240, loss is 0.4347996413707733\n",
      "epoch: 8 step: 2241, loss is 0.1558874398469925\n",
      "epoch: 8 step: 2242, loss is 0.2065465748310089\n",
      "epoch: 8 step: 2243, loss is 0.6726025342941284\n",
      "epoch: 8 step: 2244, loss is 0.15319237112998962\n",
      "epoch: 8 step: 2245, loss is 0.21035444736480713\n",
      "epoch: 8 step: 2246, loss is 0.2487086057662964\n",
      "epoch: 8 step: 2247, loss is 0.09633251279592514\n",
      "epoch: 8 step: 2248, loss is 0.2106385976076126\n",
      "epoch: 8 step: 2249, loss is 0.3866974115371704\n",
      "epoch: 8 step: 2250, loss is 0.2399449348449707\n",
      "epoch: 8 step: 2251, loss is 0.3395099639892578\n",
      "epoch: 8 step: 2252, loss is 0.11229085177183151\n",
      "epoch: 8 step: 2253, loss is 0.03756216540932655\n",
      "epoch: 8 step: 2254, loss is 0.1278107464313507\n",
      "epoch: 8 step: 2255, loss is 0.4491030275821686\n",
      "epoch: 8 step: 2256, loss is 0.09728337079286575\n",
      "epoch: 8 step: 2257, loss is 0.012136166915297508\n",
      "epoch: 8 step: 2258, loss is 0.02001233585178852\n",
      "epoch: 8 step: 2259, loss is 0.1592755764722824\n",
      "epoch: 8 step: 2260, loss is 0.1169130951166153\n",
      "epoch: 8 step: 2261, loss is 0.058984991163015366\n",
      "epoch: 8 step: 2262, loss is 0.5640103816986084\n",
      "epoch: 8 step: 2263, loss is 0.17151059210300446\n",
      "epoch: 8 step: 2264, loss is 0.5186243653297424\n",
      "epoch: 8 step: 2265, loss is 0.1690230369567871\n",
      "epoch: 8 step: 2266, loss is 0.5563437342643738\n",
      "epoch: 8 step: 2267, loss is 0.08251257985830307\n",
      "epoch: 8 step: 2268, loss is 0.027649853378534317\n",
      "epoch: 8 step: 2269, loss is 0.067005954682827\n",
      "epoch: 8 step: 2270, loss is 0.4003429710865021\n",
      "epoch: 8 step: 2271, loss is 0.11617894470691681\n",
      "epoch: 8 step: 2272, loss is 0.07401422411203384\n",
      "epoch: 8 step: 2273, loss is 0.09553436934947968\n",
      "epoch: 8 step: 2274, loss is 0.03288024663925171\n",
      "epoch: 8 step: 2275, loss is 0.024209626019001007\n",
      "epoch: 8 step: 2276, loss is 0.04110097885131836\n",
      "epoch: 8 step: 2277, loss is 0.12609192728996277\n",
      "epoch: 8 step: 2278, loss is 0.24212254583835602\n",
      "epoch: 8 step: 2279, loss is 0.719224214553833\n",
      "epoch: 8 step: 2280, loss is 0.19556063413619995\n",
      "epoch: 8 step: 2281, loss is 0.1294911801815033\n",
      "epoch: 8 step: 2282, loss is 0.8054554462432861\n",
      "epoch: 8 step: 2283, loss is 0.2595948576927185\n",
      "epoch: 8 step: 2284, loss is 0.19077107310295105\n",
      "epoch: 8 step: 2285, loss is 0.09007031470537186\n",
      "epoch: 8 step: 2286, loss is 0.2679177522659302\n",
      "epoch: 8 step: 2287, loss is 0.24825455248355865\n",
      "epoch: 8 step: 2288, loss is 0.06085303798317909\n",
      "epoch: 8 step: 2289, loss is 0.4347078502178192\n",
      "epoch: 8 step: 2290, loss is 0.5020951628684998\n",
      "epoch: 8 step: 2291, loss is 1.2433220148086548\n",
      "epoch: 8 step: 2292, loss is 0.21879996359348297\n",
      "epoch: 8 step: 2293, loss is 0.2786193788051605\n",
      "epoch: 8 step: 2294, loss is 0.13896174728870392\n",
      "epoch: 8 step: 2295, loss is 0.2658347189426422\n",
      "epoch: 8 step: 2296, loss is 0.09934156388044357\n",
      "epoch: 8 step: 2297, loss is 0.7614820003509521\n",
      "epoch: 8 step: 2298, loss is 0.14565257728099823\n",
      "epoch: 8 step: 2299, loss is 0.3588178753852844\n",
      "epoch: 8 step: 2300, loss is 0.17349807918071747\n",
      "epoch: 8 step: 2301, loss is 0.1486188769340515\n",
      "epoch: 8 step: 2302, loss is 0.6139667630195618\n",
      "epoch: 8 step: 2303, loss is 0.09120851010084152\n",
      "epoch: 8 step: 2304, loss is 0.108125239610672\n",
      "epoch: 8 step: 2305, loss is 0.0854574516415596\n",
      "epoch: 8 step: 2306, loss is 0.2636284828186035\n",
      "epoch: 8 step: 2307, loss is 0.7039696574211121\n",
      "epoch: 8 step: 2308, loss is 0.08821934461593628\n",
      "epoch: 8 step: 2309, loss is 0.010954910889267921\n",
      "epoch: 8 step: 2310, loss is 0.10854526609182358\n",
      "epoch: 8 step: 2311, loss is 0.3592688739299774\n",
      "epoch: 8 step: 2312, loss is 0.3070681393146515\n",
      "epoch: 8 step: 2313, loss is 0.28031617403030396\n",
      "epoch: 8 step: 2314, loss is 0.03389855474233627\n",
      "epoch: 8 step: 2315, loss is 0.04608500748872757\n",
      "epoch: 8 step: 2316, loss is 0.3150579035282135\n",
      "epoch: 8 step: 2317, loss is 0.35889220237731934\n",
      "epoch: 8 step: 2318, loss is 0.11687098443508148\n",
      "epoch: 8 step: 2319, loss is 0.1681881844997406\n",
      "epoch: 8 step: 2320, loss is 0.5164974331855774\n",
      "epoch: 8 step: 2321, loss is 0.244159534573555\n",
      "epoch: 8 step: 2322, loss is 0.20431166887283325\n",
      "epoch: 8 step: 2323, loss is 0.31186288595199585\n",
      "epoch: 8 step: 2324, loss is 0.037922512739896774\n",
      "epoch: 8 step: 2325, loss is 0.27372244000434875\n",
      "epoch: 8 step: 2326, loss is 0.10285256803035736\n",
      "epoch: 8 step: 2327, loss is 0.13988889753818512\n",
      "epoch: 8 step: 2328, loss is 0.0765678733587265\n",
      "epoch: 8 step: 2329, loss is 0.1160179153084755\n",
      "epoch: 8 step: 2330, loss is 0.20540666580200195\n",
      "epoch: 8 step: 2331, loss is 0.27025654911994934\n",
      "epoch: 8 step: 2332, loss is 0.04437445476651192\n",
      "epoch: 8 step: 2333, loss is 0.27727624773979187\n",
      "epoch: 8 step: 2334, loss is 0.1516174077987671\n",
      "epoch: 8 step: 2335, loss is 0.1343252807855606\n",
      "epoch: 8 step: 2336, loss is 0.15182249248027802\n",
      "epoch: 8 step: 2337, loss is 0.438519686460495\n",
      "epoch: 8 step: 2338, loss is 0.18666519224643707\n",
      "epoch: 8 step: 2339, loss is 0.0581434965133667\n",
      "epoch: 8 step: 2340, loss is 0.3098352551460266\n",
      "epoch: 8 step: 2341, loss is 0.1945473998785019\n",
      "epoch: 8 step: 2342, loss is 0.054876990616321564\n",
      "epoch: 8 step: 2343, loss is 0.1536238193511963\n",
      "epoch: 8 step: 2344, loss is 0.19067755341529846\n",
      "epoch: 8 step: 2345, loss is 0.12703759968280792\n",
      "epoch: 8 step: 2346, loss is 0.04933791235089302\n",
      "epoch: 8 step: 2347, loss is 0.005005158018320799\n",
      "epoch: 8 step: 2348, loss is 0.07605679333209991\n",
      "epoch: 8 step: 2349, loss is 0.5680943131446838\n",
      "epoch: 8 step: 2350, loss is 0.08231144398450851\n",
      "epoch: 8 step: 2351, loss is 0.483693927526474\n",
      "epoch: 8 step: 2352, loss is 0.026478147134184837\n",
      "epoch: 8 step: 2353, loss is 0.3082292675971985\n",
      "epoch: 8 step: 2354, loss is 0.056508030742406845\n",
      "epoch: 8 step: 2355, loss is 0.5429404377937317\n",
      "epoch: 8 step: 2356, loss is 0.6368891596794128\n",
      "epoch: 8 step: 2357, loss is 0.1446550488471985\n",
      "epoch: 8 step: 2358, loss is 0.38429710268974304\n",
      "epoch: 8 step: 2359, loss is 0.13098125159740448\n",
      "epoch: 8 step: 2360, loss is 0.20570826530456543\n",
      "epoch: 8 step: 2361, loss is 0.08705022931098938\n",
      "epoch: 8 step: 2362, loss is 0.38646072149276733\n",
      "epoch: 8 step: 2363, loss is 0.2127700001001358\n",
      "epoch: 8 step: 2364, loss is 0.13076147437095642\n",
      "epoch: 8 step: 2365, loss is 0.06328831613063812\n",
      "epoch: 8 step: 2366, loss is 0.11108466237783432\n",
      "epoch: 8 step: 2367, loss is 0.04704531654715538\n",
      "epoch: 8 step: 2368, loss is 0.15500949323177338\n",
      "epoch: 8 step: 2369, loss is 0.32013407349586487\n",
      "epoch: 8 step: 2370, loss is 0.2505481541156769\n",
      "epoch: 8 step: 2371, loss is 0.49740442633628845\n",
      "epoch: 8 step: 2372, loss is 0.16587947309017181\n",
      "epoch: 8 step: 2373, loss is 0.6245880126953125\n",
      "epoch: 8 step: 2374, loss is 0.009833707474172115\n",
      "epoch: 8 step: 2375, loss is 0.6611661314964294\n",
      "epoch: 8 step: 2376, loss is 0.1204596757888794\n",
      "epoch: 8 step: 2377, loss is 0.12994326651096344\n",
      "epoch: 8 step: 2378, loss is 0.08898624777793884\n",
      "epoch: 8 step: 2379, loss is 0.05594412609934807\n",
      "epoch: 8 step: 2380, loss is 0.18450191617012024\n",
      "epoch: 8 step: 2381, loss is 0.07395844906568527\n",
      "epoch: 8 step: 2382, loss is 0.334013432264328\n",
      "epoch: 8 step: 2383, loss is 0.33857300877571106\n",
      "epoch: 8 step: 2384, loss is 0.11778809130191803\n",
      "epoch: 8 step: 2385, loss is 0.3281404674053192\n",
      "epoch: 8 step: 2386, loss is 0.5212062001228333\n",
      "epoch: 8 step: 2387, loss is 0.45530372858047485\n",
      "epoch: 8 step: 2388, loss is 0.22624270617961884\n",
      "epoch: 8 step: 2389, loss is 0.03702680766582489\n",
      "epoch: 8 step: 2390, loss is 0.03672781214118004\n",
      "epoch: 8 step: 2391, loss is 0.8581348657608032\n",
      "epoch: 8 step: 2392, loss is 0.7475317120552063\n",
      "epoch: 8 step: 2393, loss is 0.08446058630943298\n",
      "epoch: 8 step: 2394, loss is 0.08286641538143158\n",
      "epoch: 8 step: 2395, loss is 0.528870165348053\n",
      "epoch: 8 step: 2396, loss is 0.3676791489124298\n",
      "epoch: 8 step: 2397, loss is 0.29047563672065735\n",
      "epoch: 8 step: 2398, loss is 0.4744548201560974\n",
      "epoch: 8 step: 2399, loss is 0.16895458102226257\n",
      "epoch: 8 step: 2400, loss is 0.19497327506542206\n",
      "epoch: 8 step: 2401, loss is 0.2191256582736969\n",
      "epoch: 8 step: 2402, loss is 0.1121959388256073\n",
      "epoch: 8 step: 2403, loss is 0.509940505027771\n",
      "epoch: 8 step: 2404, loss is 0.04505448415875435\n",
      "epoch: 8 step: 2405, loss is 0.18961472809314728\n",
      "epoch: 8 step: 2406, loss is 0.06721708923578262\n",
      "epoch: 8 step: 2407, loss is 0.5346848964691162\n",
      "epoch: 8 step: 2408, loss is 0.19910866022109985\n",
      "epoch: 8 step: 2409, loss is 0.2404249608516693\n",
      "epoch: 8 step: 2410, loss is 0.07069828361272812\n",
      "epoch: 8 step: 2411, loss is 0.09501634538173676\n",
      "epoch: 8 step: 2412, loss is 0.12048226594924927\n",
      "epoch: 8 step: 2413, loss is 0.09009455144405365\n",
      "epoch: 8 step: 2414, loss is 0.5340691208839417\n",
      "epoch: 8 step: 2415, loss is 0.34538930654525757\n",
      "epoch: 8 step: 2416, loss is 0.0984288826584816\n",
      "epoch: 8 step: 2417, loss is 0.11665168404579163\n",
      "epoch: 8 step: 2418, loss is 0.1982303261756897\n",
      "epoch: 8 step: 2419, loss is 0.2709965407848358\n",
      "epoch: 8 step: 2420, loss is 0.08779851347208023\n",
      "epoch: 8 step: 2421, loss is 0.5416135787963867\n",
      "epoch: 8 step: 2422, loss is 0.24835233390331268\n",
      "epoch: 8 step: 2423, loss is 0.21227246522903442\n",
      "epoch: 8 step: 2424, loss is 0.4298785626888275\n",
      "epoch: 8 step: 2425, loss is 0.6190561056137085\n",
      "epoch: 8 step: 2426, loss is 0.11295227706432343\n",
      "epoch: 8 step: 2427, loss is 0.15630188584327698\n",
      "epoch: 8 step: 2428, loss is 0.3158037066459656\n",
      "epoch: 8 step: 2429, loss is 0.4060153663158417\n",
      "epoch: 8 step: 2430, loss is 0.3600043058395386\n",
      "epoch: 8 step: 2431, loss is 0.282775342464447\n",
      "epoch: 8 step: 2432, loss is 0.10930626839399338\n",
      "epoch: 8 step: 2433, loss is 0.604581892490387\n",
      "epoch: 8 step: 2434, loss is 0.10155937075614929\n",
      "epoch: 8 step: 2435, loss is 0.072828508913517\n",
      "epoch: 8 step: 2436, loss is 0.1088680550456047\n",
      "epoch: 8 step: 2437, loss is 0.2958844006061554\n",
      "epoch: 8 step: 2438, loss is 0.27688515186309814\n",
      "epoch: 8 step: 2439, loss is 0.0585801936686039\n",
      "epoch: 8 step: 2440, loss is 0.2046910971403122\n",
      "epoch: 8 step: 2441, loss is 0.7165038585662842\n",
      "epoch: 8 step: 2442, loss is 0.30553045868873596\n",
      "epoch: 8 step: 2443, loss is 0.08605244010686874\n",
      "epoch: 8 step: 2444, loss is 0.11096177995204926\n",
      "epoch: 8 step: 2445, loss is 0.3148251175880432\n",
      "epoch: 8 step: 2446, loss is 0.4652095139026642\n",
      "epoch: 8 step: 2447, loss is 0.4908433258533478\n",
      "epoch: 8 step: 2448, loss is 0.03421349823474884\n",
      "epoch: 8 step: 2449, loss is 0.11450175940990448\n",
      "epoch: 8 step: 2450, loss is 0.1603708267211914\n",
      "epoch: 8 step: 2451, loss is 0.23660287261009216\n",
      "epoch: 8 step: 2452, loss is 0.19930195808410645\n",
      "epoch: 8 step: 2453, loss is 0.18798935413360596\n",
      "epoch: 8 step: 2454, loss is 0.11929444968700409\n",
      "epoch: 8 step: 2455, loss is 0.18262679874897003\n",
      "epoch: 8 step: 2456, loss is 0.44750887155532837\n",
      "epoch: 8 step: 2457, loss is 0.1945568025112152\n",
      "epoch: 8 step: 2458, loss is 0.2724830210208893\n",
      "epoch: 8 step: 2459, loss is 0.08259426057338715\n",
      "epoch: 8 step: 2460, loss is 0.2513212263584137\n",
      "epoch: 8 step: 2461, loss is 0.6404424905776978\n",
      "epoch: 8 step: 2462, loss is 0.19141347706317902\n",
      "epoch: 8 step: 2463, loss is 0.6713604927062988\n",
      "epoch: 8 step: 2464, loss is 0.2556067109107971\n",
      "epoch: 8 step: 2465, loss is 0.23152510821819305\n",
      "epoch: 8 step: 2466, loss is 0.12354610860347748\n",
      "epoch: 8 step: 2467, loss is 0.281678706407547\n",
      "epoch: 8 step: 2468, loss is 0.2207113653421402\n",
      "epoch: 8 step: 2469, loss is 0.21537058055400848\n",
      "epoch: 8 step: 2470, loss is 0.23972617089748383\n",
      "epoch: 8 step: 2471, loss is 0.10928510129451752\n",
      "epoch: 8 step: 2472, loss is 0.19386079907417297\n",
      "epoch: 8 step: 2473, loss is 0.1141541600227356\n",
      "epoch: 8 step: 2474, loss is 0.37241238355636597\n",
      "epoch: 8 step: 2475, loss is 0.17917340993881226\n",
      "epoch: 8 step: 2476, loss is 0.1784675270318985\n",
      "epoch: 8 step: 2477, loss is 0.07979623228311539\n",
      "epoch: 8 step: 2478, loss is 0.059771012514829636\n",
      "epoch: 8 step: 2479, loss is 0.6616302132606506\n",
      "epoch: 8 step: 2480, loss is 0.1029626727104187\n",
      "epoch: 8 step: 2481, loss is 0.36330628395080566\n",
      "epoch: 8 step: 2482, loss is 0.2516869604587555\n",
      "epoch: 8 step: 2483, loss is 0.2086452841758728\n",
      "epoch: 8 step: 2484, loss is 0.09236980229616165\n",
      "epoch: 8 step: 2485, loss is 0.13380248844623566\n",
      "epoch: 8 step: 2486, loss is 0.3259343206882477\n",
      "epoch: 8 step: 2487, loss is 0.5049816966056824\n",
      "epoch: 8 step: 2488, loss is 0.1366880089044571\n",
      "epoch: 8 step: 2489, loss is 0.12818384170532227\n",
      "epoch: 8 step: 2490, loss is 0.2317049652338028\n",
      "epoch: 8 step: 2491, loss is 0.3364820182323456\n",
      "epoch: 8 step: 2492, loss is 0.2569802701473236\n",
      "epoch: 8 step: 2493, loss is 0.21093182265758514\n",
      "epoch: 8 step: 2494, loss is 0.12682673335075378\n",
      "epoch: 8 step: 2495, loss is 0.09950783848762512\n",
      "epoch: 8 step: 2496, loss is 0.3559286594390869\n",
      "epoch: 8 step: 2497, loss is 0.04113917052745819\n",
      "epoch: 8 step: 2498, loss is 0.12150291353464127\n",
      "epoch: 8 step: 2499, loss is 0.42168760299682617\n",
      "epoch: 8 step: 2500, loss is 0.0869433730840683\n",
      "epoch: 8 step: 2501, loss is 0.2641522288322449\n",
      "epoch: 8 step: 2502, loss is 0.5064104199409485\n",
      "epoch: 8 step: 2503, loss is 0.35403382778167725\n",
      "epoch: 8 step: 2504, loss is 0.36939889192581177\n",
      "epoch: 8 step: 2505, loss is 0.0920012891292572\n",
      "epoch: 8 step: 2506, loss is 0.3752281963825226\n",
      "epoch: 8 step: 2507, loss is 0.18246757984161377\n",
      "epoch: 8 step: 2508, loss is 0.18751314282417297\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: unknown JFIF revision number 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 9 step: 1, loss is 0.17572495341300964\n",
      "epoch: 9 step: 2, loss is 0.2603878378868103\n",
      "epoch: 9 step: 3, loss is 0.1716996282339096\n",
      "epoch: 9 step: 4, loss is 0.5208238363265991\n",
      "epoch: 9 step: 5, loss is 0.26201507449150085\n",
      "epoch: 9 step: 6, loss is 0.09356184303760529\n",
      "epoch: 9 step: 7, loss is 0.07166824489831924\n",
      "epoch: 9 step: 8, loss is 0.3219011723995209\n",
      "epoch: 9 step: 9, loss is 0.10997441411018372\n",
      "epoch: 9 step: 10, loss is 0.15733066201210022\n",
      "epoch: 9 step: 11, loss is 0.08393912017345428\n",
      "epoch: 9 step: 12, loss is 0.04652966186404228\n",
      "epoch: 9 step: 13, loss is 0.13936394453048706\n",
      "epoch: 9 step: 14, loss is 0.08156430721282959\n",
      "epoch: 9 step: 15, loss is 0.14917954802513123\n",
      "epoch: 9 step: 16, loss is 0.32112953066825867\n",
      "epoch: 9 step: 17, loss is 0.04240585118532181\n",
      "epoch: 9 step: 18, loss is 0.3009236752986908\n",
      "epoch: 9 step: 19, loss is 0.200850710272789\n",
      "epoch: 9 step: 20, loss is 0.04676799848675728\n",
      "epoch: 9 step: 21, loss is 0.20418818295001984\n",
      "epoch: 9 step: 22, loss is 0.059920649975538254\n",
      "epoch: 9 step: 23, loss is 0.11633504927158356\n",
      "epoch: 9 step: 24, loss is 0.24429677426815033\n",
      "epoch: 9 step: 25, loss is 0.3316660523414612\n",
      "epoch: 9 step: 26, loss is 0.051817506551742554\n",
      "epoch: 9 step: 27, loss is 0.20473693311214447\n",
      "epoch: 9 step: 28, loss is 0.1293993890285492\n",
      "epoch: 9 step: 29, loss is 0.14360220730304718\n",
      "epoch: 9 step: 30, loss is 0.15955184400081635\n",
      "epoch: 9 step: 31, loss is 0.6667320728302002\n",
      "epoch: 9 step: 32, loss is 0.03759336844086647\n",
      "epoch: 9 step: 33, loss is 0.05522306635975838\n",
      "epoch: 9 step: 34, loss is 0.13066864013671875\n",
      "epoch: 9 step: 35, loss is 0.11589663475751877\n",
      "epoch: 9 step: 36, loss is 0.07300504297018051\n",
      "epoch: 9 step: 37, loss is 0.00786256231367588\n",
      "epoch: 9 step: 38, loss is 0.20561175048351288\n",
      "epoch: 9 step: 39, loss is 0.03118077851831913\n",
      "epoch: 9 step: 40, loss is 0.018042119219899178\n",
      "epoch: 9 step: 41, loss is 0.3845531940460205\n",
      "epoch: 9 step: 42, loss is 0.5059841871261597\n",
      "epoch: 9 step: 43, loss is 0.37894508242607117\n",
      "epoch: 9 step: 44, loss is 0.13360340893268585\n",
      "epoch: 9 step: 45, loss is 0.25578203797340393\n",
      "epoch: 9 step: 46, loss is 0.16433726251125336\n",
      "epoch: 9 step: 47, loss is 0.01723843812942505\n",
      "epoch: 9 step: 48, loss is 0.17870934307575226\n",
      "epoch: 9 step: 49, loss is 0.09834229946136475\n",
      "epoch: 9 step: 50, loss is 0.02997751347720623\n",
      "epoch: 9 step: 51, loss is 0.546463131904602\n",
      "epoch: 9 step: 52, loss is 0.10412601381540298\n",
      "epoch: 9 step: 53, loss is 0.3010466992855072\n",
      "epoch: 9 step: 54, loss is 0.19736650586128235\n",
      "epoch: 9 step: 55, loss is 1.0094183683395386\n",
      "epoch: 9 step: 56, loss is 0.2285056859254837\n",
      "epoch: 9 step: 57, loss is 0.2694443464279175\n",
      "epoch: 9 step: 58, loss is 0.040235746651887894\n",
      "epoch: 9 step: 59, loss is 0.09282464534044266\n",
      "epoch: 9 step: 60, loss is 0.5651136636734009\n",
      "epoch: 9 step: 61, loss is 0.30135369300842285\n",
      "epoch: 9 step: 62, loss is 0.24645276367664337\n",
      "epoch: 9 step: 63, loss is 0.4361450970172882\n",
      "epoch: 9 step: 64, loss is 0.47617748379707336\n",
      "epoch: 9 step: 65, loss is 0.38275521993637085\n",
      "epoch: 9 step: 66, loss is 0.05920328572392464\n",
      "epoch: 9 step: 67, loss is 0.2706901729106903\n",
      "epoch: 9 step: 68, loss is 0.48995545506477356\n",
      "epoch: 9 step: 69, loss is 0.20080247521400452\n",
      "epoch: 9 step: 70, loss is 0.314854234457016\n",
      "epoch: 9 step: 71, loss is 0.1754138171672821\n",
      "epoch: 9 step: 72, loss is 0.37786880135536194\n",
      "epoch: 9 step: 73, loss is 0.511183500289917\n",
      "epoch: 9 step: 74, loss is 0.23253899812698364\n",
      "epoch: 9 step: 75, loss is 0.2350814938545227\n",
      "epoch: 9 step: 76, loss is 0.04590308666229248\n",
      "epoch: 9 step: 77, loss is 0.3553602695465088\n",
      "epoch: 9 step: 78, loss is 0.24977463483810425\n",
      "epoch: 9 step: 79, loss is 0.24015775322914124\n",
      "epoch: 9 step: 80, loss is 0.1821705400943756\n",
      "epoch: 9 step: 81, loss is 0.15309570729732513\n",
      "epoch: 9 step: 82, loss is 0.37522000074386597\n",
      "epoch: 9 step: 83, loss is 0.13804292678833008\n",
      "epoch: 9 step: 84, loss is 0.3183216154575348\n",
      "epoch: 9 step: 85, loss is 0.028456147760152817\n",
      "epoch: 9 step: 86, loss is 0.2729836404323578\n",
      "epoch: 9 step: 87, loss is 0.07453914731740952\n",
      "epoch: 9 step: 88, loss is 0.06638173758983612\n",
      "epoch: 9 step: 89, loss is 0.08800456672906876\n",
      "epoch: 9 step: 90, loss is 0.4726838767528534\n",
      "epoch: 9 step: 91, loss is 0.4223049581050873\n",
      "epoch: 9 step: 92, loss is 0.240217387676239\n",
      "epoch: 9 step: 93, loss is 0.0984567403793335\n",
      "epoch: 9 step: 94, loss is 0.006102065090090036\n",
      "epoch: 9 step: 95, loss is 0.2489423155784607\n",
      "epoch: 9 step: 96, loss is 0.35109782218933105\n",
      "epoch: 9 step: 97, loss is 0.17206856608390808\n",
      "epoch: 9 step: 98, loss is 0.09238574653863907\n",
      "epoch: 9 step: 99, loss is 0.2509516775608063\n",
      "epoch: 9 step: 100, loss is 0.1489734798669815\n",
      "epoch: 9 step: 101, loss is 0.08545060455799103\n",
      "epoch: 9 step: 102, loss is 0.47870320081710815\n",
      "epoch: 9 step: 103, loss is 0.16584664583206177\n",
      "epoch: 9 step: 104, loss is 0.4935559034347534\n",
      "epoch: 9 step: 105, loss is 0.026430832222104073\n",
      "epoch: 9 step: 106, loss is 0.02650570496916771\n",
      "epoch: 9 step: 107, loss is 0.6079341769218445\n",
      "epoch: 9 step: 108, loss is 0.09381625801324844\n",
      "epoch: 9 step: 109, loss is 0.2985924184322357\n",
      "epoch: 9 step: 110, loss is 0.12355861812829971\n",
      "epoch: 9 step: 111, loss is 0.18269824981689453\n",
      "epoch: 9 step: 112, loss is 0.13332149386405945\n",
      "epoch: 9 step: 113, loss is 0.16025952994823456\n",
      "epoch: 9 step: 114, loss is 0.0545172393321991\n",
      "epoch: 9 step: 115, loss is 0.26749658584594727\n",
      "epoch: 9 step: 116, loss is 0.039289675652980804\n",
      "epoch: 9 step: 117, loss is 0.02735131047666073\n",
      "epoch: 9 step: 118, loss is 0.3494996130466461\n",
      "epoch: 9 step: 119, loss is 0.009292522445321083\n",
      "epoch: 9 step: 120, loss is 0.6875268220901489\n",
      "epoch: 9 step: 121, loss is 0.2522825300693512\n",
      "epoch: 9 step: 122, loss is 0.19686143100261688\n",
      "epoch: 9 step: 123, loss is 0.7452701330184937\n",
      "epoch: 9 step: 124, loss is 0.027669444680213928\n",
      "epoch: 9 step: 125, loss is 0.40376055240631104\n",
      "epoch: 9 step: 126, loss is 0.0724424496293068\n",
      "epoch: 9 step: 127, loss is 0.24192051589488983\n",
      "epoch: 9 step: 128, loss is 0.1216205358505249\n",
      "epoch: 9 step: 129, loss is 0.1217297613620758\n",
      "epoch: 9 step: 130, loss is 0.23084349930286407\n",
      "epoch: 9 step: 131, loss is 0.18983642756938934\n",
      "epoch: 9 step: 132, loss is 0.03095884807407856\n",
      "epoch: 9 step: 133, loss is 0.03682367131114006\n",
      "epoch: 9 step: 134, loss is 0.08294185250997543\n",
      "epoch: 9 step: 135, loss is 0.08288496732711792\n",
      "epoch: 9 step: 136, loss is 0.09648670256137848\n",
      "epoch: 9 step: 137, loss is 0.03408332169055939\n",
      "epoch: 9 step: 138, loss is 0.3058911859989166\n",
      "epoch: 9 step: 139, loss is 0.05361485853791237\n",
      "epoch: 9 step: 140, loss is 0.3106004297733307\n",
      "epoch: 9 step: 141, loss is 0.4127926528453827\n",
      "epoch: 9 step: 142, loss is 0.12270884215831757\n",
      "epoch: 9 step: 143, loss is 0.06556887924671173\n",
      "epoch: 9 step: 144, loss is 0.04246164858341217\n",
      "epoch: 9 step: 145, loss is 0.1212356686592102\n",
      "epoch: 9 step: 146, loss is 0.14856836199760437\n",
      "epoch: 9 step: 147, loss is 0.02019982971251011\n",
      "epoch: 9 step: 148, loss is 0.09320377558469772\n",
      "epoch: 9 step: 149, loss is 0.2964501678943634\n",
      "epoch: 9 step: 150, loss is 0.21719489991664886\n",
      "epoch: 9 step: 151, loss is 0.021666981279850006\n",
      "epoch: 9 step: 152, loss is 0.30152812600135803\n",
      "epoch: 9 step: 153, loss is 0.152631476521492\n",
      "epoch: 9 step: 154, loss is 0.29377585649490356\n",
      "epoch: 9 step: 155, loss is 0.31381505727767944\n",
      "epoch: 9 step: 156, loss is 0.12243714183568954\n",
      "epoch: 9 step: 157, loss is 0.09384722262620926\n",
      "epoch: 9 step: 158, loss is 0.35052603483200073\n",
      "epoch: 9 step: 159, loss is 0.08917635679244995\n",
      "epoch: 9 step: 160, loss is 0.13763388991355896\n",
      "epoch: 9 step: 161, loss is 0.046178337186574936\n",
      "epoch: 9 step: 162, loss is 0.04143211618065834\n",
      "epoch: 9 step: 163, loss is 0.3713366389274597\n",
      "epoch: 9 step: 164, loss is 0.30625349283218384\n",
      "epoch: 9 step: 165, loss is 0.7274168133735657\n",
      "epoch: 9 step: 166, loss is 0.8192252516746521\n",
      "epoch: 9 step: 167, loss is 0.5129562616348267\n",
      "epoch: 9 step: 168, loss is 0.4820369482040405\n",
      "epoch: 9 step: 169, loss is 0.5296318531036377\n",
      "epoch: 9 step: 170, loss is 0.06257151067256927\n",
      "epoch: 9 step: 171, loss is 0.3450470268726349\n",
      "epoch: 9 step: 172, loss is 0.20247869193553925\n",
      "epoch: 9 step: 173, loss is 0.042629193514585495\n",
      "epoch: 9 step: 174, loss is 0.23935873806476593\n",
      "epoch: 9 step: 175, loss is 0.0456886850297451\n",
      "epoch: 9 step: 176, loss is 0.43549400568008423\n",
      "epoch: 9 step: 177, loss is 0.1431107372045517\n",
      "epoch: 9 step: 178, loss is 0.2645955979824066\n",
      "epoch: 9 step: 179, loss is 0.3150154650211334\n",
      "epoch: 9 step: 180, loss is 0.2950906753540039\n",
      "epoch: 9 step: 181, loss is 0.30029574036598206\n",
      "epoch: 9 step: 182, loss is 0.5282450318336487\n",
      "epoch: 9 step: 183, loss is 0.49110597372055054\n",
      "epoch: 9 step: 184, loss is 0.3163606822490692\n",
      "epoch: 9 step: 185, loss is 0.05047217011451721\n",
      "epoch: 9 step: 186, loss is 0.1324346661567688\n",
      "epoch: 9 step: 187, loss is 0.6095077991485596\n",
      "epoch: 9 step: 188, loss is 0.14621271193027496\n",
      "epoch: 9 step: 189, loss is 0.215992271900177\n",
      "epoch: 9 step: 190, loss is 0.5649815201759338\n",
      "epoch: 9 step: 191, loss is 0.4609055519104004\n",
      "epoch: 9 step: 192, loss is 0.5557345747947693\n",
      "epoch: 9 step: 193, loss is 0.16645924746990204\n",
      "epoch: 9 step: 194, loss is 0.4446481466293335\n",
      "epoch: 9 step: 195, loss is 0.052169959992170334\n",
      "epoch: 9 step: 196, loss is 0.044029660522937775\n",
      "epoch: 9 step: 197, loss is 0.14462517201900482\n",
      "epoch: 9 step: 198, loss is 0.12164612859487534\n",
      "epoch: 9 step: 199, loss is 0.10616527497768402\n",
      "epoch: 9 step: 200, loss is 0.028051013126969337\n",
      "epoch: 9 step: 201, loss is 0.1919071078300476\n",
      "epoch: 9 step: 202, loss is 0.22908088564872742\n",
      "epoch: 9 step: 203, loss is 0.028073428198695183\n",
      "epoch: 9 step: 204, loss is 0.3157159984111786\n",
      "epoch: 9 step: 205, loss is 0.0783381536602974\n",
      "epoch: 9 step: 206, loss is 0.07744208723306656\n",
      "epoch: 9 step: 207, loss is 0.08084522932767868\n",
      "epoch: 9 step: 208, loss is 0.1664288192987442\n",
      "epoch: 9 step: 209, loss is 0.1468459963798523\n",
      "epoch: 9 step: 210, loss is 0.2408219426870346\n",
      "epoch: 9 step: 211, loss is 0.12589772045612335\n",
      "epoch: 9 step: 212, loss is 0.2900908887386322\n",
      "epoch: 9 step: 213, loss is 0.3455227017402649\n",
      "epoch: 9 step: 214, loss is 0.6327568292617798\n",
      "epoch: 9 step: 215, loss is 0.445578008890152\n",
      "epoch: 9 step: 216, loss is 0.2335061877965927\n",
      "epoch: 9 step: 217, loss is 0.048550091683864594\n",
      "epoch: 9 step: 218, loss is 0.1014714315533638\n",
      "epoch: 9 step: 219, loss is 0.24699349701404572\n",
      "epoch: 9 step: 220, loss is 0.3291151523590088\n",
      "epoch: 9 step: 221, loss is 0.03798767551779747\n",
      "epoch: 9 step: 222, loss is 0.12377262860536575\n",
      "epoch: 9 step: 223, loss is 0.20802481472492218\n",
      "epoch: 9 step: 224, loss is 0.6040424108505249\n",
      "epoch: 9 step: 225, loss is 0.13474498689174652\n",
      "epoch: 9 step: 226, loss is 0.3450431227684021\n",
      "epoch: 9 step: 227, loss is 0.02479570172727108\n",
      "epoch: 9 step: 228, loss is 0.2748386561870575\n",
      "epoch: 9 step: 229, loss is 0.12696634232997894\n",
      "epoch: 9 step: 230, loss is 0.05779493227601051\n",
      "epoch: 9 step: 231, loss is 0.19464102387428284\n",
      "epoch: 9 step: 232, loss is 0.08430161327123642\n",
      "epoch: 9 step: 233, loss is 0.22771544754505157\n",
      "epoch: 9 step: 234, loss is 0.9027336835861206\n",
      "epoch: 9 step: 235, loss is 0.2270454317331314\n",
      "epoch: 9 step: 236, loss is 0.3061528503894806\n",
      "epoch: 9 step: 237, loss is 0.12234635651111603\n",
      "epoch: 9 step: 238, loss is 0.11937565356492996\n",
      "epoch: 9 step: 239, loss is 0.0990159884095192\n",
      "epoch: 9 step: 240, loss is 0.050597090274095535\n",
      "epoch: 9 step: 241, loss is 0.057666175067424774\n",
      "epoch: 9 step: 242, loss is 0.16333839297294617\n",
      "epoch: 9 step: 243, loss is 0.9183341264724731\n",
      "epoch: 9 step: 244, loss is 0.17902298271656036\n",
      "epoch: 9 step: 245, loss is 0.29370880126953125\n",
      "epoch: 9 step: 246, loss is 0.26025113463401794\n",
      "epoch: 9 step: 247, loss is 0.2564168870449066\n",
      "epoch: 9 step: 248, loss is 0.3486734926700592\n",
      "epoch: 9 step: 249, loss is 0.01758095994591713\n",
      "epoch: 9 step: 250, loss is 0.18585661053657532\n",
      "epoch: 9 step: 251, loss is 0.2686534821987152\n",
      "epoch: 9 step: 252, loss is 0.08212190121412277\n",
      "epoch: 9 step: 253, loss is 0.48711803555488586\n",
      "epoch: 9 step: 254, loss is 0.6416910886764526\n",
      "epoch: 9 step: 255, loss is 0.19756104052066803\n",
      "epoch: 9 step: 256, loss is 0.3087587356567383\n",
      "epoch: 9 step: 257, loss is 0.12370549142360687\n",
      "epoch: 9 step: 258, loss is 0.22549989819526672\n",
      "epoch: 9 step: 259, loss is 0.10674931854009628\n",
      "epoch: 9 step: 260, loss is 0.2296074628829956\n",
      "epoch: 9 step: 261, loss is 0.841600239276886\n",
      "epoch: 9 step: 262, loss is 0.4201429784297943\n",
      "epoch: 9 step: 263, loss is 0.4341875910758972\n",
      "epoch: 9 step: 264, loss is 0.24825230240821838\n",
      "epoch: 9 step: 265, loss is 0.23846079409122467\n",
      "epoch: 9 step: 266, loss is 0.11299274861812592\n",
      "epoch: 9 step: 267, loss is 0.10302278399467468\n",
      "epoch: 9 step: 268, loss is 0.28643229603767395\n",
      "epoch: 9 step: 269, loss is 0.20988214015960693\n",
      "epoch: 9 step: 270, loss is 0.3503410518169403\n",
      "epoch: 9 step: 271, loss is 0.1110016405582428\n",
      "epoch: 9 step: 272, loss is 0.16314274072647095\n",
      "epoch: 9 step: 273, loss is 0.08853595703840256\n",
      "epoch: 9 step: 274, loss is 0.30624696612358093\n",
      "epoch: 9 step: 275, loss is 0.07751727849245071\n",
      "epoch: 9 step: 276, loss is 0.4992205500602722\n",
      "epoch: 9 step: 277, loss is 0.06284094601869583\n",
      "epoch: 9 step: 278, loss is 0.2173086553812027\n",
      "epoch: 9 step: 279, loss is 0.567798912525177\n",
      "epoch: 9 step: 280, loss is 0.06234021484851837\n",
      "epoch: 9 step: 281, loss is 0.0660368800163269\n",
      "epoch: 9 step: 282, loss is 0.13168281316757202\n",
      "epoch: 9 step: 283, loss is 0.11722773313522339\n",
      "epoch: 9 step: 284, loss is 0.41717806458473206\n",
      "epoch: 9 step: 285, loss is 0.05692928656935692\n",
      "epoch: 9 step: 286, loss is 0.2535030245780945\n",
      "epoch: 9 step: 287, loss is 0.04771130904555321\n",
      "epoch: 9 step: 288, loss is 0.620313823223114\n",
      "epoch: 9 step: 289, loss is 0.3013642430305481\n",
      "epoch: 9 step: 290, loss is 0.04352464899420738\n",
      "epoch: 9 step: 291, loss is 0.10991411656141281\n",
      "epoch: 9 step: 292, loss is 0.06342354416847229\n",
      "epoch: 9 step: 293, loss is 0.16191457211971283\n",
      "epoch: 9 step: 294, loss is 0.7557791471481323\n",
      "epoch: 9 step: 295, loss is 0.08406949043273926\n",
      "epoch: 9 step: 296, loss is 0.09694213420152664\n",
      "epoch: 9 step: 297, loss is 0.07813066989183426\n",
      "epoch: 9 step: 298, loss is 0.08109959959983826\n",
      "epoch: 9 step: 299, loss is 0.10754481703042984\n",
      "epoch: 9 step: 300, loss is 0.08462240546941757\n",
      "epoch: 9 step: 301, loss is 0.33369705080986023\n",
      "epoch: 9 step: 302, loss is 0.1035626232624054\n",
      "epoch: 9 step: 303, loss is 0.352070689201355\n",
      "epoch: 9 step: 304, loss is 0.04198937863111496\n",
      "epoch: 9 step: 305, loss is 0.13842186331748962\n",
      "epoch: 9 step: 306, loss is 0.0349954292178154\n",
      "epoch: 9 step: 307, loss is 0.10995800048112869\n",
      "epoch: 9 step: 308, loss is 0.042089544236660004\n",
      "epoch: 9 step: 309, loss is 0.3901975154876709\n",
      "epoch: 9 step: 310, loss is 0.15658143162727356\n",
      "epoch: 9 step: 311, loss is 0.18383947014808655\n",
      "epoch: 9 step: 312, loss is 0.10488831251859665\n",
      "epoch: 9 step: 313, loss is 0.031947970390319824\n",
      "epoch: 9 step: 314, loss is 0.16219063103199005\n",
      "epoch: 9 step: 315, loss is 0.0906587466597557\n",
      "epoch: 9 step: 316, loss is 0.3969097137451172\n",
      "epoch: 9 step: 317, loss is 0.0465870127081871\n",
      "epoch: 9 step: 318, loss is 0.909990131855011\n",
      "epoch: 9 step: 319, loss is 0.5944960713386536\n",
      "epoch: 9 step: 320, loss is 0.06716705858707428\n",
      "epoch: 9 step: 321, loss is 0.16091716289520264\n",
      "epoch: 9 step: 322, loss is 0.09839662909507751\n",
      "epoch: 9 step: 323, loss is 0.917522668838501\n",
      "epoch: 9 step: 324, loss is 0.09998182952404022\n",
      "epoch: 9 step: 325, loss is 0.07239696383476257\n",
      "epoch: 9 step: 326, loss is 0.03073485568165779\n",
      "epoch: 9 step: 327, loss is 0.27395451068878174\n",
      "epoch: 9 step: 328, loss is 0.09201287478208542\n",
      "epoch: 9 step: 329, loss is 0.15798187255859375\n",
      "epoch: 9 step: 330, loss is 0.47034329175949097\n",
      "epoch: 9 step: 331, loss is 0.015165727585554123\n",
      "epoch: 9 step: 332, loss is 0.18904416263103485\n",
      "epoch: 9 step: 333, loss is 0.11272288858890533\n",
      "epoch: 9 step: 334, loss is 0.11592584103345871\n",
      "epoch: 9 step: 335, loss is 0.04642608016729355\n",
      "epoch: 9 step: 336, loss is 1.1892585754394531\n",
      "epoch: 9 step: 337, loss is 0.026755614206194878\n",
      "epoch: 9 step: 338, loss is 0.48942601680755615\n",
      "epoch: 9 step: 339, loss is 0.1080092191696167\n",
      "epoch: 9 step: 340, loss is 0.5201788544654846\n",
      "epoch: 9 step: 341, loss is 0.2302284836769104\n",
      "epoch: 9 step: 342, loss is 0.026621511206030846\n",
      "epoch: 9 step: 343, loss is 0.09961790591478348\n",
      "epoch: 9 step: 344, loss is 0.11914432793855667\n",
      "epoch: 9 step: 345, loss is 0.11701976507902145\n",
      "epoch: 9 step: 346, loss is 0.677089512348175\n",
      "epoch: 9 step: 347, loss is 0.2740754783153534\n",
      "epoch: 9 step: 348, loss is 0.2581935524940491\n",
      "epoch: 9 step: 349, loss is 0.18605206906795502\n",
      "epoch: 9 step: 350, loss is 0.1404016613960266\n",
      "epoch: 9 step: 351, loss is 0.05315347760915756\n",
      "epoch: 9 step: 352, loss is 0.5774634480476379\n",
      "epoch: 9 step: 353, loss is 0.1459023803472519\n",
      "epoch: 9 step: 354, loss is 0.19213363528251648\n",
      "epoch: 9 step: 355, loss is 0.09560851007699966\n",
      "epoch: 9 step: 356, loss is 0.3068428337574005\n",
      "epoch: 9 step: 357, loss is 0.2348739206790924\n",
      "epoch: 9 step: 358, loss is 0.26683515310287476\n",
      "epoch: 9 step: 359, loss is 0.15287084877490997\n",
      "epoch: 9 step: 360, loss is 0.10311165452003479\n",
      "epoch: 9 step: 361, loss is 0.366061270236969\n",
      "epoch: 9 step: 362, loss is 0.29535073041915894\n",
      "epoch: 9 step: 363, loss is 0.20971748232841492\n",
      "epoch: 9 step: 364, loss is 0.09481701999902725\n",
      "epoch: 9 step: 365, loss is 0.09114424139261246\n",
      "epoch: 9 step: 366, loss is 0.0934004932641983\n",
      "epoch: 9 step: 367, loss is 0.3407956659793854\n",
      "epoch: 9 step: 368, loss is 0.04257933422923088\n",
      "epoch: 9 step: 369, loss is 0.713233470916748\n",
      "epoch: 9 step: 370, loss is 0.023159651085734367\n",
      "epoch: 9 step: 371, loss is 0.1590564101934433\n",
      "epoch: 9 step: 372, loss is 0.1801546812057495\n",
      "epoch: 9 step: 373, loss is 0.20805789530277252\n",
      "epoch: 9 step: 374, loss is 0.2137867957353592\n",
      "epoch: 9 step: 375, loss is 0.020601719617843628\n",
      "epoch: 9 step: 376, loss is 0.4019530415534973\n",
      "epoch: 9 step: 377, loss is 0.04188660904765129\n",
      "epoch: 9 step: 378, loss is 0.6035974025726318\n",
      "epoch: 9 step: 379, loss is 0.2659185528755188\n",
      "epoch: 9 step: 380, loss is 0.29828378558158875\n",
      "epoch: 9 step: 381, loss is 0.31770381331443787\n",
      "epoch: 9 step: 382, loss is 0.36301925778388977\n",
      "epoch: 9 step: 383, loss is 0.1686064898967743\n",
      "epoch: 9 step: 384, loss is 0.17616881430149078\n",
      "epoch: 9 step: 385, loss is 0.11036460101604462\n",
      "epoch: 9 step: 386, loss is 0.14459221065044403\n",
      "epoch: 9 step: 387, loss is 0.2803002893924713\n",
      "epoch: 9 step: 388, loss is 0.3504891097545624\n",
      "epoch: 9 step: 389, loss is 0.17954646050930023\n",
      "epoch: 9 step: 390, loss is 0.22045738995075226\n",
      "epoch: 9 step: 391, loss is 0.23528918623924255\n",
      "epoch: 9 step: 392, loss is 0.16692228615283966\n",
      "epoch: 9 step: 393, loss is 0.41443073749542236\n",
      "epoch: 9 step: 394, loss is 0.0688629075884819\n",
      "epoch: 9 step: 395, loss is 0.4353299140930176\n",
      "epoch: 9 step: 396, loss is 0.28725844621658325\n",
      "epoch: 9 step: 397, loss is 0.31007087230682373\n",
      "epoch: 9 step: 398, loss is 0.47612330317497253\n",
      "epoch: 9 step: 399, loss is 0.13782739639282227\n",
      "epoch: 9 step: 400, loss is 0.1191837415099144\n",
      "epoch: 9 step: 401, loss is 0.11603841185569763\n",
      "epoch: 9 step: 402, loss is 0.16130071878433228\n",
      "epoch: 9 step: 403, loss is 0.46260979771614075\n",
      "epoch: 9 step: 404, loss is 0.014775112271308899\n",
      "epoch: 9 step: 405, loss is 0.43770483136177063\n",
      "epoch: 9 step: 406, loss is 0.08729331940412521\n",
      "epoch: 9 step: 407, loss is 0.5708823800086975\n",
      "epoch: 9 step: 408, loss is 0.4050644338130951\n",
      "epoch: 9 step: 409, loss is 0.2858583331108093\n",
      "epoch: 9 step: 410, loss is 0.20141419768333435\n",
      "epoch: 9 step: 411, loss is 0.1979093700647354\n",
      "epoch: 9 step: 412, loss is 0.5030533671379089\n",
      "epoch: 9 step: 413, loss is 0.04366600513458252\n",
      "epoch: 9 step: 414, loss is 0.15808731317520142\n",
      "epoch: 9 step: 415, loss is 0.15385328233242035\n",
      "epoch: 9 step: 416, loss is 0.39860039949417114\n",
      "epoch: 9 step: 417, loss is 0.08658168464899063\n",
      "epoch: 9 step: 418, loss is 0.03674643114209175\n",
      "epoch: 9 step: 419, loss is 0.34007173776626587\n",
      "epoch: 9 step: 420, loss is 0.2789303660392761\n",
      "epoch: 9 step: 421, loss is 0.10789171606302261\n",
      "epoch: 9 step: 422, loss is 0.21857064962387085\n",
      "epoch: 9 step: 423, loss is 0.348064661026001\n",
      "epoch: 9 step: 424, loss is 0.22936365008354187\n",
      "epoch: 9 step: 425, loss is 0.17959770560264587\n",
      "epoch: 9 step: 426, loss is 0.24603596329689026\n",
      "epoch: 9 step: 427, loss is 0.04748501628637314\n",
      "epoch: 9 step: 428, loss is 0.3254088759422302\n",
      "epoch: 9 step: 429, loss is 0.15657678246498108\n",
      "epoch: 9 step: 430, loss is 0.022038783878087997\n",
      "epoch: 9 step: 431, loss is 0.30245986580848694\n",
      "epoch: 9 step: 432, loss is 0.03897789120674133\n",
      "epoch: 9 step: 433, loss is 0.06161957606673241\n",
      "epoch: 9 step: 434, loss is 0.5880624055862427\n",
      "epoch: 9 step: 435, loss is 0.1897096335887909\n",
      "epoch: 9 step: 436, loss is 0.31732824444770813\n",
      "epoch: 9 step: 437, loss is 0.12603960931301117\n",
      "epoch: 9 step: 438, loss is 0.11097963899374008\n",
      "epoch: 9 step: 439, loss is 0.09745804965496063\n",
      "epoch: 9 step: 440, loss is 0.7036466002464294\n",
      "epoch: 9 step: 441, loss is 0.08983881771564484\n",
      "epoch: 9 step: 442, loss is 0.5971426963806152\n",
      "epoch: 9 step: 443, loss is 0.01243322528898716\n",
      "epoch: 9 step: 444, loss is 0.09101568162441254\n",
      "epoch: 9 step: 445, loss is 0.18158991634845734\n",
      "epoch: 9 step: 446, loss is 0.1490868479013443\n",
      "epoch: 9 step: 447, loss is 0.3263651728630066\n",
      "epoch: 9 step: 448, loss is 0.06886985152959824\n",
      "epoch: 9 step: 449, loss is 0.06602592021226883\n",
      "epoch: 9 step: 450, loss is 0.03706636652350426\n",
      "epoch: 9 step: 451, loss is 0.27757400274276733\n",
      "epoch: 9 step: 452, loss is 0.11278769373893738\n",
      "epoch: 9 step: 453, loss is 0.1982555389404297\n",
      "epoch: 9 step: 454, loss is 0.23437920212745667\n",
      "epoch: 9 step: 455, loss is 0.11575939506292343\n",
      "epoch: 9 step: 456, loss is 0.056634895503520966\n",
      "epoch: 9 step: 457, loss is 0.47760361433029175\n",
      "epoch: 9 step: 458, loss is 0.1160709336400032\n",
      "epoch: 9 step: 459, loss is 0.22054263949394226\n",
      "epoch: 9 step: 460, loss is 0.09168940037488937\n",
      "epoch: 9 step: 461, loss is 0.2735276520252228\n",
      "epoch: 9 step: 462, loss is 0.08754123002290726\n",
      "epoch: 9 step: 463, loss is 0.08571788668632507\n",
      "epoch: 9 step: 464, loss is 0.06491956859827042\n",
      "epoch: 9 step: 465, loss is 0.31319212913513184\n",
      "epoch: 9 step: 466, loss is 0.14270591735839844\n",
      "epoch: 9 step: 467, loss is 0.029815617948770523\n",
      "epoch: 9 step: 468, loss is 1.3786519765853882\n",
      "epoch: 9 step: 469, loss is 0.23949533700942993\n",
      "epoch: 9 step: 470, loss is 0.0538472980260849\n",
      "epoch: 9 step: 471, loss is 0.18873216211795807\n",
      "epoch: 9 step: 472, loss is 0.37789496779441833\n",
      "epoch: 9 step: 473, loss is 0.08449890464544296\n",
      "epoch: 9 step: 474, loss is 0.3640855848789215\n",
      "epoch: 9 step: 475, loss is 0.3685002326965332\n",
      "epoch: 9 step: 476, loss is 0.07575748860836029\n",
      "epoch: 9 step: 477, loss is 0.14884737133979797\n",
      "epoch: 9 step: 478, loss is 0.16057993471622467\n",
      "epoch: 9 step: 479, loss is 0.056036900728940964\n",
      "epoch: 9 step: 480, loss is 0.6609976887702942\n",
      "epoch: 9 step: 481, loss is 0.5731099247932434\n",
      "epoch: 9 step: 482, loss is 0.6135310530662537\n",
      "epoch: 9 step: 483, loss is 0.9975239038467407\n",
      "epoch: 9 step: 484, loss is 0.42345094680786133\n",
      "epoch: 9 step: 485, loss is 0.09686047583818436\n",
      "epoch: 9 step: 486, loss is 0.03207690268754959\n",
      "epoch: 9 step: 487, loss is 0.29302796721458435\n",
      "epoch: 9 step: 488, loss is 0.46948790550231934\n",
      "epoch: 9 step: 489, loss is 0.2940134108066559\n",
      "epoch: 9 step: 490, loss is 0.03475165739655495\n",
      "epoch: 9 step: 491, loss is 0.4412839710712433\n",
      "epoch: 9 step: 492, loss is 0.04614399001002312\n",
      "epoch: 9 step: 493, loss is 0.12744586169719696\n",
      "epoch: 9 step: 494, loss is 0.3949824571609497\n",
      "epoch: 9 step: 495, loss is 0.3187161684036255\n",
      "epoch: 9 step: 496, loss is 0.502052903175354\n",
      "epoch: 9 step: 497, loss is 0.16363763809204102\n",
      "epoch: 9 step: 498, loss is 0.08176758140325546\n",
      "epoch: 9 step: 499, loss is 0.1030087023973465\n",
      "epoch: 9 step: 500, loss is 0.06392167508602142\n",
      "epoch: 9 step: 501, loss is 0.09912292659282684\n",
      "epoch: 9 step: 502, loss is 0.5256423950195312\n",
      "epoch: 9 step: 503, loss is 0.10453104227781296\n",
      "epoch: 9 step: 504, loss is 0.06728892773389816\n",
      "epoch: 9 step: 505, loss is 0.3272162675857544\n",
      "epoch: 9 step: 506, loss is 0.7760331630706787\n",
      "epoch: 9 step: 507, loss is 0.31357866525650024\n",
      "epoch: 9 step: 508, loss is 0.08293318748474121\n",
      "epoch: 9 step: 509, loss is 0.15369407832622528\n",
      "epoch: 9 step: 510, loss is 0.11299349367618561\n",
      "epoch: 9 step: 511, loss is 0.3241852819919586\n",
      "epoch: 9 step: 512, loss is 0.24732966721057892\n",
      "epoch: 9 step: 513, loss is 0.11823860555887222\n",
      "epoch: 9 step: 514, loss is 0.03511572629213333\n",
      "epoch: 9 step: 515, loss is 0.01254270225763321\n",
      "epoch: 9 step: 516, loss is 0.40394893288612366\n",
      "epoch: 9 step: 517, loss is 0.3544924259185791\n",
      "epoch: 9 step: 518, loss is 0.14945361018180847\n",
      "epoch: 9 step: 519, loss is 0.08609677851200104\n",
      "epoch: 9 step: 520, loss is 0.14824923872947693\n",
      "epoch: 9 step: 521, loss is 0.19401651620864868\n",
      "epoch: 9 step: 522, loss is 0.37180042266845703\n",
      "epoch: 9 step: 523, loss is 0.28754329681396484\n",
      "epoch: 9 step: 524, loss is 0.22804459929466248\n",
      "epoch: 9 step: 525, loss is 0.13834033906459808\n",
      "epoch: 9 step: 526, loss is 0.09349042922258377\n",
      "epoch: 9 step: 527, loss is 0.06625410169363022\n",
      "epoch: 9 step: 528, loss is 0.1632240116596222\n",
      "epoch: 9 step: 529, loss is 0.1906353086233139\n",
      "epoch: 9 step: 530, loss is 0.4667260944843292\n",
      "epoch: 9 step: 531, loss is 0.05865663290023804\n",
      "epoch: 9 step: 532, loss is 0.04566432908177376\n",
      "epoch: 9 step: 533, loss is 0.3002051115036011\n",
      "epoch: 9 step: 534, loss is 0.1036720797419548\n",
      "epoch: 9 step: 535, loss is 0.2295914888381958\n",
      "epoch: 9 step: 536, loss is 0.3718428909778595\n",
      "epoch: 9 step: 537, loss is 0.16509932279586792\n",
      "epoch: 9 step: 538, loss is 0.2876996099948883\n",
      "epoch: 9 step: 539, loss is 0.37816503643989563\n",
      "epoch: 9 step: 540, loss is 0.06597652286291122\n",
      "epoch: 9 step: 541, loss is 0.09650172293186188\n",
      "epoch: 9 step: 542, loss is 0.04457534849643707\n",
      "epoch: 9 step: 543, loss is 0.42815059423446655\n",
      "epoch: 9 step: 544, loss is 0.03390387445688248\n",
      "epoch: 9 step: 545, loss is 0.08693600445985794\n",
      "epoch: 9 step: 546, loss is 0.14513592422008514\n",
      "epoch: 9 step: 547, loss is 0.0840609148144722\n",
      "epoch: 9 step: 548, loss is 0.1512378454208374\n",
      "epoch: 9 step: 549, loss is 0.013857242651283741\n",
      "epoch: 9 step: 550, loss is 0.09072743356227875\n",
      "epoch: 9 step: 551, loss is 0.14609850943088531\n",
      "epoch: 9 step: 552, loss is 0.33074742555618286\n",
      "epoch: 9 step: 553, loss is 0.06443324685096741\n",
      "epoch: 9 step: 554, loss is 0.12334701418876648\n",
      "epoch: 9 step: 555, loss is 0.12843208014965057\n",
      "epoch: 9 step: 556, loss is 0.03677046298980713\n",
      "epoch: 9 step: 557, loss is 0.2417236715555191\n",
      "epoch: 9 step: 558, loss is 0.14763379096984863\n",
      "epoch: 9 step: 559, loss is 0.021606381982564926\n",
      "epoch: 9 step: 560, loss is 0.11579626053571701\n",
      "epoch: 9 step: 561, loss is 0.15892770886421204\n",
      "epoch: 9 step: 562, loss is 0.23028354346752167\n",
      "epoch: 9 step: 563, loss is 0.3777126967906952\n",
      "epoch: 9 step: 564, loss is 0.0666297897696495\n",
      "epoch: 9 step: 565, loss is 0.09014766663312912\n",
      "epoch: 9 step: 566, loss is 0.04205681011080742\n",
      "epoch: 9 step: 567, loss is 0.17182029783725739\n",
      "epoch: 9 step: 568, loss is 0.26648250222206116\n",
      "epoch: 9 step: 569, loss is 0.6083691716194153\n",
      "epoch: 9 step: 570, loss is 0.17495344579219818\n",
      "epoch: 9 step: 571, loss is 0.5165936350822449\n",
      "epoch: 9 step: 572, loss is 0.11857234686613083\n",
      "epoch: 9 step: 573, loss is 0.26515069603919983\n",
      "epoch: 9 step: 574, loss is 0.3710319697856903\n",
      "epoch: 9 step: 575, loss is 0.33860698342323303\n",
      "epoch: 9 step: 576, loss is 0.5496130585670471\n",
      "epoch: 9 step: 577, loss is 0.11535433679819107\n",
      "epoch: 9 step: 578, loss is 0.31150251626968384\n",
      "epoch: 9 step: 579, loss is 0.5310811400413513\n",
      "epoch: 9 step: 580, loss is 0.10112997144460678\n",
      "epoch: 9 step: 581, loss is 0.24197006225585938\n",
      "epoch: 9 step: 582, loss is 0.03810762241482735\n",
      "epoch: 9 step: 583, loss is 0.2406376451253891\n",
      "epoch: 9 step: 584, loss is 0.07520617544651031\n",
      "epoch: 9 step: 585, loss is 0.273594468832016\n",
      "epoch: 9 step: 586, loss is 0.04669416323304176\n",
      "epoch: 9 step: 587, loss is 0.18081656098365784\n",
      "epoch: 9 step: 588, loss is 0.3854438364505768\n",
      "epoch: 9 step: 589, loss is 0.1018247902393341\n",
      "epoch: 9 step: 590, loss is 0.0962110385298729\n",
      "epoch: 9 step: 591, loss is 0.38011541962623596\n",
      "epoch: 9 step: 592, loss is 0.5485187768936157\n",
      "epoch: 9 step: 593, loss is 0.1530528962612152\n",
      "epoch: 9 step: 594, loss is 0.050796028226614\n",
      "epoch: 9 step: 595, loss is 0.2532568573951721\n",
      "epoch: 9 step: 596, loss is 0.050469499081373215\n",
      "epoch: 9 step: 597, loss is 0.23243458569049835\n",
      "epoch: 9 step: 598, loss is 0.2713712751865387\n",
      "epoch: 9 step: 599, loss is 0.14131857454776764\n",
      "epoch: 9 step: 600, loss is 0.09994494169950485\n",
      "epoch: 9 step: 601, loss is 0.10330546647310257\n",
      "epoch: 9 step: 602, loss is 0.25483474135398865\n",
      "epoch: 9 step: 603, loss is 0.5652942061424255\n",
      "epoch: 9 step: 604, loss is 0.10570121556520462\n",
      "epoch: 9 step: 605, loss is 0.0778614729642868\n",
      "epoch: 9 step: 606, loss is 0.3175494074821472\n",
      "epoch: 9 step: 607, loss is 0.3351558446884155\n",
      "epoch: 9 step: 608, loss is 0.38197770714759827\n",
      "epoch: 9 step: 609, loss is 0.4531082808971405\n",
      "epoch: 9 step: 610, loss is 0.4144682288169861\n",
      "epoch: 9 step: 611, loss is 0.07003244757652283\n",
      "epoch: 9 step: 612, loss is 0.31165266036987305\n",
      "epoch: 9 step: 613, loss is 0.3034788966178894\n",
      "epoch: 9 step: 614, loss is 0.025763917714357376\n",
      "epoch: 9 step: 615, loss is 0.11034411191940308\n",
      "epoch: 9 step: 616, loss is 0.5572791695594788\n",
      "epoch: 9 step: 617, loss is 0.29325345158576965\n",
      "epoch: 9 step: 618, loss is 0.01592780277132988\n",
      "epoch: 9 step: 619, loss is 0.5748387575149536\n",
      "epoch: 9 step: 620, loss is 0.09771937131881714\n",
      "epoch: 9 step: 621, loss is 0.1827324628829956\n",
      "epoch: 9 step: 622, loss is 0.18977224826812744\n",
      "epoch: 9 step: 623, loss is 0.08266216516494751\n",
      "epoch: 9 step: 624, loss is 0.07091762125492096\n",
      "epoch: 9 step: 625, loss is 0.06154200807213783\n",
      "epoch: 9 step: 626, loss is 0.3111534118652344\n",
      "epoch: 9 step: 627, loss is 0.03163614124059677\n",
      "epoch: 9 step: 628, loss is 0.1224820464849472\n",
      "epoch: 9 step: 629, loss is 0.19458146393299103\n",
      "epoch: 9 step: 630, loss is 0.12291692942380905\n",
      "epoch: 9 step: 631, loss is 0.13753776252269745\n",
      "epoch: 9 step: 632, loss is 0.4022381603717804\n",
      "epoch: 9 step: 633, loss is 0.28572753071784973\n",
      "epoch: 9 step: 634, loss is 0.24685268104076385\n",
      "epoch: 9 step: 635, loss is 0.07444554567337036\n",
      "epoch: 9 step: 636, loss is 0.039841800928115845\n",
      "epoch: 9 step: 637, loss is 0.09662587940692902\n",
      "epoch: 9 step: 638, loss is 0.23480509221553802\n",
      "epoch: 9 step: 639, loss is 0.028048312291502953\n",
      "epoch: 9 step: 640, loss is 0.11482025682926178\n",
      "epoch: 9 step: 641, loss is 0.15553168952465057\n",
      "epoch: 9 step: 642, loss is 0.8555328845977783\n",
      "epoch: 9 step: 643, loss is 0.5571369528770447\n",
      "epoch: 9 step: 644, loss is 0.170350581407547\n",
      "epoch: 9 step: 645, loss is 0.15354198217391968\n",
      "epoch: 9 step: 646, loss is 0.10864593088626862\n",
      "epoch: 9 step: 647, loss is 0.15367817878723145\n",
      "epoch: 9 step: 648, loss is 0.15318958461284637\n",
      "epoch: 9 step: 649, loss is 0.28986743092536926\n",
      "epoch: 9 step: 650, loss is 0.2182244211435318\n",
      "epoch: 9 step: 651, loss is 0.21241195499897003\n",
      "epoch: 9 step: 652, loss is 0.1763228178024292\n",
      "epoch: 9 step: 653, loss is 0.1073395162820816\n",
      "epoch: 9 step: 654, loss is 0.45888206362724304\n",
      "epoch: 9 step: 655, loss is 0.026290060952305794\n",
      "epoch: 9 step: 656, loss is 0.2050582319498062\n",
      "epoch: 9 step: 657, loss is 0.48227638006210327\n",
      "epoch: 9 step: 658, loss is 0.22097207605838776\n",
      "epoch: 9 step: 659, loss is 0.017762189731001854\n",
      "epoch: 9 step: 660, loss is 0.5208999514579773\n",
      "epoch: 9 step: 661, loss is 0.024173814803361893\n",
      "epoch: 9 step: 662, loss is 0.17964529991149902\n",
      "epoch: 9 step: 663, loss is 0.15555869042873383\n",
      "epoch: 9 step: 664, loss is 0.1219014972448349\n",
      "epoch: 9 step: 665, loss is 0.2681623697280884\n",
      "epoch: 9 step: 666, loss is 0.10156207531690598\n",
      "epoch: 9 step: 667, loss is 0.14102333784103394\n",
      "epoch: 9 step: 668, loss is 0.07954297959804535\n",
      "epoch: 9 step: 669, loss is 0.36281269788742065\n",
      "epoch: 9 step: 670, loss is 0.11033789813518524\n",
      "epoch: 9 step: 671, loss is 0.05893564224243164\n",
      "epoch: 9 step: 672, loss is 0.2802683413028717\n",
      "epoch: 9 step: 673, loss is 0.031017091125249863\n",
      "epoch: 9 step: 674, loss is 0.06259475648403168\n",
      "epoch: 9 step: 675, loss is 0.11903253197669983\n",
      "epoch: 9 step: 676, loss is 0.170465350151062\n",
      "epoch: 9 step: 677, loss is 0.02741660363972187\n",
      "epoch: 9 step: 678, loss is 0.20266394317150116\n",
      "epoch: 9 step: 679, loss is 0.05467935651540756\n",
      "epoch: 9 step: 680, loss is 0.2308337241411209\n",
      "epoch: 9 step: 681, loss is 0.1663358062505722\n",
      "epoch: 9 step: 682, loss is 0.13798297941684723\n",
      "epoch: 9 step: 683, loss is 0.2934236228466034\n",
      "epoch: 9 step: 684, loss is 0.2738351821899414\n",
      "epoch: 9 step: 685, loss is 0.05496528372168541\n",
      "epoch: 9 step: 686, loss is 0.20158888399600983\n",
      "epoch: 9 step: 687, loss is 0.06563138216733932\n",
      "epoch: 9 step: 688, loss is 0.021148554980754852\n",
      "epoch: 9 step: 689, loss is 0.08365076780319214\n",
      "epoch: 9 step: 690, loss is 0.36195576190948486\n",
      "epoch: 9 step: 691, loss is 0.010177841410040855\n",
      "epoch: 9 step: 692, loss is 0.13953040540218353\n",
      "epoch: 9 step: 693, loss is 0.2771025598049164\n",
      "epoch: 9 step: 694, loss is 0.07090345770120621\n",
      "epoch: 9 step: 695, loss is 0.5493407249450684\n",
      "epoch: 9 step: 696, loss is 0.07158111035823822\n",
      "epoch: 9 step: 697, loss is 0.08324357122182846\n",
      "epoch: 9 step: 698, loss is 0.34464824199676514\n",
      "epoch: 9 step: 699, loss is 0.045455481857061386\n",
      "epoch: 9 step: 700, loss is 0.010011209174990654\n",
      "epoch: 9 step: 701, loss is 0.19804486632347107\n",
      "epoch: 9 step: 702, loss is 0.5237569808959961\n",
      "epoch: 9 step: 703, loss is 0.1364900916814804\n",
      "epoch: 9 step: 704, loss is 0.1928638070821762\n",
      "epoch: 9 step: 705, loss is 0.03173195198178291\n",
      "epoch: 9 step: 706, loss is 0.1852579265832901\n",
      "epoch: 9 step: 707, loss is 0.017685526981949806\n",
      "epoch: 9 step: 708, loss is 0.021870004013180733\n",
      "epoch: 9 step: 709, loss is 0.03886076435446739\n",
      "epoch: 9 step: 710, loss is 0.1832294762134552\n",
      "epoch: 9 step: 711, loss is 0.07874912023544312\n",
      "epoch: 9 step: 712, loss is 0.19731320440769196\n",
      "epoch: 9 step: 713, loss is 0.42216941714286804\n",
      "epoch: 9 step: 714, loss is 0.6437158584594727\n",
      "epoch: 9 step: 715, loss is 0.22768427431583405\n",
      "epoch: 9 step: 716, loss is 0.20283366739749908\n",
      "epoch: 9 step: 717, loss is 0.28860270977020264\n",
      "epoch: 9 step: 718, loss is 0.3339146673679352\n",
      "epoch: 9 step: 719, loss is 0.4915420413017273\n",
      "epoch: 9 step: 720, loss is 0.31449323892593384\n",
      "epoch: 9 step: 721, loss is 0.2398396134376526\n",
      "epoch: 9 step: 722, loss is 0.1003851592540741\n",
      "epoch: 9 step: 723, loss is 0.09378322958946228\n",
      "epoch: 9 step: 724, loss is 0.8356846570968628\n",
      "epoch: 9 step: 725, loss is 0.07460677623748779\n",
      "epoch: 9 step: 726, loss is 0.052646975964307785\n",
      "epoch: 9 step: 727, loss is 0.14529988169670105\n",
      "epoch: 9 step: 728, loss is 0.07879318296909332\n",
      "epoch: 9 step: 729, loss is 0.311369925737381\n",
      "epoch: 9 step: 730, loss is 0.06680262088775635\n",
      "epoch: 9 step: 731, loss is 0.18578195571899414\n",
      "epoch: 9 step: 732, loss is 0.05517806485295296\n",
      "epoch: 9 step: 733, loss is 0.15484099090099335\n",
      "epoch: 9 step: 734, loss is 0.2470087856054306\n",
      "epoch: 9 step: 735, loss is 0.22243763506412506\n",
      "epoch: 9 step: 736, loss is 0.4270433485507965\n",
      "epoch: 9 step: 737, loss is 0.2653522193431854\n",
      "epoch: 9 step: 738, loss is 0.2424936443567276\n",
      "epoch: 9 step: 739, loss is 0.38307151198387146\n",
      "epoch: 9 step: 740, loss is 0.12303410470485687\n",
      "epoch: 9 step: 741, loss is 0.158981055021286\n",
      "epoch: 9 step: 742, loss is 0.07027249038219452\n",
      "epoch: 9 step: 743, loss is 0.23879492282867432\n",
      "epoch: 9 step: 744, loss is 0.36498749256134033\n",
      "epoch: 9 step: 745, loss is 0.44296690821647644\n",
      "epoch: 9 step: 746, loss is 0.4730689525604248\n",
      "epoch: 9 step: 747, loss is 0.1416953206062317\n",
      "epoch: 9 step: 748, loss is 0.1039634644985199\n",
      "epoch: 9 step: 749, loss is 0.386319637298584\n",
      "epoch: 9 step: 750, loss is 0.3240495026111603\n",
      "epoch: 9 step: 751, loss is 0.8695358633995056\n",
      "epoch: 9 step: 752, loss is 0.1543530970811844\n",
      "epoch: 9 step: 753, loss is 0.267498642206192\n",
      "epoch: 9 step: 754, loss is 0.16889135539531708\n",
      "epoch: 9 step: 755, loss is 0.273914635181427\n",
      "epoch: 9 step: 756, loss is 0.08931083977222443\n",
      "epoch: 9 step: 757, loss is 0.4749930500984192\n",
      "epoch: 9 step: 758, loss is 0.09223820269107819\n",
      "epoch: 9 step: 759, loss is 0.1427498310804367\n",
      "epoch: 9 step: 760, loss is 0.15307803452014923\n",
      "epoch: 9 step: 761, loss is 0.14671339094638824\n",
      "epoch: 9 step: 762, loss is 0.1581266075372696\n",
      "epoch: 9 step: 763, loss is 0.20586486160755157\n",
      "epoch: 9 step: 764, loss is 0.09350835531949997\n",
      "epoch: 9 step: 765, loss is 0.06395306438207626\n",
      "epoch: 9 step: 766, loss is 0.08050739020109177\n",
      "epoch: 9 step: 767, loss is 0.0550740621984005\n",
      "epoch: 9 step: 768, loss is 0.22188551723957062\n",
      "epoch: 9 step: 769, loss is 0.04485185444355011\n",
      "epoch: 9 step: 770, loss is 0.7378891706466675\n",
      "epoch: 9 step: 771, loss is 0.1858656406402588\n",
      "epoch: 9 step: 772, loss is 0.6089956760406494\n",
      "epoch: 9 step: 773, loss is 0.05029382184147835\n",
      "epoch: 9 step: 774, loss is 0.03214947506785393\n",
      "epoch: 9 step: 775, loss is 0.3682965934276581\n",
      "epoch: 9 step: 776, loss is 0.2220226526260376\n",
      "epoch: 9 step: 777, loss is 0.2534870505332947\n",
      "epoch: 9 step: 778, loss is 0.10743986070156097\n",
      "epoch: 9 step: 779, loss is 0.0820014625787735\n",
      "epoch: 9 step: 780, loss is 0.386118084192276\n",
      "epoch: 9 step: 781, loss is 0.11107612401247025\n",
      "epoch: 9 step: 782, loss is 0.22320368885993958\n",
      "epoch: 9 step: 783, loss is 0.1228988990187645\n",
      "epoch: 9 step: 784, loss is 0.5590606331825256\n",
      "epoch: 9 step: 785, loss is 0.49540838599205017\n",
      "epoch: 9 step: 786, loss is 0.04994223639369011\n",
      "epoch: 9 step: 787, loss is 0.28079888224601746\n",
      "epoch: 9 step: 788, loss is 0.29179641604423523\n",
      "epoch: 9 step: 789, loss is 0.23329909145832062\n",
      "epoch: 9 step: 790, loss is 0.15826471149921417\n",
      "epoch: 9 step: 791, loss is 0.06335170567035675\n",
      "epoch: 9 step: 792, loss is 0.31825321912765503\n",
      "epoch: 9 step: 793, loss is 0.3476034104824066\n",
      "epoch: 9 step: 794, loss is 0.11272315680980682\n",
      "epoch: 9 step: 795, loss is 0.23359087109565735\n",
      "epoch: 9 step: 796, loss is 0.3310254216194153\n",
      "epoch: 9 step: 797, loss is 0.2826008200645447\n",
      "epoch: 9 step: 798, loss is 0.07516259700059891\n",
      "epoch: 9 step: 799, loss is 0.14010824263095856\n",
      "epoch: 9 step: 800, loss is 0.24007365107536316\n",
      "epoch: 9 step: 801, loss is 0.1219114288687706\n",
      "epoch: 9 step: 802, loss is 0.0582084134221077\n",
      "epoch: 9 step: 803, loss is 0.42823904752731323\n",
      "epoch: 9 step: 804, loss is 0.006412637885659933\n",
      "epoch: 9 step: 805, loss is 0.18326054513454437\n",
      "epoch: 9 step: 806, loss is 0.12426500767469406\n",
      "epoch: 9 step: 807, loss is 0.18244773149490356\n",
      "epoch: 9 step: 808, loss is 0.020548859611153603\n",
      "epoch: 9 step: 809, loss is 0.05989344045519829\n",
      "epoch: 9 step: 810, loss is 0.2567671239376068\n",
      "epoch: 9 step: 811, loss is 0.6273093223571777\n",
      "epoch: 9 step: 812, loss is 0.1260969638824463\n",
      "epoch: 9 step: 813, loss is 0.13181182742118835\n",
      "epoch: 9 step: 814, loss is 0.25642338395118713\n",
      "epoch: 9 step: 815, loss is 0.009829755872488022\n",
      "epoch: 9 step: 816, loss is 0.1378605216741562\n",
      "epoch: 9 step: 817, loss is 0.20307905972003937\n",
      "epoch: 9 step: 818, loss is 0.33429792523384094\n",
      "epoch: 9 step: 819, loss is 0.41096368432044983\n",
      "epoch: 9 step: 820, loss is 0.005945269018411636\n",
      "epoch: 9 step: 821, loss is 0.12782283127307892\n",
      "epoch: 9 step: 822, loss is 0.19116364419460297\n",
      "epoch: 9 step: 823, loss is 0.2985136806964874\n",
      "epoch: 9 step: 824, loss is 0.22290939092636108\n",
      "epoch: 9 step: 825, loss is 0.42496582865715027\n",
      "epoch: 9 step: 826, loss is 0.1447005420923233\n",
      "epoch: 9 step: 827, loss is 0.18552634119987488\n",
      "epoch: 9 step: 828, loss is 0.2161308228969574\n",
      "epoch: 9 step: 829, loss is 0.27359598875045776\n",
      "epoch: 9 step: 830, loss is 0.41598859429359436\n",
      "epoch: 9 step: 831, loss is 0.03312456235289574\n",
      "epoch: 9 step: 832, loss is 0.14283528923988342\n",
      "epoch: 9 step: 833, loss is 0.03912422060966492\n",
      "epoch: 9 step: 834, loss is 0.14934790134429932\n",
      "epoch: 9 step: 835, loss is 0.019122498109936714\n",
      "epoch: 9 step: 836, loss is 0.1975756734609604\n",
      "epoch: 9 step: 837, loss is 0.16590774059295654\n",
      "epoch: 9 step: 838, loss is 0.6603390574455261\n",
      "epoch: 9 step: 839, loss is 0.24062618613243103\n",
      "epoch: 9 step: 840, loss is 0.07219484448432922\n",
      "epoch: 9 step: 841, loss is 0.020106211304664612\n",
      "epoch: 9 step: 842, loss is 0.19924913346767426\n",
      "epoch: 9 step: 843, loss is 0.10430204123258591\n",
      "epoch: 9 step: 844, loss is 0.3730095624923706\n",
      "epoch: 9 step: 845, loss is 0.18153534829616547\n",
      "epoch: 9 step: 846, loss is 0.04594297334551811\n",
      "epoch: 9 step: 847, loss is 0.023379456251859665\n",
      "epoch: 9 step: 848, loss is 0.29396235942840576\n",
      "epoch: 9 step: 849, loss is 0.3078446090221405\n",
      "epoch: 9 step: 850, loss is 0.0976933091878891\n",
      "epoch: 9 step: 851, loss is 0.16912677884101868\n",
      "epoch: 9 step: 852, loss is 0.050324682146310806\n",
      "epoch: 9 step: 853, loss is 0.38936319947242737\n",
      "epoch: 9 step: 854, loss is 0.0655561238527298\n",
      "epoch: 9 step: 855, loss is 0.05281498655676842\n",
      "epoch: 9 step: 856, loss is 0.9021288156509399\n",
      "epoch: 9 step: 857, loss is 0.22064021229743958\n",
      "epoch: 9 step: 858, loss is 0.020861022174358368\n",
      "epoch: 9 step: 859, loss is 0.14747583866119385\n",
      "epoch: 9 step: 860, loss is 0.4271485209465027\n",
      "epoch: 9 step: 861, loss is 0.10093842446804047\n",
      "epoch: 9 step: 862, loss is 0.5566678643226624\n",
      "epoch: 9 step: 863, loss is 0.08478587120771408\n",
      "epoch: 9 step: 864, loss is 0.1725158542394638\n",
      "epoch: 9 step: 865, loss is 0.050965528935194016\n",
      "epoch: 9 step: 866, loss is 0.38469141721725464\n",
      "epoch: 9 step: 867, loss is 0.24862053990364075\n",
      "epoch: 9 step: 868, loss is 0.33634042739868164\n",
      "epoch: 9 step: 869, loss is 0.4070056676864624\n",
      "epoch: 9 step: 870, loss is 0.2530202269554138\n",
      "epoch: 9 step: 871, loss is 0.1788550615310669\n",
      "epoch: 9 step: 872, loss is 0.0693841353058815\n",
      "epoch: 9 step: 873, loss is 0.17649802565574646\n",
      "epoch: 9 step: 874, loss is 0.31202179193496704\n",
      "epoch: 9 step: 875, loss is 0.05131440982222557\n",
      "epoch: 9 step: 876, loss is 0.28717663884162903\n",
      "epoch: 9 step: 877, loss is 0.13647454977035522\n",
      "epoch: 9 step: 878, loss is 0.15018469095230103\n",
      "epoch: 9 step: 879, loss is 0.7440365552902222\n",
      "epoch: 9 step: 880, loss is 0.19427095353603363\n",
      "epoch: 9 step: 881, loss is 0.20830073952674866\n",
      "epoch: 9 step: 882, loss is 0.3275843560695648\n",
      "epoch: 9 step: 883, loss is 0.21755890548229218\n",
      "epoch: 9 step: 884, loss is 0.2960226535797119\n",
      "epoch: 9 step: 885, loss is 0.14452199637889862\n",
      "epoch: 9 step: 886, loss is 0.3299669027328491\n",
      "epoch: 9 step: 887, loss is 0.4320893883705139\n",
      "epoch: 9 step: 888, loss is 0.12280994653701782\n",
      "epoch: 9 step: 889, loss is 0.07913529872894287\n",
      "epoch: 9 step: 890, loss is 0.06752361357212067\n",
      "epoch: 9 step: 891, loss is 0.36854323744773865\n",
      "epoch: 9 step: 892, loss is 0.20921245217323303\n",
      "epoch: 9 step: 893, loss is 0.06444403529167175\n",
      "epoch: 9 step: 894, loss is 0.13569395244121552\n",
      "epoch: 9 step: 895, loss is 0.8033761978149414\n",
      "epoch: 9 step: 896, loss is 0.6999855041503906\n",
      "epoch: 9 step: 897, loss is 0.08000090718269348\n",
      "epoch: 9 step: 898, loss is 0.14558091759681702\n",
      "epoch: 9 step: 899, loss is 0.04298657551407814\n",
      "epoch: 9 step: 900, loss is 0.034901104867458344\n",
      "epoch: 9 step: 901, loss is 0.26490288972854614\n",
      "epoch: 9 step: 902, loss is 0.23090557754039764\n",
      "epoch: 9 step: 903, loss is 0.02282513678073883\n",
      "epoch: 9 step: 904, loss is 0.11517510563135147\n",
      "epoch: 9 step: 905, loss is 0.010032071731984615\n",
      "epoch: 9 step: 906, loss is 0.6151191592216492\n",
      "epoch: 9 step: 907, loss is 0.2873048484325409\n",
      "epoch: 9 step: 908, loss is 0.07531990110874176\n",
      "epoch: 9 step: 909, loss is 0.16517692804336548\n",
      "epoch: 9 step: 910, loss is 0.24725483357906342\n",
      "epoch: 9 step: 911, loss is 0.42464202642440796\n",
      "epoch: 9 step: 912, loss is 0.10198643058538437\n",
      "epoch: 9 step: 913, loss is 0.052256230264902115\n",
      "epoch: 9 step: 914, loss is 0.10032150894403458\n",
      "epoch: 9 step: 915, loss is 0.05987777188420296\n",
      "epoch: 9 step: 916, loss is 0.13995105028152466\n",
      "epoch: 9 step: 917, loss is 0.20815670490264893\n",
      "epoch: 9 step: 918, loss is 0.16299138963222504\n",
      "epoch: 9 step: 919, loss is 0.2433611899614334\n",
      "epoch: 9 step: 920, loss is 0.364712119102478\n",
      "epoch: 9 step: 921, loss is 0.3682900667190552\n",
      "epoch: 9 step: 922, loss is 0.2599338889122009\n",
      "epoch: 9 step: 923, loss is 0.28054147958755493\n",
      "epoch: 9 step: 924, loss is 0.2658502161502838\n",
      "epoch: 9 step: 925, loss is 0.06293336302042007\n",
      "epoch: 9 step: 926, loss is 0.17516647279262543\n",
      "epoch: 9 step: 927, loss is 0.028795650228857994\n",
      "epoch: 9 step: 928, loss is 0.5084754824638367\n",
      "epoch: 9 step: 929, loss is 0.12776722013950348\n",
      "epoch: 9 step: 930, loss is 0.0240592323243618\n",
      "epoch: 9 step: 931, loss is 0.2353033423423767\n",
      "epoch: 9 step: 932, loss is 0.11466708779335022\n",
      "epoch: 9 step: 933, loss is 0.13818588852882385\n",
      "epoch: 9 step: 934, loss is 0.06449484080076218\n",
      "epoch: 9 step: 935, loss is 0.2273019254207611\n",
      "epoch: 9 step: 936, loss is 0.09488780796527863\n",
      "epoch: 9 step: 937, loss is 0.10879778116941452\n",
      "epoch: 9 step: 938, loss is 0.05526312440633774\n",
      "epoch: 9 step: 939, loss is 0.1550423502922058\n",
      "epoch: 9 step: 940, loss is 0.23071248829364777\n",
      "epoch: 9 step: 941, loss is 0.3296095132827759\n",
      "epoch: 9 step: 942, loss is 0.1255473494529724\n",
      "epoch: 9 step: 943, loss is 0.12207647413015366\n",
      "epoch: 9 step: 944, loss is 0.4259866178035736\n",
      "epoch: 9 step: 945, loss is 0.2704326808452606\n",
      "epoch: 9 step: 946, loss is 0.2356472611427307\n",
      "epoch: 9 step: 947, loss is 0.5493468046188354\n",
      "epoch: 9 step: 948, loss is 0.06220095604658127\n",
      "epoch: 9 step: 949, loss is 0.14989708364009857\n",
      "epoch: 9 step: 950, loss is 0.032519206404685974\n",
      "epoch: 9 step: 951, loss is 0.47883689403533936\n",
      "epoch: 9 step: 952, loss is 0.11795360594987869\n",
      "epoch: 9 step: 953, loss is 0.422457754611969\n",
      "epoch: 9 step: 954, loss is 0.14102588593959808\n",
      "epoch: 9 step: 955, loss is 1.0378923416137695\n",
      "epoch: 9 step: 956, loss is 0.2654789686203003\n",
      "epoch: 9 step: 957, loss is 0.38169291615486145\n",
      "epoch: 9 step: 958, loss is 0.22048139572143555\n",
      "epoch: 9 step: 959, loss is 0.06591110676527023\n",
      "epoch: 9 step: 960, loss is 0.20164817571640015\n",
      "epoch: 9 step: 961, loss is 0.21376550197601318\n",
      "epoch: 9 step: 962, loss is 0.10560304671525955\n",
      "epoch: 9 step: 963, loss is 0.48883309960365295\n",
      "epoch: 9 step: 964, loss is 0.07448709011077881\n",
      "epoch: 9 step: 965, loss is 0.06753305345773697\n",
      "epoch: 9 step: 966, loss is 0.06400073319673538\n",
      "epoch: 9 step: 967, loss is 0.3882557153701782\n",
      "epoch: 9 step: 968, loss is 0.12400534003973007\n",
      "epoch: 9 step: 969, loss is 0.7675093412399292\n",
      "epoch: 9 step: 970, loss is 0.35431984066963196\n",
      "epoch: 9 step: 971, loss is 0.11134815961122513\n",
      "epoch: 9 step: 972, loss is 0.1046581044793129\n",
      "epoch: 9 step: 973, loss is 0.2151690572500229\n",
      "epoch: 9 step: 974, loss is 0.10052426159381866\n",
      "epoch: 9 step: 975, loss is 0.10867685079574585\n",
      "epoch: 9 step: 976, loss is 0.21840868890285492\n",
      "epoch: 9 step: 977, loss is 0.2903331518173218\n",
      "epoch: 9 step: 978, loss is 0.10051356256008148\n",
      "epoch: 9 step: 979, loss is 0.477961927652359\n",
      "epoch: 9 step: 980, loss is 0.14332586526870728\n",
      "epoch: 9 step: 981, loss is 0.5383223295211792\n",
      "epoch: 9 step: 982, loss is 0.2987978756427765\n",
      "epoch: 9 step: 983, loss is 0.33415812253952026\n",
      "epoch: 9 step: 984, loss is 0.1700078845024109\n",
      "epoch: 9 step: 985, loss is 0.08108772337436676\n",
      "epoch: 9 step: 986, loss is 0.274469256401062\n",
      "epoch: 9 step: 987, loss is 0.032638195902109146\n",
      "epoch: 9 step: 988, loss is 0.09325293451547623\n",
      "epoch: 9 step: 989, loss is 0.13462631404399872\n",
      "epoch: 9 step: 990, loss is 0.04015281796455383\n",
      "epoch: 9 step: 991, loss is 0.048555754125118256\n",
      "epoch: 9 step: 992, loss is 0.015448048710823059\n",
      "epoch: 9 step: 993, loss is 0.04030712693929672\n",
      "epoch: 9 step: 994, loss is 0.37810155749320984\n",
      "epoch: 9 step: 995, loss is 0.3700537085533142\n",
      "epoch: 9 step: 996, loss is 0.1655532568693161\n",
      "epoch: 9 step: 997, loss is 0.10565266758203506\n",
      "epoch: 9 step: 998, loss is 0.7223320603370667\n",
      "epoch: 9 step: 999, loss is 0.3277902901172638\n",
      "epoch: 9 step: 1000, loss is 0.029358582571148872\n",
      "epoch: 9 step: 1001, loss is 0.20784291625022888\n",
      "epoch: 9 step: 1002, loss is 0.3895273506641388\n",
      "epoch: 9 step: 1003, loss is 0.437919020652771\n",
      "epoch: 9 step: 1004, loss is 0.11629986763000488\n",
      "epoch: 9 step: 1005, loss is 0.8004647493362427\n",
      "epoch: 9 step: 1006, loss is 0.052542973309755325\n",
      "epoch: 9 step: 1007, loss is 0.41069620847702026\n",
      "epoch: 9 step: 1008, loss is 0.15891578793525696\n",
      "epoch: 9 step: 1009, loss is 0.24546106159687042\n",
      "epoch: 9 step: 1010, loss is 0.24416615068912506\n",
      "epoch: 9 step: 1011, loss is 0.13017147779464722\n",
      "epoch: 9 step: 1012, loss is 0.402468204498291\n",
      "epoch: 9 step: 1013, loss is 0.16931895911693573\n",
      "epoch: 9 step: 1014, loss is 0.18177133798599243\n",
      "epoch: 9 step: 1015, loss is 0.49145758152008057\n",
      "epoch: 9 step: 1016, loss is 0.3503461182117462\n",
      "epoch: 9 step: 1017, loss is 0.15880140662193298\n",
      "epoch: 9 step: 1018, loss is 0.15686827898025513\n",
      "epoch: 9 step: 1019, loss is 0.10662632435560226\n",
      "epoch: 9 step: 1020, loss is 0.16280849277973175\n",
      "epoch: 9 step: 1021, loss is 0.053088825196027756\n",
      "epoch: 9 step: 1022, loss is 0.34144890308380127\n",
      "epoch: 9 step: 1023, loss is 0.25833961367607117\n",
      "epoch: 9 step: 1024, loss is 0.11808691918849945\n",
      "epoch: 9 step: 1025, loss is 0.19528411328792572\n",
      "epoch: 9 step: 1026, loss is 0.1213817447423935\n",
      "epoch: 9 step: 1027, loss is 0.12567219138145447\n",
      "epoch: 9 step: 1028, loss is 0.05242336168885231\n",
      "epoch: 9 step: 1029, loss is 0.01617475226521492\n",
      "epoch: 9 step: 1030, loss is 0.02150055393576622\n",
      "epoch: 9 step: 1031, loss is 0.4681057929992676\n",
      "epoch: 9 step: 1032, loss is 0.46053823828697205\n",
      "epoch: 9 step: 1033, loss is 0.3771332800388336\n",
      "epoch: 9 step: 1034, loss is 0.09172144532203674\n",
      "epoch: 9 step: 1035, loss is 0.26300960779190063\n",
      "epoch: 9 step: 1036, loss is 0.19761450588703156\n",
      "epoch: 9 step: 1037, loss is 0.14170929789543152\n",
      "epoch: 9 step: 1038, loss is 0.3654382824897766\n",
      "epoch: 9 step: 1039, loss is 0.050284430384635925\n",
      "epoch: 9 step: 1040, loss is 0.07309792190790176\n",
      "epoch: 9 step: 1041, loss is 0.1321409046649933\n",
      "epoch: 9 step: 1042, loss is 0.22327257692813873\n",
      "epoch: 9 step: 1043, loss is 0.15803593397140503\n",
      "epoch: 9 step: 1044, loss is 0.1279286891222\n",
      "epoch: 9 step: 1045, loss is 0.7090408802032471\n",
      "epoch: 9 step: 1046, loss is 0.21138659119606018\n",
      "epoch: 9 step: 1047, loss is 0.05238781124353409\n",
      "epoch: 9 step: 1048, loss is 0.04553290829062462\n",
      "epoch: 9 step: 1049, loss is 0.06676173955202103\n",
      "epoch: 9 step: 1050, loss is 0.21360239386558533\n",
      "epoch: 9 step: 1051, loss is 0.5185685157775879\n",
      "epoch: 9 step: 1052, loss is 0.04334893077611923\n",
      "epoch: 9 step: 1053, loss is 0.1032768115401268\n",
      "epoch: 9 step: 1054, loss is 0.20662519335746765\n",
      "epoch: 9 step: 1055, loss is 0.06553240865468979\n",
      "epoch: 9 step: 1056, loss is 0.22954262793064117\n",
      "epoch: 9 step: 1057, loss is 0.1945350468158722\n",
      "epoch: 9 step: 1058, loss is 0.547816276550293\n",
      "epoch: 9 step: 1059, loss is 0.3467644155025482\n",
      "epoch: 9 step: 1060, loss is 0.12624822556972504\n",
      "epoch: 9 step: 1061, loss is 0.15003523230552673\n",
      "epoch: 9 step: 1062, loss is 0.27308982610702515\n",
      "epoch: 9 step: 1063, loss is 0.07544949650764465\n",
      "epoch: 9 step: 1064, loss is 0.1577056646347046\n",
      "epoch: 9 step: 1065, loss is 0.11490742862224579\n",
      "epoch: 9 step: 1066, loss is 0.031225798651576042\n",
      "epoch: 9 step: 1067, loss is 0.05296684056520462\n",
      "epoch: 9 step: 1068, loss is 0.08417381346225739\n",
      "epoch: 9 step: 1069, loss is 0.03589310497045517\n",
      "epoch: 9 step: 1070, loss is 0.0844700038433075\n",
      "epoch: 9 step: 1071, loss is 0.12604714930057526\n",
      "epoch: 9 step: 1072, loss is 0.3876684904098511\n",
      "epoch: 9 step: 1073, loss is 0.552919864654541\n",
      "epoch: 9 step: 1074, loss is 0.4506833255290985\n",
      "epoch: 9 step: 1075, loss is 0.6631741523742676\n",
      "epoch: 9 step: 1076, loss is 0.2927442491054535\n",
      "epoch: 9 step: 1077, loss is 0.24154826998710632\n",
      "epoch: 9 step: 1078, loss is 0.054617926478385925\n",
      "epoch: 9 step: 1079, loss is 0.09163448214530945\n",
      "epoch: 9 step: 1080, loss is 0.2874131500720978\n",
      "epoch: 9 step: 1081, loss is 0.027449389919638634\n",
      "epoch: 9 step: 1082, loss is 0.021992096677422523\n",
      "epoch: 9 step: 1083, loss is 0.15811605751514435\n",
      "epoch: 9 step: 1084, loss is 0.35132279992103577\n",
      "epoch: 9 step: 1085, loss is 0.31617528200149536\n",
      "epoch: 9 step: 1086, loss is 0.25604283809661865\n",
      "epoch: 9 step: 1087, loss is 0.3435859680175781\n",
      "epoch: 9 step: 1088, loss is 0.31506678462028503\n",
      "epoch: 9 step: 1089, loss is 0.6168893575668335\n",
      "epoch: 9 step: 1090, loss is 0.20377179980278015\n",
      "epoch: 9 step: 1091, loss is 0.04991844296455383\n",
      "epoch: 9 step: 1092, loss is 0.11692100763320923\n",
      "epoch: 9 step: 1093, loss is 0.2101699262857437\n",
      "epoch: 9 step: 1094, loss is 0.2706403136253357\n",
      "epoch: 9 step: 1095, loss is 0.47442036867141724\n",
      "epoch: 9 step: 1096, loss is 0.6544963717460632\n",
      "epoch: 9 step: 1097, loss is 0.17967884242534637\n",
      "epoch: 9 step: 1098, loss is 0.29540494084358215\n",
      "epoch: 9 step: 1099, loss is 0.3835805058479309\n",
      "epoch: 9 step: 1100, loss is 0.09487894177436829\n",
      "epoch: 9 step: 1101, loss is 0.17718544602394104\n",
      "epoch: 9 step: 1102, loss is 0.1280624270439148\n",
      "epoch: 9 step: 1103, loss is 0.6911596655845642\n",
      "epoch: 9 step: 1104, loss is 0.09719637781381607\n",
      "epoch: 9 step: 1105, loss is 0.5952885150909424\n",
      "epoch: 9 step: 1106, loss is 0.16552889347076416\n",
      "epoch: 9 step: 1107, loss is 0.21409274637699127\n",
      "epoch: 9 step: 1108, loss is 0.26698875427246094\n",
      "epoch: 9 step: 1109, loss is 0.2712375819683075\n",
      "epoch: 9 step: 1110, loss is 0.08273940533399582\n",
      "epoch: 9 step: 1111, loss is 0.023570042103528976\n",
      "epoch: 9 step: 1112, loss is 0.01643810048699379\n",
      "epoch: 9 step: 1113, loss is 0.17427204549312592\n",
      "epoch: 9 step: 1114, loss is 0.10651210695505142\n",
      "epoch: 9 step: 1115, loss is 0.34839168190956116\n",
      "epoch: 9 step: 1116, loss is 0.03371512517333031\n",
      "epoch: 9 step: 1117, loss is 0.13680540025234222\n",
      "epoch: 9 step: 1118, loss is 0.370999276638031\n",
      "epoch: 9 step: 1119, loss is 0.6500147581100464\n",
      "epoch: 9 step: 1120, loss is 0.05486549809575081\n",
      "epoch: 9 step: 1121, loss is 0.2618477940559387\n",
      "epoch: 9 step: 1122, loss is 0.062309108674526215\n",
      "epoch: 9 step: 1123, loss is 0.17511481046676636\n",
      "epoch: 9 step: 1124, loss is 0.43494510650634766\n",
      "epoch: 9 step: 1125, loss is 0.09208518266677856\n",
      "epoch: 9 step: 1126, loss is 0.04215933755040169\n",
      "epoch: 9 step: 1127, loss is 0.21690677106380463\n",
      "epoch: 9 step: 1128, loss is 0.05592904984951019\n",
      "epoch: 9 step: 1129, loss is 0.2453785538673401\n",
      "epoch: 9 step: 1130, loss is 0.31902703642845154\n",
      "epoch: 9 step: 1131, loss is 0.15738670527935028\n",
      "epoch: 9 step: 1132, loss is 0.1460115611553192\n",
      "epoch: 9 step: 1133, loss is 0.2812352478504181\n",
      "epoch: 9 step: 1134, loss is 0.03254453465342522\n",
      "epoch: 9 step: 1135, loss is 0.10106530785560608\n",
      "epoch: 9 step: 1136, loss is 0.10875354707241058\n",
      "epoch: 9 step: 1137, loss is 0.5278990864753723\n",
      "epoch: 9 step: 1138, loss is 0.08334502577781677\n",
      "epoch: 9 step: 1139, loss is 0.3791697025299072\n",
      "epoch: 9 step: 1140, loss is 0.1256551891565323\n",
      "epoch: 9 step: 1141, loss is 0.04230908676981926\n",
      "epoch: 9 step: 1142, loss is 0.13263124227523804\n",
      "epoch: 9 step: 1143, loss is 0.020586419850587845\n",
      "epoch: 9 step: 1144, loss is 0.40016183257102966\n",
      "epoch: 9 step: 1145, loss is 0.1379173845052719\n",
      "epoch: 9 step: 1146, loss is 0.1023714616894722\n",
      "epoch: 9 step: 1147, loss is 0.0474126860499382\n",
      "epoch: 9 step: 1148, loss is 0.200966015458107\n",
      "epoch: 9 step: 1149, loss is 0.2481471747159958\n",
      "epoch: 9 step: 1150, loss is 0.15672184526920319\n",
      "epoch: 9 step: 1151, loss is 0.5308729410171509\n",
      "epoch: 9 step: 1152, loss is 0.16202063858509064\n",
      "epoch: 9 step: 1153, loss is 0.21384896337985992\n",
      "epoch: 9 step: 1154, loss is 0.0793064758181572\n",
      "epoch: 9 step: 1155, loss is 0.25334927439689636\n",
      "epoch: 9 step: 1156, loss is 0.05426361784338951\n",
      "epoch: 9 step: 1157, loss is 0.031879037618637085\n",
      "epoch: 9 step: 1158, loss is 0.24898040294647217\n",
      "epoch: 9 step: 1159, loss is 0.31407102942466736\n",
      "epoch: 9 step: 1160, loss is 0.06742659956216812\n",
      "epoch: 9 step: 1161, loss is 0.5896031856536865\n",
      "epoch: 9 step: 1162, loss is 0.5711833834648132\n",
      "epoch: 9 step: 1163, loss is 0.31820952892303467\n",
      "epoch: 9 step: 1164, loss is 0.04867878928780556\n",
      "epoch: 9 step: 1165, loss is 0.05823265016078949\n",
      "epoch: 9 step: 1166, loss is 0.06976593285799026\n",
      "epoch: 9 step: 1167, loss is 0.06594575941562653\n",
      "epoch: 9 step: 1168, loss is 0.43674230575561523\n",
      "epoch: 9 step: 1169, loss is 0.22704267501831055\n",
      "epoch: 9 step: 1170, loss is 0.04208187013864517\n",
      "epoch: 9 step: 1171, loss is 0.09327704459428787\n",
      "epoch: 9 step: 1172, loss is 0.2766256332397461\n",
      "epoch: 9 step: 1173, loss is 0.014353128150105476\n",
      "epoch: 9 step: 1174, loss is 0.0665469840168953\n",
      "epoch: 9 step: 1175, loss is 0.28584054112434387\n",
      "epoch: 9 step: 1176, loss is 0.5016565322875977\n",
      "epoch: 9 step: 1177, loss is 0.2127504199743271\n",
      "epoch: 9 step: 1178, loss is 0.1023782342672348\n",
      "epoch: 9 step: 1179, loss is 0.4523065388202667\n",
      "epoch: 9 step: 1180, loss is 0.07690643519163132\n",
      "epoch: 9 step: 1181, loss is 0.16444236040115356\n",
      "epoch: 9 step: 1182, loss is 0.2715226113796234\n",
      "epoch: 9 step: 1183, loss is 0.26666536927223206\n",
      "epoch: 9 step: 1184, loss is 0.19693471491336823\n",
      "epoch: 9 step: 1185, loss is 0.12363042682409286\n",
      "epoch: 9 step: 1186, loss is 0.15878723561763763\n",
      "epoch: 9 step: 1187, loss is 0.39327555894851685\n",
      "epoch: 9 step: 1188, loss is 0.1390710324048996\n",
      "epoch: 9 step: 1189, loss is 0.43979358673095703\n",
      "epoch: 9 step: 1190, loss is 0.022170890122652054\n",
      "epoch: 9 step: 1191, loss is 0.12492900341749191\n",
      "epoch: 9 step: 1192, loss is 0.13201552629470825\n",
      "epoch: 9 step: 1193, loss is 0.037509381771087646\n",
      "epoch: 9 step: 1194, loss is 0.1307833343744278\n",
      "epoch: 9 step: 1195, loss is 0.03982938826084137\n",
      "epoch: 9 step: 1196, loss is 0.7726287841796875\n",
      "epoch: 9 step: 1197, loss is 0.035777803510427475\n",
      "epoch: 9 step: 1198, loss is 0.3315911889076233\n",
      "epoch: 9 step: 1199, loss is 0.09057661890983582\n",
      "epoch: 9 step: 1200, loss is 0.02522796019911766\n",
      "epoch: 9 step: 1201, loss is 0.3967832922935486\n",
      "epoch: 9 step: 1202, loss is 0.014588889665901661\n",
      "epoch: 9 step: 1203, loss is 0.1167185977101326\n",
      "epoch: 9 step: 1204, loss is 0.1619417816400528\n",
      "epoch: 9 step: 1205, loss is 0.16904346644878387\n",
      "epoch: 9 step: 1206, loss is 0.502545177936554\n",
      "epoch: 9 step: 1207, loss is 0.2479826807975769\n",
      "epoch: 9 step: 1208, loss is 0.2608514130115509\n",
      "epoch: 9 step: 1209, loss is 0.09425295889377594\n",
      "epoch: 9 step: 1210, loss is 0.6355757713317871\n",
      "epoch: 9 step: 1211, loss is 0.08399061858654022\n",
      "epoch: 9 step: 1212, loss is 0.21319518983364105\n",
      "epoch: 9 step: 1213, loss is 0.32294198870658875\n",
      "epoch: 9 step: 1214, loss is 0.14012393355369568\n",
      "epoch: 9 step: 1215, loss is 0.24817131459712982\n",
      "epoch: 9 step: 1216, loss is 0.6996322870254517\n",
      "epoch: 9 step: 1217, loss is 0.5677682757377625\n",
      "epoch: 9 step: 1218, loss is 0.5347394347190857\n",
      "epoch: 9 step: 1219, loss is 0.18952031433582306\n",
      "epoch: 9 step: 1220, loss is 0.0559077262878418\n",
      "epoch: 9 step: 1221, loss is 0.21642577648162842\n",
      "epoch: 9 step: 1222, loss is 0.36562514305114746\n",
      "epoch: 9 step: 1223, loss is 0.3928682506084442\n",
      "epoch: 9 step: 1224, loss is 0.0903826504945755\n",
      "epoch: 9 step: 1225, loss is 0.0729251503944397\n",
      "epoch: 9 step: 1226, loss is 0.1559091955423355\n",
      "epoch: 9 step: 1227, loss is 0.13308848440647125\n",
      "epoch: 9 step: 1228, loss is 0.11066963523626328\n",
      "epoch: 9 step: 1229, loss is 0.04631662368774414\n",
      "epoch: 9 step: 1230, loss is 0.05828956514596939\n",
      "epoch: 9 step: 1231, loss is 0.04431333765387535\n",
      "epoch: 9 step: 1232, loss is 0.517429769039154\n",
      "epoch: 9 step: 1233, loss is 0.11831208318471909\n",
      "epoch: 9 step: 1234, loss is 0.04410553351044655\n",
      "epoch: 9 step: 1235, loss is 0.4643958508968353\n",
      "epoch: 9 step: 1236, loss is 0.18223422765731812\n",
      "epoch: 9 step: 1237, loss is 0.05944402515888214\n",
      "epoch: 9 step: 1238, loss is 0.23581580817699432\n",
      "epoch: 9 step: 1239, loss is 0.16432909667491913\n",
      "epoch: 9 step: 1240, loss is 0.4407775402069092\n",
      "epoch: 9 step: 1241, loss is 0.07504872232675552\n",
      "epoch: 9 step: 1242, loss is 0.9245494604110718\n",
      "epoch: 9 step: 1243, loss is 0.01765027455985546\n",
      "epoch: 9 step: 1244, loss is 0.15235204994678497\n",
      "epoch: 9 step: 1245, loss is 0.07130252569913864\n",
      "epoch: 9 step: 1246, loss is 0.0676426887512207\n",
      "epoch: 9 step: 1247, loss is 0.26067620515823364\n",
      "epoch: 9 step: 1248, loss is 0.12130460143089294\n",
      "epoch: 9 step: 1249, loss is 0.11731588840484619\n",
      "epoch: 9 step: 1250, loss is 0.1690855473279953\n",
      "epoch: 9 step: 1251, loss is 0.15811295807361603\n",
      "epoch: 9 step: 1252, loss is 0.047912999987602234\n",
      "epoch: 9 step: 1253, loss is 0.12832027673721313\n",
      "epoch: 9 step: 1254, loss is 0.05955184996128082\n",
      "epoch: 9 step: 1255, loss is 0.5141472816467285\n",
      "epoch: 9 step: 1256, loss is 0.1596013456583023\n",
      "epoch: 9 step: 1257, loss is 0.7782006859779358\n",
      "epoch: 9 step: 1258, loss is 0.28860774636268616\n",
      "epoch: 9 step: 1259, loss is 0.026706485077738762\n",
      "epoch: 9 step: 1260, loss is 0.1456153690814972\n",
      "epoch: 9 step: 1261, loss is 1.1753233671188354\n",
      "epoch: 9 step: 1262, loss is 0.02875755913555622\n",
      "epoch: 9 step: 1263, loss is 0.48284974694252014\n",
      "epoch: 9 step: 1264, loss is 0.026454193517565727\n",
      "epoch: 9 step: 1265, loss is 0.09150214493274689\n",
      "epoch: 9 step: 1266, loss is 0.02238779515028\n",
      "epoch: 9 step: 1267, loss is 0.10752811282873154\n",
      "epoch: 9 step: 1268, loss is 0.08675727993249893\n",
      "epoch: 9 step: 1269, loss is 0.1934017539024353\n",
      "epoch: 9 step: 1270, loss is 0.14343848824501038\n",
      "epoch: 9 step: 1271, loss is 0.3356363773345947\n",
      "epoch: 9 step: 1272, loss is 0.10547251999378204\n",
      "epoch: 9 step: 1273, loss is 0.41127321124076843\n",
      "epoch: 9 step: 1274, loss is 0.16858136653900146\n",
      "epoch: 9 step: 1275, loss is 0.1671830266714096\n",
      "epoch: 9 step: 1276, loss is 0.06340832263231277\n",
      "epoch: 9 step: 1277, loss is 0.3261348307132721\n",
      "epoch: 9 step: 1278, loss is 0.10046371817588806\n",
      "epoch: 9 step: 1279, loss is 0.03881123289465904\n",
      "epoch: 9 step: 1280, loss is 0.10158345103263855\n",
      "epoch: 9 step: 1281, loss is 0.23388688266277313\n",
      "epoch: 9 step: 1282, loss is 0.11241243779659271\n",
      "epoch: 9 step: 1283, loss is 0.4301399290561676\n",
      "epoch: 9 step: 1284, loss is 0.44764161109924316\n",
      "epoch: 9 step: 1285, loss is 0.21045266091823578\n",
      "epoch: 9 step: 1286, loss is 0.14068128168582916\n",
      "epoch: 9 step: 1287, loss is 0.5772072672843933\n",
      "epoch: 9 step: 1288, loss is 0.06623769551515579\n",
      "epoch: 9 step: 1289, loss is 0.16979551315307617\n",
      "epoch: 9 step: 1290, loss is 0.155075803399086\n",
      "epoch: 9 step: 1291, loss is 0.05078257992863655\n",
      "epoch: 9 step: 1292, loss is 0.12058066576719284\n",
      "epoch: 9 step: 1293, loss is 0.21910710632801056\n",
      "epoch: 9 step: 1294, loss is 0.24053595960140228\n",
      "epoch: 9 step: 1295, loss is 0.21546916663646698\n",
      "epoch: 9 step: 1296, loss is 0.26749512553215027\n",
      "epoch: 9 step: 1297, loss is 0.040170520544052124\n",
      "epoch: 9 step: 1298, loss is 0.017959650605916977\n",
      "epoch: 9 step: 1299, loss is 0.24259346723556519\n",
      "epoch: 9 step: 1300, loss is 0.23263853788375854\n",
      "epoch: 9 step: 1301, loss is 0.12131213396787643\n",
      "epoch: 9 step: 1302, loss is 0.03421178087592125\n",
      "epoch: 9 step: 1303, loss is 0.03562513366341591\n",
      "epoch: 9 step: 1304, loss is 0.09970913827419281\n",
      "epoch: 9 step: 1305, loss is 0.31074392795562744\n",
      "epoch: 9 step: 1306, loss is 0.2852662205696106\n",
      "epoch: 9 step: 1307, loss is 0.13409247994422913\n",
      "epoch: 9 step: 1308, loss is 0.1421997845172882\n",
      "epoch: 9 step: 1309, loss is 0.0804557129740715\n",
      "epoch: 9 step: 1310, loss is 0.10798465460538864\n",
      "epoch: 9 step: 1311, loss is 0.08822423219680786\n",
      "epoch: 9 step: 1312, loss is 0.057747092097997665\n",
      "epoch: 9 step: 1313, loss is 0.19610263407230377\n",
      "epoch: 9 step: 1314, loss is 0.30612310767173767\n",
      "epoch: 9 step: 1315, loss is 0.4946601986885071\n",
      "epoch: 9 step: 1316, loss is 0.14618775248527527\n",
      "epoch: 9 step: 1317, loss is 0.4360513687133789\n",
      "epoch: 9 step: 1318, loss is 0.0705418810248375\n",
      "epoch: 9 step: 1319, loss is 0.1927642971277237\n",
      "epoch: 9 step: 1320, loss is 0.15696430206298828\n",
      "epoch: 9 step: 1321, loss is 0.28166675567626953\n",
      "epoch: 9 step: 1322, loss is 0.4573313891887665\n",
      "epoch: 9 step: 1323, loss is 0.3482419550418854\n",
      "epoch: 9 step: 1324, loss is 0.6432328224182129\n",
      "epoch: 9 step: 1325, loss is 0.011800973676145077\n",
      "epoch: 9 step: 1326, loss is 0.06350359320640564\n",
      "epoch: 9 step: 1327, loss is 0.613849937915802\n",
      "epoch: 9 step: 1328, loss is 0.02653786912560463\n",
      "epoch: 9 step: 1329, loss is 0.5191531181335449\n",
      "epoch: 9 step: 1330, loss is 0.027348889037966728\n",
      "epoch: 9 step: 1331, loss is 0.055370379239320755\n",
      "epoch: 9 step: 1332, loss is 0.2905305325984955\n",
      "epoch: 9 step: 1333, loss is 0.6786673069000244\n",
      "epoch: 9 step: 1334, loss is 0.04469570890069008\n",
      "epoch: 9 step: 1335, loss is 0.33999165892601013\n",
      "epoch: 9 step: 1336, loss is 0.5443015098571777\n",
      "epoch: 9 step: 1337, loss is 0.250974178314209\n",
      "epoch: 9 step: 1338, loss is 0.2238333374261856\n",
      "epoch: 9 step: 1339, loss is 0.36771050095558167\n",
      "epoch: 9 step: 1340, loss is 0.023327594622969627\n",
      "epoch: 9 step: 1341, loss is 0.1255127191543579\n",
      "epoch: 9 step: 1342, loss is 0.33990007638931274\n",
      "epoch: 9 step: 1343, loss is 0.39424359798431396\n",
      "epoch: 9 step: 1344, loss is 0.08592194318771362\n",
      "epoch: 9 step: 1345, loss is 0.18440183997154236\n",
      "epoch: 9 step: 1346, loss is 0.16305159032344818\n",
      "epoch: 9 step: 1347, loss is 0.07391858100891113\n",
      "epoch: 9 step: 1348, loss is 0.17450520396232605\n",
      "epoch: 9 step: 1349, loss is 0.2743067145347595\n",
      "epoch: 9 step: 1350, loss is 0.5264725089073181\n",
      "epoch: 9 step: 1351, loss is 0.013454164378345013\n",
      "epoch: 9 step: 1352, loss is 0.2288767695426941\n",
      "epoch: 9 step: 1353, loss is 0.1888258457183838\n",
      "epoch: 9 step: 1354, loss is 0.0663660317659378\n",
      "epoch: 9 step: 1355, loss is 0.2099660485982895\n",
      "epoch: 9 step: 1356, loss is 0.14816300570964813\n",
      "epoch: 9 step: 1357, loss is 0.1100892722606659\n",
      "epoch: 9 step: 1358, loss is 1.142015814781189\n",
      "epoch: 9 step: 1359, loss is 0.01927175559103489\n",
      "epoch: 9 step: 1360, loss is 0.13913488388061523\n",
      "epoch: 9 step: 1361, loss is 0.8415320515632629\n",
      "epoch: 9 step: 1362, loss is 0.3410719931125641\n",
      "epoch: 9 step: 1363, loss is 0.30211830139160156\n",
      "epoch: 9 step: 1364, loss is 0.07932912558317184\n",
      "epoch: 9 step: 1365, loss is 0.019742781296372414\n",
      "epoch: 9 step: 1366, loss is 0.11616986244916916\n",
      "epoch: 9 step: 1367, loss is 0.33456283807754517\n",
      "epoch: 9 step: 1368, loss is 0.14297787845134735\n",
      "epoch: 9 step: 1369, loss is 0.09485477209091187\n",
      "epoch: 9 step: 1370, loss is 0.21062630414962769\n",
      "epoch: 9 step: 1371, loss is 0.030054662376642227\n",
      "epoch: 9 step: 1372, loss is 0.08983401954174042\n",
      "epoch: 9 step: 1373, loss is 0.3180054724216461\n",
      "epoch: 9 step: 1374, loss is 0.23296117782592773\n",
      "epoch: 9 step: 1375, loss is 0.23119127750396729\n",
      "epoch: 9 step: 1376, loss is 0.22193746268749237\n",
      "epoch: 9 step: 1377, loss is 0.7592228055000305\n",
      "epoch: 9 step: 1378, loss is 0.20851030945777893\n",
      "epoch: 9 step: 1379, loss is 0.07166110724210739\n",
      "epoch: 9 step: 1380, loss is 0.10022206604480743\n",
      "epoch: 9 step: 1381, loss is 0.2083386331796646\n",
      "epoch: 9 step: 1382, loss is 0.29646095633506775\n",
      "epoch: 9 step: 1383, loss is 0.31509286165237427\n",
      "epoch: 9 step: 1384, loss is 0.2737940549850464\n",
      "epoch: 9 step: 1385, loss is 0.009074524976313114\n",
      "epoch: 9 step: 1386, loss is 0.17207059264183044\n",
      "epoch: 9 step: 1387, loss is 0.4473090171813965\n",
      "epoch: 9 step: 1388, loss is 0.4079793095588684\n",
      "epoch: 9 step: 1389, loss is 0.21055690944194794\n",
      "epoch: 9 step: 1390, loss is 0.3251190185546875\n",
      "epoch: 9 step: 1391, loss is 0.42873772978782654\n",
      "epoch: 9 step: 1392, loss is 0.25883278250694275\n",
      "epoch: 9 step: 1393, loss is 0.306766539812088\n",
      "epoch: 9 step: 1394, loss is 0.0801670029759407\n",
      "epoch: 9 step: 1395, loss is 0.31035688519477844\n",
      "epoch: 9 step: 1396, loss is 0.49603036046028137\n",
      "epoch: 9 step: 1397, loss is 0.28607234358787537\n",
      "epoch: 9 step: 1398, loss is 0.4095224440097809\n",
      "epoch: 9 step: 1399, loss is 0.10491562634706497\n",
      "epoch: 9 step: 1400, loss is 0.08214012533426285\n",
      "epoch: 9 step: 1401, loss is 0.38051652908325195\n",
      "epoch: 9 step: 1402, loss is 0.1497560739517212\n",
      "epoch: 9 step: 1403, loss is 0.03511900454759598\n",
      "epoch: 9 step: 1404, loss is 0.12205711752176285\n",
      "epoch: 9 step: 1405, loss is 0.20170743763446808\n",
      "epoch: 9 step: 1406, loss is 0.09371333569288254\n",
      "epoch: 9 step: 1407, loss is 0.05064372718334198\n",
      "epoch: 9 step: 1408, loss is 0.5496661067008972\n",
      "epoch: 9 step: 1409, loss is 0.11293315142393112\n",
      "epoch: 9 step: 1410, loss is 0.18037688732147217\n",
      "epoch: 9 step: 1411, loss is 0.06220279633998871\n",
      "epoch: 9 step: 1412, loss is 0.07263989746570587\n",
      "epoch: 9 step: 1413, loss is 0.07498817890882492\n",
      "epoch: 9 step: 1414, loss is 0.19453339278697968\n",
      "epoch: 9 step: 1415, loss is 0.0812203586101532\n",
      "epoch: 9 step: 1416, loss is 0.23061403632164001\n",
      "epoch: 9 step: 1417, loss is 0.22767041623592377\n",
      "epoch: 9 step: 1418, loss is 0.03208814933896065\n",
      "epoch: 9 step: 1419, loss is 0.01360184233635664\n",
      "epoch: 9 step: 1420, loss is 0.06744122505187988\n",
      "epoch: 9 step: 1421, loss is 0.10492482781410217\n",
      "epoch: 9 step: 1422, loss is 0.06138456240296364\n",
      "epoch: 9 step: 1423, loss is 0.025393260642886162\n",
      "epoch: 9 step: 1424, loss is 0.14493855834007263\n",
      "epoch: 9 step: 1425, loss is 0.026162398979067802\n",
      "epoch: 9 step: 1426, loss is 0.6723061800003052\n",
      "epoch: 9 step: 1427, loss is 0.05458024516701698\n",
      "epoch: 9 step: 1428, loss is 0.16249659657478333\n",
      "epoch: 9 step: 1429, loss is 0.1001284196972847\n",
      "epoch: 9 step: 1430, loss is 0.1369139552116394\n",
      "epoch: 9 step: 1431, loss is 0.30054494738578796\n",
      "epoch: 9 step: 1432, loss is 0.09885700047016144\n",
      "epoch: 9 step: 1433, loss is 0.3724099099636078\n",
      "epoch: 9 step: 1434, loss is 0.16628676652908325\n",
      "epoch: 9 step: 1435, loss is 0.03512590005993843\n",
      "epoch: 9 step: 1436, loss is 0.025661814957857132\n",
      "epoch: 9 step: 1437, loss is 0.4237247705459595\n",
      "epoch: 9 step: 1438, loss is 0.068377785384655\n",
      "epoch: 9 step: 1439, loss is 0.19221752882003784\n",
      "epoch: 9 step: 1440, loss is 0.4974384307861328\n",
      "epoch: 9 step: 1441, loss is 0.21150603890419006\n",
      "epoch: 9 step: 1442, loss is 0.13638995587825775\n",
      "epoch: 9 step: 1443, loss is 0.06380055099725723\n",
      "epoch: 9 step: 1444, loss is 0.09409074485301971\n",
      "epoch: 9 step: 1445, loss is 0.05157189816236496\n",
      "epoch: 9 step: 1446, loss is 0.18999852240085602\n",
      "epoch: 9 step: 1447, loss is 0.33976855874061584\n",
      "epoch: 9 step: 1448, loss is 0.13228081166744232\n",
      "epoch: 9 step: 1449, loss is 0.1846732497215271\n",
      "epoch: 9 step: 1450, loss is 0.5081989169120789\n",
      "epoch: 9 step: 1451, loss is 0.24407155811786652\n",
      "epoch: 9 step: 1452, loss is 0.14793135225772858\n",
      "epoch: 9 step: 1453, loss is 0.04905438423156738\n",
      "epoch: 9 step: 1454, loss is 0.10874230414628983\n",
      "epoch: 9 step: 1455, loss is 0.09111150354146957\n",
      "epoch: 9 step: 1456, loss is 0.2699032127857208\n",
      "epoch: 9 step: 1457, loss is 0.08537421375513077\n",
      "epoch: 9 step: 1458, loss is 0.6741228103637695\n",
      "epoch: 9 step: 1459, loss is 0.01481584832072258\n",
      "epoch: 9 step: 1460, loss is 0.3080579340457916\n",
      "epoch: 9 step: 1461, loss is 0.0529092401266098\n",
      "epoch: 9 step: 1462, loss is 0.11325131356716156\n",
      "epoch: 9 step: 1463, loss is 0.3990110754966736\n",
      "epoch: 9 step: 1464, loss is 0.2623792290687561\n",
      "epoch: 9 step: 1465, loss is 0.03175169602036476\n",
      "epoch: 9 step: 1466, loss is 0.026188964024186134\n",
      "epoch: 9 step: 1467, loss is 0.4496619701385498\n",
      "epoch: 9 step: 1468, loss is 0.10427548736333847\n",
      "epoch: 9 step: 1469, loss is 0.43106138706207275\n",
      "epoch: 9 step: 1470, loss is 0.1069989800453186\n",
      "epoch: 9 step: 1471, loss is 0.10487255454063416\n",
      "epoch: 9 step: 1472, loss is 0.20840126276016235\n",
      "epoch: 9 step: 1473, loss is 0.13904744386672974\n",
      "epoch: 9 step: 1474, loss is 0.150918111205101\n",
      "epoch: 9 step: 1475, loss is 0.12160860747098923\n",
      "epoch: 9 step: 1476, loss is 0.030565517023205757\n",
      "epoch: 9 step: 1477, loss is 0.062141869217157364\n",
      "epoch: 9 step: 1478, loss is 0.014185543172061443\n",
      "epoch: 9 step: 1479, loss is 0.23485617339611053\n",
      "epoch: 9 step: 1480, loss is 0.08265386521816254\n",
      "epoch: 9 step: 1481, loss is 0.15290746092796326\n",
      "epoch: 9 step: 1482, loss is 0.4006580412387848\n",
      "epoch: 9 step: 1483, loss is 0.014164417050778866\n",
      "epoch: 9 step: 1484, loss is 0.21652370691299438\n",
      "epoch: 9 step: 1485, loss is 0.38855811953544617\n",
      "epoch: 9 step: 1486, loss is 0.03615966811776161\n",
      "epoch: 9 step: 1487, loss is 0.7234194874763489\n",
      "epoch: 9 step: 1488, loss is 0.016758374869823456\n",
      "epoch: 9 step: 1489, loss is 0.11309720575809479\n",
      "epoch: 9 step: 1490, loss is 0.0648316740989685\n",
      "epoch: 9 step: 1491, loss is 0.11170376092195511\n",
      "epoch: 9 step: 1492, loss is 0.5515986680984497\n",
      "epoch: 9 step: 1493, loss is 0.25381964445114136\n",
      "epoch: 9 step: 1494, loss is 0.011564717628061771\n",
      "epoch: 9 step: 1495, loss is 0.2293826788663864\n",
      "epoch: 9 step: 1496, loss is 0.3738921880722046\n",
      "epoch: 9 step: 1497, loss is 0.2813478112220764\n",
      "epoch: 9 step: 1498, loss is 0.31548812985420227\n",
      "epoch: 9 step: 1499, loss is 0.23419003188610077\n",
      "epoch: 9 step: 1500, loss is 0.19681315124034882\n",
      "epoch: 9 step: 1501, loss is 0.16395843029022217\n",
      "epoch: 9 step: 1502, loss is 0.03918933495879173\n",
      "epoch: 9 step: 1503, loss is 0.3282536566257477\n",
      "epoch: 9 step: 1504, loss is 0.2582004964351654\n",
      "epoch: 9 step: 1505, loss is 0.011748967692255974\n",
      "epoch: 9 step: 1506, loss is 0.2125139832496643\n",
      "epoch: 9 step: 1507, loss is 0.27340367436408997\n",
      "epoch: 9 step: 1508, loss is 0.14103898406028748\n",
      "epoch: 9 step: 1509, loss is 0.07088357210159302\n",
      "epoch: 9 step: 1510, loss is 0.4821751117706299\n",
      "epoch: 9 step: 1511, loss is 0.33414122462272644\n",
      "epoch: 9 step: 1512, loss is 0.06777729839086533\n",
      "epoch: 9 step: 1513, loss is 0.24903029203414917\n",
      "epoch: 9 step: 1514, loss is 0.19697093963623047\n",
      "epoch: 9 step: 1515, loss is 0.3696992099285126\n",
      "epoch: 9 step: 1516, loss is 0.06992215663194656\n",
      "epoch: 9 step: 1517, loss is 0.14269669353961945\n",
      "epoch: 9 step: 1518, loss is 0.44351473450660706\n",
      "epoch: 9 step: 1519, loss is 0.3831069767475128\n",
      "epoch: 9 step: 1520, loss is 0.014853384345769882\n",
      "epoch: 9 step: 1521, loss is 0.6610103845596313\n",
      "epoch: 9 step: 1522, loss is 0.14453673362731934\n",
      "epoch: 9 step: 1523, loss is 0.20804762840270996\n",
      "epoch: 9 step: 1524, loss is 0.07165621221065521\n",
      "epoch: 9 step: 1525, loss is 0.5808101296424866\n",
      "epoch: 9 step: 1526, loss is 0.3184452950954437\n",
      "epoch: 9 step: 1527, loss is 0.3411407470703125\n",
      "epoch: 9 step: 1528, loss is 0.20048953592777252\n",
      "epoch: 9 step: 1529, loss is 0.5689910054206848\n",
      "epoch: 9 step: 1530, loss is 0.2273959070444107\n",
      "epoch: 9 step: 1531, loss is 0.0547824427485466\n",
      "epoch: 9 step: 1532, loss is 0.37744346261024475\n",
      "epoch: 9 step: 1533, loss is 0.20637668669223785\n",
      "epoch: 9 step: 1534, loss is 0.03974118083715439\n",
      "epoch: 9 step: 1535, loss is 0.08161292970180511\n",
      "epoch: 9 step: 1536, loss is 0.20400947332382202\n",
      "epoch: 9 step: 1537, loss is 0.11742206662893295\n",
      "epoch: 9 step: 1538, loss is 0.747877299785614\n",
      "epoch: 9 step: 1539, loss is 0.5322166085243225\n",
      "epoch: 9 step: 1540, loss is 0.06148975342512131\n",
      "epoch: 9 step: 1541, loss is 0.1328413188457489\n",
      "epoch: 9 step: 1542, loss is 0.5122374296188354\n",
      "epoch: 9 step: 1543, loss is 0.3633776605129242\n",
      "epoch: 9 step: 1544, loss is 0.49618470668792725\n",
      "epoch: 9 step: 1545, loss is 0.09585850685834885\n",
      "epoch: 9 step: 1546, loss is 0.06270915269851685\n",
      "epoch: 9 step: 1547, loss is 0.19551925361156464\n",
      "epoch: 9 step: 1548, loss is 0.0287200715392828\n",
      "epoch: 9 step: 1549, loss is 0.08267630636692047\n",
      "epoch: 9 step: 1550, loss is 0.11770037561655045\n",
      "epoch: 9 step: 1551, loss is 0.13423192501068115\n",
      "epoch: 9 step: 1552, loss is 0.46259835362434387\n",
      "epoch: 9 step: 1553, loss is 0.1962825357913971\n",
      "epoch: 9 step: 1554, loss is 0.1462307572364807\n",
      "epoch: 9 step: 1555, loss is 0.1603507101535797\n",
      "epoch: 9 step: 1556, loss is 0.1824445277452469\n",
      "epoch: 9 step: 1557, loss is 0.597183346748352\n",
      "epoch: 9 step: 1558, loss is 0.7662953734397888\n",
      "epoch: 9 step: 1559, loss is 0.11271027475595474\n",
      "epoch: 9 step: 1560, loss is 0.2970893383026123\n",
      "epoch: 9 step: 1561, loss is 0.017342565581202507\n",
      "epoch: 9 step: 1562, loss is 0.060736704617738724\n",
      "epoch: 9 step: 1563, loss is 0.0798267051577568\n",
      "epoch: 9 step: 1564, loss is 0.3098216652870178\n",
      "epoch: 9 step: 1565, loss is 0.06510692834854126\n",
      "epoch: 9 step: 1566, loss is 0.08811786025762558\n",
      "epoch: 9 step: 1567, loss is 0.18980999290943146\n",
      "epoch: 9 step: 1568, loss is 0.19936779141426086\n",
      "epoch: 9 step: 1569, loss is 0.23645497858524323\n",
      "epoch: 9 step: 1570, loss is 0.2492379993200302\n",
      "epoch: 9 step: 1571, loss is 0.362337589263916\n",
      "epoch: 9 step: 1572, loss is 0.2040606290102005\n",
      "epoch: 9 step: 1573, loss is 0.03730519115924835\n",
      "epoch: 9 step: 1574, loss is 0.06482367217540741\n",
      "epoch: 9 step: 1575, loss is 0.28297296166419983\n",
      "epoch: 9 step: 1576, loss is 0.3432237207889557\n",
      "epoch: 9 step: 1577, loss is 0.10259737819433212\n",
      "epoch: 9 step: 1578, loss is 0.2634935677051544\n",
      "epoch: 9 step: 1579, loss is 0.468672513961792\n",
      "epoch: 9 step: 1580, loss is 0.6561028957366943\n",
      "epoch: 9 step: 1581, loss is 0.11552202701568604\n",
      "epoch: 9 step: 1582, loss is 0.175495907664299\n",
      "epoch: 9 step: 1583, loss is 0.2677621841430664\n",
      "epoch: 9 step: 1584, loss is 0.4995083510875702\n",
      "epoch: 9 step: 1585, loss is 0.19895482063293457\n",
      "epoch: 9 step: 1586, loss is 0.7777502536773682\n",
      "epoch: 9 step: 1587, loss is 0.3856491446495056\n",
      "epoch: 9 step: 1588, loss is 0.08828425407409668\n",
      "epoch: 9 step: 1589, loss is 0.120877206325531\n",
      "epoch: 9 step: 1590, loss is 0.2967226207256317\n",
      "epoch: 9 step: 1591, loss is 0.09734290093183517\n",
      "epoch: 9 step: 1592, loss is 0.14158836007118225\n",
      "epoch: 9 step: 1593, loss is 0.29102498292922974\n",
      "epoch: 9 step: 1594, loss is 0.02124038338661194\n",
      "epoch: 9 step: 1595, loss is 0.2760106027126312\n",
      "epoch: 9 step: 1596, loss is 0.15530097484588623\n",
      "epoch: 9 step: 1597, loss is 0.12420361489057541\n",
      "epoch: 9 step: 1598, loss is 0.22065359354019165\n",
      "epoch: 9 step: 1599, loss is 0.18356063961982727\n",
      "epoch: 9 step: 1600, loss is 0.035513728857040405\n",
      "epoch: 9 step: 1601, loss is 0.18233376741409302\n",
      "epoch: 9 step: 1602, loss is 0.04205961152911186\n",
      "epoch: 9 step: 1603, loss is 0.07497473061084747\n",
      "epoch: 9 step: 1604, loss is 0.12440775334835052\n",
      "epoch: 9 step: 1605, loss is 0.18924036622047424\n",
      "epoch: 9 step: 1606, loss is 0.03114074282348156\n",
      "epoch: 9 step: 1607, loss is 0.07585147023200989\n",
      "epoch: 9 step: 1608, loss is 0.48666760325431824\n",
      "epoch: 9 step: 1609, loss is 0.2295421063899994\n",
      "epoch: 9 step: 1610, loss is 0.02283535525202751\n",
      "epoch: 9 step: 1611, loss is 0.07531341910362244\n",
      "epoch: 9 step: 1612, loss is 0.06985032558441162\n",
      "epoch: 9 step: 1613, loss is 0.06594183295965195\n",
      "epoch: 9 step: 1614, loss is 0.028020143508911133\n",
      "epoch: 9 step: 1615, loss is 0.512501060962677\n",
      "epoch: 9 step: 1616, loss is 0.47929105162620544\n",
      "epoch: 9 step: 1617, loss is 0.761040210723877\n",
      "epoch: 9 step: 1618, loss is 0.11920362710952759\n",
      "epoch: 9 step: 1619, loss is 0.08649306744337082\n",
      "epoch: 9 step: 1620, loss is 0.13561473786830902\n",
      "epoch: 9 step: 1621, loss is 0.6550313234329224\n",
      "epoch: 9 step: 1622, loss is 0.40447208285331726\n",
      "epoch: 9 step: 1623, loss is 0.5794683694839478\n",
      "epoch: 9 step: 1624, loss is 0.012472078204154968\n",
      "epoch: 9 step: 1625, loss is 0.06996698677539825\n",
      "epoch: 9 step: 1626, loss is 0.2690941095352173\n",
      "epoch: 9 step: 1627, loss is 0.35073772072792053\n",
      "epoch: 9 step: 1628, loss is 0.3531940281391144\n",
      "epoch: 9 step: 1629, loss is 0.091530442237854\n",
      "epoch: 9 step: 1630, loss is 0.17280903458595276\n",
      "epoch: 9 step: 1631, loss is 0.19116446375846863\n",
      "epoch: 9 step: 1632, loss is 0.07317285239696503\n",
      "epoch: 9 step: 1633, loss is 0.22036920487880707\n",
      "epoch: 9 step: 1634, loss is 0.03906235098838806\n",
      "epoch: 9 step: 1635, loss is 0.31108999252319336\n",
      "epoch: 9 step: 1636, loss is 0.17892679572105408\n",
      "epoch: 9 step: 1637, loss is 0.05615707486867905\n",
      "epoch: 9 step: 1638, loss is 0.4063158333301544\n",
      "epoch: 9 step: 1639, loss is 0.11181916296482086\n",
      "epoch: 9 step: 1640, loss is 0.4024515748023987\n",
      "epoch: 9 step: 1641, loss is 0.2144029140472412\n",
      "epoch: 9 step: 1642, loss is 0.0935438796877861\n",
      "epoch: 9 step: 1643, loss is 0.14880266785621643\n",
      "epoch: 9 step: 1644, loss is 0.11012948304414749\n",
      "epoch: 9 step: 1645, loss is 0.6305868625640869\n",
      "epoch: 9 step: 1646, loss is 0.0901416540145874\n",
      "epoch: 9 step: 1647, loss is 0.2257819026708603\n",
      "epoch: 9 step: 1648, loss is 0.17338068783283234\n",
      "epoch: 9 step: 1649, loss is 0.09530960768461227\n",
      "epoch: 9 step: 1650, loss is 0.12269168347120285\n",
      "epoch: 9 step: 1651, loss is 0.19818703830242157\n",
      "epoch: 9 step: 1652, loss is 0.2554227113723755\n",
      "epoch: 9 step: 1653, loss is 0.27865070104599\n",
      "epoch: 9 step: 1654, loss is 0.45494788885116577\n",
      "epoch: 9 step: 1655, loss is 0.03149167820811272\n",
      "epoch: 9 step: 1656, loss is 0.11721640825271606\n",
      "epoch: 9 step: 1657, loss is 0.21265244483947754\n",
      "epoch: 9 step: 1658, loss is 0.3844601809978485\n",
      "epoch: 9 step: 1659, loss is 0.057396918535232544\n",
      "epoch: 9 step: 1660, loss is 0.6512124538421631\n",
      "epoch: 9 step: 1661, loss is 0.07370401918888092\n",
      "epoch: 9 step: 1662, loss is 0.23305940628051758\n",
      "epoch: 9 step: 1663, loss is 0.055919598788022995\n",
      "epoch: 9 step: 1664, loss is 0.07112623006105423\n",
      "epoch: 9 step: 1665, loss is 0.19204050302505493\n",
      "epoch: 9 step: 1666, loss is 0.08796826004981995\n",
      "epoch: 9 step: 1667, loss is 0.24689769744873047\n",
      "epoch: 9 step: 1668, loss is 0.6421840190887451\n",
      "epoch: 9 step: 1669, loss is 0.5289396047592163\n",
      "epoch: 9 step: 1670, loss is 0.0505388043820858\n",
      "epoch: 9 step: 1671, loss is 0.34978634119033813\n",
      "epoch: 9 step: 1672, loss is 0.17205823957920074\n",
      "epoch: 9 step: 1673, loss is 0.0976143404841423\n",
      "epoch: 9 step: 1674, loss is 0.23628732562065125\n",
      "epoch: 9 step: 1675, loss is 0.16141377389431\n",
      "epoch: 9 step: 1676, loss is 0.14811748266220093\n",
      "epoch: 9 step: 1677, loss is 0.38703808188438416\n",
      "epoch: 9 step: 1678, loss is 0.1592804491519928\n",
      "epoch: 9 step: 1679, loss is 0.2992565929889679\n",
      "epoch: 9 step: 1680, loss is 0.6447925567626953\n",
      "epoch: 9 step: 1681, loss is 0.09172181040048599\n",
      "epoch: 9 step: 1682, loss is 0.1496487259864807\n",
      "epoch: 9 step: 1683, loss is 0.09321248531341553\n",
      "epoch: 9 step: 1684, loss is 1.1164517402648926\n",
      "epoch: 9 step: 1685, loss is 0.5132734179496765\n",
      "epoch: 9 step: 1686, loss is 0.5683019161224365\n",
      "epoch: 9 step: 1687, loss is 0.1748133897781372\n",
      "epoch: 9 step: 1688, loss is 0.3242399990558624\n",
      "epoch: 9 step: 1689, loss is 0.1949433535337448\n",
      "epoch: 9 step: 1690, loss is 0.2004825621843338\n",
      "epoch: 9 step: 1691, loss is 0.3752213418483734\n",
      "epoch: 9 step: 1692, loss is 0.1648734211921692\n",
      "epoch: 9 step: 1693, loss is 0.5439920425415039\n",
      "epoch: 9 step: 1694, loss is 0.5928422808647156\n",
      "epoch: 9 step: 1695, loss is 0.11441764235496521\n",
      "epoch: 9 step: 1696, loss is 0.6851953268051147\n",
      "epoch: 9 step: 1697, loss is 0.44544297456741333\n",
      "epoch: 9 step: 1698, loss is 0.14375682175159454\n",
      "epoch: 9 step: 1699, loss is 0.1731387823820114\n",
      "epoch: 9 step: 1700, loss is 0.11335761100053787\n",
      "epoch: 9 step: 1701, loss is 0.1641334444284439\n",
      "epoch: 9 step: 1702, loss is 0.07118445634841919\n",
      "epoch: 9 step: 1703, loss is 0.05528555065393448\n",
      "epoch: 9 step: 1704, loss is 0.3578187823295593\n",
      "epoch: 9 step: 1705, loss is 0.38631048798561096\n",
      "epoch: 9 step: 1706, loss is 0.4345530867576599\n",
      "epoch: 9 step: 1707, loss is 0.2960018515586853\n",
      "epoch: 9 step: 1708, loss is 0.3389551341533661\n",
      "epoch: 9 step: 1709, loss is 0.09414266794919968\n",
      "epoch: 9 step: 1710, loss is 0.30848070979118347\n",
      "epoch: 9 step: 1711, loss is 0.2536541223526001\n",
      "epoch: 9 step: 1712, loss is 0.2610633671283722\n",
      "epoch: 9 step: 1713, loss is 0.12254711240530014\n",
      "epoch: 9 step: 1714, loss is 0.15492738783359528\n",
      "epoch: 9 step: 1715, loss is 0.4617217183113098\n",
      "epoch: 9 step: 1716, loss is 0.4076734185218811\n",
      "epoch: 9 step: 1717, loss is 0.3546375632286072\n",
      "epoch: 9 step: 1718, loss is 0.31525325775146484\n",
      "epoch: 9 step: 1719, loss is 0.4218890070915222\n",
      "epoch: 9 step: 1720, loss is 0.0776442438364029\n",
      "epoch: 9 step: 1721, loss is 0.0852840393781662\n",
      "epoch: 9 step: 1722, loss is 0.2392311990261078\n",
      "epoch: 9 step: 1723, loss is 0.25517597794532776\n",
      "epoch: 9 step: 1724, loss is 0.20450593531131744\n",
      "epoch: 9 step: 1725, loss is 0.0697251707315445\n",
      "epoch: 9 step: 1726, loss is 0.019268224015831947\n",
      "epoch: 9 step: 1727, loss is 0.09072533249855042\n",
      "epoch: 9 step: 1728, loss is 0.3360224664211273\n",
      "epoch: 9 step: 1729, loss is 0.16974349319934845\n",
      "epoch: 9 step: 1730, loss is 0.5037438869476318\n",
      "epoch: 9 step: 1731, loss is 0.04250272363424301\n",
      "epoch: 9 step: 1732, loss is 0.6551132798194885\n",
      "epoch: 9 step: 1733, loss is 0.0933661237359047\n",
      "epoch: 9 step: 1734, loss is 0.19660280644893646\n",
      "epoch: 9 step: 1735, loss is 0.07800876349210739\n",
      "epoch: 9 step: 1736, loss is 0.20438836514949799\n",
      "epoch: 9 step: 1737, loss is 0.17885811626911163\n",
      "epoch: 9 step: 1738, loss is 0.1428452879190445\n",
      "epoch: 9 step: 1739, loss is 0.6734335422515869\n",
      "epoch: 9 step: 1740, loss is 0.11390500515699387\n",
      "epoch: 9 step: 1741, loss is 0.34208551049232483\n",
      "epoch: 9 step: 1742, loss is 0.052271537482738495\n",
      "epoch: 9 step: 1743, loss is 0.2646154761314392\n",
      "epoch: 9 step: 1744, loss is 0.21628107130527496\n",
      "epoch: 9 step: 1745, loss is 0.16245824098587036\n",
      "epoch: 9 step: 1746, loss is 0.17625418305397034\n",
      "epoch: 9 step: 1747, loss is 0.4535428583621979\n",
      "epoch: 9 step: 1748, loss is 0.38368070125579834\n",
      "epoch: 9 step: 1749, loss is 0.04862353578209877\n",
      "epoch: 9 step: 1750, loss is 0.33643093705177307\n",
      "epoch: 9 step: 1751, loss is 0.4982653558254242\n",
      "epoch: 9 step: 1752, loss is 0.2489578276872635\n",
      "epoch: 9 step: 1753, loss is 0.16495294868946075\n",
      "epoch: 9 step: 1754, loss is 0.10508684068918228\n",
      "epoch: 9 step: 1755, loss is 0.5590950846672058\n",
      "epoch: 9 step: 1756, loss is 0.24182789027690887\n",
      "epoch: 9 step: 1757, loss is 0.42526331543922424\n",
      "epoch: 9 step: 1758, loss is 0.30350974202156067\n",
      "epoch: 9 step: 1759, loss is 0.1429012566804886\n",
      "epoch: 9 step: 1760, loss is 0.5647334456443787\n",
      "epoch: 9 step: 1761, loss is 0.039298638701438904\n",
      "epoch: 9 step: 1762, loss is 0.06177029013633728\n",
      "epoch: 9 step: 1763, loss is 0.29545778036117554\n",
      "epoch: 9 step: 1764, loss is 0.44620248675346375\n",
      "epoch: 9 step: 1765, loss is 0.07705538719892502\n",
      "epoch: 9 step: 1766, loss is 0.10173653066158295\n",
      "epoch: 9 step: 1767, loss is 0.009222292341291904\n",
      "epoch: 9 step: 1768, loss is 0.4789488911628723\n",
      "epoch: 9 step: 1769, loss is 0.04601791873574257\n",
      "epoch: 9 step: 1770, loss is 0.2627815008163452\n",
      "epoch: 9 step: 1771, loss is 0.056889608502388\n",
      "epoch: 9 step: 1772, loss is 0.08813613653182983\n",
      "epoch: 9 step: 1773, loss is 0.028829503804445267\n",
      "epoch: 9 step: 1774, loss is 0.08739034086465836\n",
      "epoch: 9 step: 1775, loss is 0.022034011781215668\n",
      "epoch: 9 step: 1776, loss is 0.5574116110801697\n",
      "epoch: 9 step: 1777, loss is 0.06589966267347336\n",
      "epoch: 9 step: 1778, loss is 0.27358752489089966\n",
      "epoch: 9 step: 1779, loss is 0.5554161071777344\n",
      "epoch: 9 step: 1780, loss is 0.11517492681741714\n",
      "epoch: 9 step: 1781, loss is 0.3211636245250702\n",
      "epoch: 9 step: 1782, loss is 0.2096981704235077\n",
      "epoch: 9 step: 1783, loss is 0.0639549046754837\n",
      "epoch: 9 step: 1784, loss is 0.06999886780977249\n",
      "epoch: 9 step: 1785, loss is 0.1315767765045166\n",
      "epoch: 9 step: 1786, loss is 0.9816519021987915\n",
      "epoch: 9 step: 1787, loss is 0.0857338234782219\n",
      "epoch: 9 step: 1788, loss is 0.08349694311618805\n",
      "epoch: 9 step: 1789, loss is 0.6368381381034851\n",
      "epoch: 9 step: 1790, loss is 0.22875115275382996\n",
      "epoch: 9 step: 1791, loss is 0.014313015155494213\n",
      "epoch: 9 step: 1792, loss is 0.019427206367254257\n",
      "epoch: 9 step: 1793, loss is 0.6931993961334229\n",
      "epoch: 9 step: 1794, loss is 0.19132164120674133\n",
      "epoch: 9 step: 1795, loss is 0.4773532748222351\n",
      "epoch: 9 step: 1796, loss is 0.051956504583358765\n",
      "epoch: 9 step: 1797, loss is 0.10234973579645157\n",
      "epoch: 9 step: 1798, loss is 0.2596651017665863\n",
      "epoch: 9 step: 1799, loss is 0.16206127405166626\n",
      "epoch: 9 step: 1800, loss is 0.10707134753465652\n",
      "epoch: 9 step: 1801, loss is 0.25597888231277466\n",
      "epoch: 9 step: 1802, loss is 0.29846861958503723\n",
      "epoch: 9 step: 1803, loss is 0.06127575412392616\n",
      "epoch: 9 step: 1804, loss is 0.13313136994838715\n",
      "epoch: 9 step: 1805, loss is 0.2341369241476059\n",
      "epoch: 9 step: 1806, loss is 0.5792549848556519\n",
      "epoch: 9 step: 1807, loss is 0.13870109617710114\n",
      "epoch: 9 step: 1808, loss is 0.5029190182685852\n",
      "epoch: 9 step: 1809, loss is 0.18675601482391357\n",
      "epoch: 9 step: 1810, loss is 0.11737661063671112\n",
      "epoch: 9 step: 1811, loss is 0.4163609445095062\n",
      "epoch: 9 step: 1812, loss is 0.301069438457489\n",
      "epoch: 9 step: 1813, loss is 0.08459754288196564\n",
      "epoch: 9 step: 1814, loss is 0.6698605418205261\n",
      "epoch: 9 step: 1815, loss is 0.09581223875284195\n",
      "epoch: 9 step: 1816, loss is 0.48301026225090027\n",
      "epoch: 9 step: 1817, loss is 0.5231052041053772\n",
      "epoch: 9 step: 1818, loss is 0.20726367831230164\n",
      "epoch: 9 step: 1819, loss is 0.13717156648635864\n",
      "epoch: 9 step: 1820, loss is 0.43764322996139526\n",
      "epoch: 9 step: 1821, loss is 0.5197364091873169\n",
      "epoch: 9 step: 1822, loss is 0.1561395525932312\n",
      "epoch: 9 step: 1823, loss is 0.38459256291389465\n",
      "epoch: 9 step: 1824, loss is 0.11810316890478134\n",
      "epoch: 9 step: 1825, loss is 0.11130490899085999\n",
      "epoch: 9 step: 1826, loss is 0.12276501208543777\n",
      "epoch: 9 step: 1827, loss is 0.07338330149650574\n",
      "epoch: 9 step: 1828, loss is 0.25352567434310913\n",
      "epoch: 9 step: 1829, loss is 0.26195085048675537\n",
      "epoch: 9 step: 1830, loss is 0.1505143940448761\n",
      "epoch: 9 step: 1831, loss is 0.1025259792804718\n",
      "epoch: 9 step: 1832, loss is 0.1294199526309967\n",
      "epoch: 9 step: 1833, loss is 0.07978794723749161\n",
      "epoch: 9 step: 1834, loss is 0.24431659281253815\n",
      "epoch: 9 step: 1835, loss is 0.10097044706344604\n",
      "epoch: 9 step: 1836, loss is 0.06294620782136917\n",
      "epoch: 9 step: 1837, loss is 0.38576000928878784\n",
      "epoch: 9 step: 1838, loss is 0.3196975290775299\n",
      "epoch: 9 step: 1839, loss is 0.003590365406125784\n",
      "epoch: 9 step: 1840, loss is 0.06980524212121964\n",
      "epoch: 9 step: 1841, loss is 0.29263561964035034\n",
      "epoch: 9 step: 1842, loss is 0.19640785455703735\n",
      "epoch: 9 step: 1843, loss is 0.016774140298366547\n",
      "epoch: 9 step: 1844, loss is 0.7422323822975159\n",
      "epoch: 9 step: 1845, loss is 0.08605386316776276\n",
      "epoch: 9 step: 1846, loss is 0.2317819744348526\n",
      "epoch: 9 step: 1847, loss is 0.2067769467830658\n",
      "epoch: 9 step: 1848, loss is 0.014097834937274456\n",
      "epoch: 9 step: 1849, loss is 0.007827963680028915\n",
      "epoch: 9 step: 1850, loss is 0.082808718085289\n",
      "epoch: 9 step: 1851, loss is 0.10190839320421219\n",
      "epoch: 9 step: 1852, loss is 0.08467887341976166\n",
      "epoch: 9 step: 1853, loss is 0.3004961907863617\n",
      "epoch: 9 step: 1854, loss is 0.15608341991901398\n",
      "epoch: 9 step: 1855, loss is 0.08835987001657486\n",
      "epoch: 9 step: 1856, loss is 0.06135174259543419\n",
      "epoch: 9 step: 1857, loss is 0.18292616307735443\n",
      "epoch: 9 step: 1858, loss is 0.4437030553817749\n",
      "epoch: 9 step: 1859, loss is 0.03929854929447174\n",
      "epoch: 9 step: 1860, loss is 0.06383344531059265\n",
      "epoch: 9 step: 1861, loss is 0.11752506345510483\n",
      "epoch: 9 step: 1862, loss is 0.5969533920288086\n",
      "epoch: 9 step: 1863, loss is 0.19406400620937347\n",
      "epoch: 9 step: 1864, loss is 0.04753590747714043\n",
      "epoch: 9 step: 1865, loss is 0.07106435298919678\n",
      "epoch: 9 step: 1866, loss is 0.03584520146250725\n",
      "epoch: 9 step: 1867, loss is 0.09908096492290497\n",
      "epoch: 9 step: 1868, loss is 0.2584910988807678\n",
      "epoch: 9 step: 1869, loss is 0.5433323383331299\n",
      "epoch: 9 step: 1870, loss is 0.3646802604198456\n",
      "epoch: 9 step: 1871, loss is 0.120309017598629\n",
      "epoch: 9 step: 1872, loss is 0.228716641664505\n",
      "epoch: 9 step: 1873, loss is 0.3906526267528534\n",
      "epoch: 9 step: 1874, loss is 0.29756009578704834\n",
      "epoch: 9 step: 1875, loss is 0.10839205980300903\n",
      "epoch: 9 step: 1876, loss is 0.26704028248786926\n",
      "epoch: 9 step: 1877, loss is 0.17445886135101318\n",
      "epoch: 9 step: 1878, loss is 0.11926001310348511\n",
      "epoch: 9 step: 1879, loss is 0.09261010587215424\n",
      "epoch: 9 step: 1880, loss is 0.249985471367836\n",
      "epoch: 9 step: 1881, loss is 0.3053325414657593\n",
      "epoch: 9 step: 1882, loss is 0.24471348524093628\n",
      "epoch: 9 step: 1883, loss is 0.062437355518341064\n",
      "epoch: 9 step: 1884, loss is 0.30241671204566956\n",
      "epoch: 9 step: 1885, loss is 0.3605968952178955\n",
      "epoch: 9 step: 1886, loss is 0.06825395673513412\n",
      "epoch: 9 step: 1887, loss is 0.18555514514446259\n",
      "epoch: 9 step: 1888, loss is 0.11404076218605042\n",
      "epoch: 9 step: 1889, loss is 0.046846505254507065\n",
      "epoch: 9 step: 1890, loss is 0.09755208343267441\n",
      "epoch: 9 step: 1891, loss is 0.17430424690246582\n",
      "epoch: 9 step: 1892, loss is 0.16165392100811005\n",
      "epoch: 9 step: 1893, loss is 0.06802628934383392\n",
      "epoch: 9 step: 1894, loss is 0.9939606189727783\n",
      "epoch: 9 step: 1895, loss is 0.2932176887989044\n",
      "epoch: 9 step: 1896, loss is 0.12225835025310516\n",
      "epoch: 9 step: 1897, loss is 0.25021785497665405\n",
      "epoch: 9 step: 1898, loss is 0.04521957412362099\n",
      "epoch: 9 step: 1899, loss is 0.32528090476989746\n",
      "epoch: 9 step: 1900, loss is 0.22027423977851868\n",
      "epoch: 9 step: 1901, loss is 0.04999326542019844\n",
      "epoch: 9 step: 1902, loss is 0.24036496877670288\n",
      "epoch: 9 step: 1903, loss is 0.20909127593040466\n",
      "epoch: 9 step: 1904, loss is 0.22847793996334076\n",
      "epoch: 9 step: 1905, loss is 0.04159173369407654\n",
      "epoch: 9 step: 1906, loss is 0.055129002779722214\n",
      "epoch: 9 step: 1907, loss is 0.11944276094436646\n",
      "epoch: 9 step: 1908, loss is 0.37064453959465027\n",
      "epoch: 9 step: 1909, loss is 0.10396423935890198\n",
      "epoch: 9 step: 1910, loss is 0.04265821352601051\n",
      "epoch: 9 step: 1911, loss is 0.0032332350965589285\n",
      "epoch: 9 step: 1912, loss is 0.12831462919712067\n",
      "epoch: 9 step: 1913, loss is 0.08592101186513901\n",
      "epoch: 9 step: 1914, loss is 0.005099225323647261\n",
      "epoch: 9 step: 1915, loss is 0.5119598507881165\n",
      "epoch: 9 step: 1916, loss is 0.3985722064971924\n",
      "epoch: 9 step: 1917, loss is 0.17163340747356415\n",
      "epoch: 9 step: 1918, loss is 0.1403747946023941\n",
      "epoch: 9 step: 1919, loss is 0.04659929499030113\n",
      "epoch: 9 step: 1920, loss is 0.10581966489553452\n",
      "epoch: 9 step: 1921, loss is 0.025774404406547546\n",
      "epoch: 9 step: 1922, loss is 0.391791433095932\n",
      "epoch: 9 step: 1923, loss is 0.16556428372859955\n",
      "epoch: 9 step: 1924, loss is 0.034942712634801865\n",
      "epoch: 9 step: 1925, loss is 0.23874256014823914\n",
      "epoch: 9 step: 1926, loss is 0.08370057493448257\n",
      "epoch: 9 step: 1927, loss is 0.3733396828174591\n",
      "epoch: 9 step: 1928, loss is 0.03724576532840729\n",
      "epoch: 9 step: 1929, loss is 0.10248249769210815\n",
      "epoch: 9 step: 1930, loss is 0.14951279759407043\n",
      "epoch: 9 step: 1931, loss is 0.2038969099521637\n",
      "epoch: 9 step: 1932, loss is 0.1299779713153839\n",
      "epoch: 9 step: 1933, loss is 0.035748191177845\n",
      "epoch: 9 step: 1934, loss is 0.17245011031627655\n",
      "epoch: 9 step: 1935, loss is 0.20593805611133575\n",
      "epoch: 9 step: 1936, loss is 0.24002379179000854\n",
      "epoch: 9 step: 1937, loss is 0.29776057600975037\n",
      "epoch: 9 step: 1938, loss is 0.3758786916732788\n",
      "epoch: 9 step: 1939, loss is 0.1452564150094986\n",
      "epoch: 9 step: 1940, loss is 0.12538915872573853\n",
      "epoch: 9 step: 1941, loss is 0.7715433239936829\n",
      "epoch: 9 step: 1942, loss is 0.2547268569469452\n",
      "epoch: 9 step: 1943, loss is 0.05472055822610855\n",
      "epoch: 9 step: 1944, loss is 0.13361679017543793\n",
      "epoch: 9 step: 1945, loss is 0.3009977340698242\n",
      "epoch: 9 step: 1946, loss is 0.14715446531772614\n",
      "epoch: 9 step: 1947, loss is 0.2399514764547348\n",
      "epoch: 9 step: 1948, loss is 0.05413335561752319\n",
      "epoch: 9 step: 1949, loss is 0.1898968368768692\n",
      "epoch: 9 step: 1950, loss is 0.11272089183330536\n",
      "epoch: 9 step: 1951, loss is 0.07013515383005142\n",
      "epoch: 9 step: 1952, loss is 0.11638019233942032\n",
      "epoch: 9 step: 1953, loss is 0.19898004829883575\n",
      "epoch: 9 step: 1954, loss is 0.40704891085624695\n",
      "epoch: 9 step: 1955, loss is 0.2288323938846588\n",
      "epoch: 9 step: 1956, loss is 0.09014038741588593\n",
      "epoch: 9 step: 1957, loss is 0.11131267994642258\n",
      "epoch: 9 step: 1958, loss is 0.7575562000274658\n",
      "epoch: 9 step: 1959, loss is 0.3261718153953552\n",
      "epoch: 9 step: 1960, loss is 0.2558298707008362\n",
      "epoch: 9 step: 1961, loss is 0.046407949179410934\n",
      "epoch: 9 step: 1962, loss is 0.12227985262870789\n",
      "epoch: 9 step: 1963, loss is 0.3093676269054413\n",
      "epoch: 9 step: 1964, loss is 0.10021043568849564\n",
      "epoch: 9 step: 1965, loss is 0.1964064985513687\n",
      "epoch: 9 step: 1966, loss is 0.050909411162137985\n",
      "epoch: 9 step: 1967, loss is 0.0733918771147728\n",
      "epoch: 9 step: 1968, loss is 0.36659950017929077\n",
      "epoch: 9 step: 1969, loss is 0.0758880227804184\n",
      "epoch: 9 step: 1970, loss is 0.01951700635254383\n",
      "epoch: 9 step: 1971, loss is 0.034395504742860794\n",
      "epoch: 9 step: 1972, loss is 0.4903362989425659\n",
      "epoch: 9 step: 1973, loss is 0.08327093720436096\n",
      "epoch: 9 step: 1974, loss is 0.17940451204776764\n",
      "epoch: 9 step: 1975, loss is 0.2747030258178711\n",
      "epoch: 9 step: 1976, loss is 0.06926088035106659\n",
      "epoch: 9 step: 1977, loss is 0.05084942281246185\n",
      "epoch: 9 step: 1978, loss is 0.36973243951797485\n",
      "epoch: 9 step: 1979, loss is 0.059280674904584885\n",
      "epoch: 9 step: 1980, loss is 0.7305322885513306\n",
      "epoch: 9 step: 1981, loss is 0.5053214430809021\n",
      "epoch: 9 step: 1982, loss is 0.18467575311660767\n",
      "epoch: 9 step: 1983, loss is 0.19365334510803223\n",
      "epoch: 9 step: 1984, loss is 0.23511916399002075\n",
      "epoch: 9 step: 1985, loss is 0.11725238710641861\n",
      "epoch: 9 step: 1986, loss is 0.12964753806591034\n",
      "epoch: 9 step: 1987, loss is 0.02739831805229187\n",
      "epoch: 9 step: 1988, loss is 0.1532752811908722\n",
      "epoch: 9 step: 1989, loss is 0.5984339118003845\n",
      "epoch: 9 step: 1990, loss is 0.08483246713876724\n",
      "epoch: 9 step: 1991, loss is 0.4314415156841278\n",
      "epoch: 9 step: 1992, loss is 0.058705709874629974\n",
      "epoch: 9 step: 1993, loss is 0.17538119852542877\n",
      "epoch: 9 step: 1994, loss is 0.04884278401732445\n",
      "epoch: 9 step: 1995, loss is 0.5231403708457947\n",
      "epoch: 9 step: 1996, loss is 0.3063516914844513\n",
      "epoch: 9 step: 1997, loss is 0.40833011269569397\n",
      "epoch: 9 step: 1998, loss is 0.04747341573238373\n",
      "epoch: 9 step: 1999, loss is 0.3257358968257904\n",
      "epoch: 9 step: 2000, loss is 0.34096962213516235\n",
      "epoch: 9 step: 2001, loss is 0.08808723092079163\n",
      "epoch: 9 step: 2002, loss is 0.40704599022865295\n",
      "epoch: 9 step: 2003, loss is 0.2610751986503601\n",
      "epoch: 9 step: 2004, loss is 0.05942388251423836\n",
      "epoch: 9 step: 2005, loss is 0.12485931068658829\n",
      "epoch: 9 step: 2006, loss is 0.08445791900157928\n",
      "epoch: 9 step: 2007, loss is 0.36852121353149414\n",
      "epoch: 9 step: 2008, loss is 0.03182913735508919\n",
      "epoch: 9 step: 2009, loss is 0.2437630444765091\n",
      "epoch: 9 step: 2010, loss is 0.45562058687210083\n",
      "epoch: 9 step: 2011, loss is 0.03117738477885723\n",
      "epoch: 9 step: 2012, loss is 0.74330735206604\n",
      "epoch: 9 step: 2013, loss is 0.18421442806720734\n",
      "epoch: 9 step: 2014, loss is 0.025566931813955307\n",
      "epoch: 9 step: 2015, loss is 0.0749797448515892\n",
      "epoch: 9 step: 2016, loss is 0.07905600965023041\n",
      "epoch: 9 step: 2017, loss is 0.0664268359541893\n",
      "epoch: 9 step: 2018, loss is 0.197034552693367\n",
      "epoch: 9 step: 2019, loss is 0.4256102740764618\n",
      "epoch: 9 step: 2020, loss is 0.07131892442703247\n",
      "epoch: 9 step: 2021, loss is 0.3227809965610504\n",
      "epoch: 9 step: 2022, loss is 0.8783650994300842\n",
      "epoch: 9 step: 2023, loss is 0.05490095540881157\n",
      "epoch: 9 step: 2024, loss is 0.029703378677368164\n",
      "epoch: 9 step: 2025, loss is 0.2716118395328522\n",
      "epoch: 9 step: 2026, loss is 0.04443163797259331\n",
      "epoch: 9 step: 2027, loss is 0.2176932543516159\n",
      "epoch: 9 step: 2028, loss is 0.1692911982536316\n",
      "epoch: 9 step: 2029, loss is 0.22444328665733337\n",
      "epoch: 9 step: 2030, loss is 0.15793313086032867\n",
      "epoch: 9 step: 2031, loss is 0.26025912165641785\n",
      "epoch: 9 step: 2032, loss is 0.008523252792656422\n",
      "epoch: 9 step: 2033, loss is 0.038584042340517044\n",
      "epoch: 9 step: 2034, loss is 0.20289170742034912\n",
      "epoch: 9 step: 2035, loss is 0.052110373973846436\n",
      "epoch: 9 step: 2036, loss is 0.17903319001197815\n",
      "epoch: 9 step: 2037, loss is 0.15471723675727844\n",
      "epoch: 9 step: 2038, loss is 0.04389080032706261\n",
      "epoch: 9 step: 2039, loss is 0.4731105864048004\n",
      "epoch: 9 step: 2040, loss is 1.7827129364013672\n",
      "epoch: 9 step: 2041, loss is 0.24235883355140686\n",
      "epoch: 9 step: 2042, loss is 0.13307203352451324\n",
      "epoch: 9 step: 2043, loss is 0.24207530915737152\n",
      "epoch: 9 step: 2044, loss is 0.31238675117492676\n",
      "epoch: 9 step: 2045, loss is 0.28384244441986084\n",
      "epoch: 9 step: 2046, loss is 0.17023441195487976\n",
      "epoch: 9 step: 2047, loss is 0.11730562150478363\n",
      "epoch: 9 step: 2048, loss is 0.16904407739639282\n",
      "epoch: 9 step: 2049, loss is 0.5263374447822571\n",
      "epoch: 9 step: 2050, loss is 0.16526558995246887\n",
      "epoch: 9 step: 2051, loss is 0.21707381308078766\n",
      "epoch: 9 step: 2052, loss is 0.2550767958164215\n",
      "epoch: 9 step: 2053, loss is 0.6406542062759399\n",
      "epoch: 9 step: 2054, loss is 0.024471012875437737\n",
      "epoch: 9 step: 2055, loss is 0.5285033583641052\n",
      "epoch: 9 step: 2056, loss is 0.16458535194396973\n",
      "epoch: 9 step: 2057, loss is 0.21100054681301117\n",
      "epoch: 9 step: 2058, loss is 0.13578924536705017\n",
      "epoch: 9 step: 2059, loss is 0.035931676626205444\n",
      "epoch: 9 step: 2060, loss is 0.12173046171665192\n",
      "epoch: 9 step: 2061, loss is 0.3393658697605133\n",
      "epoch: 9 step: 2062, loss is 0.4923146963119507\n",
      "epoch: 9 step: 2063, loss is 0.03953384608030319\n",
      "epoch: 9 step: 2064, loss is 0.2595610022544861\n",
      "epoch: 9 step: 2065, loss is 0.0890834704041481\n",
      "epoch: 9 step: 2066, loss is 0.07472674548625946\n",
      "epoch: 9 step: 2067, loss is 0.09117111563682556\n",
      "epoch: 9 step: 2068, loss is 0.4597725570201874\n",
      "epoch: 9 step: 2069, loss is 0.29559454321861267\n",
      "epoch: 9 step: 2070, loss is 0.10741691291332245\n",
      "epoch: 9 step: 2071, loss is 0.27566540241241455\n",
      "epoch: 9 step: 2072, loss is 0.33072447776794434\n",
      "epoch: 9 step: 2073, loss is 0.00796189159154892\n",
      "epoch: 9 step: 2074, loss is 0.11843294650316238\n",
      "epoch: 9 step: 2075, loss is 0.22937841713428497\n",
      "epoch: 9 step: 2076, loss is 0.21250498294830322\n",
      "epoch: 9 step: 2077, loss is 0.029234467074275017\n",
      "epoch: 9 step: 2078, loss is 1.1625338792800903\n",
      "epoch: 9 step: 2079, loss is 0.05085782706737518\n",
      "epoch: 9 step: 2080, loss is 0.25487542152404785\n",
      "epoch: 9 step: 2081, loss is 0.42621245980262756\n",
      "epoch: 9 step: 2082, loss is 0.332658588886261\n",
      "epoch: 9 step: 2083, loss is 0.13022811710834503\n",
      "epoch: 9 step: 2084, loss is 0.1497013419866562\n",
      "epoch: 9 step: 2085, loss is 0.10713548213243484\n",
      "epoch: 9 step: 2086, loss is 0.23413512110710144\n",
      "epoch: 9 step: 2087, loss is 0.07954052090644836\n",
      "epoch: 9 step: 2088, loss is 0.31364837288856506\n",
      "epoch: 9 step: 2089, loss is 0.21521112322807312\n",
      "epoch: 9 step: 2090, loss is 0.10319982469081879\n",
      "epoch: 9 step: 2091, loss is 0.5808098912239075\n",
      "epoch: 9 step: 2092, loss is 0.6224393844604492\n",
      "epoch: 9 step: 2093, loss is 0.2887248694896698\n",
      "epoch: 9 step: 2094, loss is 0.2040167897939682\n",
      "epoch: 9 step: 2095, loss is 0.39270713925361633\n",
      "epoch: 9 step: 2096, loss is 0.5310184359550476\n",
      "epoch: 9 step: 2097, loss is 0.24267984926700592\n",
      "epoch: 9 step: 2098, loss is 0.029239194467663765\n",
      "epoch: 9 step: 2099, loss is 0.20623494684696198\n",
      "epoch: 9 step: 2100, loss is 0.09125365316867828\n",
      "epoch: 9 step: 2101, loss is 0.6499814391136169\n",
      "epoch: 9 step: 2102, loss is 0.5001688599586487\n",
      "epoch: 9 step: 2103, loss is 0.026367999613285065\n",
      "epoch: 9 step: 2104, loss is 0.022628353908658028\n",
      "epoch: 9 step: 2105, loss is 0.0920829176902771\n",
      "epoch: 9 step: 2106, loss is 0.1917441040277481\n",
      "epoch: 9 step: 2107, loss is 0.10184597969055176\n",
      "epoch: 9 step: 2108, loss is 0.4024111032485962\n",
      "epoch: 9 step: 2109, loss is 0.3883480727672577\n",
      "epoch: 9 step: 2110, loss is 0.3013969361782074\n",
      "epoch: 9 step: 2111, loss is 0.29040858149528503\n",
      "epoch: 9 step: 2112, loss is 0.06703487038612366\n",
      "epoch: 9 step: 2113, loss is 0.7762613892555237\n",
      "epoch: 9 step: 2114, loss is 0.27393293380737305\n",
      "epoch: 9 step: 2115, loss is 0.18888390064239502\n",
      "epoch: 9 step: 2116, loss is 0.22792935371398926\n",
      "epoch: 9 step: 2117, loss is 0.25279828906059265\n",
      "epoch: 9 step: 2118, loss is 0.0923951044678688\n",
      "epoch: 9 step: 2119, loss is 0.09310191124677658\n",
      "epoch: 9 step: 2120, loss is 0.4869270920753479\n",
      "epoch: 9 step: 2121, loss is 0.45862725377082825\n",
      "epoch: 9 step: 2122, loss is 0.038900647312402725\n",
      "epoch: 9 step: 2123, loss is 0.06076082959771156\n",
      "epoch: 9 step: 2124, loss is 0.24052023887634277\n",
      "epoch: 9 step: 2125, loss is 0.012397962622344494\n",
      "epoch: 9 step: 2126, loss is 0.110990509390831\n",
      "epoch: 9 step: 2127, loss is 0.0603751577436924\n",
      "epoch: 9 step: 2128, loss is 0.10753344744443893\n",
      "epoch: 9 step: 2129, loss is 0.3432696759700775\n",
      "epoch: 9 step: 2130, loss is 0.045580726116895676\n",
      "epoch: 9 step: 2131, loss is 0.14130784571170807\n",
      "epoch: 9 step: 2132, loss is 0.13410596549510956\n",
      "epoch: 9 step: 2133, loss is 0.12516914308071136\n",
      "epoch: 9 step: 2134, loss is 0.15425141155719757\n",
      "epoch: 9 step: 2135, loss is 0.2547093629837036\n",
      "epoch: 9 step: 2136, loss is 0.42063650488853455\n",
      "epoch: 9 step: 2137, loss is 0.14703530073165894\n",
      "epoch: 9 step: 2138, loss is 0.10468856990337372\n",
      "epoch: 9 step: 2139, loss is 0.2653753459453583\n",
      "epoch: 9 step: 2140, loss is 0.3499812185764313\n",
      "epoch: 9 step: 2141, loss is 0.4779055118560791\n",
      "epoch: 9 step: 2142, loss is 0.15387222170829773\n",
      "epoch: 9 step: 2143, loss is 0.1127527579665184\n",
      "epoch: 9 step: 2144, loss is 0.08717188239097595\n",
      "epoch: 9 step: 2145, loss is 0.5001845955848694\n",
      "epoch: 9 step: 2146, loss is 0.06511439383029938\n",
      "epoch: 9 step: 2147, loss is 0.31140169501304626\n",
      "epoch: 9 step: 2148, loss is 0.09018255025148392\n",
      "epoch: 9 step: 2149, loss is 0.062065936625003815\n",
      "epoch: 9 step: 2150, loss is 0.7309762835502625\n",
      "epoch: 9 step: 2151, loss is 0.07384659349918365\n",
      "epoch: 9 step: 2152, loss is 0.11788108944892883\n",
      "epoch: 9 step: 2153, loss is 0.12081850320100784\n",
      "epoch: 9 step: 2154, loss is 0.4117795526981354\n",
      "epoch: 9 step: 2155, loss is 0.010064891539514065\n",
      "epoch: 9 step: 2156, loss is 0.2834506034851074\n",
      "epoch: 9 step: 2157, loss is 0.28745412826538086\n",
      "epoch: 9 step: 2158, loss is 0.16526950895786285\n",
      "epoch: 9 step: 2159, loss is 0.11066890507936478\n",
      "epoch: 9 step: 2160, loss is 0.5577588081359863\n",
      "epoch: 9 step: 2161, loss is 0.16142471134662628\n",
      "epoch: 9 step: 2162, loss is 0.1890283226966858\n",
      "epoch: 9 step: 2163, loss is 0.05288245528936386\n",
      "epoch: 9 step: 2164, loss is 0.34022340178489685\n",
      "epoch: 9 step: 2165, loss is 0.11601586639881134\n",
      "epoch: 9 step: 2166, loss is 0.03156033530831337\n",
      "epoch: 9 step: 2167, loss is 0.1291263848543167\n",
      "epoch: 9 step: 2168, loss is 0.4195329546928406\n",
      "epoch: 9 step: 2169, loss is 0.05904587730765343\n",
      "epoch: 9 step: 2170, loss is 0.0900542363524437\n",
      "epoch: 9 step: 2171, loss is 0.45105740427970886\n",
      "epoch: 9 step: 2172, loss is 0.28473860025405884\n",
      "epoch: 9 step: 2173, loss is 0.5051673650741577\n",
      "epoch: 9 step: 2174, loss is 0.4438903331756592\n",
      "epoch: 9 step: 2175, loss is 0.11091230809688568\n",
      "epoch: 9 step: 2176, loss is 0.5602949857711792\n",
      "epoch: 9 step: 2177, loss is 0.47856375575065613\n",
      "epoch: 9 step: 2178, loss is 0.16359363496303558\n",
      "epoch: 9 step: 2179, loss is 0.4278765320777893\n",
      "epoch: 9 step: 2180, loss is 0.24586665630340576\n",
      "epoch: 9 step: 2181, loss is 0.031281888484954834\n",
      "epoch: 9 step: 2182, loss is 0.2665318250656128\n",
      "epoch: 9 step: 2183, loss is 0.32926371693611145\n",
      "epoch: 9 step: 2184, loss is 0.2039763480424881\n",
      "epoch: 9 step: 2185, loss is 0.08237078040838242\n",
      "epoch: 9 step: 2186, loss is 0.04729458689689636\n",
      "epoch: 9 step: 2187, loss is 0.01545641664415598\n",
      "epoch: 9 step: 2188, loss is 0.35303181409835815\n",
      "epoch: 9 step: 2189, loss is 0.1010272353887558\n",
      "epoch: 9 step: 2190, loss is 0.05743549391627312\n",
      "epoch: 9 step: 2191, loss is 0.7119670510292053\n",
      "epoch: 9 step: 2192, loss is 0.5258913040161133\n",
      "epoch: 9 step: 2193, loss is 0.06771399080753326\n",
      "epoch: 9 step: 2194, loss is 0.11460568755865097\n",
      "epoch: 9 step: 2195, loss is 0.30833426117897034\n",
      "epoch: 9 step: 2196, loss is 0.27643999457359314\n",
      "epoch: 9 step: 2197, loss is 0.0877736434340477\n",
      "epoch: 9 step: 2198, loss is 0.25949740409851074\n",
      "epoch: 9 step: 2199, loss is 0.17372779548168182\n",
      "epoch: 9 step: 2200, loss is 0.2021123468875885\n",
      "epoch: 9 step: 2201, loss is 0.040613457560539246\n",
      "epoch: 9 step: 2202, loss is 0.20639142394065857\n",
      "epoch: 9 step: 2203, loss is 0.10209856927394867\n",
      "epoch: 9 step: 2204, loss is 0.02879135124385357\n",
      "epoch: 9 step: 2205, loss is 0.47379186749458313\n",
      "epoch: 9 step: 2206, loss is 0.26786476373672485\n",
      "epoch: 9 step: 2207, loss is 0.10052581876516342\n",
      "epoch: 9 step: 2208, loss is 0.11327028274536133\n",
      "epoch: 9 step: 2209, loss is 0.8388174176216125\n",
      "epoch: 9 step: 2210, loss is 0.28926587104797363\n",
      "epoch: 9 step: 2211, loss is 0.06499498337507248\n",
      "epoch: 9 step: 2212, loss is 0.5543885827064514\n",
      "epoch: 9 step: 2213, loss is 0.1427006870508194\n",
      "epoch: 9 step: 2214, loss is 0.2669070363044739\n",
      "epoch: 9 step: 2215, loss is 0.39631521701812744\n",
      "epoch: 9 step: 2216, loss is 0.13108937442302704\n",
      "epoch: 9 step: 2217, loss is 0.17971909046173096\n",
      "epoch: 9 step: 2218, loss is 0.25009289383888245\n",
      "epoch: 9 step: 2219, loss is 0.08533984422683716\n",
      "epoch: 9 step: 2220, loss is 0.16354429721832275\n",
      "epoch: 9 step: 2221, loss is 0.06311486661434174\n",
      "epoch: 9 step: 2222, loss is 0.07411430031061172\n",
      "epoch: 9 step: 2223, loss is 0.1265566498041153\n",
      "epoch: 9 step: 2224, loss is 0.04259689152240753\n",
      "epoch: 9 step: 2225, loss is 0.1367478370666504\n",
      "epoch: 9 step: 2226, loss is 0.16356274485588074\n",
      "epoch: 9 step: 2227, loss is 0.16788095235824585\n",
      "epoch: 9 step: 2228, loss is 0.2097369134426117\n",
      "epoch: 9 step: 2229, loss is 0.789898157119751\n",
      "epoch: 9 step: 2230, loss is 0.005889736115932465\n",
      "epoch: 9 step: 2231, loss is 0.27404648065567017\n",
      "epoch: 9 step: 2232, loss is 0.07015972584486008\n",
      "epoch: 9 step: 2233, loss is 0.7025646567344666\n",
      "epoch: 9 step: 2234, loss is 0.03762417286634445\n",
      "epoch: 9 step: 2235, loss is 0.0036838429514318705\n",
      "epoch: 9 step: 2236, loss is 0.033869821578264236\n",
      "epoch: 9 step: 2237, loss is 0.16967551410198212\n",
      "epoch: 9 step: 2238, loss is 0.035861603915691376\n",
      "epoch: 9 step: 2239, loss is 0.035808708518743515\n",
      "epoch: 9 step: 2240, loss is 0.00916513241827488\n",
      "epoch: 9 step: 2241, loss is 0.2521979808807373\n",
      "epoch: 9 step: 2242, loss is 0.21779774129390717\n",
      "epoch: 9 step: 2243, loss is 0.12051235139369965\n",
      "epoch: 9 step: 2244, loss is 0.28795379400253296\n",
      "epoch: 9 step: 2245, loss is 0.15756018459796906\n",
      "epoch: 9 step: 2246, loss is 0.13089939951896667\n",
      "epoch: 9 step: 2247, loss is 0.2018691897392273\n",
      "epoch: 9 step: 2248, loss is 0.06131017208099365\n",
      "epoch: 9 step: 2249, loss is 0.024744408205151558\n",
      "epoch: 9 step: 2250, loss is 0.09827153384685516\n",
      "epoch: 9 step: 2251, loss is 0.05680086463689804\n",
      "epoch: 9 step: 2252, loss is 0.10370543599128723\n",
      "epoch: 9 step: 2253, loss is 0.13361811637878418\n",
      "epoch: 9 step: 2254, loss is 0.11914987862110138\n",
      "epoch: 9 step: 2255, loss is 0.31174495816230774\n",
      "epoch: 9 step: 2256, loss is 0.10035935044288635\n",
      "epoch: 9 step: 2257, loss is 0.16868984699249268\n",
      "epoch: 9 step: 2258, loss is 0.10053788870573044\n",
      "epoch: 9 step: 2259, loss is 0.1279965490102768\n",
      "epoch: 9 step: 2260, loss is 0.051968347281217575\n",
      "epoch: 9 step: 2261, loss is 0.5725322365760803\n",
      "epoch: 9 step: 2262, loss is 0.1508178561925888\n",
      "epoch: 9 step: 2263, loss is 0.22640806436538696\n",
      "epoch: 9 step: 2264, loss is 0.2100735753774643\n",
      "epoch: 9 step: 2265, loss is 0.21395905315876007\n",
      "epoch: 9 step: 2266, loss is 0.5581973791122437\n",
      "epoch: 9 step: 2267, loss is 0.4778467118740082\n",
      "epoch: 9 step: 2268, loss is 0.3147905766963959\n",
      "epoch: 9 step: 2269, loss is 0.02437952719628811\n",
      "epoch: 9 step: 2270, loss is 0.17779697477817535\n",
      "epoch: 9 step: 2271, loss is 0.05979073792695999\n",
      "epoch: 9 step: 2272, loss is 0.5004704594612122\n",
      "epoch: 9 step: 2273, loss is 0.08150286972522736\n",
      "epoch: 9 step: 2274, loss is 0.641230583190918\n",
      "epoch: 9 step: 2275, loss is 0.08230274170637131\n",
      "epoch: 9 step: 2276, loss is 0.10758863389492035\n",
      "epoch: 9 step: 2277, loss is 0.5511958003044128\n",
      "epoch: 9 step: 2278, loss is 0.10605672746896744\n",
      "epoch: 9 step: 2279, loss is 0.08461257070302963\n",
      "epoch: 9 step: 2280, loss is 0.3426983952522278\n",
      "epoch: 9 step: 2281, loss is 0.07903111726045609\n",
      "epoch: 9 step: 2282, loss is 0.3336031436920166\n",
      "epoch: 9 step: 2283, loss is 0.31585079431533813\n",
      "epoch: 9 step: 2284, loss is 0.029012346640229225\n",
      "epoch: 9 step: 2285, loss is 0.1105533242225647\n",
      "epoch: 9 step: 2286, loss is 0.27057158946990967\n",
      "epoch: 9 step: 2287, loss is 0.137510746717453\n",
      "epoch: 9 step: 2288, loss is 0.2607555687427521\n",
      "epoch: 9 step: 2289, loss is 0.06668756157159805\n",
      "epoch: 9 step: 2290, loss is 0.1379472017288208\n",
      "epoch: 9 step: 2291, loss is 0.300608366727829\n",
      "epoch: 9 step: 2292, loss is 0.09035678207874298\n",
      "epoch: 9 step: 2293, loss is 0.00518978014588356\n",
      "epoch: 9 step: 2294, loss is 0.07859223335981369\n",
      "epoch: 9 step: 2295, loss is 0.027224622666835785\n",
      "epoch: 9 step: 2296, loss is 0.08286580443382263\n",
      "epoch: 9 step: 2297, loss is 0.061887260526418686\n",
      "epoch: 9 step: 2298, loss is 0.17766982316970825\n",
      "epoch: 9 step: 2299, loss is 0.3645479679107666\n",
      "epoch: 9 step: 2300, loss is 0.5092347264289856\n",
      "epoch: 9 step: 2301, loss is 0.09540733695030212\n",
      "epoch: 9 step: 2302, loss is 0.2516002655029297\n",
      "epoch: 9 step: 2303, loss is 0.3324982821941376\n",
      "epoch: 9 step: 2304, loss is 0.262798935174942\n",
      "epoch: 9 step: 2305, loss is 0.1993376463651657\n",
      "epoch: 9 step: 2306, loss is 0.31323736906051636\n",
      "epoch: 9 step: 2307, loss is 0.07446818798780441\n",
      "epoch: 9 step: 2308, loss is 0.0640672892332077\n",
      "epoch: 9 step: 2309, loss is 0.19788679480552673\n",
      "epoch: 9 step: 2310, loss is 0.19081969559192657\n",
      "epoch: 9 step: 2311, loss is 0.4740599989891052\n",
      "epoch: 9 step: 2312, loss is 0.024451112374663353\n",
      "epoch: 9 step: 2313, loss is 0.1279284954071045\n",
      "epoch: 9 step: 2314, loss is 0.1306268572807312\n",
      "epoch: 9 step: 2315, loss is 0.41277649998664856\n",
      "epoch: 9 step: 2316, loss is 0.1180301234126091\n",
      "epoch: 9 step: 2317, loss is 0.18080325424671173\n",
      "epoch: 9 step: 2318, loss is 0.21606147289276123\n",
      "epoch: 9 step: 2319, loss is 0.20030340552330017\n",
      "epoch: 9 step: 2320, loss is 0.012114785611629486\n",
      "epoch: 9 step: 2321, loss is 0.04631118103861809\n",
      "epoch: 9 step: 2322, loss is 0.0756271556019783\n",
      "epoch: 9 step: 2323, loss is 0.3550446927547455\n",
      "epoch: 9 step: 2324, loss is 0.051415614783763885\n",
      "epoch: 9 step: 2325, loss is 0.3674015700817108\n",
      "epoch: 9 step: 2326, loss is 0.014663724228739738\n",
      "epoch: 9 step: 2327, loss is 0.21641696989536285\n",
      "epoch: 9 step: 2328, loss is 0.0240242388099432\n",
      "epoch: 9 step: 2329, loss is 0.20020070672035217\n",
      "epoch: 9 step: 2330, loss is 0.1430940181016922\n",
      "epoch: 9 step: 2331, loss is 0.3356645405292511\n",
      "epoch: 9 step: 2332, loss is 0.24034158885478973\n",
      "epoch: 9 step: 2333, loss is 0.2842431366443634\n",
      "epoch: 9 step: 2334, loss is 0.5253599286079407\n",
      "epoch: 9 step: 2335, loss is 0.19833393394947052\n",
      "epoch: 9 step: 2336, loss is 0.005566109903156757\n",
      "epoch: 9 step: 2337, loss is 0.0704561099410057\n",
      "epoch: 9 step: 2338, loss is 0.16066516935825348\n",
      "epoch: 9 step: 2339, loss is 0.09396790713071823\n",
      "epoch: 9 step: 2340, loss is 0.05229305848479271\n",
      "epoch: 9 step: 2341, loss is 0.10962134599685669\n",
      "epoch: 9 step: 2342, loss is 0.17732231318950653\n",
      "epoch: 9 step: 2343, loss is 0.09246592223644257\n",
      "epoch: 9 step: 2344, loss is 0.0796794667840004\n",
      "epoch: 9 step: 2345, loss is 0.0038666916079819202\n",
      "epoch: 9 step: 2346, loss is 0.34210845828056335\n",
      "epoch: 9 step: 2347, loss is 0.046586647629737854\n",
      "epoch: 9 step: 2348, loss is 0.37797290086746216\n",
      "epoch: 9 step: 2349, loss is 0.22352874279022217\n",
      "epoch: 9 step: 2350, loss is 0.2921667695045471\n",
      "epoch: 9 step: 2351, loss is 0.20909824967384338\n",
      "epoch: 9 step: 2352, loss is 0.03300337493419647\n",
      "epoch: 9 step: 2353, loss is 0.40419304370880127\n",
      "epoch: 9 step: 2354, loss is 0.35154128074645996\n",
      "epoch: 9 step: 2355, loss is 0.3828783631324768\n",
      "epoch: 9 step: 2356, loss is 0.5244783759117126\n",
      "epoch: 9 step: 2357, loss is 0.027425022795796394\n",
      "epoch: 9 step: 2358, loss is 0.6929150819778442\n",
      "epoch: 9 step: 2359, loss is 0.09601374715566635\n",
      "epoch: 9 step: 2360, loss is 0.3097772002220154\n",
      "epoch: 9 step: 2361, loss is 0.07181233167648315\n",
      "epoch: 9 step: 2362, loss is 0.11088284850120544\n",
      "epoch: 9 step: 2363, loss is 0.0560750775039196\n",
      "epoch: 9 step: 2364, loss is 0.35239630937576294\n",
      "epoch: 9 step: 2365, loss is 0.24948839843273163\n",
      "epoch: 9 step: 2366, loss is 0.16523152589797974\n",
      "epoch: 9 step: 2367, loss is 0.1521073877811432\n",
      "epoch: 9 step: 2368, loss is 0.2076646387577057\n",
      "epoch: 9 step: 2369, loss is 0.11908988654613495\n",
      "epoch: 9 step: 2370, loss is 0.16798672080039978\n",
      "epoch: 9 step: 2371, loss is 0.5424711108207703\n",
      "epoch: 9 step: 2372, loss is 0.19254575669765472\n",
      "epoch: 9 step: 2373, loss is 0.6660658717155457\n",
      "epoch: 9 step: 2374, loss is 0.24398620426654816\n",
      "epoch: 9 step: 2375, loss is 0.051691100001335144\n",
      "epoch: 9 step: 2376, loss is 0.11691097170114517\n",
      "epoch: 9 step: 2377, loss is 0.05717448145151138\n",
      "epoch: 9 step: 2378, loss is 0.104510597884655\n",
      "epoch: 9 step: 2379, loss is 0.4493137300014496\n",
      "epoch: 9 step: 2380, loss is 0.16656571626663208\n",
      "epoch: 9 step: 2381, loss is 0.04092196375131607\n",
      "epoch: 9 step: 2382, loss is 0.15131372213363647\n",
      "epoch: 9 step: 2383, loss is 0.04768228903412819\n",
      "epoch: 9 step: 2384, loss is 0.1789379119873047\n",
      "epoch: 9 step: 2385, loss is 0.2078172117471695\n",
      "epoch: 9 step: 2386, loss is 1.6740976572036743\n",
      "epoch: 9 step: 2387, loss is 0.12418665736913681\n",
      "epoch: 9 step: 2388, loss is 0.23457276821136475\n",
      "epoch: 9 step: 2389, loss is 0.34241950511932373\n",
      "epoch: 9 step: 2390, loss is 0.01677686907351017\n",
      "epoch: 9 step: 2391, loss is 0.3495679497718811\n",
      "epoch: 9 step: 2392, loss is 0.14189936220645905\n",
      "epoch: 9 step: 2393, loss is 0.038392696529626846\n",
      "epoch: 9 step: 2394, loss is 0.2975691854953766\n",
      "epoch: 9 step: 2395, loss is 0.009722750633955002\n",
      "epoch: 9 step: 2396, loss is 0.22295068204402924\n",
      "epoch: 9 step: 2397, loss is 0.3554634153842926\n",
      "epoch: 9 step: 2398, loss is 0.04564138129353523\n",
      "epoch: 9 step: 2399, loss is 0.1855790913105011\n",
      "epoch: 9 step: 2400, loss is 0.45238301157951355\n",
      "epoch: 9 step: 2401, loss is 0.08601371198892593\n",
      "epoch: 9 step: 2402, loss is 0.48645520210266113\n",
      "epoch: 9 step: 2403, loss is 0.04553743451833725\n",
      "epoch: 9 step: 2404, loss is 0.09651371091604233\n",
      "epoch: 9 step: 2405, loss is 0.08616003394126892\n",
      "epoch: 9 step: 2406, loss is 0.31265032291412354\n",
      "epoch: 9 step: 2407, loss is 0.07221753895282745\n",
      "epoch: 9 step: 2408, loss is 0.029395367950201035\n",
      "epoch: 9 step: 2409, loss is 0.11626151949167252\n",
      "epoch: 9 step: 2410, loss is 0.043669458478689194\n",
      "epoch: 9 step: 2411, loss is 0.059584490954875946\n",
      "epoch: 9 step: 2412, loss is 0.23609201610088348\n",
      "epoch: 9 step: 2413, loss is 0.08745334297418594\n",
      "epoch: 9 step: 2414, loss is 0.16038689017295837\n",
      "epoch: 9 step: 2415, loss is 0.4885329306125641\n",
      "epoch: 9 step: 2416, loss is 0.1233358085155487\n",
      "epoch: 9 step: 2417, loss is 0.28360244631767273\n",
      "epoch: 9 step: 2418, loss is 0.05046410486102104\n",
      "epoch: 9 step: 2419, loss is 0.07885388284921646\n",
      "epoch: 9 step: 2420, loss is 0.37658512592315674\n",
      "epoch: 9 step: 2421, loss is 0.061539627611637115\n",
      "epoch: 9 step: 2422, loss is 0.21763311326503754\n",
      "epoch: 9 step: 2423, loss is 0.30250096321105957\n",
      "epoch: 9 step: 2424, loss is 0.10155453532934189\n",
      "epoch: 9 step: 2425, loss is 0.5211562514305115\n",
      "epoch: 9 step: 2426, loss is 0.3424168527126312\n",
      "epoch: 9 step: 2427, loss is 0.1562599390745163\n",
      "epoch: 9 step: 2428, loss is 0.28874924778938293\n",
      "epoch: 9 step: 2429, loss is 0.20795771479606628\n",
      "epoch: 9 step: 2430, loss is 0.16618867218494415\n",
      "epoch: 9 step: 2431, loss is 0.12346506118774414\n",
      "epoch: 9 step: 2432, loss is 0.1323567032814026\n",
      "epoch: 9 step: 2433, loss is 0.1813763827085495\n",
      "epoch: 9 step: 2434, loss is 0.45125335454940796\n",
      "epoch: 9 step: 2435, loss is 0.08610913157463074\n",
      "epoch: 9 step: 2436, loss is 0.11458168178796768\n",
      "epoch: 9 step: 2437, loss is 0.05438320338726044\n",
      "epoch: 9 step: 2438, loss is 0.12763340771198273\n",
      "epoch: 9 step: 2439, loss is 0.07940412312746048\n",
      "epoch: 9 step: 2440, loss is 0.18961864709854126\n",
      "epoch: 9 step: 2441, loss is 0.20693722367286682\n",
      "epoch: 9 step: 2442, loss is 0.1580127626657486\n",
      "epoch: 9 step: 2443, loss is 0.1241578757762909\n",
      "epoch: 9 step: 2444, loss is 0.17911683022975922\n",
      "epoch: 9 step: 2445, loss is 0.04013226553797722\n",
      "epoch: 9 step: 2446, loss is 0.022861743345856667\n",
      "epoch: 9 step: 2447, loss is 0.4164884388446808\n",
      "epoch: 9 step: 2448, loss is 0.33481812477111816\n",
      "epoch: 9 step: 2449, loss is 0.1281609833240509\n",
      "epoch: 9 step: 2450, loss is 0.0677124485373497\n",
      "epoch: 9 step: 2451, loss is 0.25217878818511963\n",
      "epoch: 9 step: 2452, loss is 0.1507416069507599\n",
      "epoch: 9 step: 2453, loss is 0.12566916644573212\n",
      "epoch: 9 step: 2454, loss is 0.13651449978351593\n",
      "epoch: 9 step: 2455, loss is 0.9416204690933228\n",
      "epoch: 9 step: 2456, loss is 0.06618836522102356\n",
      "epoch: 9 step: 2457, loss is 0.11197421699762344\n",
      "epoch: 9 step: 2458, loss is 0.43304243683815\n",
      "epoch: 9 step: 2459, loss is 0.19836270809173584\n",
      "epoch: 9 step: 2460, loss is 0.03386586159467697\n",
      "epoch: 9 step: 2461, loss is 0.35791799426078796\n",
      "epoch: 9 step: 2462, loss is 0.5961290001869202\n",
      "epoch: 9 step: 2463, loss is 0.046880804002285004\n",
      "epoch: 9 step: 2464, loss is 0.7902691960334778\n",
      "epoch: 9 step: 2465, loss is 0.1855163425207138\n",
      "epoch: 9 step: 2466, loss is 0.02444758266210556\n",
      "epoch: 9 step: 2467, loss is 0.03479776158928871\n",
      "epoch: 9 step: 2468, loss is 0.13371095061302185\n",
      "epoch: 9 step: 2469, loss is 0.05180706828832626\n",
      "epoch: 9 step: 2470, loss is 0.06964676082134247\n",
      "epoch: 9 step: 2471, loss is 0.24956122040748596\n",
      "epoch: 9 step: 2472, loss is 0.11182348430156708\n",
      "epoch: 9 step: 2473, loss is 0.1025395542383194\n",
      "epoch: 9 step: 2474, loss is 0.6651701331138611\n",
      "epoch: 9 step: 2475, loss is 0.17536503076553345\n",
      "epoch: 9 step: 2476, loss is 0.20228198170661926\n",
      "epoch: 9 step: 2477, loss is 0.23196834325790405\n",
      "epoch: 9 step: 2478, loss is 0.3218366205692291\n",
      "epoch: 9 step: 2479, loss is 0.4081016778945923\n",
      "epoch: 9 step: 2480, loss is 0.28292882442474365\n",
      "epoch: 9 step: 2481, loss is 0.21342012286186218\n",
      "epoch: 9 step: 2482, loss is 0.2310970276594162\n",
      "epoch: 9 step: 2483, loss is 0.052305351942777634\n",
      "epoch: 9 step: 2484, loss is 0.3805370330810547\n",
      "epoch: 9 step: 2485, loss is 0.2706719636917114\n",
      "epoch: 9 step: 2486, loss is 0.35285499691963196\n",
      "epoch: 9 step: 2487, loss is 0.7547047138214111\n",
      "epoch: 9 step: 2488, loss is 0.6974010467529297\n",
      "epoch: 9 step: 2489, loss is 0.11533091217279434\n",
      "epoch: 9 step: 2490, loss is 0.20506402850151062\n",
      "epoch: 9 step: 2491, loss is 0.40132614970207214\n",
      "epoch: 9 step: 2492, loss is 0.11777366697788239\n",
      "epoch: 9 step: 2493, loss is 0.1273416131734848\n",
      "epoch: 9 step: 2494, loss is 0.39004355669021606\n",
      "epoch: 9 step: 2495, loss is 0.012692484073340893\n",
      "epoch: 9 step: 2496, loss is 0.22044256329536438\n",
      "epoch: 9 step: 2497, loss is 0.1200946643948555\n",
      "epoch: 9 step: 2498, loss is 0.18452155590057373\n",
      "epoch: 9 step: 2499, loss is 0.658711314201355\n",
      "epoch: 9 step: 2500, loss is 0.09096994251012802\n",
      "epoch: 9 step: 2501, loss is 0.1149897426366806\n",
      "epoch: 9 step: 2502, loss is 0.10032057017087936\n",
      "epoch: 9 step: 2503, loss is 0.04918380081653595\n",
      "epoch: 9 step: 2504, loss is 0.2971673607826233\n",
      "epoch: 9 step: 2505, loss is 0.03727548196911812\n",
      "epoch: 9 step: 2506, loss is 0.5201420187950134\n",
      "epoch: 9 step: 2507, loss is 0.3959566652774811\n",
      "epoch: 9 step: 2508, loss is 0.09432952851057053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: unknown JFIF revision number 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 10 step: 1, loss is 0.02297615446150303\n",
      "epoch: 10 step: 2, loss is 0.004764561075717211\n",
      "epoch: 10 step: 3, loss is 0.3689391016960144\n",
      "epoch: 10 step: 4, loss is 0.2755938470363617\n",
      "epoch: 10 step: 5, loss is 0.2133324295282364\n",
      "epoch: 10 step: 6, loss is 0.4298229217529297\n",
      "epoch: 10 step: 7, loss is 0.06851629167795181\n",
      "epoch: 10 step: 8, loss is 0.4676697850227356\n",
      "epoch: 10 step: 9, loss is 0.210661843419075\n",
      "epoch: 10 step: 10, loss is 0.057994499802589417\n",
      "epoch: 10 step: 11, loss is 0.527441680431366\n",
      "epoch: 10 step: 12, loss is 0.14853563904762268\n",
      "epoch: 10 step: 13, loss is 0.4333086907863617\n",
      "epoch: 10 step: 14, loss is 0.025780828669667244\n",
      "epoch: 10 step: 15, loss is 0.1065327376127243\n",
      "epoch: 10 step: 16, loss is 0.1896851658821106\n",
      "epoch: 10 step: 17, loss is 0.2075122892856598\n",
      "epoch: 10 step: 18, loss is 0.06383974105119705\n",
      "epoch: 10 step: 19, loss is 0.4537537395954132\n",
      "epoch: 10 step: 20, loss is 0.5119455456733704\n",
      "epoch: 10 step: 21, loss is 0.22385498881340027\n",
      "epoch: 10 step: 22, loss is 0.4692997932434082\n",
      "epoch: 10 step: 23, loss is 0.16946133971214294\n",
      "epoch: 10 step: 24, loss is 0.2513257563114166\n",
      "epoch: 10 step: 25, loss is 0.47322773933410645\n",
      "epoch: 10 step: 26, loss is 0.1033054068684578\n",
      "epoch: 10 step: 27, loss is 0.40613171458244324\n",
      "epoch: 10 step: 28, loss is 0.2565971910953522\n",
      "epoch: 10 step: 29, loss is 0.13823486864566803\n",
      "epoch: 10 step: 30, loss is 0.41649818420410156\n",
      "epoch: 10 step: 31, loss is 0.19595983624458313\n",
      "epoch: 10 step: 32, loss is 0.1051236242055893\n",
      "epoch: 10 step: 33, loss is 0.16114866733551025\n",
      "epoch: 10 step: 34, loss is 0.276161789894104\n",
      "epoch: 10 step: 35, loss is 0.032898493111133575\n",
      "epoch: 10 step: 36, loss is 0.018360160291194916\n",
      "epoch: 10 step: 37, loss is 0.04210289940237999\n",
      "epoch: 10 step: 38, loss is 0.05113712325692177\n",
      "epoch: 10 step: 39, loss is 0.2649158537387848\n",
      "epoch: 10 step: 40, loss is 0.2597457766532898\n",
      "epoch: 10 step: 41, loss is 0.03612915799021721\n",
      "epoch: 10 step: 42, loss is 0.2553066909313202\n",
      "epoch: 10 step: 43, loss is 0.614242672920227\n",
      "epoch: 10 step: 44, loss is 0.03182166814804077\n",
      "epoch: 10 step: 45, loss is 0.13447871804237366\n",
      "epoch: 10 step: 46, loss is 0.021381035447120667\n",
      "epoch: 10 step: 47, loss is 0.028309021145105362\n",
      "epoch: 10 step: 48, loss is 0.06680776923894882\n",
      "epoch: 10 step: 49, loss is 0.08587422966957092\n",
      "epoch: 10 step: 50, loss is 0.008992614224553108\n",
      "epoch: 10 step: 51, loss is 0.04589330777525902\n",
      "epoch: 10 step: 52, loss is 0.06526728719472885\n",
      "epoch: 10 step: 53, loss is 0.0023925171699374914\n",
      "epoch: 10 step: 54, loss is 0.1912894994020462\n",
      "epoch: 10 step: 55, loss is 0.01917007379233837\n",
      "epoch: 10 step: 56, loss is 0.09993530809879303\n",
      "epoch: 10 step: 57, loss is 0.3691531717777252\n",
      "epoch: 10 step: 58, loss is 0.042928919196128845\n",
      "epoch: 10 step: 59, loss is 0.048583582043647766\n",
      "epoch: 10 step: 60, loss is 0.11867457628250122\n",
      "epoch: 10 step: 61, loss is 0.3289506733417511\n",
      "epoch: 10 step: 62, loss is 0.04573432356119156\n",
      "epoch: 10 step: 63, loss is 0.06062934175133705\n",
      "epoch: 10 step: 64, loss is 0.3523924648761749\n",
      "epoch: 10 step: 65, loss is 0.006093948148190975\n",
      "epoch: 10 step: 66, loss is 0.13791237771511078\n",
      "epoch: 10 step: 67, loss is 0.08633751422166824\n",
      "epoch: 10 step: 68, loss is 0.011270864866673946\n",
      "epoch: 10 step: 69, loss is 0.05614831671118736\n",
      "epoch: 10 step: 70, loss is 0.7833464741706848\n",
      "epoch: 10 step: 71, loss is 0.008744848892092705\n",
      "epoch: 10 step: 72, loss is 0.1862994134426117\n",
      "epoch: 10 step: 73, loss is 0.17641741037368774\n",
      "epoch: 10 step: 74, loss is 0.3125024139881134\n",
      "epoch: 10 step: 75, loss is 0.23874160647392273\n",
      "epoch: 10 step: 76, loss is 0.07869921624660492\n",
      "epoch: 10 step: 77, loss is 0.2609858214855194\n",
      "epoch: 10 step: 78, loss is 0.04978565499186516\n",
      "epoch: 10 step: 79, loss is 0.18967917561531067\n",
      "epoch: 10 step: 80, loss is 0.3228856027126312\n",
      "epoch: 10 step: 81, loss is 0.2214304357767105\n",
      "epoch: 10 step: 82, loss is 0.32901039719581604\n",
      "epoch: 10 step: 83, loss is 0.08924851566553116\n",
      "epoch: 10 step: 84, loss is 0.15306027233600616\n",
      "epoch: 10 step: 85, loss is 0.1123969778418541\n",
      "epoch: 10 step: 86, loss is 0.08077283948659897\n",
      "epoch: 10 step: 87, loss is 0.2602424621582031\n",
      "epoch: 10 step: 88, loss is 0.09636203944683075\n",
      "epoch: 10 step: 89, loss is 0.31950876116752625\n",
      "epoch: 10 step: 90, loss is 0.33401715755462646\n",
      "epoch: 10 step: 91, loss is 0.015705544501543045\n",
      "epoch: 10 step: 92, loss is 0.053826671093702316\n",
      "epoch: 10 step: 93, loss is 0.09210259467363358\n",
      "epoch: 10 step: 94, loss is 0.021004972979426384\n",
      "epoch: 10 step: 95, loss is 0.03636658191680908\n",
      "epoch: 10 step: 96, loss is 0.46264514327049255\n",
      "epoch: 10 step: 97, loss is 0.050985872745513916\n",
      "epoch: 10 step: 98, loss is 0.06740207970142365\n",
      "epoch: 10 step: 99, loss is 0.08949515968561172\n",
      "epoch: 10 step: 100, loss is 0.1830945909023285\n",
      "epoch: 10 step: 101, loss is 0.045203179121017456\n",
      "epoch: 10 step: 102, loss is 0.13329361379146576\n",
      "epoch: 10 step: 103, loss is 0.03550205007195473\n",
      "epoch: 10 step: 104, loss is 0.016459064558148384\n",
      "epoch: 10 step: 105, loss is 0.06461750715970993\n",
      "epoch: 10 step: 106, loss is 0.28493252396583557\n",
      "epoch: 10 step: 107, loss is 0.2373032122850418\n",
      "epoch: 10 step: 108, loss is 0.26812466979026794\n",
      "epoch: 10 step: 109, loss is 0.4923710823059082\n",
      "epoch: 10 step: 110, loss is 0.26589465141296387\n",
      "epoch: 10 step: 111, loss is 0.07830511778593063\n",
      "epoch: 10 step: 112, loss is 0.09301207959651947\n",
      "epoch: 10 step: 113, loss is 0.07021158933639526\n",
      "epoch: 10 step: 114, loss is 0.06772793084383011\n",
      "epoch: 10 step: 115, loss is 0.19780953228473663\n",
      "epoch: 10 step: 116, loss is 0.3020237982273102\n",
      "epoch: 10 step: 117, loss is 0.6511175632476807\n",
      "epoch: 10 step: 118, loss is 0.037684306502342224\n",
      "epoch: 10 step: 119, loss is 0.15437930822372437\n",
      "epoch: 10 step: 120, loss is 0.3205926716327667\n",
      "epoch: 10 step: 121, loss is 0.26021987199783325\n",
      "epoch: 10 step: 122, loss is 0.19269824028015137\n",
      "epoch: 10 step: 123, loss is 0.20746079087257385\n",
      "epoch: 10 step: 124, loss is 0.06653504818677902\n",
      "epoch: 10 step: 125, loss is 0.05715818330645561\n",
      "epoch: 10 step: 126, loss is 0.14424103498458862\n",
      "epoch: 10 step: 127, loss is 0.031990449875593185\n",
      "epoch: 10 step: 128, loss is 0.11920873820781708\n",
      "epoch: 10 step: 129, loss is 0.19068671762943268\n",
      "epoch: 10 step: 130, loss is 0.2620539367198944\n",
      "epoch: 10 step: 131, loss is 0.09271276742219925\n",
      "epoch: 10 step: 132, loss is 0.019989533349871635\n",
      "epoch: 10 step: 133, loss is 0.13434067368507385\n",
      "epoch: 10 step: 134, loss is 0.1475447714328766\n",
      "epoch: 10 step: 135, loss is 0.28423866629600525\n",
      "epoch: 10 step: 136, loss is 0.15175528824329376\n",
      "epoch: 10 step: 137, loss is 0.4832756221294403\n",
      "epoch: 10 step: 138, loss is 0.10311522334814072\n",
      "epoch: 10 step: 139, loss is 0.09590739011764526\n",
      "epoch: 10 step: 140, loss is 0.02191247045993805\n",
      "epoch: 10 step: 141, loss is 0.04181784763932228\n",
      "epoch: 10 step: 142, loss is 0.024336639791727066\n",
      "epoch: 10 step: 143, loss is 0.2585690915584564\n",
      "epoch: 10 step: 144, loss is 0.5168425440788269\n",
      "epoch: 10 step: 145, loss is 0.07275423407554626\n",
      "epoch: 10 step: 146, loss is 0.2985469400882721\n",
      "epoch: 10 step: 147, loss is 0.07665763795375824\n",
      "epoch: 10 step: 148, loss is 0.12042399495840073\n",
      "epoch: 10 step: 149, loss is 0.07765446603298187\n",
      "epoch: 10 step: 150, loss is 0.35866492986679077\n",
      "epoch: 10 step: 151, loss is 0.10428690910339355\n",
      "epoch: 10 step: 152, loss is 0.1834293007850647\n",
      "epoch: 10 step: 153, loss is 0.16099122166633606\n",
      "epoch: 10 step: 154, loss is 0.43112483620643616\n",
      "epoch: 10 step: 155, loss is 0.4087129235267639\n",
      "epoch: 10 step: 156, loss is 0.08454625308513641\n",
      "epoch: 10 step: 157, loss is 0.2919796407222748\n",
      "epoch: 10 step: 158, loss is 0.07248662412166595\n",
      "epoch: 10 step: 159, loss is 0.09669069200754166\n",
      "epoch: 10 step: 160, loss is 0.07905492186546326\n",
      "epoch: 10 step: 161, loss is 0.29711639881134033\n",
      "epoch: 10 step: 162, loss is 0.11710888892412186\n",
      "epoch: 10 step: 163, loss is 0.220438614487648\n",
      "epoch: 10 step: 164, loss is 0.21155233681201935\n",
      "epoch: 10 step: 165, loss is 0.27193474769592285\n",
      "epoch: 10 step: 166, loss is 0.246168315410614\n",
      "epoch: 10 step: 167, loss is 0.03834225609898567\n",
      "epoch: 10 step: 168, loss is 0.2914319932460785\n",
      "epoch: 10 step: 169, loss is 0.18691182136535645\n",
      "epoch: 10 step: 170, loss is 0.044387221336364746\n",
      "epoch: 10 step: 171, loss is 0.6946010589599609\n",
      "epoch: 10 step: 172, loss is 0.10451432317495346\n",
      "epoch: 10 step: 173, loss is 0.017967302352190018\n",
      "epoch: 10 step: 174, loss is 0.5713642835617065\n",
      "epoch: 10 step: 175, loss is 0.34030961990356445\n",
      "epoch: 10 step: 176, loss is 0.17007608711719513\n",
      "epoch: 10 step: 177, loss is 0.08840332180261612\n",
      "epoch: 10 step: 178, loss is 0.14621353149414062\n",
      "epoch: 10 step: 179, loss is 0.11652069538831711\n",
      "epoch: 10 step: 180, loss is 0.032424356788396835\n",
      "epoch: 10 step: 181, loss is 0.7268698215484619\n",
      "epoch: 10 step: 182, loss is 0.1014777347445488\n",
      "epoch: 10 step: 183, loss is 0.1632443070411682\n",
      "epoch: 10 step: 184, loss is 0.05176357552409172\n",
      "epoch: 10 step: 185, loss is 0.1728132665157318\n",
      "epoch: 10 step: 186, loss is 0.02482815645635128\n",
      "epoch: 10 step: 187, loss is 0.0027009404730051756\n",
      "epoch: 10 step: 188, loss is 0.009219332598149776\n",
      "epoch: 10 step: 189, loss is 0.2812046706676483\n",
      "epoch: 10 step: 190, loss is 0.09539362788200378\n",
      "epoch: 10 step: 191, loss is 0.16016124188899994\n",
      "epoch: 10 step: 192, loss is 0.1364995837211609\n",
      "epoch: 10 step: 193, loss is 0.08300921320915222\n",
      "epoch: 10 step: 194, loss is 0.14324304461479187\n",
      "epoch: 10 step: 195, loss is 0.020881932228803635\n",
      "epoch: 10 step: 196, loss is 0.078526571393013\n",
      "epoch: 10 step: 197, loss is 0.006470485124737024\n",
      "epoch: 10 step: 198, loss is 0.05872555822134018\n",
      "epoch: 10 step: 199, loss is 0.05973295494914055\n",
      "epoch: 10 step: 200, loss is 0.10592573136091232\n",
      "epoch: 10 step: 201, loss is 0.0629974976181984\n",
      "epoch: 10 step: 202, loss is 0.05074602738022804\n",
      "epoch: 10 step: 203, loss is 0.036443017423152924\n",
      "epoch: 10 step: 204, loss is 0.0659807100892067\n",
      "epoch: 10 step: 205, loss is 0.007993440143764019\n",
      "epoch: 10 step: 206, loss is 0.3889036774635315\n",
      "epoch: 10 step: 207, loss is 0.2530311048030853\n",
      "epoch: 10 step: 208, loss is 0.4102620780467987\n",
      "epoch: 10 step: 209, loss is 0.0330267995595932\n",
      "epoch: 10 step: 210, loss is 0.18332451581954956\n",
      "epoch: 10 step: 211, loss is 0.08999544382095337\n",
      "epoch: 10 step: 212, loss is 0.3186134696006775\n",
      "epoch: 10 step: 213, loss is 0.07438590377569199\n",
      "epoch: 10 step: 214, loss is 0.04192536696791649\n",
      "epoch: 10 step: 215, loss is 0.8060995936393738\n",
      "epoch: 10 step: 216, loss is 0.6246870756149292\n",
      "epoch: 10 step: 217, loss is 0.2193068414926529\n",
      "epoch: 10 step: 218, loss is 0.13252079486846924\n",
      "epoch: 10 step: 219, loss is 0.1374538689851761\n",
      "epoch: 10 step: 220, loss is 0.249742329120636\n",
      "epoch: 10 step: 221, loss is 0.21240949630737305\n",
      "epoch: 10 step: 222, loss is 0.03707842528820038\n",
      "epoch: 10 step: 223, loss is 0.12618787586688995\n",
      "epoch: 10 step: 224, loss is 0.17025864124298096\n",
      "epoch: 10 step: 225, loss is 0.058194223791360855\n",
      "epoch: 10 step: 226, loss is 0.06087195873260498\n",
      "epoch: 10 step: 227, loss is 0.015938546508550644\n",
      "epoch: 10 step: 228, loss is 0.11965220421552658\n",
      "epoch: 10 step: 229, loss is 0.18761982023715973\n",
      "epoch: 10 step: 230, loss is 0.1719646006822586\n",
      "epoch: 10 step: 231, loss is 0.2002216875553131\n",
      "epoch: 10 step: 232, loss is 0.41828691959381104\n",
      "epoch: 10 step: 233, loss is 0.04822351410984993\n",
      "epoch: 10 step: 234, loss is 0.15246324241161346\n",
      "epoch: 10 step: 235, loss is 0.21822860836982727\n",
      "epoch: 10 step: 236, loss is 0.05809275805950165\n",
      "epoch: 10 step: 237, loss is 0.05184924229979515\n",
      "epoch: 10 step: 238, loss is 0.04116032272577286\n",
      "epoch: 10 step: 239, loss is 0.06361328065395355\n",
      "epoch: 10 step: 240, loss is 0.2774360179901123\n",
      "epoch: 10 step: 241, loss is 0.19435057044029236\n",
      "epoch: 10 step: 242, loss is 0.1939595341682434\n",
      "epoch: 10 step: 243, loss is 0.075904980301857\n",
      "epoch: 10 step: 244, loss is 0.1728929728269577\n",
      "epoch: 10 step: 245, loss is 0.16484428942203522\n",
      "epoch: 10 step: 246, loss is 0.07205329090356827\n",
      "epoch: 10 step: 247, loss is 0.275433212518692\n",
      "epoch: 10 step: 248, loss is 0.6245278716087341\n",
      "epoch: 10 step: 249, loss is 0.022357335314154625\n",
      "epoch: 10 step: 250, loss is 0.3802814781665802\n",
      "epoch: 10 step: 251, loss is 0.21294762194156647\n",
      "epoch: 10 step: 252, loss is 0.04765748977661133\n",
      "epoch: 10 step: 253, loss is 0.1842317432165146\n",
      "epoch: 10 step: 254, loss is 0.10403469949960709\n",
      "epoch: 10 step: 255, loss is 0.596616804599762\n",
      "epoch: 10 step: 256, loss is 0.28144755959510803\n",
      "epoch: 10 step: 257, loss is 0.44423922896385193\n",
      "epoch: 10 step: 258, loss is 0.24029795825481415\n",
      "epoch: 10 step: 259, loss is 0.06228721886873245\n",
      "epoch: 10 step: 260, loss is 0.04440746828913689\n",
      "epoch: 10 step: 261, loss is 0.21231578290462494\n",
      "epoch: 10 step: 262, loss is 0.037747982889413834\n",
      "epoch: 10 step: 263, loss is 0.028297847136855125\n",
      "epoch: 10 step: 264, loss is 0.3276265263557434\n",
      "epoch: 10 step: 265, loss is 0.2773141860961914\n",
      "epoch: 10 step: 266, loss is 0.5399078726768494\n",
      "epoch: 10 step: 267, loss is 0.2092721164226532\n",
      "epoch: 10 step: 268, loss is 0.5965141654014587\n",
      "epoch: 10 step: 269, loss is 0.3004004955291748\n",
      "epoch: 10 step: 270, loss is 0.04242236167192459\n",
      "epoch: 10 step: 271, loss is 0.5369102954864502\n",
      "epoch: 10 step: 272, loss is 0.21349717676639557\n",
      "epoch: 10 step: 273, loss is 0.20399144291877747\n",
      "epoch: 10 step: 274, loss is 0.21664036810398102\n",
      "epoch: 10 step: 275, loss is 0.2631939649581909\n",
      "epoch: 10 step: 276, loss is 0.2390403002500534\n",
      "epoch: 10 step: 277, loss is 0.3168628215789795\n",
      "epoch: 10 step: 278, loss is 0.1693064272403717\n",
      "epoch: 10 step: 279, loss is 0.01743118092417717\n",
      "epoch: 10 step: 280, loss is 0.029882049188017845\n",
      "epoch: 10 step: 281, loss is 0.14654262363910675\n",
      "epoch: 10 step: 282, loss is 0.07336578518152237\n",
      "epoch: 10 step: 283, loss is 0.10977575182914734\n",
      "epoch: 10 step: 284, loss is 0.06654807180166245\n",
      "epoch: 10 step: 285, loss is 0.08080491423606873\n",
      "epoch: 10 step: 286, loss is 0.11280152201652527\n",
      "epoch: 10 step: 287, loss is 0.09074947983026505\n",
      "epoch: 10 step: 288, loss is 0.02786758355796337\n",
      "epoch: 10 step: 289, loss is 0.09356208890676498\n",
      "epoch: 10 step: 290, loss is 0.1807962954044342\n",
      "epoch: 10 step: 291, loss is 0.0029066901188343763\n",
      "epoch: 10 step: 292, loss is 0.1626747101545334\n",
      "epoch: 10 step: 293, loss is 0.20294590294361115\n",
      "epoch: 10 step: 294, loss is 0.5146554112434387\n",
      "epoch: 10 step: 295, loss is 0.3586789071559906\n",
      "epoch: 10 step: 296, loss is 0.11860077828168869\n",
      "epoch: 10 step: 297, loss is 0.14092226326465607\n",
      "epoch: 10 step: 298, loss is 0.04362872242927551\n",
      "epoch: 10 step: 299, loss is 0.011869378387928009\n",
      "epoch: 10 step: 300, loss is 0.13653381168842316\n",
      "epoch: 10 step: 301, loss is 0.021005796268582344\n",
      "epoch: 10 step: 302, loss is 0.028005070984363556\n",
      "epoch: 10 step: 303, loss is 0.03920348733663559\n",
      "epoch: 10 step: 304, loss is 0.20982962846755981\n",
      "epoch: 10 step: 305, loss is 0.5528385043144226\n",
      "epoch: 10 step: 306, loss is 0.16053558886051178\n",
      "epoch: 10 step: 307, loss is 0.4723459780216217\n",
      "epoch: 10 step: 308, loss is 0.27577370405197144\n",
      "epoch: 10 step: 309, loss is 0.07824644446372986\n",
      "epoch: 10 step: 310, loss is 0.03170042112469673\n",
      "epoch: 10 step: 311, loss is 0.1434757113456726\n",
      "epoch: 10 step: 312, loss is 0.12645192444324493\n",
      "epoch: 10 step: 313, loss is 0.15772691369056702\n",
      "epoch: 10 step: 314, loss is 0.10833121836185455\n",
      "epoch: 10 step: 315, loss is 0.053263626992702484\n",
      "epoch: 10 step: 316, loss is 0.21224941313266754\n",
      "epoch: 10 step: 317, loss is 0.13728664815425873\n",
      "epoch: 10 step: 318, loss is 0.27691230177879333\n",
      "epoch: 10 step: 319, loss is 0.8469479084014893\n",
      "epoch: 10 step: 320, loss is 0.0884837657213211\n",
      "epoch: 10 step: 321, loss is 0.07344675809144974\n",
      "epoch: 10 step: 322, loss is 0.014001281931996346\n",
      "epoch: 10 step: 323, loss is 0.06629340350627899\n",
      "epoch: 10 step: 324, loss is 0.07733064889907837\n",
      "epoch: 10 step: 325, loss is 0.44161444902420044\n",
      "epoch: 10 step: 326, loss is 0.2545184791088104\n",
      "epoch: 10 step: 327, loss is 0.011921829544007778\n",
      "epoch: 10 step: 328, loss is 0.052867863327264786\n",
      "epoch: 10 step: 329, loss is 0.3831920027732849\n",
      "epoch: 10 step: 330, loss is 0.23107168078422546\n",
      "epoch: 10 step: 331, loss is 0.008666851557791233\n",
      "epoch: 10 step: 332, loss is 0.014584330841898918\n",
      "epoch: 10 step: 333, loss is 0.11836623400449753\n",
      "epoch: 10 step: 334, loss is 0.05277731269598007\n",
      "epoch: 10 step: 335, loss is 0.09177828580141068\n",
      "epoch: 10 step: 336, loss is 0.2795385718345642\n",
      "epoch: 10 step: 337, loss is 0.08812855184078217\n",
      "epoch: 10 step: 338, loss is 0.05408031493425369\n",
      "epoch: 10 step: 339, loss is 0.10122457891702652\n",
      "epoch: 10 step: 340, loss is 0.28730276226997375\n",
      "epoch: 10 step: 341, loss is 0.4026796817779541\n",
      "epoch: 10 step: 342, loss is 0.04919440671801567\n",
      "epoch: 10 step: 343, loss is 0.1159757450222969\n",
      "epoch: 10 step: 344, loss is 0.09579864144325256\n",
      "epoch: 10 step: 345, loss is 0.02756674401462078\n",
      "epoch: 10 step: 346, loss is 0.11064818501472473\n",
      "epoch: 10 step: 347, loss is 0.35025301575660706\n",
      "epoch: 10 step: 348, loss is 0.02009444870054722\n",
      "epoch: 10 step: 349, loss is 0.041130680590867996\n",
      "epoch: 10 step: 350, loss is 0.10136646032333374\n",
      "epoch: 10 step: 351, loss is 0.04470400512218475\n",
      "epoch: 10 step: 352, loss is 0.16862402856349945\n",
      "epoch: 10 step: 353, loss is 0.38424283266067505\n",
      "epoch: 10 step: 354, loss is 0.13907362520694733\n",
      "epoch: 10 step: 355, loss is 0.21586337685585022\n",
      "epoch: 10 step: 356, loss is 0.37248408794403076\n",
      "epoch: 10 step: 357, loss is 0.07229482382535934\n",
      "epoch: 10 step: 358, loss is 0.04970814660191536\n",
      "epoch: 10 step: 359, loss is 0.15378713607788086\n",
      "epoch: 10 step: 360, loss is 0.3403555154800415\n",
      "epoch: 10 step: 361, loss is 0.2522677779197693\n",
      "epoch: 10 step: 362, loss is 0.1316351741552353\n",
      "epoch: 10 step: 363, loss is 0.07163093984127045\n",
      "epoch: 10 step: 364, loss is 0.07943263649940491\n",
      "epoch: 10 step: 365, loss is 0.24758104979991913\n",
      "epoch: 10 step: 366, loss is 0.3573910892009735\n",
      "epoch: 10 step: 367, loss is 0.041490115225315094\n",
      "epoch: 10 step: 368, loss is 0.2976292073726654\n",
      "epoch: 10 step: 369, loss is 0.010354840196669102\n",
      "epoch: 10 step: 370, loss is 1.2287495136260986\n",
      "epoch: 10 step: 371, loss is 0.10706894099712372\n",
      "epoch: 10 step: 372, loss is 0.6280907988548279\n",
      "epoch: 10 step: 373, loss is 0.04425535351037979\n",
      "epoch: 10 step: 374, loss is 0.25575947761535645\n",
      "epoch: 10 step: 375, loss is 0.3990904688835144\n",
      "epoch: 10 step: 376, loss is 0.0337434858083725\n",
      "epoch: 10 step: 377, loss is 0.652378499507904\n",
      "epoch: 10 step: 378, loss is 0.16079972684383392\n",
      "epoch: 10 step: 379, loss is 0.035036418586969376\n",
      "epoch: 10 step: 380, loss is 0.08700837194919586\n",
      "epoch: 10 step: 381, loss is 0.10435386002063751\n",
      "epoch: 10 step: 382, loss is 0.1734832227230072\n",
      "epoch: 10 step: 383, loss is 0.25683167576789856\n",
      "epoch: 10 step: 384, loss is 0.09860949963331223\n",
      "epoch: 10 step: 385, loss is 0.26650166511535645\n",
      "epoch: 10 step: 386, loss is 0.16572001576423645\n",
      "epoch: 10 step: 387, loss is 1.3731714487075806\n",
      "epoch: 10 step: 388, loss is 0.16071385145187378\n",
      "epoch: 10 step: 389, loss is 0.24951809644699097\n",
      "epoch: 10 step: 390, loss is 0.017380407080054283\n",
      "epoch: 10 step: 391, loss is 0.1059591993689537\n",
      "epoch: 10 step: 392, loss is 0.12051700055599213\n",
      "epoch: 10 step: 393, loss is 0.5381681323051453\n",
      "epoch: 10 step: 394, loss is 0.27199792861938477\n",
      "epoch: 10 step: 395, loss is 0.2616393268108368\n",
      "epoch: 10 step: 396, loss is 0.3659747838973999\n",
      "epoch: 10 step: 397, loss is 0.13252946734428406\n",
      "epoch: 10 step: 398, loss is 0.21386714279651642\n",
      "epoch: 10 step: 399, loss is 0.10096105188131332\n",
      "epoch: 10 step: 400, loss is 0.15080758929252625\n",
      "epoch: 10 step: 401, loss is 0.022016432136297226\n",
      "epoch: 10 step: 402, loss is 0.1374177783727646\n",
      "epoch: 10 step: 403, loss is 0.24405734241008759\n",
      "epoch: 10 step: 404, loss is 0.26837143301963806\n",
      "epoch: 10 step: 405, loss is 0.15027181804180145\n",
      "epoch: 10 step: 406, loss is 0.27781566977500916\n",
      "epoch: 10 step: 407, loss is 0.1557091772556305\n",
      "epoch: 10 step: 408, loss is 0.37976476550102234\n",
      "epoch: 10 step: 409, loss is 0.43902337551116943\n",
      "epoch: 10 step: 410, loss is 0.15471243858337402\n",
      "epoch: 10 step: 411, loss is 0.03502574935555458\n",
      "epoch: 10 step: 412, loss is 0.36598560214042664\n",
      "epoch: 10 step: 413, loss is 0.06141868233680725\n",
      "epoch: 10 step: 414, loss is 0.09467094391584396\n",
      "epoch: 10 step: 415, loss is 0.07666801661252975\n",
      "epoch: 10 step: 416, loss is 0.14085796475410461\n",
      "epoch: 10 step: 417, loss is 0.05103958770632744\n",
      "epoch: 10 step: 418, loss is 0.09670518338680267\n",
      "epoch: 10 step: 419, loss is 0.06607874482870102\n",
      "epoch: 10 step: 420, loss is 0.09573198109865189\n",
      "epoch: 10 step: 421, loss is 0.3599719703197479\n",
      "epoch: 10 step: 422, loss is 0.04775505140423775\n",
      "epoch: 10 step: 423, loss is 0.11279807239770889\n",
      "epoch: 10 step: 424, loss is 0.07060825079679489\n",
      "epoch: 10 step: 425, loss is 0.5993981957435608\n",
      "epoch: 10 step: 426, loss is 0.13290882110595703\n",
      "epoch: 10 step: 427, loss is 0.0671340748667717\n",
      "epoch: 10 step: 428, loss is 0.03763943538069725\n",
      "epoch: 10 step: 429, loss is 0.1701369285583496\n",
      "epoch: 10 step: 430, loss is 0.02688807249069214\n",
      "epoch: 10 step: 431, loss is 0.5740408897399902\n",
      "epoch: 10 step: 432, loss is 0.05741800367832184\n",
      "epoch: 10 step: 433, loss is 0.08760099112987518\n",
      "epoch: 10 step: 434, loss is 0.5424278974533081\n",
      "epoch: 10 step: 435, loss is 0.14205728471279144\n",
      "epoch: 10 step: 436, loss is 0.049929920583963394\n",
      "epoch: 10 step: 437, loss is 0.10950525850057602\n",
      "epoch: 10 step: 438, loss is 0.37277063727378845\n",
      "epoch: 10 step: 439, loss is 0.13298358023166656\n",
      "epoch: 10 step: 440, loss is 0.12086497247219086\n",
      "epoch: 10 step: 441, loss is 0.048289958387613297\n",
      "epoch: 10 step: 442, loss is 0.022010888904333115\n",
      "epoch: 10 step: 443, loss is 0.17379528284072876\n",
      "epoch: 10 step: 444, loss is 0.08082074671983719\n",
      "epoch: 10 step: 445, loss is 0.04516011103987694\n",
      "epoch: 10 step: 446, loss is 0.14851774275302887\n",
      "epoch: 10 step: 447, loss is 0.014456731267273426\n",
      "epoch: 10 step: 448, loss is 0.08955490589141846\n",
      "epoch: 10 step: 449, loss is 0.10660015046596527\n",
      "epoch: 10 step: 450, loss is 0.00953441672027111\n",
      "epoch: 10 step: 451, loss is 0.20282399654388428\n",
      "epoch: 10 step: 452, loss is 0.12482760101556778\n",
      "epoch: 10 step: 453, loss is 0.11583219468593597\n",
      "epoch: 10 step: 454, loss is 0.16903682053089142\n",
      "epoch: 10 step: 455, loss is 0.0353974923491478\n",
      "epoch: 10 step: 456, loss is 0.22379091382026672\n",
      "epoch: 10 step: 457, loss is 0.23907478153705597\n",
      "epoch: 10 step: 458, loss is 0.7300441861152649\n",
      "epoch: 10 step: 459, loss is 0.14342600107192993\n",
      "epoch: 10 step: 460, loss is 0.16441087424755096\n",
      "epoch: 10 step: 461, loss is 0.14065156877040863\n",
      "epoch: 10 step: 462, loss is 0.19109690189361572\n",
      "epoch: 10 step: 463, loss is 0.020395221188664436\n",
      "epoch: 10 step: 464, loss is 0.0035645663738250732\n",
      "epoch: 10 step: 465, loss is 0.07244177907705307\n",
      "epoch: 10 step: 466, loss is 0.042417313903570175\n",
      "epoch: 10 step: 467, loss is 0.0987764447927475\n",
      "epoch: 10 step: 468, loss is 0.048191435635089874\n",
      "epoch: 10 step: 469, loss is 0.03472783416509628\n",
      "epoch: 10 step: 470, loss is 0.013036220334470272\n",
      "epoch: 10 step: 471, loss is 0.33532780408859253\n",
      "epoch: 10 step: 472, loss is 0.0020556761883199215\n",
      "epoch: 10 step: 473, loss is 0.07405008375644684\n",
      "epoch: 10 step: 474, loss is 0.057688575237989426\n",
      "epoch: 10 step: 475, loss is 0.10194435715675354\n",
      "epoch: 10 step: 476, loss is 0.01217589620500803\n",
      "epoch: 10 step: 477, loss is 0.5878474712371826\n",
      "epoch: 10 step: 478, loss is 0.23089341819286346\n",
      "epoch: 10 step: 479, loss is 0.057533539831638336\n",
      "epoch: 10 step: 480, loss is 0.0026827927213162184\n",
      "epoch: 10 step: 481, loss is 0.028726717457175255\n",
      "epoch: 10 step: 482, loss is 0.2164762020111084\n",
      "epoch: 10 step: 483, loss is 0.3982241749763489\n",
      "epoch: 10 step: 484, loss is 0.1704336702823639\n",
      "epoch: 10 step: 485, loss is 0.16404643654823303\n",
      "epoch: 10 step: 486, loss is 0.01937851868569851\n",
      "epoch: 10 step: 487, loss is 0.21329818665981293\n",
      "epoch: 10 step: 488, loss is 0.42665398120880127\n",
      "epoch: 10 step: 489, loss is 0.2048693597316742\n",
      "epoch: 10 step: 490, loss is 0.014600957743823528\n",
      "epoch: 10 step: 491, loss is 0.23742561042308807\n",
      "epoch: 10 step: 492, loss is 0.26844173669815063\n",
      "epoch: 10 step: 493, loss is 0.07042095810174942\n",
      "epoch: 10 step: 494, loss is 0.2762046456336975\n",
      "epoch: 10 step: 495, loss is 0.26151707768440247\n",
      "epoch: 10 step: 496, loss is 0.5387033820152283\n",
      "epoch: 10 step: 497, loss is 0.04469987004995346\n",
      "epoch: 10 step: 498, loss is 0.4985768496990204\n",
      "epoch: 10 step: 499, loss is 0.02869110181927681\n",
      "epoch: 10 step: 500, loss is 0.27981293201446533\n",
      "epoch: 10 step: 501, loss is 0.02122832089662552\n",
      "epoch: 10 step: 502, loss is 0.7672711610794067\n",
      "epoch: 10 step: 503, loss is 0.12455140799283981\n",
      "epoch: 10 step: 504, loss is 0.017048871144652367\n",
      "epoch: 10 step: 505, loss is 0.018749888986349106\n",
      "epoch: 10 step: 506, loss is 0.07151174545288086\n",
      "epoch: 10 step: 507, loss is 0.17255482077598572\n",
      "epoch: 10 step: 508, loss is 0.516944944858551\n",
      "epoch: 10 step: 509, loss is 0.20725871622562408\n",
      "epoch: 10 step: 510, loss is 0.10653712600469589\n",
      "epoch: 10 step: 511, loss is 0.13416971266269684\n",
      "epoch: 10 step: 512, loss is 0.009044003672897816\n",
      "epoch: 10 step: 513, loss is 0.5524341464042664\n",
      "epoch: 10 step: 514, loss is 0.0699516236782074\n",
      "epoch: 10 step: 515, loss is 0.0685548484325409\n",
      "epoch: 10 step: 516, loss is 0.3448483645915985\n",
      "epoch: 10 step: 517, loss is 0.25313800573349\n",
      "epoch: 10 step: 518, loss is 0.16671375930309296\n",
      "epoch: 10 step: 519, loss is 0.13166765868663788\n",
      "epoch: 10 step: 520, loss is 0.031874023377895355\n",
      "epoch: 10 step: 521, loss is 0.032813362777233124\n",
      "epoch: 10 step: 522, loss is 0.07897154986858368\n",
      "epoch: 10 step: 523, loss is 0.08965256065130234\n",
      "epoch: 10 step: 524, loss is 0.2532430589199066\n",
      "epoch: 10 step: 525, loss is 0.23007561266422272\n",
      "epoch: 10 step: 526, loss is 0.2788945734500885\n",
      "epoch: 10 step: 527, loss is 0.5994623303413391\n",
      "epoch: 10 step: 528, loss is 0.08623147010803223\n",
      "epoch: 10 step: 529, loss is 0.10491617769002914\n",
      "epoch: 10 step: 530, loss is 0.1476445347070694\n",
      "epoch: 10 step: 531, loss is 0.020421793684363365\n",
      "epoch: 10 step: 532, loss is 0.020157426595687866\n",
      "epoch: 10 step: 533, loss is 0.036719661206007004\n",
      "epoch: 10 step: 534, loss is 0.2819841802120209\n",
      "epoch: 10 step: 535, loss is 0.18872982263565063\n",
      "epoch: 10 step: 536, loss is 0.019764374941587448\n",
      "epoch: 10 step: 537, loss is 0.5148147344589233\n",
      "epoch: 10 step: 538, loss is 0.09454651921987534\n",
      "epoch: 10 step: 539, loss is 0.09193562716245651\n",
      "epoch: 10 step: 540, loss is 0.21062922477722168\n",
      "epoch: 10 step: 541, loss is 0.38164767622947693\n",
      "epoch: 10 step: 542, loss is 0.2685610055923462\n",
      "epoch: 10 step: 543, loss is 0.33505767583847046\n",
      "epoch: 10 step: 544, loss is 0.06070197746157646\n",
      "epoch: 10 step: 545, loss is 0.3632503151893616\n",
      "epoch: 10 step: 546, loss is 0.18289430439472198\n",
      "epoch: 10 step: 547, loss is 0.15322601795196533\n",
      "epoch: 10 step: 548, loss is 0.054869022220373154\n",
      "epoch: 10 step: 549, loss is 0.048427168279886246\n",
      "epoch: 10 step: 550, loss is 0.2243688553571701\n",
      "epoch: 10 step: 551, loss is 0.3165076971054077\n",
      "epoch: 10 step: 552, loss is 0.04456028714776039\n",
      "epoch: 10 step: 553, loss is 0.180748850107193\n",
      "epoch: 10 step: 554, loss is 0.08293145895004272\n",
      "epoch: 10 step: 555, loss is 0.01163983903825283\n",
      "epoch: 10 step: 556, loss is 0.7511075139045715\n",
      "epoch: 10 step: 557, loss is 0.3042677938938141\n",
      "epoch: 10 step: 558, loss is 0.041005175560712814\n",
      "epoch: 10 step: 559, loss is 0.04666035249829292\n",
      "epoch: 10 step: 560, loss is 0.03463207557797432\n",
      "epoch: 10 step: 561, loss is 0.11220148950815201\n",
      "epoch: 10 step: 562, loss is 0.02884998731315136\n",
      "epoch: 10 step: 563, loss is 0.03920814022421837\n",
      "epoch: 10 step: 564, loss is 0.045349832624197006\n",
      "epoch: 10 step: 565, loss is 0.04526227340102196\n",
      "epoch: 10 step: 566, loss is 0.33318641781806946\n",
      "epoch: 10 step: 567, loss is 0.1388564109802246\n",
      "epoch: 10 step: 568, loss is 0.10875756293535233\n",
      "epoch: 10 step: 569, loss is 0.1970076560974121\n",
      "epoch: 10 step: 570, loss is 0.16864408552646637\n",
      "epoch: 10 step: 571, loss is 0.5217888355255127\n",
      "epoch: 10 step: 572, loss is 0.3518667221069336\n",
      "epoch: 10 step: 573, loss is 0.03929442539811134\n",
      "epoch: 10 step: 574, loss is 0.4288756549358368\n",
      "epoch: 10 step: 575, loss is 0.13764850795269012\n",
      "epoch: 10 step: 576, loss is 0.06820850819349289\n",
      "epoch: 10 step: 577, loss is 0.11445418745279312\n",
      "epoch: 10 step: 578, loss is 0.028980350121855736\n",
      "epoch: 10 step: 579, loss is 0.2617682218551636\n",
      "epoch: 10 step: 580, loss is 0.17616653442382812\n",
      "epoch: 10 step: 581, loss is 0.03822886198759079\n",
      "epoch: 10 step: 582, loss is 0.11827728152275085\n",
      "epoch: 10 step: 583, loss is 0.10528843849897385\n",
      "epoch: 10 step: 584, loss is 0.11806660145521164\n",
      "epoch: 10 step: 585, loss is 0.2418850064277649\n",
      "epoch: 10 step: 586, loss is 0.12638166546821594\n",
      "epoch: 10 step: 587, loss is 0.050620127469301224\n",
      "epoch: 10 step: 588, loss is 0.36636579036712646\n",
      "epoch: 10 step: 589, loss is 0.050949592143297195\n",
      "epoch: 10 step: 590, loss is 0.04436544328927994\n",
      "epoch: 10 step: 591, loss is 0.01659143529832363\n",
      "epoch: 10 step: 592, loss is 0.11404277384281158\n",
      "epoch: 10 step: 593, loss is 0.3608992397785187\n",
      "epoch: 10 step: 594, loss is 0.07402513921260834\n",
      "epoch: 10 step: 595, loss is 0.5228103399276733\n",
      "epoch: 10 step: 596, loss is 0.4711248278617859\n",
      "epoch: 10 step: 597, loss is 0.15547111630439758\n",
      "epoch: 10 step: 598, loss is 0.22332219779491425\n",
      "epoch: 10 step: 599, loss is 0.9246503710746765\n",
      "epoch: 10 step: 600, loss is 0.1104588732123375\n",
      "epoch: 10 step: 601, loss is 0.033543433994054794\n",
      "epoch: 10 step: 602, loss is 0.1294594258069992\n",
      "epoch: 10 step: 603, loss is 0.06315606832504272\n",
      "epoch: 10 step: 604, loss is 0.08557941019535065\n",
      "epoch: 10 step: 605, loss is 0.21479184925556183\n",
      "epoch: 10 step: 606, loss is 0.4683763384819031\n",
      "epoch: 10 step: 607, loss is 0.7015902996063232\n",
      "epoch: 10 step: 608, loss is 0.13805440068244934\n",
      "epoch: 10 step: 609, loss is 0.25734925270080566\n",
      "epoch: 10 step: 610, loss is 0.1568366438150406\n",
      "epoch: 10 step: 611, loss is 0.2958456873893738\n",
      "epoch: 10 step: 612, loss is 0.048721637576818466\n",
      "epoch: 10 step: 613, loss is 0.020810628309845924\n",
      "epoch: 10 step: 614, loss is 0.06078965216875076\n",
      "epoch: 10 step: 615, loss is 0.09351653605699539\n",
      "epoch: 10 step: 616, loss is 0.6126352548599243\n",
      "epoch: 10 step: 617, loss is 0.29445451498031616\n",
      "epoch: 10 step: 618, loss is 0.2766490876674652\n",
      "epoch: 10 step: 619, loss is 0.25746870040893555\n",
      "epoch: 10 step: 620, loss is 0.4089145064353943\n",
      "epoch: 10 step: 621, loss is 0.039268579334020615\n",
      "epoch: 10 step: 622, loss is 0.12958206236362457\n",
      "epoch: 10 step: 623, loss is 0.09396298974752426\n",
      "epoch: 10 step: 624, loss is 0.316670686006546\n",
      "epoch: 10 step: 625, loss is 0.08101595193147659\n",
      "epoch: 10 step: 626, loss is 0.13794949650764465\n",
      "epoch: 10 step: 627, loss is 0.01267301570624113\n",
      "epoch: 10 step: 628, loss is 0.05702873691916466\n",
      "epoch: 10 step: 629, loss is 0.37321048974990845\n",
      "epoch: 10 step: 630, loss is 0.12933596968650818\n",
      "epoch: 10 step: 631, loss is 0.424835205078125\n",
      "epoch: 10 step: 632, loss is 0.020739663392305374\n",
      "epoch: 10 step: 633, loss is 0.02039448171854019\n",
      "epoch: 10 step: 634, loss is 0.06964042782783508\n",
      "epoch: 10 step: 635, loss is 0.08045680075883865\n",
      "epoch: 10 step: 636, loss is 0.09708899259567261\n",
      "epoch: 10 step: 637, loss is 0.05455409362912178\n",
      "epoch: 10 step: 638, loss is 0.3294690251350403\n",
      "epoch: 10 step: 639, loss is 0.2215905785560608\n",
      "epoch: 10 step: 640, loss is 0.10000225156545639\n",
      "epoch: 10 step: 641, loss is 0.04921061173081398\n",
      "epoch: 10 step: 642, loss is 0.09819811582565308\n",
      "epoch: 10 step: 643, loss is 0.027031082659959793\n",
      "epoch: 10 step: 644, loss is 0.1199001744389534\n",
      "epoch: 10 step: 645, loss is 0.44587746262550354\n",
      "epoch: 10 step: 646, loss is 0.19300873577594757\n",
      "epoch: 10 step: 647, loss is 0.03710617870092392\n",
      "epoch: 10 step: 648, loss is 0.1418744921684265\n",
      "epoch: 10 step: 649, loss is 0.14283163845539093\n",
      "epoch: 10 step: 650, loss is 0.017260318621993065\n",
      "epoch: 10 step: 651, loss is 0.04912736639380455\n",
      "epoch: 10 step: 652, loss is 0.02897724136710167\n",
      "epoch: 10 step: 653, loss is 0.3126630485057831\n",
      "epoch: 10 step: 654, loss is 0.20386888086795807\n",
      "epoch: 10 step: 655, loss is 0.10021743923425674\n",
      "epoch: 10 step: 656, loss is 0.12272907048463821\n",
      "epoch: 10 step: 657, loss is 0.043073464184999466\n",
      "epoch: 10 step: 658, loss is 0.024658873677253723\n",
      "epoch: 10 step: 659, loss is 0.1294364035129547\n",
      "epoch: 10 step: 660, loss is 0.013370017521083355\n",
      "epoch: 10 step: 661, loss is 0.1299830973148346\n",
      "epoch: 10 step: 662, loss is 0.26890861988067627\n",
      "epoch: 10 step: 663, loss is 0.02135331742465496\n",
      "epoch: 10 step: 664, loss is 0.2585946321487427\n",
      "epoch: 10 step: 665, loss is 0.28111138939857483\n",
      "epoch: 10 step: 666, loss is 0.15514472126960754\n",
      "epoch: 10 step: 667, loss is 0.1053951159119606\n",
      "epoch: 10 step: 668, loss is 0.2702610492706299\n",
      "epoch: 10 step: 669, loss is 0.046760182827711105\n",
      "epoch: 10 step: 670, loss is 0.46884194016456604\n",
      "epoch: 10 step: 671, loss is 0.3454431891441345\n",
      "epoch: 10 step: 672, loss is 0.027634719386696815\n",
      "epoch: 10 step: 673, loss is 0.10855372995138168\n",
      "epoch: 10 step: 674, loss is 0.02020229399204254\n",
      "epoch: 10 step: 675, loss is 0.3073761463165283\n",
      "epoch: 10 step: 676, loss is 0.18761220574378967\n",
      "epoch: 10 step: 677, loss is 0.1361038088798523\n",
      "epoch: 10 step: 678, loss is 0.6247506141662598\n",
      "epoch: 10 step: 679, loss is 0.35227200388908386\n",
      "epoch: 10 step: 680, loss is 0.20732276141643524\n",
      "epoch: 10 step: 681, loss is 0.007450192701071501\n",
      "epoch: 10 step: 682, loss is 0.21843235194683075\n",
      "epoch: 10 step: 683, loss is 0.5065761208534241\n",
      "epoch: 10 step: 684, loss is 0.18147946894168854\n",
      "epoch: 10 step: 685, loss is 0.03388453647494316\n",
      "epoch: 10 step: 686, loss is 0.03173382207751274\n",
      "epoch: 10 step: 687, loss is 0.33018070459365845\n",
      "epoch: 10 step: 688, loss is 0.2872568666934967\n",
      "epoch: 10 step: 689, loss is 0.25195157527923584\n",
      "epoch: 10 step: 690, loss is 0.14783358573913574\n",
      "epoch: 10 step: 691, loss is 0.18406885862350464\n",
      "epoch: 10 step: 692, loss is 0.298806756734848\n",
      "epoch: 10 step: 693, loss is 0.15301652252674103\n",
      "epoch: 10 step: 694, loss is 0.14784066379070282\n",
      "epoch: 10 step: 695, loss is 0.12253427505493164\n",
      "epoch: 10 step: 696, loss is 0.31500333547592163\n",
      "epoch: 10 step: 697, loss is 0.07928018271923065\n",
      "epoch: 10 step: 698, loss is 0.4592066705226898\n",
      "epoch: 10 step: 699, loss is 0.11060767620801926\n",
      "epoch: 10 step: 700, loss is 0.11176333576440811\n",
      "epoch: 10 step: 701, loss is 0.06576099991798401\n",
      "epoch: 10 step: 702, loss is 0.808578610420227\n",
      "epoch: 10 step: 703, loss is 1.072623372077942\n",
      "epoch: 10 step: 704, loss is 0.16466248035430908\n",
      "epoch: 10 step: 705, loss is 0.14705729484558105\n",
      "epoch: 10 step: 706, loss is 0.05150137469172478\n",
      "epoch: 10 step: 707, loss is 0.21936491131782532\n",
      "epoch: 10 step: 708, loss is 0.14237207174301147\n",
      "epoch: 10 step: 709, loss is 0.17085780203342438\n",
      "epoch: 10 step: 710, loss is 0.022091316059231758\n",
      "epoch: 10 step: 711, loss is 0.011341694742441177\n",
      "epoch: 10 step: 712, loss is 0.19833365082740784\n",
      "epoch: 10 step: 713, loss is 0.014390075579285622\n",
      "epoch: 10 step: 714, loss is 0.23653259873390198\n",
      "epoch: 10 step: 715, loss is 0.39360108971595764\n",
      "epoch: 10 step: 716, loss is 0.15354293584823608\n",
      "epoch: 10 step: 717, loss is 0.44582241773605347\n",
      "epoch: 10 step: 718, loss is 0.20805244147777557\n",
      "epoch: 10 step: 719, loss is 0.0797225832939148\n",
      "epoch: 10 step: 720, loss is 0.2806008458137512\n",
      "epoch: 10 step: 721, loss is 0.1839069277048111\n",
      "epoch: 10 step: 722, loss is 0.25973525643348694\n",
      "epoch: 10 step: 723, loss is 0.0028957948088645935\n",
      "epoch: 10 step: 724, loss is 0.08646619319915771\n",
      "epoch: 10 step: 725, loss is 0.34741947054862976\n",
      "epoch: 10 step: 726, loss is 0.13109475374221802\n",
      "epoch: 10 step: 727, loss is 0.03474755957722664\n",
      "epoch: 10 step: 728, loss is 0.05083230510354042\n",
      "epoch: 10 step: 729, loss is 0.1431322991847992\n",
      "epoch: 10 step: 730, loss is 0.040227364748716354\n",
      "epoch: 10 step: 731, loss is 0.04372033104300499\n",
      "epoch: 10 step: 732, loss is 0.1474873274564743\n",
      "epoch: 10 step: 733, loss is 0.06356269866228104\n",
      "epoch: 10 step: 734, loss is 0.20423299074172974\n",
      "epoch: 10 step: 735, loss is 0.034025341272354126\n",
      "epoch: 10 step: 736, loss is 0.20919382572174072\n",
      "epoch: 10 step: 737, loss is 0.2760077118873596\n",
      "epoch: 10 step: 738, loss is 0.14050456881523132\n",
      "epoch: 10 step: 739, loss is 0.2453993707895279\n",
      "epoch: 10 step: 740, loss is 0.05134066939353943\n",
      "epoch: 10 step: 741, loss is 0.025794828310608864\n",
      "epoch: 10 step: 742, loss is 0.15062212944030762\n",
      "epoch: 10 step: 743, loss is 0.2556924521923065\n",
      "epoch: 10 step: 744, loss is 0.39583560824394226\n",
      "epoch: 10 step: 745, loss is 0.017911667004227638\n",
      "epoch: 10 step: 746, loss is 0.050550442188978195\n",
      "epoch: 10 step: 747, loss is 0.04305534064769745\n",
      "epoch: 10 step: 748, loss is 0.050416499376297\n",
      "epoch: 10 step: 749, loss is 0.05743487551808357\n",
      "epoch: 10 step: 750, loss is 0.006095175631344318\n",
      "epoch: 10 step: 751, loss is 0.14524775743484497\n",
      "epoch: 10 step: 752, loss is 0.17966578900814056\n",
      "epoch: 10 step: 753, loss is 0.12725625932216644\n",
      "epoch: 10 step: 754, loss is 0.13097837567329407\n",
      "epoch: 10 step: 755, loss is 0.4156295955181122\n",
      "epoch: 10 step: 756, loss is 0.08960618078708649\n",
      "epoch: 10 step: 757, loss is 0.08106699585914612\n",
      "epoch: 10 step: 758, loss is 0.03697828948497772\n",
      "epoch: 10 step: 759, loss is 0.0049246144481003284\n",
      "epoch: 10 step: 760, loss is 0.018949376419186592\n",
      "epoch: 10 step: 761, loss is 0.005451798904687166\n",
      "epoch: 10 step: 762, loss is 0.03503676876425743\n",
      "epoch: 10 step: 763, loss is 0.020666897296905518\n",
      "epoch: 10 step: 764, loss is 0.03002263978123665\n",
      "epoch: 10 step: 765, loss is 0.18713517487049103\n",
      "epoch: 10 step: 766, loss is 0.0027987491339445114\n",
      "epoch: 10 step: 767, loss is 0.4279787540435791\n",
      "epoch: 10 step: 768, loss is 0.22037385404109955\n",
      "epoch: 10 step: 769, loss is 0.5399656891822815\n",
      "epoch: 10 step: 770, loss is 0.01114745158702135\n",
      "epoch: 10 step: 771, loss is 0.07763814926147461\n",
      "epoch: 10 step: 772, loss is 0.37640902400016785\n",
      "epoch: 10 step: 773, loss is 0.07571599632501602\n",
      "epoch: 10 step: 774, loss is 0.07766342908143997\n",
      "epoch: 10 step: 775, loss is 0.12100037932395935\n",
      "epoch: 10 step: 776, loss is 1.0062265396118164\n",
      "epoch: 10 step: 777, loss is 0.3457593023777008\n",
      "epoch: 10 step: 778, loss is 0.010024048388004303\n",
      "epoch: 10 step: 779, loss is 0.05360546335577965\n",
      "epoch: 10 step: 780, loss is 0.07659754902124405\n",
      "epoch: 10 step: 781, loss is 0.034798514097929\n",
      "epoch: 10 step: 782, loss is 0.2859897315502167\n",
      "epoch: 10 step: 783, loss is 0.663141667842865\n",
      "epoch: 10 step: 784, loss is 0.41137152910232544\n",
      "epoch: 10 step: 785, loss is 0.016239769756793976\n",
      "epoch: 10 step: 786, loss is 0.15220412611961365\n",
      "epoch: 10 step: 787, loss is 0.06003746762871742\n",
      "epoch: 10 step: 788, loss is 0.06767929345369339\n",
      "epoch: 10 step: 789, loss is 0.5367752313613892\n",
      "epoch: 10 step: 790, loss is 0.010170609690248966\n",
      "epoch: 10 step: 791, loss is 0.28417614102363586\n",
      "epoch: 10 step: 792, loss is 0.11717669665813446\n",
      "epoch: 10 step: 793, loss is 0.21762938797473907\n",
      "epoch: 10 step: 794, loss is 0.015079326927661896\n",
      "epoch: 10 step: 795, loss is 0.17087893187999725\n",
      "epoch: 10 step: 796, loss is 0.05580177903175354\n",
      "epoch: 10 step: 797, loss is 0.2230340838432312\n",
      "epoch: 10 step: 798, loss is 0.1488044410943985\n",
      "epoch: 10 step: 799, loss is 0.33670759201049805\n",
      "epoch: 10 step: 800, loss is 0.5719277858734131\n",
      "epoch: 10 step: 801, loss is 0.15822701156139374\n",
      "epoch: 10 step: 802, loss is 0.27359047532081604\n",
      "epoch: 10 step: 803, loss is 0.08708152920007706\n",
      "epoch: 10 step: 804, loss is 0.027329349890351295\n",
      "epoch: 10 step: 805, loss is 0.010525563731789589\n",
      "epoch: 10 step: 806, loss is 0.6083481311798096\n",
      "epoch: 10 step: 807, loss is 0.060722243040800095\n",
      "epoch: 10 step: 808, loss is 0.012829604558646679\n",
      "epoch: 10 step: 809, loss is 0.1511693149805069\n",
      "epoch: 10 step: 810, loss is 0.1752157211303711\n",
      "epoch: 10 step: 811, loss is 0.030291210860013962\n",
      "epoch: 10 step: 812, loss is 0.6203718781471252\n",
      "epoch: 10 step: 813, loss is 0.25548702478408813\n",
      "epoch: 10 step: 814, loss is 0.32314321398735046\n",
      "epoch: 10 step: 815, loss is 0.9091130495071411\n",
      "epoch: 10 step: 816, loss is 0.26625341176986694\n",
      "epoch: 10 step: 817, loss is 0.04324282705783844\n",
      "epoch: 10 step: 818, loss is 0.437804251909256\n",
      "epoch: 10 step: 819, loss is 0.44545212388038635\n",
      "epoch: 10 step: 820, loss is 0.36378633975982666\n",
      "epoch: 10 step: 821, loss is 0.4399634897708893\n",
      "epoch: 10 step: 822, loss is 0.13869604468345642\n",
      "epoch: 10 step: 823, loss is 0.03983353078365326\n",
      "epoch: 10 step: 824, loss is 0.12439610064029694\n",
      "epoch: 10 step: 825, loss is 0.20568227767944336\n",
      "epoch: 10 step: 826, loss is 0.078913114964962\n",
      "epoch: 10 step: 827, loss is 0.0996522530913353\n",
      "epoch: 10 step: 828, loss is 0.08293546736240387\n",
      "epoch: 10 step: 829, loss is 0.5618793368339539\n",
      "epoch: 10 step: 830, loss is 0.30981168150901794\n",
      "epoch: 10 step: 831, loss is 0.04893624037504196\n",
      "epoch: 10 step: 832, loss is 0.3081822395324707\n",
      "epoch: 10 step: 833, loss is 0.04331178963184357\n",
      "epoch: 10 step: 834, loss is 0.11055608093738556\n",
      "epoch: 10 step: 835, loss is 0.22291657328605652\n",
      "epoch: 10 step: 836, loss is 0.18957039713859558\n",
      "epoch: 10 step: 837, loss is 0.09334419667720795\n",
      "epoch: 10 step: 838, loss is 0.1291707307100296\n",
      "epoch: 10 step: 839, loss is 0.33001473546028137\n",
      "epoch: 10 step: 840, loss is 0.24871617555618286\n",
      "epoch: 10 step: 841, loss is 0.23120591044425964\n",
      "epoch: 10 step: 842, loss is 0.04088398069143295\n",
      "epoch: 10 step: 843, loss is 0.11269355565309525\n",
      "epoch: 10 step: 844, loss is 0.03206738084554672\n",
      "epoch: 10 step: 845, loss is 0.09528526663780212\n",
      "epoch: 10 step: 846, loss is 0.07529974728822708\n",
      "epoch: 10 step: 847, loss is 0.4799893796443939\n",
      "epoch: 10 step: 848, loss is 0.18485262989997864\n",
      "epoch: 10 step: 849, loss is 0.0594041533768177\n",
      "epoch: 10 step: 850, loss is 0.01751936972141266\n",
      "epoch: 10 step: 851, loss is 0.08852868527173996\n",
      "epoch: 10 step: 852, loss is 0.17553040385246277\n",
      "epoch: 10 step: 853, loss is 0.08134368062019348\n",
      "epoch: 10 step: 854, loss is 0.14937223494052887\n",
      "epoch: 10 step: 855, loss is 0.27843335270881653\n",
      "epoch: 10 step: 856, loss is 0.04410719871520996\n",
      "epoch: 10 step: 857, loss is 0.09127545356750488\n",
      "epoch: 10 step: 858, loss is 0.13044953346252441\n",
      "epoch: 10 step: 859, loss is 0.05469389259815216\n",
      "epoch: 10 step: 860, loss is 0.002935894764959812\n",
      "epoch: 10 step: 861, loss is 0.15013012290000916\n",
      "epoch: 10 step: 862, loss is 0.5150136947631836\n",
      "epoch: 10 step: 863, loss is 0.2591477632522583\n",
      "epoch: 10 step: 864, loss is 0.5663145184516907\n",
      "epoch: 10 step: 865, loss is 0.001728325616568327\n",
      "epoch: 10 step: 866, loss is 0.015047779306769371\n",
      "epoch: 10 step: 867, loss is 0.4084802269935608\n",
      "epoch: 10 step: 868, loss is 0.13037094473838806\n",
      "epoch: 10 step: 869, loss is 0.388136625289917\n",
      "epoch: 10 step: 870, loss is 0.01180177740752697\n",
      "epoch: 10 step: 871, loss is 0.3003096580505371\n",
      "epoch: 10 step: 872, loss is 0.21025191247463226\n",
      "epoch: 10 step: 873, loss is 0.8191006183624268\n",
      "epoch: 10 step: 874, loss is 0.16215240955352783\n",
      "epoch: 10 step: 875, loss is 0.3354151248931885\n",
      "epoch: 10 step: 876, loss is 0.026491286233067513\n",
      "epoch: 10 step: 877, loss is 0.7412543296813965\n",
      "epoch: 10 step: 878, loss is 0.16922646760940552\n",
      "epoch: 10 step: 879, loss is 0.008182743564248085\n",
      "epoch: 10 step: 880, loss is 0.2010752260684967\n",
      "epoch: 10 step: 881, loss is 0.11456090211868286\n",
      "epoch: 10 step: 882, loss is 0.2757776081562042\n",
      "epoch: 10 step: 883, loss is 0.09850296378135681\n",
      "epoch: 10 step: 884, loss is 0.27294662594795227\n",
      "epoch: 10 step: 885, loss is 0.02376113273203373\n",
      "epoch: 10 step: 886, loss is 0.20908519625663757\n",
      "epoch: 10 step: 887, loss is 0.05415946617722511\n",
      "epoch: 10 step: 888, loss is 0.49574726819992065\n",
      "epoch: 10 step: 889, loss is 0.7306378483772278\n",
      "epoch: 10 step: 890, loss is 0.06512010842561722\n",
      "epoch: 10 step: 891, loss is 0.6098905205726624\n",
      "epoch: 10 step: 892, loss is 0.2585810422897339\n",
      "epoch: 10 step: 893, loss is 0.049911413341760635\n",
      "epoch: 10 step: 894, loss is 0.38244184851646423\n",
      "epoch: 10 step: 895, loss is 0.11007257550954819\n",
      "epoch: 10 step: 896, loss is 0.19503161311149597\n",
      "epoch: 10 step: 897, loss is 0.28215932846069336\n",
      "epoch: 10 step: 898, loss is 0.4138502776622772\n",
      "epoch: 10 step: 899, loss is 0.04887007549405098\n",
      "epoch: 10 step: 900, loss is 0.13173717260360718\n",
      "epoch: 10 step: 901, loss is 0.5966445207595825\n",
      "epoch: 10 step: 902, loss is 0.23300771415233612\n",
      "epoch: 10 step: 903, loss is 0.1790914386510849\n",
      "epoch: 10 step: 904, loss is 0.10465206950902939\n",
      "epoch: 10 step: 905, loss is 0.03310887888073921\n",
      "epoch: 10 step: 906, loss is 0.18946866691112518\n",
      "epoch: 10 step: 907, loss is 0.04303385317325592\n",
      "epoch: 10 step: 908, loss is 0.26901698112487793\n",
      "epoch: 10 step: 909, loss is 0.05310083553195\n",
      "epoch: 10 step: 910, loss is 0.17344366014003754\n",
      "epoch: 10 step: 911, loss is 0.22960945963859558\n",
      "epoch: 10 step: 912, loss is 0.09389282017946243\n",
      "epoch: 10 step: 913, loss is 0.18071474134922028\n",
      "epoch: 10 step: 914, loss is 0.21276850998401642\n",
      "epoch: 10 step: 915, loss is 0.1488722860813141\n",
      "epoch: 10 step: 916, loss is 0.07474924623966217\n",
      "epoch: 10 step: 917, loss is 0.014164594002068043\n",
      "epoch: 10 step: 918, loss is 0.3271022439002991\n",
      "epoch: 10 step: 919, loss is 0.3858855068683624\n",
      "epoch: 10 step: 920, loss is 0.078649140894413\n",
      "epoch: 10 step: 921, loss is 0.15757520496845245\n",
      "epoch: 10 step: 922, loss is 0.16318973898887634\n",
      "epoch: 10 step: 923, loss is 0.5516685843467712\n",
      "epoch: 10 step: 924, loss is 0.4979185163974762\n",
      "epoch: 10 step: 925, loss is 0.2704138457775116\n",
      "epoch: 10 step: 926, loss is 0.05316409841179848\n",
      "epoch: 10 step: 927, loss is 0.36856475472450256\n",
      "epoch: 10 step: 928, loss is 0.009738866239786148\n",
      "epoch: 10 step: 929, loss is 0.24794010818004608\n",
      "epoch: 10 step: 930, loss is 0.13210299611091614\n",
      "epoch: 10 step: 931, loss is 0.04534079134464264\n",
      "epoch: 10 step: 932, loss is 0.03461931645870209\n",
      "epoch: 10 step: 933, loss is 0.07992714643478394\n",
      "epoch: 10 step: 934, loss is 0.30728763341903687\n",
      "epoch: 10 step: 935, loss is 0.05081837624311447\n",
      "epoch: 10 step: 936, loss is 0.1479455679655075\n",
      "epoch: 10 step: 937, loss is 0.39619046449661255\n",
      "epoch: 10 step: 938, loss is 0.5061324238777161\n",
      "epoch: 10 step: 939, loss is 0.016311923041939735\n",
      "epoch: 10 step: 940, loss is 0.5607127547264099\n",
      "epoch: 10 step: 941, loss is 0.32603850960731506\n",
      "epoch: 10 step: 942, loss is 0.051466867327690125\n",
      "epoch: 10 step: 943, loss is 0.4152717590332031\n",
      "epoch: 10 step: 944, loss is 0.21131250262260437\n",
      "epoch: 10 step: 945, loss is 0.439628928899765\n",
      "epoch: 10 step: 946, loss is 0.28098300099372864\n",
      "epoch: 10 step: 947, loss is 0.08868761360645294\n",
      "epoch: 10 step: 948, loss is 0.1356133371591568\n",
      "epoch: 10 step: 949, loss is 0.017393037676811218\n",
      "epoch: 10 step: 950, loss is 0.1318443864583969\n",
      "epoch: 10 step: 951, loss is 0.19985701143741608\n",
      "epoch: 10 step: 952, loss is 0.3928637206554413\n",
      "epoch: 10 step: 953, loss is 0.4498893916606903\n",
      "epoch: 10 step: 954, loss is 0.1816016435623169\n",
      "epoch: 10 step: 955, loss is 0.2554139196872711\n",
      "epoch: 10 step: 956, loss is 0.27862343192100525\n",
      "epoch: 10 step: 957, loss is 0.3478253483772278\n",
      "epoch: 10 step: 958, loss is 0.1269005686044693\n",
      "epoch: 10 step: 959, loss is 0.33271604776382446\n",
      "epoch: 10 step: 960, loss is 0.49942147731781006\n",
      "epoch: 10 step: 961, loss is 0.13384030759334564\n",
      "epoch: 10 step: 962, loss is 0.027255795896053314\n",
      "epoch: 10 step: 963, loss is 0.3101149797439575\n",
      "epoch: 10 step: 964, loss is 0.13240180909633636\n",
      "epoch: 10 step: 965, loss is 0.20928353071212769\n",
      "epoch: 10 step: 966, loss is 0.26815563440322876\n",
      "epoch: 10 step: 967, loss is 0.35026177763938904\n",
      "epoch: 10 step: 968, loss is 0.09690482169389725\n",
      "epoch: 10 step: 969, loss is 0.018153999000787735\n",
      "epoch: 10 step: 970, loss is 0.29468467831611633\n",
      "epoch: 10 step: 971, loss is 0.05323687195777893\n",
      "epoch: 10 step: 972, loss is 0.20864337682724\n",
      "epoch: 10 step: 973, loss is 0.25975778698921204\n",
      "epoch: 10 step: 974, loss is 0.04804573208093643\n",
      "epoch: 10 step: 975, loss is 0.015950052067637444\n",
      "epoch: 10 step: 976, loss is 0.025046244263648987\n",
      "epoch: 10 step: 977, loss is 0.21538057923316956\n",
      "epoch: 10 step: 978, loss is 0.16352307796478271\n",
      "epoch: 10 step: 979, loss is 0.0813886746764183\n",
      "epoch: 10 step: 980, loss is 0.39222538471221924\n",
      "epoch: 10 step: 981, loss is 0.044805098325014114\n",
      "epoch: 10 step: 982, loss is 0.17599360644817352\n",
      "epoch: 10 step: 983, loss is 0.022220775485038757\n",
      "epoch: 10 step: 984, loss is 0.07278703153133392\n",
      "epoch: 10 step: 985, loss is 0.011497212573885918\n",
      "epoch: 10 step: 986, loss is 0.10444271564483643\n",
      "epoch: 10 step: 987, loss is 0.23085492849349976\n",
      "epoch: 10 step: 988, loss is 0.007666257210075855\n",
      "epoch: 10 step: 989, loss is 0.3727760910987854\n",
      "epoch: 10 step: 990, loss is 0.022248301655054092\n",
      "epoch: 10 step: 991, loss is 0.015494700521230698\n",
      "epoch: 10 step: 992, loss is 0.185731902718544\n",
      "epoch: 10 step: 993, loss is 0.040844909846782684\n",
      "epoch: 10 step: 994, loss is 0.2588999569416046\n",
      "epoch: 10 step: 995, loss is 0.12075252085924149\n",
      "epoch: 10 step: 996, loss is 0.03645819053053856\n",
      "epoch: 10 step: 997, loss is 0.13869088888168335\n",
      "epoch: 10 step: 998, loss is 0.14189279079437256\n",
      "epoch: 10 step: 999, loss is 0.07290295511484146\n",
      "epoch: 10 step: 1000, loss is 0.11408669501543045\n",
      "epoch: 10 step: 1001, loss is 0.13167189061641693\n",
      "epoch: 10 step: 1002, loss is 0.02634395658969879\n",
      "epoch: 10 step: 1003, loss is 0.3757791519165039\n",
      "epoch: 10 step: 1004, loss is 0.02466811239719391\n",
      "epoch: 10 step: 1005, loss is 0.01105477660894394\n",
      "epoch: 10 step: 1006, loss is 0.020797578617930412\n",
      "epoch: 10 step: 1007, loss is 0.051277440041303635\n",
      "epoch: 10 step: 1008, loss is 0.28304222226142883\n",
      "epoch: 10 step: 1009, loss is 0.42532211542129517\n",
      "epoch: 10 step: 1010, loss is 0.08613528311252594\n",
      "epoch: 10 step: 1011, loss is 0.009650389663875103\n",
      "epoch: 10 step: 1012, loss is 0.013964558951556683\n",
      "epoch: 10 step: 1013, loss is 0.010305839590728283\n",
      "epoch: 10 step: 1014, loss is 0.03304297477006912\n",
      "epoch: 10 step: 1015, loss is 0.21322029829025269\n",
      "epoch: 10 step: 1016, loss is 0.016317233443260193\n",
      "epoch: 10 step: 1017, loss is 0.3402593731880188\n",
      "epoch: 10 step: 1018, loss is 0.13863952457904816\n",
      "epoch: 10 step: 1019, loss is 0.07528208941221237\n",
      "epoch: 10 step: 1020, loss is 0.018636781722307205\n",
      "epoch: 10 step: 1021, loss is 0.4129813015460968\n",
      "epoch: 10 step: 1022, loss is 0.26639315485954285\n",
      "epoch: 10 step: 1023, loss is 0.08233996480703354\n",
      "epoch: 10 step: 1024, loss is 0.010623571462929249\n",
      "epoch: 10 step: 1025, loss is 0.05098280310630798\n",
      "epoch: 10 step: 1026, loss is 0.19868780672550201\n",
      "epoch: 10 step: 1027, loss is 0.6272575855255127\n",
      "epoch: 10 step: 1028, loss is 0.0697491392493248\n",
      "epoch: 10 step: 1029, loss is 0.029731180518865585\n",
      "epoch: 10 step: 1030, loss is 0.13854821026325226\n",
      "epoch: 10 step: 1031, loss is 0.07372967898845673\n",
      "epoch: 10 step: 1032, loss is 0.14036265015602112\n",
      "epoch: 10 step: 1033, loss is 0.05605670437216759\n",
      "epoch: 10 step: 1034, loss is 0.26614370942115784\n",
      "epoch: 10 step: 1035, loss is 0.381577730178833\n",
      "epoch: 10 step: 1036, loss is 0.7300256490707397\n",
      "epoch: 10 step: 1037, loss is 0.02539501152932644\n",
      "epoch: 10 step: 1038, loss is 0.18214450776576996\n",
      "epoch: 10 step: 1039, loss is 0.056042078882455826\n",
      "epoch: 10 step: 1040, loss is 0.07740344852209091\n",
      "epoch: 10 step: 1041, loss is 0.25891390442848206\n",
      "epoch: 10 step: 1042, loss is 1.2695585489273071\n",
      "epoch: 10 step: 1043, loss is 0.3924177587032318\n",
      "epoch: 10 step: 1044, loss is 0.2283255159854889\n",
      "epoch: 10 step: 1045, loss is 0.9189103245735168\n",
      "epoch: 10 step: 1046, loss is 0.06363660842180252\n",
      "epoch: 10 step: 1047, loss is 0.31221699714660645\n",
      "epoch: 10 step: 1048, loss is 0.17669586837291718\n",
      "epoch: 10 step: 1049, loss is 0.1578262895345688\n",
      "epoch: 10 step: 1050, loss is 0.3166748881340027\n",
      "epoch: 10 step: 1051, loss is 0.07609613239765167\n",
      "epoch: 10 step: 1052, loss is 0.28998929262161255\n",
      "epoch: 10 step: 1053, loss is 0.03629269823431969\n",
      "epoch: 10 step: 1054, loss is 0.27304807305336\n",
      "epoch: 10 step: 1055, loss is 0.04282253235578537\n",
      "epoch: 10 step: 1056, loss is 0.06227114424109459\n",
      "epoch: 10 step: 1057, loss is 0.07499475032091141\n",
      "epoch: 10 step: 1058, loss is 0.008743835613131523\n",
      "epoch: 10 step: 1059, loss is 0.17738671600818634\n",
      "epoch: 10 step: 1060, loss is 0.1557195484638214\n",
      "epoch: 10 step: 1061, loss is 0.3406738042831421\n",
      "epoch: 10 step: 1062, loss is 0.08716191351413727\n",
      "epoch: 10 step: 1063, loss is 0.7920914888381958\n",
      "epoch: 10 step: 1064, loss is 0.09587172418832779\n",
      "epoch: 10 step: 1065, loss is 0.09492576867341995\n",
      "epoch: 10 step: 1066, loss is 0.33955156803131104\n",
      "epoch: 10 step: 1067, loss is 0.12487063556909561\n",
      "epoch: 10 step: 1068, loss is 0.10906621813774109\n",
      "epoch: 10 step: 1069, loss is 0.11798319965600967\n",
      "epoch: 10 step: 1070, loss is 0.5404075980186462\n",
      "epoch: 10 step: 1071, loss is 0.469512403011322\n",
      "epoch: 10 step: 1072, loss is 0.10167471319437027\n",
      "epoch: 10 step: 1073, loss is 0.23056825995445251\n",
      "epoch: 10 step: 1074, loss is 0.35137391090393066\n",
      "epoch: 10 step: 1075, loss is 0.026058191433548927\n",
      "epoch: 10 step: 1076, loss is 0.11560118198394775\n",
      "epoch: 10 step: 1077, loss is 0.5061648488044739\n",
      "epoch: 10 step: 1078, loss is 0.2831597626209259\n",
      "epoch: 10 step: 1079, loss is 0.42707592248916626\n",
      "epoch: 10 step: 1080, loss is 0.42538386583328247\n",
      "epoch: 10 step: 1081, loss is 0.23765206336975098\n",
      "epoch: 10 step: 1082, loss is 0.06468729674816132\n",
      "epoch: 10 step: 1083, loss is 0.06772592663764954\n",
      "epoch: 10 step: 1084, loss is 1.1019338369369507\n",
      "epoch: 10 step: 1085, loss is 0.8294811248779297\n",
      "epoch: 10 step: 1086, loss is 0.23215851187705994\n",
      "epoch: 10 step: 1087, loss is 0.22290357947349548\n",
      "epoch: 10 step: 1088, loss is 0.2632286250591278\n",
      "epoch: 10 step: 1089, loss is 0.22617612779140472\n",
      "epoch: 10 step: 1090, loss is 0.2271326780319214\n",
      "epoch: 10 step: 1091, loss is 0.06900094449520111\n",
      "epoch: 10 step: 1092, loss is 0.1641964614391327\n",
      "epoch: 10 step: 1093, loss is 0.13039197027683258\n",
      "epoch: 10 step: 1094, loss is 0.10888971388339996\n",
      "epoch: 10 step: 1095, loss is 0.402554452419281\n",
      "epoch: 10 step: 1096, loss is 0.1730562448501587\n",
      "epoch: 10 step: 1097, loss is 0.05172059312462807\n",
      "epoch: 10 step: 1098, loss is 0.11086117476224899\n",
      "epoch: 10 step: 1099, loss is 0.5383901596069336\n",
      "epoch: 10 step: 1100, loss is 0.15872949361801147\n",
      "epoch: 10 step: 1101, loss is 0.09938380867242813\n",
      "epoch: 10 step: 1102, loss is 0.21326296031475067\n",
      "epoch: 10 step: 1103, loss is 0.15201713144779205\n",
      "epoch: 10 step: 1104, loss is 0.10443170368671417\n",
      "epoch: 10 step: 1105, loss is 0.3649444878101349\n",
      "epoch: 10 step: 1106, loss is 0.2842363119125366\n",
      "epoch: 10 step: 1107, loss is 0.2504529058933258\n",
      "epoch: 10 step: 1108, loss is 0.15730708837509155\n",
      "epoch: 10 step: 1109, loss is 0.2832697033882141\n",
      "epoch: 10 step: 1110, loss is 0.0852009728550911\n",
      "epoch: 10 step: 1111, loss is 0.40527626872062683\n",
      "epoch: 10 step: 1112, loss is 0.18240894377231598\n",
      "epoch: 10 step: 1113, loss is 0.2580130696296692\n",
      "epoch: 10 step: 1114, loss is 0.10450763255357742\n",
      "epoch: 10 step: 1115, loss is 0.05381680652499199\n",
      "epoch: 10 step: 1116, loss is 0.15420211851596832\n",
      "epoch: 10 step: 1117, loss is 0.027366863563656807\n",
      "epoch: 10 step: 1118, loss is 0.06497679650783539\n",
      "epoch: 10 step: 1119, loss is 0.24583446979522705\n",
      "epoch: 10 step: 1120, loss is 0.02479736879467964\n",
      "epoch: 10 step: 1121, loss is 0.02587653510272503\n",
      "epoch: 10 step: 1122, loss is 0.4206816852092743\n",
      "epoch: 10 step: 1123, loss is 0.1442711353302002\n",
      "epoch: 10 step: 1124, loss is 0.10246073454618454\n",
      "epoch: 10 step: 1125, loss is 0.24910153448581696\n",
      "epoch: 10 step: 1126, loss is 0.05870446562767029\n",
      "epoch: 10 step: 1127, loss is 0.21923710405826569\n",
      "epoch: 10 step: 1128, loss is 0.466921865940094\n",
      "epoch: 10 step: 1129, loss is 0.07817526906728745\n",
      "epoch: 10 step: 1130, loss is 0.012034055776894093\n",
      "epoch: 10 step: 1131, loss is 0.1131805032491684\n",
      "epoch: 10 step: 1132, loss is 0.0238320454955101\n",
      "epoch: 10 step: 1133, loss is 0.017678016796708107\n",
      "epoch: 10 step: 1134, loss is 0.05182493478059769\n",
      "epoch: 10 step: 1135, loss is 0.0690980851650238\n",
      "epoch: 10 step: 1136, loss is 0.21928617358207703\n",
      "epoch: 10 step: 1137, loss is 0.22027498483657837\n",
      "epoch: 10 step: 1138, loss is 0.17752553522586823\n",
      "epoch: 10 step: 1139, loss is 0.009983457624912262\n",
      "epoch: 10 step: 1140, loss is 0.015263695269823074\n",
      "epoch: 10 step: 1141, loss is 0.025166409090161324\n",
      "epoch: 10 step: 1142, loss is 0.14888577163219452\n",
      "epoch: 10 step: 1143, loss is 0.027372198179364204\n",
      "epoch: 10 step: 1144, loss is 0.05328209325671196\n",
      "epoch: 10 step: 1145, loss is 0.030698809772729874\n",
      "epoch: 10 step: 1146, loss is 0.015130981802940369\n",
      "epoch: 10 step: 1147, loss is 0.08004448562860489\n",
      "epoch: 10 step: 1148, loss is 0.5389668941497803\n",
      "epoch: 10 step: 1149, loss is 0.490101158618927\n",
      "epoch: 10 step: 1150, loss is 0.25024867057800293\n",
      "epoch: 10 step: 1151, loss is 0.1434353142976761\n",
      "epoch: 10 step: 1152, loss is 0.7019443511962891\n",
      "epoch: 10 step: 1153, loss is 0.16761992871761322\n",
      "epoch: 10 step: 1154, loss is 0.02855931781232357\n",
      "epoch: 10 step: 1155, loss is 0.22945813834667206\n",
      "epoch: 10 step: 1156, loss is 0.030659029260277748\n",
      "epoch: 10 step: 1157, loss is 0.22652417421340942\n",
      "epoch: 10 step: 1158, loss is 0.289529412984848\n",
      "epoch: 10 step: 1159, loss is 0.49964821338653564\n",
      "epoch: 10 step: 1160, loss is 0.2697087824344635\n",
      "epoch: 10 step: 1161, loss is 0.050579410046339035\n",
      "epoch: 10 step: 1162, loss is 0.04989931732416153\n",
      "epoch: 10 step: 1163, loss is 0.18205228447914124\n",
      "epoch: 10 step: 1164, loss is 0.2733892798423767\n",
      "epoch: 10 step: 1165, loss is 0.017358429729938507\n",
      "epoch: 10 step: 1166, loss is 0.20490671694278717\n",
      "epoch: 10 step: 1167, loss is 0.15572918951511383\n",
      "epoch: 10 step: 1168, loss is 0.0581992007791996\n",
      "epoch: 10 step: 1169, loss is 0.021361295133829117\n",
      "epoch: 10 step: 1170, loss is 0.026459133252501488\n",
      "epoch: 10 step: 1171, loss is 0.10192722827196121\n",
      "epoch: 10 step: 1172, loss is 0.20949040353298187\n",
      "epoch: 10 step: 1173, loss is 0.01076873391866684\n",
      "epoch: 10 step: 1174, loss is 0.018438780680298805\n",
      "epoch: 10 step: 1175, loss is 0.5700168609619141\n",
      "epoch: 10 step: 1176, loss is 0.3152337074279785\n",
      "epoch: 10 step: 1177, loss is 0.2840708792209625\n",
      "epoch: 10 step: 1178, loss is 0.1529538780450821\n",
      "epoch: 10 step: 1179, loss is 0.08983611315488815\n",
      "epoch: 10 step: 1180, loss is 0.0968853309750557\n",
      "epoch: 10 step: 1181, loss is 0.08748931437730789\n",
      "epoch: 10 step: 1182, loss is 0.4339102506637573\n",
      "epoch: 10 step: 1183, loss is 0.24112065136432648\n",
      "epoch: 10 step: 1184, loss is 0.08298921585083008\n",
      "epoch: 10 step: 1185, loss is 0.06171815097332001\n",
      "epoch: 10 step: 1186, loss is 0.26962190866470337\n",
      "epoch: 10 step: 1187, loss is 0.1850174218416214\n",
      "epoch: 10 step: 1188, loss is 0.14765618741512299\n",
      "epoch: 10 step: 1189, loss is 0.12879785895347595\n",
      "epoch: 10 step: 1190, loss is 0.047935523092746735\n",
      "epoch: 10 step: 1191, loss is 0.1721346378326416\n",
      "epoch: 10 step: 1192, loss is 0.04408722743391991\n",
      "epoch: 10 step: 1193, loss is 0.4624606668949127\n",
      "epoch: 10 step: 1194, loss is 0.027224835008382797\n",
      "epoch: 10 step: 1195, loss is 0.31748631596565247\n",
      "epoch: 10 step: 1196, loss is 0.466147243976593\n",
      "epoch: 10 step: 1197, loss is 0.22849254310131073\n",
      "epoch: 10 step: 1198, loss is 0.0602593831717968\n",
      "epoch: 10 step: 1199, loss is 0.06339124590158463\n",
      "epoch: 10 step: 1200, loss is 0.2517404556274414\n",
      "epoch: 10 step: 1201, loss is 0.27068030834198\n",
      "epoch: 10 step: 1202, loss is 0.7179359793663025\n",
      "epoch: 10 step: 1203, loss is 0.05214674770832062\n",
      "epoch: 10 step: 1204, loss is 0.3875596225261688\n",
      "epoch: 10 step: 1205, loss is 0.16091707348823547\n",
      "epoch: 10 step: 1206, loss is 0.019450563937425613\n",
      "epoch: 10 step: 1207, loss is 0.05598413571715355\n",
      "epoch: 10 step: 1208, loss is 0.047608353197574615\n",
      "epoch: 10 step: 1209, loss is 0.08181777596473694\n",
      "epoch: 10 step: 1210, loss is 0.384377121925354\n",
      "epoch: 10 step: 1211, loss is 0.013305194675922394\n",
      "epoch: 10 step: 1212, loss is 0.07043328881263733\n",
      "epoch: 10 step: 1213, loss is 0.11857781559228897\n",
      "epoch: 10 step: 1214, loss is 0.3079969882965088\n",
      "epoch: 10 step: 1215, loss is 0.10713165253400803\n",
      "epoch: 10 step: 1216, loss is 0.010779156349599361\n",
      "epoch: 10 step: 1217, loss is 0.43980956077575684\n",
      "epoch: 10 step: 1218, loss is 0.22646987438201904\n",
      "epoch: 10 step: 1219, loss is 0.014185345731675625\n",
      "epoch: 10 step: 1220, loss is 0.32977330684661865\n",
      "epoch: 10 step: 1221, loss is 0.46413084864616394\n",
      "epoch: 10 step: 1222, loss is 0.07755395770072937\n",
      "epoch: 10 step: 1223, loss is 0.10002298653125763\n",
      "epoch: 10 step: 1224, loss is 0.009246517904102802\n",
      "epoch: 10 step: 1225, loss is 0.11671997606754303\n",
      "epoch: 10 step: 1226, loss is 0.3993672728538513\n",
      "epoch: 10 step: 1227, loss is 0.3105419874191284\n",
      "epoch: 10 step: 1228, loss is 0.14181090891361237\n",
      "epoch: 10 step: 1229, loss is 0.21724866330623627\n",
      "epoch: 10 step: 1230, loss is 0.07059884071350098\n",
      "epoch: 10 step: 1231, loss is 0.019772496074438095\n",
      "epoch: 10 step: 1232, loss is 0.41440442204475403\n",
      "epoch: 10 step: 1233, loss is 0.10365290194749832\n",
      "epoch: 10 step: 1234, loss is 0.009990164078772068\n",
      "epoch: 10 step: 1235, loss is 0.2299460619688034\n",
      "epoch: 10 step: 1236, loss is 0.41152676939964294\n",
      "epoch: 10 step: 1237, loss is 0.16459481418132782\n",
      "epoch: 10 step: 1238, loss is 0.06868088245391846\n",
      "epoch: 10 step: 1239, loss is 0.05788655951619148\n",
      "epoch: 10 step: 1240, loss is 0.26594626903533936\n",
      "epoch: 10 step: 1241, loss is 0.3213212788105011\n",
      "epoch: 10 step: 1242, loss is 0.05746978148818016\n",
      "epoch: 10 step: 1243, loss is 0.03056350350379944\n",
      "epoch: 10 step: 1244, loss is 0.11273099482059479\n",
      "epoch: 10 step: 1245, loss is 0.06532438099384308\n",
      "epoch: 10 step: 1246, loss is 0.08842076361179352\n",
      "epoch: 10 step: 1247, loss is 0.2558092772960663\n",
      "epoch: 10 step: 1248, loss is 0.05667274072766304\n",
      "epoch: 10 step: 1249, loss is 0.2540489733219147\n",
      "epoch: 10 step: 1250, loss is 0.24217908084392548\n",
      "epoch: 10 step: 1251, loss is 0.0742717832326889\n",
      "epoch: 10 step: 1252, loss is 0.046991847455501556\n",
      "epoch: 10 step: 1253, loss is 0.1648523062467575\n",
      "epoch: 10 step: 1254, loss is 0.01164437085390091\n",
      "epoch: 10 step: 1255, loss is 0.16941261291503906\n",
      "epoch: 10 step: 1256, loss is 0.04419371485710144\n",
      "epoch: 10 step: 1257, loss is 0.31601428985595703\n",
      "epoch: 10 step: 1258, loss is 0.07953173667192459\n",
      "epoch: 10 step: 1259, loss is 0.03499345853924751\n",
      "epoch: 10 step: 1260, loss is 0.02536526694893837\n",
      "epoch: 10 step: 1261, loss is 0.15644414722919464\n",
      "epoch: 10 step: 1262, loss is 0.04387790709733963\n",
      "epoch: 10 step: 1263, loss is 0.3756859600543976\n",
      "epoch: 10 step: 1264, loss is 0.2386251837015152\n",
      "epoch: 10 step: 1265, loss is 0.19884783029556274\n",
      "epoch: 10 step: 1266, loss is 0.0020657526329159737\n",
      "epoch: 10 step: 1267, loss is 0.14869239926338196\n",
      "epoch: 10 step: 1268, loss is 0.011020549573004246\n",
      "epoch: 10 step: 1269, loss is 0.14091433584690094\n",
      "epoch: 10 step: 1270, loss is 0.10524654388427734\n",
      "epoch: 10 step: 1271, loss is 1.0238405466079712\n",
      "epoch: 10 step: 1272, loss is 0.3855808675289154\n",
      "epoch: 10 step: 1273, loss is 0.120298832654953\n",
      "epoch: 10 step: 1274, loss is 0.2859741449356079\n",
      "epoch: 10 step: 1275, loss is 0.16208422183990479\n",
      "epoch: 10 step: 1276, loss is 0.03695176914334297\n",
      "epoch: 10 step: 1277, loss is 0.02400992251932621\n",
      "epoch: 10 step: 1278, loss is 0.1553620845079422\n",
      "epoch: 10 step: 1279, loss is 0.26286226511001587\n",
      "epoch: 10 step: 1280, loss is 0.06500054895877838\n",
      "epoch: 10 step: 1281, loss is 0.12451066076755524\n",
      "epoch: 10 step: 1282, loss is 0.19190751016139984\n",
      "epoch: 10 step: 1283, loss is 0.016481125727295876\n",
      "epoch: 10 step: 1284, loss is 0.3711625933647156\n",
      "epoch: 10 step: 1285, loss is 0.041500598192214966\n",
      "epoch: 10 step: 1286, loss is 0.39559483528137207\n",
      "epoch: 10 step: 1287, loss is 0.40095841884613037\n",
      "epoch: 10 step: 1288, loss is 0.2576691210269928\n",
      "epoch: 10 step: 1289, loss is 0.34272897243499756\n",
      "epoch: 10 step: 1290, loss is 0.16047212481498718\n",
      "epoch: 10 step: 1291, loss is 0.0707128643989563\n",
      "epoch: 10 step: 1292, loss is 0.5919644236564636\n",
      "epoch: 10 step: 1293, loss is 0.02815197966992855\n",
      "epoch: 10 step: 1294, loss is 0.0367448590695858\n",
      "epoch: 10 step: 1295, loss is 0.1621675342321396\n",
      "epoch: 10 step: 1296, loss is 0.15953323245048523\n",
      "epoch: 10 step: 1297, loss is 0.18165872991085052\n",
      "epoch: 10 step: 1298, loss is 0.36275535821914673\n",
      "epoch: 10 step: 1299, loss is 0.047176118940114975\n",
      "epoch: 10 step: 1300, loss is 0.11653122305870056\n",
      "epoch: 10 step: 1301, loss is 0.18546852469444275\n",
      "epoch: 10 step: 1302, loss is 0.06430047005414963\n",
      "epoch: 10 step: 1303, loss is 0.31237855553627014\n",
      "epoch: 10 step: 1304, loss is 0.2303767204284668\n",
      "epoch: 10 step: 1305, loss is 0.09477473795413971\n",
      "epoch: 10 step: 1306, loss is 0.03737402707338333\n",
      "epoch: 10 step: 1307, loss is 0.04089392349123955\n",
      "epoch: 10 step: 1308, loss is 0.23308885097503662\n",
      "epoch: 10 step: 1309, loss is 0.08128399401903152\n",
      "epoch: 10 step: 1310, loss is 0.035451896488666534\n",
      "epoch: 10 step: 1311, loss is 0.10461439192295074\n",
      "epoch: 10 step: 1312, loss is 0.22822102904319763\n",
      "epoch: 10 step: 1313, loss is 0.14063283801078796\n",
      "epoch: 10 step: 1314, loss is 0.10653183609247208\n",
      "epoch: 10 step: 1315, loss is 0.06796996295452118\n",
      "epoch: 10 step: 1316, loss is 0.039129190146923065\n",
      "epoch: 10 step: 1317, loss is 0.18718020617961884\n",
      "epoch: 10 step: 1318, loss is 0.255713552236557\n",
      "epoch: 10 step: 1319, loss is 0.020012516528367996\n",
      "epoch: 10 step: 1320, loss is 0.08755868673324585\n",
      "epoch: 10 step: 1321, loss is 0.03269326314330101\n",
      "epoch: 10 step: 1322, loss is 0.04075824096798897\n",
      "epoch: 10 step: 1323, loss is 0.10467608273029327\n",
      "epoch: 10 step: 1324, loss is 0.532588541507721\n",
      "epoch: 10 step: 1325, loss is 0.2858940362930298\n",
      "epoch: 10 step: 1326, loss is 0.10662955045700073\n",
      "epoch: 10 step: 1327, loss is 0.43576812744140625\n",
      "epoch: 10 step: 1328, loss is 0.04730851948261261\n",
      "epoch: 10 step: 1329, loss is 0.1263488382101059\n",
      "epoch: 10 step: 1330, loss is 0.03917819261550903\n",
      "epoch: 10 step: 1331, loss is 0.28390568494796753\n",
      "epoch: 10 step: 1332, loss is 0.03130311891436577\n",
      "epoch: 10 step: 1333, loss is 0.3782489001750946\n",
      "epoch: 10 step: 1334, loss is 0.33955609798431396\n",
      "epoch: 10 step: 1335, loss is 0.021588165313005447\n",
      "epoch: 10 step: 1336, loss is 0.2961655259132385\n",
      "epoch: 10 step: 1337, loss is 0.0749184638261795\n",
      "epoch: 10 step: 1338, loss is 0.3694407343864441\n",
      "epoch: 10 step: 1339, loss is 0.26691389083862305\n",
      "epoch: 10 step: 1340, loss is 0.4332733154296875\n",
      "epoch: 10 step: 1341, loss is 0.13951097428798676\n",
      "epoch: 10 step: 1342, loss is 0.04321553558111191\n",
      "epoch: 10 step: 1343, loss is 0.052458398044109344\n",
      "epoch: 10 step: 1344, loss is 0.1486981362104416\n",
      "epoch: 10 step: 1345, loss is 0.22373171150684357\n",
      "epoch: 10 step: 1346, loss is 0.641427218914032\n",
      "epoch: 10 step: 1347, loss is 0.43433159589767456\n",
      "epoch: 10 step: 1348, loss is 0.23616494238376617\n",
      "epoch: 10 step: 1349, loss is 0.15035127103328705\n",
      "epoch: 10 step: 1350, loss is 0.01762058027088642\n",
      "epoch: 10 step: 1351, loss is 0.40665730834007263\n",
      "epoch: 10 step: 1352, loss is 0.03960103169083595\n",
      "epoch: 10 step: 1353, loss is 0.3045080900192261\n",
      "epoch: 10 step: 1354, loss is 0.1134355440735817\n",
      "epoch: 10 step: 1355, loss is 0.054601266980171204\n",
      "epoch: 10 step: 1356, loss is 0.4702165126800537\n",
      "epoch: 10 step: 1357, loss is 0.3997751474380493\n",
      "epoch: 10 step: 1358, loss is 0.14304247498512268\n",
      "epoch: 10 step: 1359, loss is 0.6928812861442566\n",
      "epoch: 10 step: 1360, loss is 0.074897401034832\n",
      "epoch: 10 step: 1361, loss is 0.1403616964817047\n",
      "epoch: 10 step: 1362, loss is 0.021866604685783386\n",
      "epoch: 10 step: 1363, loss is 0.0574774406850338\n",
      "epoch: 10 step: 1364, loss is 0.10737461596727371\n",
      "epoch: 10 step: 1365, loss is 0.5377548336982727\n",
      "epoch: 10 step: 1366, loss is 0.047198764979839325\n",
      "epoch: 10 step: 1367, loss is 0.20694229006767273\n",
      "epoch: 10 step: 1368, loss is 0.07947111129760742\n",
      "epoch: 10 step: 1369, loss is 0.22501623630523682\n",
      "epoch: 10 step: 1370, loss is 0.1334652453660965\n",
      "epoch: 10 step: 1371, loss is 0.8552722930908203\n",
      "epoch: 10 step: 1372, loss is 0.26842230558395386\n",
      "epoch: 10 step: 1373, loss is 0.17196474969387054\n",
      "epoch: 10 step: 1374, loss is 0.48807185888290405\n",
      "epoch: 10 step: 1375, loss is 0.1886282116174698\n",
      "epoch: 10 step: 1376, loss is 0.04685528948903084\n",
      "epoch: 10 step: 1377, loss is 0.2818092107772827\n",
      "epoch: 10 step: 1378, loss is 0.18096992373466492\n",
      "epoch: 10 step: 1379, loss is 1.0089869499206543\n",
      "epoch: 10 step: 1380, loss is 0.47371888160705566\n",
      "epoch: 10 step: 1381, loss is 0.24990332126617432\n",
      "epoch: 10 step: 1382, loss is 0.20136767625808716\n",
      "epoch: 10 step: 1383, loss is 0.11422115564346313\n",
      "epoch: 10 step: 1384, loss is 0.12114289402961731\n",
      "epoch: 10 step: 1385, loss is 0.08130230009555817\n",
      "epoch: 10 step: 1386, loss is 0.07346118986606598\n",
      "epoch: 10 step: 1387, loss is 0.36174553632736206\n",
      "epoch: 10 step: 1388, loss is 0.3449998199939728\n",
      "epoch: 10 step: 1389, loss is 0.12612228095531464\n",
      "epoch: 10 step: 1390, loss is 0.1342453509569168\n",
      "epoch: 10 step: 1391, loss is 0.21997882425785065\n",
      "epoch: 10 step: 1392, loss is 0.10921236872673035\n",
      "epoch: 10 step: 1393, loss is 0.024659525603055954\n",
      "epoch: 10 step: 1394, loss is 0.106878861784935\n",
      "epoch: 10 step: 1395, loss is 0.07223977893590927\n",
      "epoch: 10 step: 1396, loss is 0.34633365273475647\n",
      "epoch: 10 step: 1397, loss is 0.7921929359436035\n",
      "epoch: 10 step: 1398, loss is 0.36316290497779846\n",
      "epoch: 10 step: 1399, loss is 0.4952565133571625\n",
      "epoch: 10 step: 1400, loss is 0.19299927353858948\n",
      "epoch: 10 step: 1401, loss is 0.15443731844425201\n",
      "epoch: 10 step: 1402, loss is 0.05550860986113548\n",
      "epoch: 10 step: 1403, loss is 0.38529273867607117\n",
      "epoch: 10 step: 1404, loss is 0.10175390541553497\n",
      "epoch: 10 step: 1405, loss is 0.5547224283218384\n",
      "epoch: 10 step: 1406, loss is 0.07458250969648361\n",
      "epoch: 10 step: 1407, loss is 0.1546226441860199\n",
      "epoch: 10 step: 1408, loss is 0.05559741333127022\n",
      "epoch: 10 step: 1409, loss is 0.08201796561479568\n",
      "epoch: 10 step: 1410, loss is 0.18335586786270142\n",
      "epoch: 10 step: 1411, loss is 0.178346648812294\n",
      "epoch: 10 step: 1412, loss is 0.20541982352733612\n",
      "epoch: 10 step: 1413, loss is 0.03344336152076721\n",
      "epoch: 10 step: 1414, loss is 0.2097197324037552\n",
      "epoch: 10 step: 1415, loss is 0.11679881811141968\n",
      "epoch: 10 step: 1416, loss is 0.15901736915111542\n",
      "epoch: 10 step: 1417, loss is 0.03494510054588318\n",
      "epoch: 10 step: 1418, loss is 0.11244270950555801\n",
      "epoch: 10 step: 1419, loss is 0.24595709145069122\n",
      "epoch: 10 step: 1420, loss is 0.19726422429084778\n",
      "epoch: 10 step: 1421, loss is 0.2314668744802475\n",
      "epoch: 10 step: 1422, loss is 0.43111997842788696\n",
      "epoch: 10 step: 1423, loss is 0.3284838795661926\n",
      "epoch: 10 step: 1424, loss is 0.2041701078414917\n",
      "epoch: 10 step: 1425, loss is 0.09327879548072815\n",
      "epoch: 10 step: 1426, loss is 0.06631665676832199\n",
      "epoch: 10 step: 1427, loss is 0.24024347960948944\n",
      "epoch: 10 step: 1428, loss is 0.13350945711135864\n",
      "epoch: 10 step: 1429, loss is 0.16543307900428772\n",
      "epoch: 10 step: 1430, loss is 0.12353096902370453\n",
      "epoch: 10 step: 1431, loss is 0.30918991565704346\n",
      "epoch: 10 step: 1432, loss is 0.2824261486530304\n",
      "epoch: 10 step: 1433, loss is 0.34792181849479675\n",
      "epoch: 10 step: 1434, loss is 0.270708829164505\n",
      "epoch: 10 step: 1435, loss is 0.3223344683647156\n",
      "epoch: 10 step: 1436, loss is 0.047078195959329605\n",
      "epoch: 10 step: 1437, loss is 0.1887725591659546\n",
      "epoch: 10 step: 1438, loss is 0.27438393235206604\n",
      "epoch: 10 step: 1439, loss is 0.2716550827026367\n",
      "epoch: 10 step: 1440, loss is 0.177764430642128\n",
      "epoch: 10 step: 1441, loss is 0.19907107949256897\n",
      "epoch: 10 step: 1442, loss is 0.021736979484558105\n",
      "epoch: 10 step: 1443, loss is 0.42791247367858887\n",
      "epoch: 10 step: 1444, loss is 0.9008057117462158\n",
      "epoch: 10 step: 1445, loss is 0.7825412750244141\n",
      "epoch: 10 step: 1446, loss is 0.81904536485672\n",
      "epoch: 10 step: 1447, loss is 0.1803753525018692\n",
      "epoch: 10 step: 1448, loss is 0.023283304646611214\n",
      "epoch: 10 step: 1449, loss is 0.0838543027639389\n",
      "epoch: 10 step: 1450, loss is 0.07727687805891037\n",
      "epoch: 10 step: 1451, loss is 0.5919153690338135\n",
      "epoch: 10 step: 1452, loss is 0.2611411213874817\n",
      "epoch: 10 step: 1453, loss is 0.19615104794502258\n",
      "epoch: 10 step: 1454, loss is 0.050302136689424515\n",
      "epoch: 10 step: 1455, loss is 0.1145494282245636\n",
      "epoch: 10 step: 1456, loss is 0.1574457287788391\n",
      "epoch: 10 step: 1457, loss is 0.11893726885318756\n",
      "epoch: 10 step: 1458, loss is 0.22658775746822357\n",
      "epoch: 10 step: 1459, loss is 0.11708010733127594\n",
      "epoch: 10 step: 1460, loss is 0.09902473539113998\n",
      "epoch: 10 step: 1461, loss is 0.06182791292667389\n",
      "epoch: 10 step: 1462, loss is 0.2962636649608612\n",
      "epoch: 10 step: 1463, loss is 0.2090466320514679\n",
      "epoch: 10 step: 1464, loss is 0.09155285358428955\n",
      "epoch: 10 step: 1465, loss is 0.6787421703338623\n",
      "epoch: 10 step: 1466, loss is 0.13160473108291626\n",
      "epoch: 10 step: 1467, loss is 0.5126869678497314\n",
      "epoch: 10 step: 1468, loss is 0.761569082736969\n",
      "epoch: 10 step: 1469, loss is 0.05335080251097679\n",
      "epoch: 10 step: 1470, loss is 0.23737621307373047\n",
      "epoch: 10 step: 1471, loss is 0.46850141882896423\n",
      "epoch: 10 step: 1472, loss is 0.022770876064896584\n",
      "epoch: 10 step: 1473, loss is 0.02625519223511219\n",
      "epoch: 10 step: 1474, loss is 0.1209263950586319\n",
      "epoch: 10 step: 1475, loss is 0.07156657427549362\n",
      "epoch: 10 step: 1476, loss is 0.04867139831185341\n",
      "epoch: 10 step: 1477, loss is 0.607081413269043\n",
      "epoch: 10 step: 1478, loss is 0.0962112694978714\n",
      "epoch: 10 step: 1479, loss is 0.088612399995327\n",
      "epoch: 10 step: 1480, loss is 0.18213701248168945\n",
      "epoch: 10 step: 1481, loss is 0.3455162048339844\n",
      "epoch: 10 step: 1482, loss is 0.2145468145608902\n",
      "epoch: 10 step: 1483, loss is 0.5056002736091614\n",
      "epoch: 10 step: 1484, loss is 0.09706603735685349\n",
      "epoch: 10 step: 1485, loss is 0.11407470703125\n",
      "epoch: 10 step: 1486, loss is 0.1661365032196045\n",
      "epoch: 10 step: 1487, loss is 0.1813296228647232\n",
      "epoch: 10 step: 1488, loss is 0.15708257257938385\n",
      "epoch: 10 step: 1489, loss is 0.35313647985458374\n",
      "epoch: 10 step: 1490, loss is 0.09960523247718811\n",
      "epoch: 10 step: 1491, loss is 0.15847469866275787\n",
      "epoch: 10 step: 1492, loss is 0.0622415654361248\n",
      "epoch: 10 step: 1493, loss is 0.16464102268218994\n",
      "epoch: 10 step: 1494, loss is 0.06035807728767395\n",
      "epoch: 10 step: 1495, loss is 0.0109314676374197\n",
      "epoch: 10 step: 1496, loss is 0.04677305370569229\n",
      "epoch: 10 step: 1497, loss is 0.10761076956987381\n",
      "epoch: 10 step: 1498, loss is 0.05463903397321701\n",
      "epoch: 10 step: 1499, loss is 0.12102815508842468\n",
      "epoch: 10 step: 1500, loss is 0.0840521901845932\n",
      "epoch: 10 step: 1501, loss is 0.042361654341220856\n",
      "epoch: 10 step: 1502, loss is 0.3552039563655853\n",
      "epoch: 10 step: 1503, loss is 0.05861445516347885\n",
      "epoch: 10 step: 1504, loss is 0.031010452657938004\n",
      "epoch: 10 step: 1505, loss is 0.02381291799247265\n",
      "epoch: 10 step: 1506, loss is 0.07352785021066666\n",
      "epoch: 10 step: 1507, loss is 0.22221499681472778\n",
      "epoch: 10 step: 1508, loss is 0.10434678941965103\n",
      "epoch: 10 step: 1509, loss is 0.1034407988190651\n",
      "epoch: 10 step: 1510, loss is 0.0891747772693634\n",
      "epoch: 10 step: 1511, loss is 0.4155767261981964\n",
      "epoch: 10 step: 1512, loss is 0.2067764848470688\n",
      "epoch: 10 step: 1513, loss is 0.018767395988106728\n",
      "epoch: 10 step: 1514, loss is 0.04581071063876152\n",
      "epoch: 10 step: 1515, loss is 0.21233421564102173\n",
      "epoch: 10 step: 1516, loss is 0.0077473619021475315\n",
      "epoch: 10 step: 1517, loss is 0.26035985350608826\n",
      "epoch: 10 step: 1518, loss is 0.02266177535057068\n",
      "epoch: 10 step: 1519, loss is 0.043338559567928314\n",
      "epoch: 10 step: 1520, loss is 0.016183042898774147\n",
      "epoch: 10 step: 1521, loss is 0.1702471673488617\n",
      "epoch: 10 step: 1522, loss is 0.050811078399419785\n",
      "epoch: 10 step: 1523, loss is 0.08717227727174759\n",
      "epoch: 10 step: 1524, loss is 0.08516592532396317\n",
      "epoch: 10 step: 1525, loss is 0.18103493750095367\n",
      "epoch: 10 step: 1526, loss is 0.08463968336582184\n",
      "epoch: 10 step: 1527, loss is 0.46876177191734314\n",
      "epoch: 10 step: 1528, loss is 0.0900055393576622\n",
      "epoch: 10 step: 1529, loss is 0.1217040866613388\n",
      "epoch: 10 step: 1530, loss is 0.1885102391242981\n",
      "epoch: 10 step: 1531, loss is 0.2389003038406372\n",
      "epoch: 10 step: 1532, loss is 0.16338863968849182\n",
      "epoch: 10 step: 1533, loss is 0.09029851853847504\n",
      "epoch: 10 step: 1534, loss is 0.061132024973630905\n",
      "epoch: 10 step: 1535, loss is 0.02117624320089817\n",
      "epoch: 10 step: 1536, loss is 0.028556223958730698\n",
      "epoch: 10 step: 1537, loss is 0.13801774382591248\n",
      "epoch: 10 step: 1538, loss is 0.37241700291633606\n",
      "epoch: 10 step: 1539, loss is 0.1538686454296112\n",
      "epoch: 10 step: 1540, loss is 0.07414362579584122\n",
      "epoch: 10 step: 1541, loss is 0.3059472441673279\n",
      "epoch: 10 step: 1542, loss is 0.13246771693229675\n",
      "epoch: 10 step: 1543, loss is 0.028175808489322662\n",
      "epoch: 10 step: 1544, loss is 0.06443583965301514\n",
      "epoch: 10 step: 1545, loss is 0.10109656304121017\n",
      "epoch: 10 step: 1546, loss is 0.01691524125635624\n",
      "epoch: 10 step: 1547, loss is 0.028065389022231102\n",
      "epoch: 10 step: 1548, loss is 0.11460991203784943\n",
      "epoch: 10 step: 1549, loss is 0.004374330397695303\n",
      "epoch: 10 step: 1550, loss is 0.08066995441913605\n",
      "epoch: 10 step: 1551, loss is 0.03930293023586273\n",
      "epoch: 10 step: 1552, loss is 0.06516354531049728\n",
      "epoch: 10 step: 1553, loss is 0.0470745749771595\n",
      "epoch: 10 step: 1554, loss is 0.1380305141210556\n",
      "epoch: 10 step: 1555, loss is 0.00974236335605383\n",
      "epoch: 10 step: 1556, loss is 0.010820646770298481\n",
      "epoch: 10 step: 1557, loss is 0.6460244655609131\n",
      "epoch: 10 step: 1558, loss is 0.050787266343832016\n",
      "epoch: 10 step: 1559, loss is 0.1351025551557541\n",
      "epoch: 10 step: 1560, loss is 0.3300546407699585\n",
      "epoch: 10 step: 1561, loss is 0.30850183963775635\n",
      "epoch: 10 step: 1562, loss is 0.025247221812605858\n",
      "epoch: 10 step: 1563, loss is 0.041818052530288696\n",
      "epoch: 10 step: 1564, loss is 0.048899468034505844\n",
      "epoch: 10 step: 1565, loss is 0.1473410278558731\n",
      "epoch: 10 step: 1566, loss is 0.9353563785552979\n",
      "epoch: 10 step: 1567, loss is 0.022730804979801178\n",
      "epoch: 10 step: 1568, loss is 0.37180274724960327\n",
      "epoch: 10 step: 1569, loss is 0.17853043973445892\n",
      "epoch: 10 step: 1570, loss is 0.7333351969718933\n",
      "epoch: 10 step: 1571, loss is 0.06252290308475494\n",
      "epoch: 10 step: 1572, loss is 0.08486107736825943\n",
      "epoch: 10 step: 1573, loss is 0.14337632060050964\n",
      "epoch: 10 step: 1574, loss is 0.12470722198486328\n",
      "epoch: 10 step: 1575, loss is 0.0743899717926979\n",
      "epoch: 10 step: 1576, loss is 0.12862029671669006\n",
      "epoch: 10 step: 1577, loss is 0.008723564445972443\n",
      "epoch: 10 step: 1578, loss is 0.057213298976421356\n",
      "epoch: 10 step: 1579, loss is 0.0984363704919815\n",
      "epoch: 10 step: 1580, loss is 0.3297266364097595\n",
      "epoch: 10 step: 1581, loss is 0.08696896582841873\n",
      "epoch: 10 step: 1582, loss is 0.12993250787258148\n",
      "epoch: 10 step: 1583, loss is 0.3037462532520294\n",
      "epoch: 10 step: 1584, loss is 0.2626678943634033\n",
      "epoch: 10 step: 1585, loss is 0.49733784794807434\n",
      "epoch: 10 step: 1586, loss is 0.5581678152084351\n",
      "epoch: 10 step: 1587, loss is 0.17273955047130585\n",
      "epoch: 10 step: 1588, loss is 0.1781259924173355\n",
      "epoch: 10 step: 1589, loss is 0.3078063726425171\n",
      "epoch: 10 step: 1590, loss is 0.6607197523117065\n",
      "epoch: 10 step: 1591, loss is 0.026577996090054512\n",
      "epoch: 10 step: 1592, loss is 0.45363396406173706\n",
      "epoch: 10 step: 1593, loss is 0.05753423273563385\n",
      "epoch: 10 step: 1594, loss is 0.09040113538503647\n",
      "epoch: 10 step: 1595, loss is 0.11544935405254364\n",
      "epoch: 10 step: 1596, loss is 0.536073625087738\n",
      "epoch: 10 step: 1597, loss is 0.2397172600030899\n",
      "epoch: 10 step: 1598, loss is 0.38457345962524414\n",
      "epoch: 10 step: 1599, loss is 0.17259442806243896\n",
      "epoch: 10 step: 1600, loss is 0.23802754282951355\n",
      "epoch: 10 step: 1601, loss is 0.06461841613054276\n",
      "epoch: 10 step: 1602, loss is 0.3169858753681183\n",
      "epoch: 10 step: 1603, loss is 0.07521021366119385\n",
      "epoch: 10 step: 1604, loss is 0.2084987610578537\n",
      "epoch: 10 step: 1605, loss is 0.0612044483423233\n",
      "epoch: 10 step: 1606, loss is 0.13952499628067017\n",
      "epoch: 10 step: 1607, loss is 0.07141703367233276\n",
      "epoch: 10 step: 1608, loss is 0.012175431475043297\n",
      "epoch: 10 step: 1609, loss is 0.1319834291934967\n",
      "epoch: 10 step: 1610, loss is 0.2398930937051773\n",
      "epoch: 10 step: 1611, loss is 0.08349499106407166\n",
      "epoch: 10 step: 1612, loss is 0.026790235191583633\n",
      "epoch: 10 step: 1613, loss is 0.022071972489356995\n",
      "epoch: 10 step: 1614, loss is 0.054524775594472885\n",
      "epoch: 10 step: 1615, loss is 0.262149840593338\n",
      "epoch: 10 step: 1616, loss is 0.25924330949783325\n",
      "epoch: 10 step: 1617, loss is 0.09010527282953262\n",
      "epoch: 10 step: 1618, loss is 0.0715901255607605\n",
      "epoch: 10 step: 1619, loss is 0.42258188128471375\n",
      "epoch: 10 step: 1620, loss is 0.014786851592361927\n",
      "epoch: 10 step: 1621, loss is 0.17267972230911255\n",
      "epoch: 10 step: 1622, loss is 0.1099223792552948\n",
      "epoch: 10 step: 1623, loss is 0.8783705830574036\n",
      "epoch: 10 step: 1624, loss is 0.7303351163864136\n",
      "epoch: 10 step: 1625, loss is 0.04622102528810501\n",
      "epoch: 10 step: 1626, loss is 0.008902267552912235\n",
      "epoch: 10 step: 1627, loss is 0.07547232508659363\n",
      "epoch: 10 step: 1628, loss is 0.05871076136827469\n",
      "epoch: 10 step: 1629, loss is 0.05827871337532997\n",
      "epoch: 10 step: 1630, loss is 0.014013715088367462\n",
      "epoch: 10 step: 1631, loss is 0.1455719918012619\n",
      "epoch: 10 step: 1632, loss is 0.6955364942550659\n",
      "epoch: 10 step: 1633, loss is 0.09525559842586517\n",
      "epoch: 10 step: 1634, loss is 0.32890745997428894\n",
      "epoch: 10 step: 1635, loss is 0.5122100114822388\n",
      "epoch: 10 step: 1636, loss is 0.153177410364151\n",
      "epoch: 10 step: 1637, loss is 0.3754742443561554\n",
      "epoch: 10 step: 1638, loss is 0.35883960127830505\n",
      "epoch: 10 step: 1639, loss is 0.28054079413414\n",
      "epoch: 10 step: 1640, loss is 0.08846971392631531\n",
      "epoch: 10 step: 1641, loss is 0.41762223839759827\n",
      "epoch: 10 step: 1642, loss is 0.09143625199794769\n",
      "epoch: 10 step: 1643, loss is 0.22330151498317719\n",
      "epoch: 10 step: 1644, loss is 0.18793444335460663\n",
      "epoch: 10 step: 1645, loss is 0.6351199150085449\n",
      "epoch: 10 step: 1646, loss is 0.5807222127914429\n",
      "epoch: 10 step: 1647, loss is 0.14680130779743195\n",
      "epoch: 10 step: 1648, loss is 0.12080012261867523\n",
      "epoch: 10 step: 1649, loss is 0.17065800726413727\n",
      "epoch: 10 step: 1650, loss is 0.1380307823419571\n",
      "epoch: 10 step: 1651, loss is 0.22296637296676636\n",
      "epoch: 10 step: 1652, loss is 0.1615586131811142\n",
      "epoch: 10 step: 1653, loss is 0.05102182924747467\n",
      "epoch: 10 step: 1654, loss is 0.04771576449275017\n",
      "epoch: 10 step: 1655, loss is 0.26076313853263855\n",
      "epoch: 10 step: 1656, loss is 0.39155149459838867\n",
      "epoch: 10 step: 1657, loss is 0.15260091423988342\n",
      "epoch: 10 step: 1658, loss is 0.04871176555752754\n",
      "epoch: 10 step: 1659, loss is 0.3993295729160309\n",
      "epoch: 10 step: 1660, loss is 0.14657160639762878\n",
      "epoch: 10 step: 1661, loss is 0.11945456266403198\n",
      "epoch: 10 step: 1662, loss is 0.09326956421136856\n",
      "epoch: 10 step: 1663, loss is 0.21363049745559692\n",
      "epoch: 10 step: 1664, loss is 0.07327219843864441\n",
      "epoch: 10 step: 1665, loss is 0.34657296538352966\n",
      "epoch: 10 step: 1666, loss is 0.18846549093723297\n",
      "epoch: 10 step: 1667, loss is 0.31600767374038696\n",
      "epoch: 10 step: 1668, loss is 0.04551553726196289\n",
      "epoch: 10 step: 1669, loss is 0.17668458819389343\n",
      "epoch: 10 step: 1670, loss is 0.019581854343414307\n",
      "epoch: 10 step: 1671, loss is 0.16748514771461487\n",
      "epoch: 10 step: 1672, loss is 0.27732908725738525\n",
      "epoch: 10 step: 1673, loss is 0.3369176983833313\n",
      "epoch: 10 step: 1674, loss is 0.2935629189014435\n",
      "epoch: 10 step: 1675, loss is 0.6157980561256409\n",
      "epoch: 10 step: 1676, loss is 0.24572008848190308\n",
      "epoch: 10 step: 1677, loss is 0.5504916310310364\n",
      "epoch: 10 step: 1678, loss is 0.26823335886001587\n",
      "epoch: 10 step: 1679, loss is 0.017648080363869667\n",
      "epoch: 10 step: 1680, loss is 0.42576003074645996\n",
      "epoch: 10 step: 1681, loss is 0.29381075501441956\n",
      "epoch: 10 step: 1682, loss is 0.3920077085494995\n",
      "epoch: 10 step: 1683, loss is 0.13883636891841888\n",
      "epoch: 10 step: 1684, loss is 0.4273710250854492\n",
      "epoch: 10 step: 1685, loss is 0.06032474339008331\n",
      "epoch: 10 step: 1686, loss is 0.010796478018164635\n",
      "epoch: 10 step: 1687, loss is 0.2628724277019501\n",
      "epoch: 10 step: 1688, loss is 0.6011499166488647\n",
      "epoch: 10 step: 1689, loss is 0.07160571217536926\n",
      "epoch: 10 step: 1690, loss is 0.037232864648103714\n",
      "epoch: 10 step: 1691, loss is 0.4505655765533447\n",
      "epoch: 10 step: 1692, loss is 0.13997092843055725\n",
      "epoch: 10 step: 1693, loss is 0.12344413250684738\n",
      "epoch: 10 step: 1694, loss is 0.3625161647796631\n",
      "epoch: 10 step: 1695, loss is 0.4556652903556824\n",
      "epoch: 10 step: 1696, loss is 0.1868801712989807\n",
      "epoch: 10 step: 1697, loss is 0.4446510374546051\n",
      "epoch: 10 step: 1698, loss is 0.11392630636692047\n",
      "epoch: 10 step: 1699, loss is 0.06084958836436272\n",
      "epoch: 10 step: 1700, loss is 0.1762138456106186\n",
      "epoch: 10 step: 1701, loss is 0.044546473771333694\n",
      "epoch: 10 step: 1702, loss is 0.17687638103961945\n",
      "epoch: 10 step: 1703, loss is 0.058067161589860916\n",
      "epoch: 10 step: 1704, loss is 0.22254247963428497\n",
      "epoch: 10 step: 1705, loss is 0.24229216575622559\n",
      "epoch: 10 step: 1706, loss is 0.10318336635828018\n",
      "epoch: 10 step: 1707, loss is 0.07915611565113068\n",
      "epoch: 10 step: 1708, loss is 0.13989666104316711\n",
      "epoch: 10 step: 1709, loss is 0.14088204503059387\n",
      "epoch: 10 step: 1710, loss is 0.022265110164880753\n",
      "epoch: 10 step: 1711, loss is 0.13957373797893524\n",
      "epoch: 10 step: 1712, loss is 0.3335646390914917\n",
      "epoch: 10 step: 1713, loss is 0.12189141660928726\n",
      "epoch: 10 step: 1714, loss is 0.0719485804438591\n",
      "epoch: 10 step: 1715, loss is 0.05384080857038498\n",
      "epoch: 10 step: 1716, loss is 0.22446656227111816\n",
      "epoch: 10 step: 1717, loss is 0.18936759233474731\n",
      "epoch: 10 step: 1718, loss is 0.34945693612098694\n",
      "epoch: 10 step: 1719, loss is 0.009280855767428875\n",
      "epoch: 10 step: 1720, loss is 0.15013891458511353\n",
      "epoch: 10 step: 1721, loss is 0.08147479593753815\n",
      "epoch: 10 step: 1722, loss is 0.04820770397782326\n",
      "epoch: 10 step: 1723, loss is 0.15677917003631592\n",
      "epoch: 10 step: 1724, loss is 0.41334861516952515\n",
      "epoch: 10 step: 1725, loss is 0.028827831149101257\n",
      "epoch: 10 step: 1726, loss is 0.11671877652406693\n",
      "epoch: 10 step: 1727, loss is 0.05458194389939308\n",
      "epoch: 10 step: 1728, loss is 0.008602163754403591\n",
      "epoch: 10 step: 1729, loss is 0.03286740928888321\n",
      "epoch: 10 step: 1730, loss is 0.03373158350586891\n",
      "epoch: 10 step: 1731, loss is 0.012402025982737541\n",
      "epoch: 10 step: 1732, loss is 0.09289944171905518\n",
      "epoch: 10 step: 1733, loss is 0.0934995710849762\n",
      "epoch: 10 step: 1734, loss is 0.31668248772621155\n",
      "epoch: 10 step: 1735, loss is 0.13322032988071442\n",
      "epoch: 10 step: 1736, loss is 0.3548129200935364\n",
      "epoch: 10 step: 1737, loss is 0.005991335492581129\n",
      "epoch: 10 step: 1738, loss is 0.08002975583076477\n",
      "epoch: 10 step: 1739, loss is 0.06405109912157059\n",
      "epoch: 10 step: 1740, loss is 0.07759477943181992\n",
      "epoch: 10 step: 1741, loss is 0.29322877526283264\n",
      "epoch: 10 step: 1742, loss is 0.11479795724153519\n",
      "epoch: 10 step: 1743, loss is 0.0645432397723198\n",
      "epoch: 10 step: 1744, loss is 0.023847034201025963\n",
      "epoch: 10 step: 1745, loss is 0.0474185086786747\n",
      "epoch: 10 step: 1746, loss is 0.0930744856595993\n",
      "epoch: 10 step: 1747, loss is 0.16908542811870575\n",
      "epoch: 10 step: 1748, loss is 0.0931517705321312\n",
      "epoch: 10 step: 1749, loss is 0.015315988101065159\n",
      "epoch: 10 step: 1750, loss is 0.290686696767807\n",
      "epoch: 10 step: 1751, loss is 0.3608894646167755\n",
      "epoch: 10 step: 1752, loss is 0.16917738318443298\n",
      "epoch: 10 step: 1753, loss is 0.10604420304298401\n",
      "epoch: 10 step: 1754, loss is 0.29953891038894653\n",
      "epoch: 10 step: 1755, loss is 0.2405257374048233\n",
      "epoch: 10 step: 1756, loss is 0.16271810233592987\n",
      "epoch: 10 step: 1757, loss is 0.14126379787921906\n",
      "epoch: 10 step: 1758, loss is 0.20523269474506378\n",
      "epoch: 10 step: 1759, loss is 0.006853272207081318\n",
      "epoch: 10 step: 1760, loss is 0.02445998042821884\n",
      "epoch: 10 step: 1761, loss is 0.050103809684515\n",
      "epoch: 10 step: 1762, loss is 0.07174969464540482\n",
      "epoch: 10 step: 1763, loss is 0.20472344756126404\n",
      "epoch: 10 step: 1764, loss is 0.04707297310233116\n",
      "epoch: 10 step: 1765, loss is 0.3955742418766022\n",
      "epoch: 10 step: 1766, loss is 0.40059104561805725\n",
      "epoch: 10 step: 1767, loss is 0.16181987524032593\n",
      "epoch: 10 step: 1768, loss is 0.012460673227906227\n",
      "epoch: 10 step: 1769, loss is 0.0755474716424942\n",
      "epoch: 10 step: 1770, loss is 0.16482795774936676\n",
      "epoch: 10 step: 1771, loss is 0.6653817892074585\n",
      "epoch: 10 step: 1772, loss is 0.003668147837743163\n",
      "epoch: 10 step: 1773, loss is 0.15513655543327332\n",
      "epoch: 10 step: 1774, loss is 0.0766172781586647\n",
      "epoch: 10 step: 1775, loss is 0.2618907392024994\n",
      "epoch: 10 step: 1776, loss is 0.9189899563789368\n",
      "epoch: 10 step: 1777, loss is 0.6170292496681213\n",
      "epoch: 10 step: 1778, loss is 0.04098853841423988\n",
      "epoch: 10 step: 1779, loss is 0.02376912161707878\n",
      "epoch: 10 step: 1780, loss is 0.06272243708372116\n",
      "epoch: 10 step: 1781, loss is 0.02367538958787918\n",
      "epoch: 10 step: 1782, loss is 0.5876545310020447\n",
      "epoch: 10 step: 1783, loss is 0.10540606081485748\n",
      "epoch: 10 step: 1784, loss is 0.06798943132162094\n",
      "epoch: 10 step: 1785, loss is 0.4929531514644623\n",
      "epoch: 10 step: 1786, loss is 0.97283935546875\n",
      "epoch: 10 step: 1787, loss is 0.31500235199928284\n",
      "epoch: 10 step: 1788, loss is 0.20146802067756653\n",
      "epoch: 10 step: 1789, loss is 0.02501153200864792\n",
      "epoch: 10 step: 1790, loss is 0.19078370928764343\n",
      "epoch: 10 step: 1791, loss is 0.12542007863521576\n",
      "epoch: 10 step: 1792, loss is 0.2966059446334839\n",
      "epoch: 10 step: 1793, loss is 0.34659114480018616\n",
      "epoch: 10 step: 1794, loss is 0.4054889380931854\n",
      "epoch: 10 step: 1795, loss is 0.4267845153808594\n",
      "epoch: 10 step: 1796, loss is 0.014993813820183277\n",
      "epoch: 10 step: 1797, loss is 0.13164553046226501\n",
      "epoch: 10 step: 1798, loss is 0.1167726218700409\n",
      "epoch: 10 step: 1799, loss is 0.08386514335870743\n",
      "epoch: 10 step: 1800, loss is 0.04929868131875992\n",
      "epoch: 10 step: 1801, loss is 0.2002522051334381\n",
      "epoch: 10 step: 1802, loss is 0.21026431024074554\n",
      "epoch: 10 step: 1803, loss is 0.046923451125621796\n",
      "epoch: 10 step: 1804, loss is 0.4180377125740051\n",
      "epoch: 10 step: 1805, loss is 0.562741756439209\n",
      "epoch: 10 step: 1806, loss is 0.005538224242627621\n",
      "epoch: 10 step: 1807, loss is 0.2842693030834198\n",
      "epoch: 10 step: 1808, loss is 0.11419737339019775\n",
      "epoch: 10 step: 1809, loss is 0.08641277253627777\n",
      "epoch: 10 step: 1810, loss is 0.1838938444852829\n",
      "epoch: 10 step: 1811, loss is 0.17947092652320862\n",
      "epoch: 10 step: 1812, loss is 0.43738093972206116\n",
      "epoch: 10 step: 1813, loss is 0.048930417746305466\n",
      "epoch: 10 step: 1814, loss is 0.1206224337220192\n",
      "epoch: 10 step: 1815, loss is 0.060281720012426376\n",
      "epoch: 10 step: 1816, loss is 0.38180461525917053\n",
      "epoch: 10 step: 1817, loss is 0.16404442489147186\n",
      "epoch: 10 step: 1818, loss is 0.08760582655668259\n",
      "epoch: 10 step: 1819, loss is 0.028176378458738327\n",
      "epoch: 10 step: 1820, loss is 0.23384344577789307\n",
      "epoch: 10 step: 1821, loss is 0.20684409141540527\n",
      "epoch: 10 step: 1822, loss is 0.24611064791679382\n",
      "epoch: 10 step: 1823, loss is 0.14670467376708984\n",
      "epoch: 10 step: 1824, loss is 0.28929242491722107\n",
      "epoch: 10 step: 1825, loss is 0.044332973659038544\n",
      "epoch: 10 step: 1826, loss is 0.6402985453605652\n",
      "epoch: 10 step: 1827, loss is 0.2263384610414505\n",
      "epoch: 10 step: 1828, loss is 0.024471288546919823\n",
      "epoch: 10 step: 1829, loss is 0.05416150018572807\n",
      "epoch: 10 step: 1830, loss is 0.020975518971681595\n",
      "epoch: 10 step: 1831, loss is 0.024644020944833755\n",
      "epoch: 10 step: 1832, loss is 0.12306880950927734\n",
      "epoch: 10 step: 1833, loss is 0.06798495352268219\n",
      "epoch: 10 step: 1834, loss is 0.03750874102115631\n",
      "epoch: 10 step: 1835, loss is 0.19757328927516937\n",
      "epoch: 10 step: 1836, loss is 0.16630645096302032\n",
      "epoch: 10 step: 1837, loss is 0.31977275013923645\n",
      "epoch: 10 step: 1838, loss is 0.23300078511238098\n",
      "epoch: 10 step: 1839, loss is 0.19489501416683197\n",
      "epoch: 10 step: 1840, loss is 0.25041887164115906\n",
      "epoch: 10 step: 1841, loss is 0.0036927456967532635\n",
      "epoch: 10 step: 1842, loss is 0.23786789178848267\n",
      "epoch: 10 step: 1843, loss is 0.02240072563290596\n",
      "epoch: 10 step: 1844, loss is 0.11817371100187302\n",
      "epoch: 10 step: 1845, loss is 0.02961624227464199\n",
      "epoch: 10 step: 1846, loss is 0.44543370604515076\n",
      "epoch: 10 step: 1847, loss is 0.33513543009757996\n",
      "epoch: 10 step: 1848, loss is 0.04405752941966057\n",
      "epoch: 10 step: 1849, loss is 0.05125918239355087\n",
      "epoch: 10 step: 1850, loss is 0.2356814742088318\n",
      "epoch: 10 step: 1851, loss is 0.12128213793039322\n",
      "epoch: 10 step: 1852, loss is 0.11682570725679398\n",
      "epoch: 10 step: 1853, loss is 0.13402381539344788\n",
      "epoch: 10 step: 1854, loss is 0.37388819456100464\n",
      "epoch: 10 step: 1855, loss is 0.14342765510082245\n",
      "epoch: 10 step: 1856, loss is 0.0389002300798893\n",
      "epoch: 10 step: 1857, loss is 0.1144346296787262\n",
      "epoch: 10 step: 1858, loss is 0.47088363766670227\n",
      "epoch: 10 step: 1859, loss is 0.03879791498184204\n",
      "epoch: 10 step: 1860, loss is 0.06239211559295654\n",
      "epoch: 10 step: 1861, loss is 0.5019963383674622\n",
      "epoch: 10 step: 1862, loss is 0.014852985739707947\n",
      "epoch: 10 step: 1863, loss is 0.0741017535328865\n",
      "epoch: 10 step: 1864, loss is 0.06764537841081619\n",
      "epoch: 10 step: 1865, loss is 0.18841014802455902\n",
      "epoch: 10 step: 1866, loss is 0.15436643362045288\n",
      "epoch: 10 step: 1867, loss is 0.03672291338443756\n",
      "epoch: 10 step: 1868, loss is 0.08636517822742462\n",
      "epoch: 10 step: 1869, loss is 0.14546231925487518\n",
      "epoch: 10 step: 1870, loss is 0.06866856664419174\n",
      "epoch: 10 step: 1871, loss is 0.2522123157978058\n",
      "epoch: 10 step: 1872, loss is 0.22957336902618408\n",
      "epoch: 10 step: 1873, loss is 0.02954353392124176\n",
      "epoch: 10 step: 1874, loss is 0.3563007116317749\n",
      "epoch: 10 step: 1875, loss is 0.3463088870048523\n",
      "epoch: 10 step: 1876, loss is 0.3502221703529358\n",
      "epoch: 10 step: 1877, loss is 0.19108131527900696\n",
      "epoch: 10 step: 1878, loss is 0.030833080410957336\n",
      "epoch: 10 step: 1879, loss is 0.015767695382237434\n",
      "epoch: 10 step: 1880, loss is 0.10785024613142014\n",
      "epoch: 10 step: 1881, loss is 0.14498694241046906\n",
      "epoch: 10 step: 1882, loss is 0.031010912731289864\n",
      "epoch: 10 step: 1883, loss is 0.14745087921619415\n",
      "epoch: 10 step: 1884, loss is 0.21790777146816254\n",
      "epoch: 10 step: 1885, loss is 0.05047440528869629\n",
      "epoch: 10 step: 1886, loss is 0.2985338568687439\n",
      "epoch: 10 step: 1887, loss is 0.13084682822227478\n",
      "epoch: 10 step: 1888, loss is 0.02666650339961052\n",
      "epoch: 10 step: 1889, loss is 0.17485657334327698\n",
      "epoch: 10 step: 1890, loss is 0.005534793715924025\n",
      "epoch: 10 step: 1891, loss is 0.17863288521766663\n",
      "epoch: 10 step: 1892, loss is 0.3673314154148102\n",
      "epoch: 10 step: 1893, loss is 0.27105459570884705\n",
      "epoch: 10 step: 1894, loss is 0.14223477244377136\n",
      "epoch: 10 step: 1895, loss is 0.5347352027893066\n",
      "epoch: 10 step: 1896, loss is 0.03628140315413475\n",
      "epoch: 10 step: 1897, loss is 0.18832871317863464\n",
      "epoch: 10 step: 1898, loss is 0.8471361994743347\n",
      "epoch: 10 step: 1899, loss is 0.25524094700813293\n",
      "epoch: 10 step: 1900, loss is 0.046463239938020706\n",
      "epoch: 10 step: 1901, loss is 0.0075967819429934025\n",
      "epoch: 10 step: 1902, loss is 0.012656508944928646\n",
      "epoch: 10 step: 1903, loss is 0.1032310202717781\n",
      "epoch: 10 step: 1904, loss is 0.11103814840316772\n",
      "epoch: 10 step: 1905, loss is 0.057188116014003754\n",
      "epoch: 10 step: 1906, loss is 0.0026296491269022226\n",
      "epoch: 10 step: 1907, loss is 0.11578911542892456\n",
      "epoch: 10 step: 1908, loss is 0.14401154220104218\n",
      "epoch: 10 step: 1909, loss is 0.5559489130973816\n",
      "epoch: 10 step: 1910, loss is 0.13439688086509705\n",
      "epoch: 10 step: 1911, loss is 0.07836682349443436\n",
      "epoch: 10 step: 1912, loss is 0.06299114972352982\n",
      "epoch: 10 step: 1913, loss is 0.022727733477950096\n",
      "epoch: 10 step: 1914, loss is 0.16119059920310974\n",
      "epoch: 10 step: 1915, loss is 0.048132576048374176\n",
      "epoch: 10 step: 1916, loss is 0.08535594493150711\n",
      "epoch: 10 step: 1917, loss is 0.2581091821193695\n",
      "epoch: 10 step: 1918, loss is 0.19424845278263092\n",
      "epoch: 10 step: 1919, loss is 0.14143602550029755\n",
      "epoch: 10 step: 1920, loss is 0.03472749516367912\n",
      "epoch: 10 step: 1921, loss is 0.09487607330083847\n",
      "epoch: 10 step: 1922, loss is 0.20014186203479767\n",
      "epoch: 10 step: 1923, loss is 0.24373896420001984\n",
      "epoch: 10 step: 1924, loss is 0.139155313372612\n",
      "epoch: 10 step: 1925, loss is 0.2559818625450134\n",
      "epoch: 10 step: 1926, loss is 0.050413187593221664\n",
      "epoch: 10 step: 1927, loss is 0.0016485699452459812\n",
      "epoch: 10 step: 1928, loss is 0.17191672325134277\n",
      "epoch: 10 step: 1929, loss is 0.1454555094242096\n",
      "epoch: 10 step: 1930, loss is 0.1293269395828247\n",
      "epoch: 10 step: 1931, loss is 0.18567274510860443\n",
      "epoch: 10 step: 1932, loss is 0.11678367108106613\n",
      "epoch: 10 step: 1933, loss is 0.41196557879447937\n",
      "epoch: 10 step: 1934, loss is 0.05145652964711189\n",
      "epoch: 10 step: 1935, loss is 0.04421616718173027\n",
      "epoch: 10 step: 1936, loss is 0.016958698630332947\n",
      "epoch: 10 step: 1937, loss is 0.3230651021003723\n",
      "epoch: 10 step: 1938, loss is 0.19712980091571808\n",
      "epoch: 10 step: 1939, loss is 0.19464430212974548\n",
      "epoch: 10 step: 1940, loss is 0.20945829153060913\n",
      "epoch: 10 step: 1941, loss is 0.031176041811704636\n",
      "epoch: 10 step: 1942, loss is 0.05856330692768097\n",
      "epoch: 10 step: 1943, loss is 0.18300694227218628\n",
      "epoch: 10 step: 1944, loss is 0.11901677399873734\n",
      "epoch: 10 step: 1945, loss is 0.06701275706291199\n",
      "epoch: 10 step: 1946, loss is 0.7921134233474731\n",
      "epoch: 10 step: 1947, loss is 0.08672332763671875\n",
      "epoch: 10 step: 1948, loss is 0.2793317437171936\n",
      "epoch: 10 step: 1949, loss is 0.060166578739881516\n",
      "epoch: 10 step: 1950, loss is 0.46650058031082153\n",
      "epoch: 10 step: 1951, loss is 0.055730149149894714\n",
      "epoch: 10 step: 1952, loss is 0.11320618540048599\n",
      "epoch: 10 step: 1953, loss is 0.032470375299453735\n",
      "epoch: 10 step: 1954, loss is 0.3507005274295807\n",
      "epoch: 10 step: 1955, loss is 0.012782999314367771\n",
      "epoch: 10 step: 1956, loss is 0.014457381330430508\n",
      "epoch: 10 step: 1957, loss is 0.09369634836912155\n",
      "epoch: 10 step: 1958, loss is 0.1988522857427597\n",
      "epoch: 10 step: 1959, loss is 0.035072553902864456\n",
      "epoch: 10 step: 1960, loss is 0.3551335334777832\n",
      "epoch: 10 step: 1961, loss is 0.19559136033058167\n",
      "epoch: 10 step: 1962, loss is 0.6133342385292053\n",
      "epoch: 10 step: 1963, loss is 0.30061620473861694\n",
      "epoch: 10 step: 1964, loss is 0.07037556916475296\n",
      "epoch: 10 step: 1965, loss is 0.01854178123176098\n",
      "epoch: 10 step: 1966, loss is 0.01650375686585903\n",
      "epoch: 10 step: 1967, loss is 0.22948984801769257\n",
      "epoch: 10 step: 1968, loss is 0.25785040855407715\n",
      "epoch: 10 step: 1969, loss is 0.32599830627441406\n",
      "epoch: 10 step: 1970, loss is 0.19002166390419006\n",
      "epoch: 10 step: 1971, loss is 0.03168400377035141\n",
      "epoch: 10 step: 1972, loss is 0.33249783515930176\n",
      "epoch: 10 step: 1973, loss is 0.00927439983934164\n",
      "epoch: 10 step: 1974, loss is 0.1998802274465561\n",
      "epoch: 10 step: 1975, loss is 0.08426514267921448\n",
      "epoch: 10 step: 1976, loss is 1.1448605060577393\n",
      "epoch: 10 step: 1977, loss is 0.06694501638412476\n",
      "epoch: 10 step: 1978, loss is 0.07724742591381073\n",
      "epoch: 10 step: 1979, loss is 0.08644779771566391\n",
      "epoch: 10 step: 1980, loss is 0.01711277663707733\n",
      "epoch: 10 step: 1981, loss is 0.04720460623502731\n",
      "epoch: 10 step: 1982, loss is 0.007206891663372517\n",
      "epoch: 10 step: 1983, loss is 0.17734242975711823\n",
      "epoch: 10 step: 1984, loss is 0.2525348365306854\n",
      "epoch: 10 step: 1985, loss is 0.0903400182723999\n",
      "epoch: 10 step: 1986, loss is 0.046421490609645844\n",
      "epoch: 10 step: 1987, loss is 0.26263150572776794\n",
      "epoch: 10 step: 1988, loss is 0.1739083081483841\n",
      "epoch: 10 step: 1989, loss is 0.08525170385837555\n",
      "epoch: 10 step: 1990, loss is 0.01959090679883957\n",
      "epoch: 10 step: 1991, loss is 0.22510476410388947\n",
      "epoch: 10 step: 1992, loss is 0.02803690917789936\n",
      "epoch: 10 step: 1993, loss is 0.16341331601142883\n",
      "epoch: 10 step: 1994, loss is 0.16721799969673157\n",
      "epoch: 10 step: 1995, loss is 0.0776137262582779\n",
      "epoch: 10 step: 1996, loss is 0.2329293191432953\n",
      "epoch: 10 step: 1997, loss is 0.34567466378211975\n",
      "epoch: 10 step: 1998, loss is 0.134969100356102\n",
      "epoch: 10 step: 1999, loss is 0.2230585515499115\n",
      "epoch: 10 step: 2000, loss is 0.5023784637451172\n",
      "epoch: 10 step: 2001, loss is 0.03373989462852478\n",
      "epoch: 10 step: 2002, loss is 0.60638427734375\n",
      "epoch: 10 step: 2003, loss is 0.08862967044115067\n",
      "epoch: 10 step: 2004, loss is 0.10484503954648972\n",
      "epoch: 10 step: 2005, loss is 0.009850766509771347\n",
      "epoch: 10 step: 2006, loss is 0.19105884432792664\n",
      "epoch: 10 step: 2007, loss is 0.13550949096679688\n",
      "epoch: 10 step: 2008, loss is 0.03190170228481293\n",
      "epoch: 10 step: 2009, loss is 0.1713099479675293\n",
      "epoch: 10 step: 2010, loss is 0.005586669314652681\n",
      "epoch: 10 step: 2011, loss is 0.5411382913589478\n",
      "epoch: 10 step: 2012, loss is 0.166335329413414\n",
      "epoch: 10 step: 2013, loss is 0.010247273370623589\n",
      "epoch: 10 step: 2014, loss is 0.1764349788427353\n",
      "epoch: 10 step: 2015, loss is 0.050316229462623596\n",
      "epoch: 10 step: 2016, loss is 0.029903264716267586\n",
      "epoch: 10 step: 2017, loss is 0.16430528461933136\n",
      "epoch: 10 step: 2018, loss is 0.23963084816932678\n",
      "epoch: 10 step: 2019, loss is 0.43888214230537415\n",
      "epoch: 10 step: 2020, loss is 0.44083961844444275\n",
      "epoch: 10 step: 2021, loss is 0.04316117614507675\n",
      "epoch: 10 step: 2022, loss is 0.3916090428829193\n",
      "epoch: 10 step: 2023, loss is 0.11920249462127686\n",
      "epoch: 10 step: 2024, loss is 0.027697067707777023\n",
      "epoch: 10 step: 2025, loss is 0.33570441603660583\n",
      "epoch: 10 step: 2026, loss is 0.4082489013671875\n",
      "epoch: 10 step: 2027, loss is 0.1737315058708191\n",
      "epoch: 10 step: 2028, loss is 0.023420047014951706\n",
      "epoch: 10 step: 2029, loss is 0.11745771765708923\n",
      "epoch: 10 step: 2030, loss is 0.056853268295526505\n",
      "epoch: 10 step: 2031, loss is 0.054203566163778305\n",
      "epoch: 10 step: 2032, loss is 0.10430953651666641\n",
      "epoch: 10 step: 2033, loss is 0.12410089373588562\n",
      "epoch: 10 step: 2034, loss is 0.12246488779783249\n",
      "epoch: 10 step: 2035, loss is 0.33385175466537476\n",
      "epoch: 10 step: 2036, loss is 0.12036719918251038\n",
      "epoch: 10 step: 2037, loss is 0.08449972420930862\n",
      "epoch: 10 step: 2038, loss is 0.5374440550804138\n",
      "epoch: 10 step: 2039, loss is 0.14922235906124115\n",
      "epoch: 10 step: 2040, loss is 0.32981905341148376\n",
      "epoch: 10 step: 2041, loss is 0.07248013466596603\n",
      "epoch: 10 step: 2042, loss is 0.20718903839588165\n",
      "epoch: 10 step: 2043, loss is 0.04599175229668617\n",
      "epoch: 10 step: 2044, loss is 0.4704759120941162\n",
      "epoch: 10 step: 2045, loss is 0.019950727000832558\n",
      "epoch: 10 step: 2046, loss is 0.31884416937828064\n",
      "epoch: 10 step: 2047, loss is 0.03009883128106594\n",
      "epoch: 10 step: 2048, loss is 0.1241702064871788\n",
      "epoch: 10 step: 2049, loss is 0.048504430800676346\n",
      "epoch: 10 step: 2050, loss is 0.048524174839258194\n",
      "epoch: 10 step: 2051, loss is 0.1232687309384346\n",
      "epoch: 10 step: 2052, loss is 0.7079253196716309\n",
      "epoch: 10 step: 2053, loss is 0.031440768390893936\n",
      "epoch: 10 step: 2054, loss is 0.2494974583387375\n",
      "epoch: 10 step: 2055, loss is 0.06668761372566223\n",
      "epoch: 10 step: 2056, loss is 0.2509641945362091\n",
      "epoch: 10 step: 2057, loss is 0.09630560129880905\n",
      "epoch: 10 step: 2058, loss is 0.5847391486167908\n",
      "epoch: 10 step: 2059, loss is 0.1733589768409729\n",
      "epoch: 10 step: 2060, loss is 0.18321357667446136\n",
      "epoch: 10 step: 2061, loss is 0.1675228625535965\n",
      "epoch: 10 step: 2062, loss is 0.38936084508895874\n",
      "epoch: 10 step: 2063, loss is 0.08037396520376205\n",
      "epoch: 10 step: 2064, loss is 0.17627787590026855\n",
      "epoch: 10 step: 2065, loss is 0.08993703126907349\n",
      "epoch: 10 step: 2066, loss is 0.04503529518842697\n",
      "epoch: 10 step: 2067, loss is 0.07229703664779663\n",
      "epoch: 10 step: 2068, loss is 0.16369836032390594\n",
      "epoch: 10 step: 2069, loss is 0.08566602319478989\n",
      "epoch: 10 step: 2070, loss is 0.045517824590206146\n",
      "epoch: 10 step: 2071, loss is 0.036486729979515076\n",
      "epoch: 10 step: 2072, loss is 0.8599033355712891\n",
      "epoch: 10 step: 2073, loss is 0.00652896286919713\n",
      "epoch: 10 step: 2074, loss is 0.1819298416376114\n",
      "epoch: 10 step: 2075, loss is 0.21754363179206848\n",
      "epoch: 10 step: 2076, loss is 0.16275723278522491\n",
      "epoch: 10 step: 2077, loss is 0.03428744897246361\n",
      "epoch: 10 step: 2078, loss is 0.27829477190971375\n",
      "epoch: 10 step: 2079, loss is 0.17101381719112396\n",
      "epoch: 10 step: 2080, loss is 0.097681425511837\n",
      "epoch: 10 step: 2081, loss is 0.503953218460083\n",
      "epoch: 10 step: 2082, loss is 0.1884506642818451\n",
      "epoch: 10 step: 2083, loss is 0.3473348021507263\n",
      "epoch: 10 step: 2084, loss is 0.008571667596697807\n",
      "epoch: 10 step: 2085, loss is 0.055179938673973083\n",
      "epoch: 10 step: 2086, loss is 0.22250564396381378\n",
      "epoch: 10 step: 2087, loss is 0.13598297536373138\n",
      "epoch: 10 step: 2088, loss is 0.06474436074495316\n",
      "epoch: 10 step: 2089, loss is 0.22086811065673828\n",
      "epoch: 10 step: 2090, loss is 0.2790938913822174\n",
      "epoch: 10 step: 2091, loss is 0.1742098033428192\n",
      "epoch: 10 step: 2092, loss is 0.12613892555236816\n",
      "epoch: 10 step: 2093, loss is 0.43492743372917175\n",
      "epoch: 10 step: 2094, loss is 0.0592329315841198\n",
      "epoch: 10 step: 2095, loss is 0.11564355343580246\n",
      "epoch: 10 step: 2096, loss is 0.03486518934369087\n",
      "epoch: 10 step: 2097, loss is 0.2235599160194397\n",
      "epoch: 10 step: 2098, loss is 0.1723346710205078\n",
      "epoch: 10 step: 2099, loss is 0.1374301016330719\n",
      "epoch: 10 step: 2100, loss is 0.0437856987118721\n",
      "epoch: 10 step: 2101, loss is 0.05838541314005852\n",
      "epoch: 10 step: 2102, loss is 0.07357802987098694\n",
      "epoch: 10 step: 2103, loss is 0.023954879492521286\n",
      "epoch: 10 step: 2104, loss is 0.06165536120533943\n",
      "epoch: 10 step: 2105, loss is 0.05716438218951225\n",
      "epoch: 10 step: 2106, loss is 0.05162665992975235\n",
      "epoch: 10 step: 2107, loss is 0.16529478132724762\n",
      "epoch: 10 step: 2108, loss is 0.1454210877418518\n",
      "epoch: 10 step: 2109, loss is 0.10126287490129471\n",
      "epoch: 10 step: 2110, loss is 0.2217416614294052\n",
      "epoch: 10 step: 2111, loss is 0.025632530450820923\n",
      "epoch: 10 step: 2112, loss is 0.015123064629733562\n",
      "epoch: 10 step: 2113, loss is 0.24963589012622833\n",
      "epoch: 10 step: 2114, loss is 0.5039437413215637\n",
      "epoch: 10 step: 2115, loss is 0.11213508248329163\n",
      "epoch: 10 step: 2116, loss is 0.006246962584555149\n",
      "epoch: 10 step: 2117, loss is 0.0729895606637001\n",
      "epoch: 10 step: 2118, loss is 0.027776222676038742\n",
      "epoch: 10 step: 2119, loss is 0.07472743839025497\n",
      "epoch: 10 step: 2120, loss is 0.560880720615387\n",
      "epoch: 10 step: 2121, loss is 0.4523420035839081\n",
      "epoch: 10 step: 2122, loss is 0.435145765542984\n",
      "epoch: 10 step: 2123, loss is 0.0171644426882267\n",
      "epoch: 10 step: 2124, loss is 0.04349120706319809\n",
      "epoch: 10 step: 2125, loss is 0.09758047759532928\n",
      "epoch: 10 step: 2126, loss is 0.083037368953228\n",
      "epoch: 10 step: 2127, loss is 0.4288197457790375\n",
      "epoch: 10 step: 2128, loss is 0.03900106996297836\n",
      "epoch: 10 step: 2129, loss is 0.29227468371391296\n",
      "epoch: 10 step: 2130, loss is 0.05331973731517792\n",
      "epoch: 10 step: 2131, loss is 0.1645447462797165\n",
      "epoch: 10 step: 2132, loss is 0.15473569929599762\n",
      "epoch: 10 step: 2133, loss is 0.8762362599372864\n",
      "epoch: 10 step: 2134, loss is 0.3223485052585602\n",
      "epoch: 10 step: 2135, loss is 0.05300324782729149\n",
      "epoch: 10 step: 2136, loss is 0.04172913730144501\n",
      "epoch: 10 step: 2137, loss is 0.2644280791282654\n",
      "epoch: 10 step: 2138, loss is 0.0067334421910345554\n",
      "epoch: 10 step: 2139, loss is 0.02586568333208561\n",
      "epoch: 10 step: 2140, loss is 0.15219399333000183\n",
      "epoch: 10 step: 2141, loss is 0.21886517107486725\n",
      "epoch: 10 step: 2142, loss is 0.049018848687410355\n",
      "epoch: 10 step: 2143, loss is 0.44011446833610535\n",
      "epoch: 10 step: 2144, loss is 0.20920531451702118\n",
      "epoch: 10 step: 2145, loss is 0.029642527922987938\n",
      "epoch: 10 step: 2146, loss is 0.24862180650234222\n",
      "epoch: 10 step: 2147, loss is 0.35393816232681274\n",
      "epoch: 10 step: 2148, loss is 0.5614141821861267\n",
      "epoch: 10 step: 2149, loss is 0.02036638744175434\n",
      "epoch: 10 step: 2150, loss is 0.10552728176116943\n",
      "epoch: 10 step: 2151, loss is 0.1959254890680313\n",
      "epoch: 10 step: 2152, loss is 0.14501209557056427\n",
      "epoch: 10 step: 2153, loss is 0.06697315722703934\n",
      "epoch: 10 step: 2154, loss is 0.03828222304582596\n",
      "epoch: 10 step: 2155, loss is 0.11718796193599701\n",
      "epoch: 10 step: 2156, loss is 0.25259268283843994\n",
      "epoch: 10 step: 2157, loss is 0.14296868443489075\n",
      "epoch: 10 step: 2158, loss is 0.010303741320967674\n",
      "epoch: 10 step: 2159, loss is 0.02078991010785103\n",
      "epoch: 10 step: 2160, loss is 0.06945390999317169\n",
      "epoch: 10 step: 2161, loss is 0.01848195679485798\n",
      "epoch: 10 step: 2162, loss is 0.06491407006978989\n",
      "epoch: 10 step: 2163, loss is 0.10699574649333954\n",
      "epoch: 10 step: 2164, loss is 0.20486882328987122\n",
      "epoch: 10 step: 2165, loss is 0.41251838207244873\n",
      "epoch: 10 step: 2166, loss is 0.2038719207048416\n",
      "epoch: 10 step: 2167, loss is 0.046613458544015884\n",
      "epoch: 10 step: 2168, loss is 0.019397249445319176\n",
      "epoch: 10 step: 2169, loss is 0.06010165810585022\n",
      "epoch: 10 step: 2170, loss is 0.015696197748184204\n",
      "epoch: 10 step: 2171, loss is 0.5536315441131592\n",
      "epoch: 10 step: 2172, loss is 0.017187433317303658\n",
      "epoch: 10 step: 2173, loss is 0.4834021031856537\n",
      "epoch: 10 step: 2174, loss is 0.10581482201814651\n",
      "epoch: 10 step: 2175, loss is 0.08786366879940033\n",
      "epoch: 10 step: 2176, loss is 0.4974081516265869\n",
      "epoch: 10 step: 2177, loss is 0.02876385860145092\n",
      "epoch: 10 step: 2178, loss is 0.41581854224205017\n",
      "epoch: 10 step: 2179, loss is 0.23801881074905396\n",
      "epoch: 10 step: 2180, loss is 0.03573444485664368\n",
      "epoch: 10 step: 2181, loss is 0.018240250647068024\n",
      "epoch: 10 step: 2182, loss is 0.035863228142261505\n",
      "epoch: 10 step: 2183, loss is 0.4833884835243225\n",
      "epoch: 10 step: 2184, loss is 0.059946149587631226\n",
      "epoch: 10 step: 2185, loss is 0.4578479528427124\n",
      "epoch: 10 step: 2186, loss is 0.39440715312957764\n",
      "epoch: 10 step: 2187, loss is 0.005925513803958893\n",
      "epoch: 10 step: 2188, loss is 0.053441714495420456\n",
      "epoch: 10 step: 2189, loss is 0.09027033299207687\n",
      "epoch: 10 step: 2190, loss is 0.08235684037208557\n",
      "epoch: 10 step: 2191, loss is 0.0598825179040432\n",
      "epoch: 10 step: 2192, loss is 0.19465796649456024\n",
      "epoch: 10 step: 2193, loss is 0.5001345276832581\n",
      "epoch: 10 step: 2194, loss is 0.1883206069469452\n",
      "epoch: 10 step: 2195, loss is 0.223697230219841\n",
      "epoch: 10 step: 2196, loss is 0.1442699134349823\n",
      "epoch: 10 step: 2197, loss is 0.07764951884746552\n",
      "epoch: 10 step: 2198, loss is 0.08784136921167374\n",
      "epoch: 10 step: 2199, loss is 0.033704426139593124\n",
      "epoch: 10 step: 2200, loss is 0.022531094029545784\n",
      "epoch: 10 step: 2201, loss is 0.09121087938547134\n",
      "epoch: 10 step: 2202, loss is 0.16696922481060028\n",
      "epoch: 10 step: 2203, loss is 0.36598944664001465\n",
      "epoch: 10 step: 2204, loss is 0.06576981395483017\n",
      "epoch: 10 step: 2205, loss is 0.2724597454071045\n",
      "epoch: 10 step: 2206, loss is 0.1300588846206665\n",
      "epoch: 10 step: 2207, loss is 0.13917088508605957\n",
      "epoch: 10 step: 2208, loss is 0.018356667831540108\n",
      "epoch: 10 step: 2209, loss is 0.5205002427101135\n",
      "epoch: 10 step: 2210, loss is 0.9519124031066895\n",
      "epoch: 10 step: 2211, loss is 0.1740468144416809\n",
      "epoch: 10 step: 2212, loss is 0.14077457785606384\n",
      "epoch: 10 step: 2213, loss is 0.4842066466808319\n",
      "epoch: 10 step: 2214, loss is 0.013957375660538673\n",
      "epoch: 10 step: 2215, loss is 0.184697687625885\n",
      "epoch: 10 step: 2216, loss is 0.1034880205988884\n",
      "epoch: 10 step: 2217, loss is 0.05292594060301781\n",
      "epoch: 10 step: 2218, loss is 0.07746264338493347\n",
      "epoch: 10 step: 2219, loss is 0.220159649848938\n",
      "epoch: 10 step: 2220, loss is 0.07462071627378464\n",
      "epoch: 10 step: 2221, loss is 0.294060081243515\n",
      "epoch: 10 step: 2222, loss is 0.0974811464548111\n",
      "epoch: 10 step: 2223, loss is 0.30617934465408325\n",
      "epoch: 10 step: 2224, loss is 0.029948007315397263\n",
      "epoch: 10 step: 2225, loss is 0.4903598427772522\n",
      "epoch: 10 step: 2226, loss is 0.19610661268234253\n",
      "epoch: 10 step: 2227, loss is 0.2044067680835724\n",
      "epoch: 10 step: 2228, loss is 0.09890833497047424\n",
      "epoch: 10 step: 2229, loss is 0.045734211802482605\n",
      "epoch: 10 step: 2230, loss is 0.5306307673454285\n",
      "epoch: 10 step: 2231, loss is 0.3818771541118622\n",
      "epoch: 10 step: 2232, loss is 0.12488467246294022\n",
      "epoch: 10 step: 2233, loss is 0.07373597472906113\n",
      "epoch: 10 step: 2234, loss is 0.11573043465614319\n",
      "epoch: 10 step: 2235, loss is 0.416680246591568\n",
      "epoch: 10 step: 2236, loss is 0.7158976793289185\n",
      "epoch: 10 step: 2237, loss is 0.09581874310970306\n",
      "epoch: 10 step: 2238, loss is 0.08394678682088852\n",
      "epoch: 10 step: 2239, loss is 0.1380031704902649\n",
      "epoch: 10 step: 2240, loss is 0.32009363174438477\n",
      "epoch: 10 step: 2241, loss is 0.11208013445138931\n",
      "epoch: 10 step: 2242, loss is 0.25268471240997314\n",
      "epoch: 10 step: 2243, loss is 0.2959953844547272\n",
      "epoch: 10 step: 2244, loss is 0.050018616020679474\n",
      "epoch: 10 step: 2245, loss is 0.04970560967922211\n",
      "epoch: 10 step: 2246, loss is 0.28587430715560913\n",
      "epoch: 10 step: 2247, loss is 0.21630904078483582\n",
      "epoch: 10 step: 2248, loss is 0.23110158741474152\n",
      "epoch: 10 step: 2249, loss is 0.21270576119422913\n",
      "epoch: 10 step: 2250, loss is 0.09178829193115234\n",
      "epoch: 10 step: 2251, loss is 0.04464343562722206\n",
      "epoch: 10 step: 2252, loss is 0.3214540481567383\n",
      "epoch: 10 step: 2253, loss is 0.3710203468799591\n",
      "epoch: 10 step: 2254, loss is 0.0071380166336894035\n",
      "epoch: 10 step: 2255, loss is 0.21550917625427246\n",
      "epoch: 10 step: 2256, loss is 0.11519429832696915\n",
      "epoch: 10 step: 2257, loss is 0.06347977370023727\n",
      "epoch: 10 step: 2258, loss is 0.048527393490076065\n",
      "epoch: 10 step: 2259, loss is 0.06003708019852638\n",
      "epoch: 10 step: 2260, loss is 0.21566641330718994\n",
      "epoch: 10 step: 2261, loss is 0.22734051942825317\n",
      "epoch: 10 step: 2262, loss is 0.3540208339691162\n",
      "epoch: 10 step: 2263, loss is 0.18629446625709534\n",
      "epoch: 10 step: 2264, loss is 0.6255571842193604\n",
      "epoch: 10 step: 2265, loss is 0.3497697412967682\n",
      "epoch: 10 step: 2266, loss is 0.4178237020969391\n",
      "epoch: 10 step: 2267, loss is 0.04093596711754799\n",
      "epoch: 10 step: 2268, loss is 0.3895031213760376\n",
      "epoch: 10 step: 2269, loss is 0.005084630101919174\n",
      "epoch: 10 step: 2270, loss is 0.42476409673690796\n",
      "epoch: 10 step: 2271, loss is 0.16034942865371704\n",
      "epoch: 10 step: 2272, loss is 0.052456315606832504\n",
      "epoch: 10 step: 2273, loss is 0.617667555809021\n",
      "epoch: 10 step: 2274, loss is 0.20866096019744873\n",
      "epoch: 10 step: 2275, loss is 0.010061138309538364\n",
      "epoch: 10 step: 2276, loss is 0.10219666361808777\n",
      "epoch: 10 step: 2277, loss is 0.08324089646339417\n",
      "epoch: 10 step: 2278, loss is 0.09379387646913528\n",
      "epoch: 10 step: 2279, loss is 0.35304537415504456\n",
      "epoch: 10 step: 2280, loss is 0.18178822100162506\n",
      "epoch: 10 step: 2281, loss is 0.04368385300040245\n",
      "epoch: 10 step: 2282, loss is 0.05211713910102844\n",
      "epoch: 10 step: 2283, loss is 0.7053248882293701\n",
      "epoch: 10 step: 2284, loss is 0.23165392875671387\n",
      "epoch: 10 step: 2285, loss is 0.027975577861070633\n",
      "epoch: 10 step: 2286, loss is 0.20186205208301544\n",
      "epoch: 10 step: 2287, loss is 0.1980839967727661\n",
      "epoch: 10 step: 2288, loss is 0.14382533729076385\n",
      "epoch: 10 step: 2289, loss is 0.5511958003044128\n",
      "epoch: 10 step: 2290, loss is 0.33150869607925415\n",
      "epoch: 10 step: 2291, loss is 0.1792295277118683\n",
      "epoch: 10 step: 2292, loss is 0.3224448561668396\n",
      "epoch: 10 step: 2293, loss is 0.08142589032649994\n",
      "epoch: 10 step: 2294, loss is 0.047621846199035645\n",
      "epoch: 10 step: 2295, loss is 0.1335955262184143\n",
      "epoch: 10 step: 2296, loss is 0.11913476884365082\n",
      "epoch: 10 step: 2297, loss is 0.3799237608909607\n",
      "epoch: 10 step: 2298, loss is 0.29555171728134155\n",
      "epoch: 10 step: 2299, loss is 0.28025972843170166\n",
      "epoch: 10 step: 2300, loss is 0.2533424496650696\n",
      "epoch: 10 step: 2301, loss is 0.5970725417137146\n",
      "epoch: 10 step: 2302, loss is 0.1769220381975174\n",
      "epoch: 10 step: 2303, loss is 0.014445898123085499\n",
      "epoch: 10 step: 2304, loss is 0.034284502267837524\n",
      "epoch: 10 step: 2305, loss is 0.05646884813904762\n",
      "epoch: 10 step: 2306, loss is 0.24116238951683044\n",
      "epoch: 10 step: 2307, loss is 0.10491064190864563\n",
      "epoch: 10 step: 2308, loss is 0.30995020270347595\n",
      "epoch: 10 step: 2309, loss is 0.3777512013912201\n",
      "epoch: 10 step: 2310, loss is 0.26048657298088074\n",
      "epoch: 10 step: 2311, loss is 0.07917643338441849\n",
      "epoch: 10 step: 2312, loss is 0.0846681296825409\n",
      "epoch: 10 step: 2313, loss is 0.1497950702905655\n",
      "epoch: 10 step: 2314, loss is 0.0404047816991806\n",
      "epoch: 10 step: 2315, loss is 0.029653098434209824\n",
      "epoch: 10 step: 2316, loss is 0.12527436017990112\n",
      "epoch: 10 step: 2317, loss is 0.03553120419383049\n",
      "epoch: 10 step: 2318, loss is 0.04191439971327782\n",
      "epoch: 10 step: 2319, loss is 0.230337992310524\n",
      "epoch: 10 step: 2320, loss is 0.03538816422224045\n",
      "epoch: 10 step: 2321, loss is 0.6278830766677856\n",
      "epoch: 10 step: 2322, loss is 0.17012919485569\n",
      "epoch: 10 step: 2323, loss is 0.019881771877408028\n",
      "epoch: 10 step: 2324, loss is 0.19630736112594604\n",
      "epoch: 10 step: 2325, loss is 0.07718726247549057\n",
      "epoch: 10 step: 2326, loss is 0.09885231405496597\n",
      "epoch: 10 step: 2327, loss is 0.40782421827316284\n",
      "epoch: 10 step: 2328, loss is 0.03201562166213989\n",
      "epoch: 10 step: 2329, loss is 0.3405486047267914\n",
      "epoch: 10 step: 2330, loss is 0.07815365493297577\n",
      "epoch: 10 step: 2331, loss is 0.132293701171875\n",
      "epoch: 10 step: 2332, loss is 0.34678369760513306\n",
      "epoch: 10 step: 2333, loss is 0.48792046308517456\n",
      "epoch: 10 step: 2334, loss is 0.11527086794376373\n",
      "epoch: 10 step: 2335, loss is 0.03877925127744675\n",
      "epoch: 10 step: 2336, loss is 0.2372952699661255\n",
      "epoch: 10 step: 2337, loss is 0.11760974675416946\n",
      "epoch: 10 step: 2338, loss is 0.23581597208976746\n",
      "epoch: 10 step: 2339, loss is 0.14528973400592804\n",
      "epoch: 10 step: 2340, loss is 0.04436846822500229\n",
      "epoch: 10 step: 2341, loss is 0.3148147761821747\n",
      "epoch: 10 step: 2342, loss is 0.12928485870361328\n",
      "epoch: 10 step: 2343, loss is 0.09348610788583755\n",
      "epoch: 10 step: 2344, loss is 0.02630782499909401\n",
      "epoch: 10 step: 2345, loss is 0.7200317978858948\n",
      "epoch: 10 step: 2346, loss is 0.1133333221077919\n",
      "epoch: 10 step: 2347, loss is 0.09369899332523346\n",
      "epoch: 10 step: 2348, loss is 0.04317818209528923\n",
      "epoch: 10 step: 2349, loss is 0.14914169907569885\n",
      "epoch: 10 step: 2350, loss is 0.29382723569869995\n",
      "epoch: 10 step: 2351, loss is 0.33979740738868713\n",
      "epoch: 10 step: 2352, loss is 0.772826075553894\n",
      "epoch: 10 step: 2353, loss is 0.6542405486106873\n",
      "epoch: 10 step: 2354, loss is 0.3032926321029663\n",
      "epoch: 10 step: 2355, loss is 0.21025097370147705\n",
      "epoch: 10 step: 2356, loss is 0.42611244320869446\n",
      "epoch: 10 step: 2357, loss is 0.07776937633752823\n",
      "epoch: 10 step: 2358, loss is 0.03988022729754448\n",
      "epoch: 10 step: 2359, loss is 0.35346490144729614\n",
      "epoch: 10 step: 2360, loss is 0.1550595760345459\n",
      "epoch: 10 step: 2361, loss is 0.027279749512672424\n",
      "epoch: 10 step: 2362, loss is 0.3210934102535248\n",
      "epoch: 10 step: 2363, loss is 0.12376401573419571\n",
      "epoch: 10 step: 2364, loss is 0.18087337911128998\n",
      "epoch: 10 step: 2365, loss is 0.5320543050765991\n",
      "epoch: 10 step: 2366, loss is 0.35032525658607483\n",
      "epoch: 10 step: 2367, loss is 0.16175532341003418\n",
      "epoch: 10 step: 2368, loss is 0.04035535454750061\n",
      "epoch: 10 step: 2369, loss is 0.07119373977184296\n",
      "epoch: 10 step: 2370, loss is 0.04384143278002739\n",
      "epoch: 10 step: 2371, loss is 0.6163532733917236\n",
      "epoch: 10 step: 2372, loss is 0.09235022217035294\n",
      "epoch: 10 step: 2373, loss is 0.2301589995622635\n",
      "epoch: 10 step: 2374, loss is 0.18152368068695068\n",
      "epoch: 10 step: 2375, loss is 0.34043198823928833\n",
      "epoch: 10 step: 2376, loss is 0.17828363180160522\n",
      "epoch: 10 step: 2377, loss is 0.018412627279758453\n",
      "epoch: 10 step: 2378, loss is 0.2122831791639328\n",
      "epoch: 10 step: 2379, loss is 0.203338161110878\n",
      "epoch: 10 step: 2380, loss is 0.44247522950172424\n",
      "epoch: 10 step: 2381, loss is 0.23401272296905518\n",
      "epoch: 10 step: 2382, loss is 0.19560374319553375\n",
      "epoch: 10 step: 2383, loss is 0.2477630227804184\n",
      "epoch: 10 step: 2384, loss is 0.18078653514385223\n",
      "epoch: 10 step: 2385, loss is 0.31186410784721375\n",
      "epoch: 10 step: 2386, loss is 0.03815449774265289\n",
      "epoch: 10 step: 2387, loss is 0.07791469246149063\n",
      "epoch: 10 step: 2388, loss is 0.16814352571964264\n",
      "epoch: 10 step: 2389, loss is 0.2006535679101944\n",
      "epoch: 10 step: 2390, loss is 0.07227004319429398\n",
      "epoch: 10 step: 2391, loss is 0.1479146033525467\n",
      "epoch: 10 step: 2392, loss is 0.09706360101699829\n",
      "epoch: 10 step: 2393, loss is 0.22628633677959442\n",
      "epoch: 10 step: 2394, loss is 0.10105235129594803\n",
      "epoch: 10 step: 2395, loss is 0.07570477575063705\n",
      "epoch: 10 step: 2396, loss is 0.05702011659741402\n",
      "epoch: 10 step: 2397, loss is 0.038497116416692734\n",
      "epoch: 10 step: 2398, loss is 0.04250841587781906\n",
      "epoch: 10 step: 2399, loss is 0.0809277668595314\n",
      "epoch: 10 step: 2400, loss is 0.045560579746961594\n",
      "epoch: 10 step: 2401, loss is 0.2849179208278656\n",
      "epoch: 10 step: 2402, loss is 0.26661503314971924\n",
      "epoch: 10 step: 2403, loss is 0.04571196436882019\n",
      "epoch: 10 step: 2404, loss is 0.47636038064956665\n",
      "epoch: 10 step: 2405, loss is 0.2353019118309021\n",
      "epoch: 10 step: 2406, loss is 0.027091125026345253\n",
      "epoch: 10 step: 2407, loss is 0.0676988735795021\n",
      "epoch: 10 step: 2408, loss is 0.06790748983621597\n",
      "epoch: 10 step: 2409, loss is 0.08092896640300751\n",
      "epoch: 10 step: 2410, loss is 0.04495341330766678\n",
      "epoch: 10 step: 2411, loss is 0.19946953654289246\n",
      "epoch: 10 step: 2412, loss is 0.03483730927109718\n",
      "epoch: 10 step: 2413, loss is 0.027408601716160774\n",
      "epoch: 10 step: 2414, loss is 0.037757422775030136\n",
      "epoch: 10 step: 2415, loss is 0.1919783055782318\n",
      "epoch: 10 step: 2416, loss is 0.11837698519229889\n",
      "epoch: 10 step: 2417, loss is 0.12333144247531891\n",
      "epoch: 10 step: 2418, loss is 0.09076928347349167\n",
      "epoch: 10 step: 2419, loss is 0.18430039286613464\n",
      "epoch: 10 step: 2420, loss is 0.037448782473802567\n",
      "epoch: 10 step: 2421, loss is 0.1860223114490509\n",
      "epoch: 10 step: 2422, loss is 0.42452874779701233\n",
      "epoch: 10 step: 2423, loss is 0.262596070766449\n",
      "epoch: 10 step: 2424, loss is 0.07638268917798996\n",
      "epoch: 10 step: 2425, loss is 0.05778414011001587\n",
      "epoch: 10 step: 2426, loss is 0.48045840859413147\n",
      "epoch: 10 step: 2427, loss is 0.32090580463409424\n",
      "epoch: 10 step: 2428, loss is 0.026837583631277084\n",
      "epoch: 10 step: 2429, loss is 0.07365137338638306\n",
      "epoch: 10 step: 2430, loss is 0.061500538140535355\n",
      "epoch: 10 step: 2431, loss is 0.07193116843700409\n",
      "epoch: 10 step: 2432, loss is 0.8973593711853027\n",
      "epoch: 10 step: 2433, loss is 0.47886574268341064\n",
      "epoch: 10 step: 2434, loss is 0.23414427042007446\n",
      "epoch: 10 step: 2435, loss is 0.06244591996073723\n",
      "epoch: 10 step: 2436, loss is 0.06654138118028641\n",
      "epoch: 10 step: 2437, loss is 0.25300759077072144\n",
      "epoch: 10 step: 2438, loss is 0.054566700011491776\n",
      "epoch: 10 step: 2439, loss is 0.029679276049137115\n",
      "epoch: 10 step: 2440, loss is 0.574272871017456\n",
      "epoch: 10 step: 2441, loss is 0.03414188697934151\n",
      "epoch: 10 step: 2442, loss is 0.4941302239894867\n",
      "epoch: 10 step: 2443, loss is 0.04995405301451683\n",
      "epoch: 10 step: 2444, loss is 0.20153887569904327\n",
      "epoch: 10 step: 2445, loss is 0.10864400863647461\n",
      "epoch: 10 step: 2446, loss is 0.6474388241767883\n",
      "epoch: 10 step: 2447, loss is 0.8242743611335754\n",
      "epoch: 10 step: 2448, loss is 0.20967181026935577\n",
      "epoch: 10 step: 2449, loss is 0.3001270294189453\n",
      "epoch: 10 step: 2450, loss is 0.053385958075523376\n",
      "epoch: 10 step: 2451, loss is 0.031059646978974342\n",
      "epoch: 10 step: 2452, loss is 0.35848453640937805\n",
      "epoch: 10 step: 2453, loss is 0.7733213305473328\n",
      "epoch: 10 step: 2454, loss is 1.0911918878555298\n",
      "epoch: 10 step: 2455, loss is 0.09349915385246277\n",
      "epoch: 10 step: 2456, loss is 0.08178173005580902\n",
      "epoch: 10 step: 2457, loss is 0.2266894429922104\n",
      "epoch: 10 step: 2458, loss is 0.1216384619474411\n",
      "epoch: 10 step: 2459, loss is 0.2455144226551056\n",
      "epoch: 10 step: 2460, loss is 0.4456246495246887\n",
      "epoch: 10 step: 2461, loss is 0.06757815182209015\n",
      "epoch: 10 step: 2462, loss is 0.5467813611030579\n",
      "epoch: 10 step: 2463, loss is 0.2559473514556885\n",
      "epoch: 10 step: 2464, loss is 0.08200827986001968\n",
      "epoch: 10 step: 2465, loss is 0.7039068937301636\n",
      "epoch: 10 step: 2466, loss is 0.10421007126569748\n",
      "epoch: 10 step: 2467, loss is 0.4495201110839844\n",
      "epoch: 10 step: 2468, loss is 0.1072361022233963\n",
      "epoch: 10 step: 2469, loss is 0.16069269180297852\n",
      "epoch: 10 step: 2470, loss is 0.21108976006507874\n",
      "epoch: 10 step: 2471, loss is 0.11760003119707108\n",
      "epoch: 10 step: 2472, loss is 0.17062164843082428\n",
      "epoch: 10 step: 2473, loss is 0.28255337476730347\n",
      "epoch: 10 step: 2474, loss is 0.09505141526460648\n",
      "epoch: 10 step: 2475, loss is 0.08129837363958359\n",
      "epoch: 10 step: 2476, loss is 0.178094282746315\n",
      "epoch: 10 step: 2477, loss is 0.04782729223370552\n",
      "epoch: 10 step: 2478, loss is 0.05629748851060867\n",
      "epoch: 10 step: 2479, loss is 0.09722744673490524\n",
      "epoch: 10 step: 2480, loss is 0.12420084327459335\n",
      "epoch: 10 step: 2481, loss is 0.27084895968437195\n",
      "epoch: 10 step: 2482, loss is 0.05711938813328743\n",
      "epoch: 10 step: 2483, loss is 0.05897121503949165\n",
      "epoch: 10 step: 2484, loss is 0.23238414525985718\n",
      "epoch: 10 step: 2485, loss is 0.04808344691991806\n",
      "epoch: 10 step: 2486, loss is 0.04372577369213104\n",
      "epoch: 10 step: 2487, loss is 0.04708055034279823\n",
      "epoch: 10 step: 2488, loss is 0.09307921677827835\n",
      "epoch: 10 step: 2489, loss is 0.020998293533921242\n",
      "epoch: 10 step: 2490, loss is 0.05287296324968338\n",
      "epoch: 10 step: 2491, loss is 0.042544152587652206\n",
      "epoch: 10 step: 2492, loss is 0.23780925571918488\n",
      "epoch: 10 step: 2493, loss is 0.027355670928955078\n",
      "epoch: 10 step: 2494, loss is 0.017193753272294998\n",
      "epoch: 10 step: 2495, loss is 0.23103848099708557\n",
      "epoch: 10 step: 2496, loss is 0.0895339697599411\n",
      "epoch: 10 step: 2497, loss is 0.39821115136146545\n",
      "epoch: 10 step: 2498, loss is 0.06693383306264877\n",
      "epoch: 10 step: 2499, loss is 0.2757924199104309\n",
      "epoch: 10 step: 2500, loss is 0.17416493594646454\n",
      "epoch: 10 step: 2501, loss is 0.12526963651180267\n",
      "epoch: 10 step: 2502, loss is 0.04453705623745918\n",
      "epoch: 10 step: 2503, loss is 0.05194256827235222\n",
      "epoch: 10 step: 2504, loss is 0.031120765954256058\n",
      "epoch: 10 step: 2505, loss is 0.3528289198875427\n",
      "epoch: 10 step: 2506, loss is 0.06095898896455765\n",
      "epoch: 10 step: 2507, loss is 0.24831174314022064\n",
      "epoch: 10 step: 2508, loss is 0.20278038084506989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: unknown JFIF revision number 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: {'accuracy': 0.9507168458781362}\n",
      "[array([0, 0, 0, 0, 0, 1, 0, 1]), array([1, 1, 1, 0, 0, 0, 1, 1]), array([1, 0, 1, 1, 0, 0, 0, 0]), array([1, 0, 0, 1, 1, 1, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0]), array([0, 1, 1, 1, 1, 0, 0, 1]), array([1, 1, 0, 0, 1, 0, 0, 1]), array([0, 0, 0, 0, 1, 0, 0, 0]), array([0, 1, 1, 0, 0, 0, 0, 0]), array([0, 0, 0, 0, 1, 1, 0, 0]), array([1, 1, 1, 1, 0, 0, 0, 1]), array([1, 1, 1, 1, 0, 0, 0, 0]), array([1, 1, 1, 0, 1, 0, 0, 1]), array([1, 1, 0, 0, 1, 0, 1, 1]), array([1, 0, 0, 0, 1, 1, 0, 0]), array([0, 0, 0, 0, 1, 1, 0, 1]), array([0, 1, 0, 0, 0, 0, 1, 0]), array([1, 1, 1, 0, 0, 1, 1, 1]), array([0, 1, 1, 1, 0, 0, 0, 1]), array([0, 0, 0, 0, 1, 0, 1, 0]), array([1, 0, 1, 1, 0, 1, 1, 0]), array([0, 1, 0, 0, 0, 1, 0, 0]), array([0, 1, 1, 0, 0, 0, 0, 1]), array([0, 1, 1, 0, 1, 1, 1, 1]), array([1, 0, 1, 1, 0, 0, 1, 0]), array([0, 0, 0, 1, 1, 1, 0, 1]), array([0, 1, 1, 0, 0, 1, 1, 1]), array([1, 0, 1, 0, 1, 0, 1, 0]), array([0, 1, 1, 0, 1, 0, 0, 1]), array([0, 1, 0, 0, 1, 0, 0, 0]), array([1, 1, 1, 1, 1, 1, 0, 0]), array([1, 1, 0, 1, 0, 0, 0, 0]), array([0, 1, 0, 0, 0, 0, 1, 1]), array([1, 0, 0, 1, 1, 0, 0, 1]), array([0, 0, 0, 1, 0, 1, 0, 1]), array([0, 0, 1, 1, 1, 1, 0, 0]), array([1, 0, 1, 1, 1, 0, 0, 1]), array([1, 0, 1, 0, 1, 0, 1, 0]), array([1, 1, 0, 1, 0, 0, 1, 1]), array([1, 0, 1, 0, 1, 0, 1, 1]), array([1, 0, 1, 0, 0, 1, 0, 0]), array([0, 0, 0, 1, 0, 0, 0, 1]), array([1, 1, 1, 0, 1, 1, 1, 1]), array([0, 1, 0, 1, 0, 1, 0, 1]), array([1, 1, 1, 1, 0, 1, 1, 0]), array([0, 1, 1, 0, 0, 0, 0, 1]), array([1, 0, 1, 1, 0, 1, 0, 0]), array([1, 0, 1, 1, 1, 1, 1, 0]), array([1, 0, 0, 1, 1, 1, 1, 1]), array([1, 0, 1, 0, 0, 1, 0, 0]), array([0, 1, 0, 0, 1, 0, 1, 1]), array([0, 0, 0, 0, 1, 0, 1, 0]), array([1, 0, 1, 1, 1, 0, 1, 1]), array([0, 0, 0, 0, 0, 1, 1, 0]), array([1, 0, 1, 1, 0, 1, 1, 1]), array([1, 1, 1, 1, 1, 1, 1, 1]), array([0, 0, 0, 1, 1, 1, 1, 0]), array([0, 1, 0, 0, 0, 0, 0, 0]), array([1, 0, 1, 1, 1, 0, 1, 1]), array([1, 1, 0, 0, 0, 1, 1, 0]), array([1, 1, 0, 0, 1, 0, 1, 1]), array([1, 0, 1, 1, 0, 0, 0, 0]), array([1, 0, 0, 1, 0, 1, 1, 0]), array([0, 0, 1, 1, 1, 1, 0, 1]), array([1, 1, 0, 0, 1, 0, 1, 0]), array([1, 0, 1, 1, 1, 1, 0, 1]), array([1, 1, 0, 0, 0, 1, 1, 0]), array([0, 0, 1, 0, 1, 1, 1, 0]), array([0, 1, 0, 1, 1, 0, 1, 1]), array([1, 1, 1, 0, 1, 1, 1, 1]), array([1, 1, 0, 1, 1, 1, 0, 0]), array([0, 1, 1, 1, 1, 0, 1, 1]), array([0, 0, 1, 0, 1, 1, 1, 0]), array([0, 0, 0, 1, 0, 1, 1, 0]), array([0, 1, 1, 0, 1, 1, 0, 0]), array([0, 0, 0, 0, 0, 0, 0, 0]), array([1, 0, 1, 1, 0, 0, 0, 1]), array([1, 1, 1, 1, 1, 0, 1, 0]), array([0, 1, 1, 1, 0, 1, 0, 1]), array([0, 0, 0, 1, 0, 1, 0, 1]), array([0, 0, 1, 0, 0, 0, 1, 1]), array([0, 0, 0, 1, 1, 1, 0, 0]), array([0, 0, 1, 1, 0, 0, 0, 1]), array([1, 0, 1, 1, 0, 0, 0, 1]), array([0, 0, 0, 0, 0, 0, 0, 0]), array([1, 1, 0, 0, 0, 1, 1, 0]), array([0, 1, 1, 1, 1, 0, 0, 1]), array([1, 0, 0, 0, 1, 0, 1, 1]), array([1, 1, 0, 1, 0, 0, 0, 1]), array([1, 0, 1, 1, 0, 0, 1, 0]), array([0, 1, 0, 1, 1, 0, 1, 0]), array([1, 0, 0, 0, 0, 0, 1, 0]), array([1, 1, 1, 1, 1, 0, 1, 1]), array([0, 1, 1, 0, 1, 0, 0, 0]), array([1, 1, 0, 0, 1, 1, 0, 1]), array([0, 0, 0, 1, 0, 0, 0, 1]), array([1, 0, 1, 0, 1, 1, 0, 1]), array([0, 1, 0, 0, 1, 1, 1, 1]), array([1, 0, 0, 0, 1, 1, 0, 0]), array([0, 0, 1, 0, 1, 0, 0, 0]), array([1, 1, 1, 1, 0, 1, 1, 0]), array([1, 0, 1, 1, 0, 1, 1, 1]), array([0, 0, 0, 0, 1, 0, 1, 1]), array([0, 1, 1, 0, 0, 0, 0, 0]), array([0, 1, 1, 0, 1, 0, 0, 0]), array([1, 1, 0, 1, 0, 1, 0, 0]), array([1, 0, 0, 1, 1, 0, 1, 1]), array([1, 1, 0, 0, 1, 1, 1, 0]), array([1, 1, 0, 0, 0, 0, 0, 1]), array([1, 1, 0, 1, 1, 0, 0, 1]), array([1, 0, 1, 1, 0, 1, 1, 1]), array([0, 1, 0, 1, 0, 1, 1, 1]), array([0, 0, 1, 0, 1, 1, 0, 0]), array([1, 1, 0, 1, 0, 1, 1, 1]), array([0, 1, 1, 0, 1, 1, 0, 1]), array([1, 0, 0, 0, 0, 0, 0, 1]), array([1, 1, 1, 1, 1, 1, 0, 0]), array([1, 0, 1, 0, 0, 0, 0, 0]), array([0, 0, 1, 0, 1, 0, 0, 1]), array([1, 0, 1, 1, 1, 0, 1, 1]), array([1, 1, 1, 0, 1, 1, 1, 1]), array([0, 0, 0, 1, 0, 0, 0, 0]), array([0, 0, 1, 1, 1, 1, 1, 0]), array([0, 1, 1, 1, 1, 1, 0, 1]), array([0, 0, 1, 0, 0, 1, 1, 0]), array([0, 0, 1, 1, 1, 1, 1, 1]), array([0, 0, 0, 1, 0, 1, 1, 1]), array([1, 1, 0, 0, 1, 0, 1, 1]), array([0, 0, 0, 0, 1, 1, 1, 1]), array([0, 0, 1, 0, 1, 1, 0, 1]), array([0, 0, 1, 1, 0, 0, 0, 1]), array([1, 0, 1, 0, 0, 0, 1, 0]), array([1, 1, 1, 0, 0, 0, 0, 0]), array([0, 1, 1, 1, 1, 0, 0, 1]), array([1, 0, 1, 1, 1, 0, 0, 0]), array([0, 1, 0, 0, 0, 1, 1, 1]), array([1, 0, 0, 0, 1, 0, 1, 1]), array([1, 1, 0, 1, 0, 1, 0, 0]), array([0, 0, 1, 0, 0, 1, 0, 1]), array([1, 0, 1, 0, 1, 1, 0, 1]), array([1, 1, 0, 0, 1, 1, 1, 0]), array([1, 1, 0, 0, 1, 0, 0, 0]), array([0, 1, 1, 1, 1, 1, 0, 0]), array([0, 1, 1, 1, 0, 1, 1, 0]), array([1, 1, 0, 1, 1, 0, 1, 0]), array([1, 0, 0, 0, 1, 1, 1, 0]), array([0, 0, 0, 0, 1, 0, 0, 1]), array([0, 1, 1, 1, 1, 0, 1, 0]), array([1, 1, 1, 0, 0, 0, 1, 0]), array([0, 0, 0, 1, 0, 1, 1, 1]), array([0, 1, 0, 0, 0, 1, 1, 1]), array([1, 0, 1, 1, 1, 0, 0, 0]), array([1, 1, 0, 0, 1, 1, 0, 1]), array([0, 0, 0, 1, 1, 0, 1, 1]), array([0, 1, 1, 0, 1, 1, 1, 0]), array([0, 0, 1, 0, 0, 1, 0, 1]), array([1, 1, 0, 1, 1, 0, 1, 1]), array([1, 0, 0, 1, 0, 0, 0, 0]), array([0, 1, 0, 0, 1, 0, 0, 1]), array([0, 0, 0, 1, 1, 0, 1, 0]), array([1, 0, 1, 0, 1, 0, 0, 1]), array([1, 0, 1, 0, 1, 0, 1, 1]), array([1, 1, 1, 1, 0, 0, 0, 1]), array([1, 0, 1, 0, 1, 0, 0, 0]), array([0, 1, 0, 0, 0, 0, 0, 1]), array([1, 0, 0, 0, 1, 1, 1, 0]), array([0, 0, 1, 1, 0, 0, 0, 1]), array([1, 1, 1, 1, 0, 0, 0, 1]), array([1, 1, 1, 1, 0, 0, 1, 1]), array([1, 0, 1, 1, 1, 0, 1, 0]), array([1, 0, 0, 1, 0, 1, 0, 0]), array([1, 1, 1, 0, 0, 0, 0, 0]), array([0, 1, 1, 1, 0, 0, 1, 1]), array([0, 0, 0, 1, 1, 0, 0, 0]), array([0, 1, 0, 1, 1, 1, 1, 1]), array([1, 1, 0, 1, 1, 1, 0, 1]), array([0, 0, 0, 1, 0, 0, 1, 1]), array([1, 1, 0, 1, 1, 0, 0, 0]), array([0, 1, 0, 1, 0, 1, 0, 1]), array([0, 1, 1, 1, 0, 1, 0, 0]), array([1, 0, 0, 0, 1, 1, 0, 0]), array([0, 0, 0, 1, 1, 1, 0, 1]), array([0, 0, 0, 1, 1, 0, 0, 1]), array([1, 0, 1, 1, 1, 1, 0, 1]), array([1, 1, 0, 0, 1, 0, 1, 1]), array([1, 0, 1, 1, 0, 0, 0, 1]), array([1, 0, 0, 1, 1, 0, 1, 1]), array([1, 1, 0, 1, 1, 1, 1, 1]), array([0, 1, 0, 1, 1, 1, 1, 0]), array([0, 1, 1, 1, 1, 1, 1, 0]), array([1, 0, 0, 1, 1, 0, 1, 0]), array([0, 0, 0, 0, 1, 0, 1, 0]), array([0, 1, 1, 1, 0, 1, 0, 0]), array([0, 0, 0, 0, 1, 0, 1, 1]), array([0, 0, 0, 1, 0, 0, 0, 1]), array([1, 0, 0, 0, 1, 1, 0, 1]), array([1, 1, 0, 0, 1, 1, 1, 0]), array([1, 0, 1, 0, 1, 0, 1, 0]), array([0, 1, 0, 0, 1, 1, 0, 0]), array([0, 1, 1, 1, 0, 0, 0, 0]), array([0, 0, 0, 1, 1, 0, 0, 1]), array([1, 0, 0, 1, 1, 0, 1, 1]), array([1, 0, 0, 0, 1, 1, 0, 1]), array([0, 1, 1, 0, 0, 0, 0, 0]), array([0, 0, 1, 0, 0, 1, 1, 1]), array([0, 1, 1, 0, 0, 0, 0, 1]), array([1, 0, 1, 0, 0, 1, 0, 0]), array([1, 0, 1, 1, 0, 1, 0, 1]), array([0, 1, 1, 0, 0, 1, 0, 1]), array([1, 0, 1, 0, 1, 1, 1, 0]), array([1, 0, 0, 0, 0, 1, 0, 0]), array([1, 0, 0, 1, 1, 0, 1, 1]), array([0, 1, 1, 0, 1, 0, 0, 0]), array([1, 0, 1, 0, 0, 0, 0, 0]), array([0, 1, 0, 1, 1, 0, 0, 1]), array([1, 1, 1, 0, 1, 1, 1, 1]), array([1, 1, 1, 1, 0, 1, 1, 0]), array([1, 0, 0, 1, 1, 1, 0, 1]), array([0, 0, 0, 1, 0, 1, 1, 1]), array([1, 1, 1, 1, 1, 0, 1, 0]), array([0, 0, 1, 1, 0, 0, 0, 0]), array([0, 1, 1, 0, 1, 1, 0, 0]), array([0, 0, 0, 1, 0, 0, 0, 1]), array([1, 0, 0, 1, 0, 0, 1, 0]), array([0, 1, 0, 0, 1, 0, 1, 1]), array([1, 1, 0, 1, 1, 0, 0, 0]), array([1, 0, 1, 0, 1, 0, 1, 1]), array([1, 0, 0, 1, 0, 1, 0, 1]), array([1, 1, 1, 1, 1, 1, 0, 0]), array([1, 1, 1, 0, 0, 0, 0, 0]), array([1, 1, 1, 1, 1, 0, 1, 0]), array([1, 1, 1, 0, 0, 0, 1, 1]), array([0, 1, 1, 0, 1, 0, 0, 1]), array([0, 1, 0, 1, 1, 0, 0, 0]), array([1, 0, 0, 1, 0, 1, 0, 1]), array([0, 1, 1, 1, 0, 0, 1, 0]), array([0, 0, 1, 0, 0, 0, 1, 1]), array([1, 1, 1, 1, 0, 1, 0, 0]), array([0, 1, 0, 1, 0, 1, 0, 0]), array([0, 0, 1, 1, 0, 1, 0, 0]), array([1, 1, 1, 1, 0, 0, 0, 0]), array([1, 1, 0, 0, 0, 0, 0, 1]), array([1, 1, 0, 0, 1, 0, 0, 0]), array([1, 1, 1, 1, 1, 1, 1, 0]), array([1, 1, 1, 1, 0, 1, 0, 1]), array([0, 1, 0, 1, 0, 0, 0, 0]), array([0, 1, 0, 0, 1, 1, 0, 1]), array([0, 1, 1, 1, 0, 0, 1, 0]), array([0, 1, 0, 1, 0, 0, 0, 1]), array([0, 1, 1, 1, 1, 0, 1, 1]), array([1, 1, 0, 1, 0, 0, 1, 0]), array([0, 1, 0, 1, 0, 1, 0, 0]), array([1, 1, 0, 1, 0, 0, 1, 1]), array([0, 1, 0, 0, 0, 1, 1, 1]), array([1, 0, 1, 0, 0, 1, 1, 1]), array([1, 1, 1, 1, 1, 1, 1, 1]), array([1, 1, 1, 1, 0, 0, 1, 0]), array([0, 1, 0, 0, 0, 0, 1, 0]), array([1, 1, 1, 0, 0, 0, 1, 0]), array([0, 1, 0, 0, 0, 1, 0, 0]), array([0, 0, 1, 1, 1, 1, 0, 0]), array([1, 1, 0, 0, 0, 1, 1, 0]), array([1, 1, 0, 0, 1, 0, 1, 0]), array([1, 1, 0, 1, 1, 1, 1, 1]), array([0, 1, 0, 0, 1, 0, 1, 1]), array([0, 0, 1, 1, 1, 1, 0, 0]), array([0, 1, 0, 1, 0, 0, 0, 0]), array([0, 0, 0, 0, 1, 1, 1, 0]), array([1, 0, 1, 1, 1, 1, 0, 0]), array([1, 0, 1, 1, 1, 0, 1, 1]), array([0, 0, 1, 0, 0, 0, 0, 1]), array([1, 1, 0, 0, 1, 0, 0, 0]), array([1, 1, 1, 0, 1, 0, 0, 1]), array([0, 0, 0, 1, 0, 0, 0, 1]), array([0, 1, 1, 0, 1, 1, 1, 1]), array([1, 0, 0, 1, 0, 1, 0, 1]), array([1, 0, 1, 0, 1, 0, 0, 1]), array([1, 0, 0, 1, 0, 1, 1, 0]), array([1, 0, 0, 1, 0, 1, 0, 0]), array([0, 0, 1, 0, 1, 1, 1, 1]), array([1, 1, 0, 0, 1, 1, 0, 1]), array([0, 0, 0, 1, 1, 1, 0, 1]), array([0, 0, 1, 1, 1, 1, 1, 1]), array([1, 0, 0, 1, 0, 1, 1, 0]), array([0, 1, 1, 0, 1, 1, 0, 1]), array([0, 0, 0, 1, 1, 1, 1, 1]), array([0, 0, 1, 0, 1, 1, 1, 0]), array([1, 0, 0, 0, 0, 0, 1, 0]), array([1, 0, 0, 0, 0, 0, 0, 0]), array([0, 1, 0, 0, 0, 1, 1, 1]), array([0, 1, 1, 1, 0, 1, 0, 1]), array([1, 0, 1, 1, 0, 0, 0, 1]), array([0, 0, 0, 1, 1, 1, 0, 1]), array([1, 1, 1, 0, 0, 0, 1, 0]), array([0, 1, 1, 1, 0, 1, 0, 1]), array([0, 0, 0, 1, 0, 1, 1, 1]), array([0, 1, 0, 0, 1, 0, 0, 1]), array([0, 0, 0, 1, 1, 1, 1, 0]), array([1, 0, 1, 0, 1, 1, 0, 0]), array([1, 0, 1, 1, 0, 0, 0, 0]), array([1, 0, 1, 0, 0, 0, 0, 1]), array([1, 0, 0, 1, 1, 1, 0, 0]), array([0, 1, 0, 0, 0, 0, 0, 1]), array([1, 1, 0, 0, 0, 0, 1, 0]), array([1, 0, 0, 0, 0, 0, 0, 0]), array([0, 1, 1, 0, 0, 0, 0, 0]), array([1, 0, 1, 1, 1, 1, 1, 0]), array([1, 0, 0, 1, 1, 1, 0, 0]), array([1, 0, 0, 0, 0, 1, 1, 0]), array([1, 1, 1, 0])]\n"
     ]
    }
   ],
   "source": [
    "import mindspore.dataset as ds\n",
    "import numpy as np\n",
    "from mindspore import nn, Model\n",
    "from mindspore.train.callback import LossMonitor\n",
    "from mindspore.dataset import vision\n",
    "\n",
    "\n",
    "class PetImageClassifier:\n",
    "    def __init__(self, image_size=227, batch_size=8, learning_rate=0.001, momentum=0.9, epochs=10):\n",
    "        self.image_size = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.momentum = momentum\n",
    "        self.epochs = epochs\n",
    "\n",
    "        # Define dataset transformations\n",
    "        self.mean = [0.5 * 255] * 3\n",
    "        self.std = [0.5 * 255] * 3\n",
    "        self.transforms = [\n",
    "            vision.Resize((self.image_size, self.image_size)),\n",
    "            vision.Normalize(mean=self.mean, std=self.std),\n",
    "            vision.HWC2CHW()\n",
    "        ]\n",
    "\n",
    "        # Load train dataset\n",
    "        self.train_dataset = ds.ImageFolderDataset(\n",
    "            dataset_dir='./dataset/PetImages/train',\n",
    "            decode=True).map(\n",
    "            operations=self.transforms, num_parallel_workers=1).batch(self.batch_size)\n",
    "        self.train_dataset, self.val_dataset = self.train_dataset.split([0.9, 0.1])\n",
    "\n",
    "        # Load test dataset\n",
    "        self.test_dataset = ds.ImageFolderDataset(\n",
    "            dataset_dir='./dataset/PetImages/eval',\n",
    "            decode=True).map(\n",
    "            operations=self.transforms, num_parallel_workers=1).batch(self.batch_size)\n",
    "\n",
    "        self.net = AlexNet()\n",
    "        self.loss = nn.SoftmaxCrossEntropyWithLogits(sparse=True, reduction='mean')\n",
    "        self.opt = nn.Momentum(self.net.trainable_params(), learning_rate=self.learning_rate, momentum=self.momentum)\n",
    "        self.model = Model(self.net, loss_fn=self.loss, optimizer=self.opt, metrics={'accuracy'})\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train(\n",
    "            epoch=self.epochs,\n",
    "            train_dataset=self.train_dataset,\n",
    "            callbacks=[LossMonitor()],\n",
    "            dataset_sink_mode=True)\n",
    "\n",
    "    def test(self):\n",
    "        accuracy = self.model.eval(self.val_dataset, dataset_sink_mode=False)\n",
    "        print(f'Test accuracy: {accuracy}')\n",
    "\n",
    "    def predict(self):\n",
    "        predictions = []\n",
    "        for data in self.test_dataset.create_dict_iterator():\n",
    "            inputs = data['image']\n",
    "            output = self.model.predict(inputs)\n",
    "            pred = np.argmax(output.asnumpy(), axis=1)\n",
    "            predictions.append(pred)\n",
    "        print(predictions)\n",
    "\n",
    "\n",
    "class AlexNet(nn.Cell):\n",
    "    def __init__(self):\n",
    "        super(AlexNet, self).__init__()\n",
    "        self.features = nn.SequentialCell(\n",
    "            nn.Conv2d(3, 96, 11, stride=4, pad_mode=\"valid\"),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(96, 256, 5, pad_mode=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(256, 384, 3, pad_mode=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 384, 3, pad_mode=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(384, 256, 3, pad_mode=\"same\"),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        self.classifier = nn.SequentialCell(\n",
    "            nn.Dense(6 * 6 * 256, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(4096, 4096),\n",
    "            nn.ReLU(),\n",
    "            nn.Dense(4096, 100)\n",
    "        )\n",
    "\n",
    "    def construct(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Create an instance of PetImageClassifier and train the model\n",
    "    pet_image_classifier = PetImageClassifier()\n",
    "    pet_image_classifier.train()\n",
    "\n",
    "    # Test the trained model\n",
    "    pet_image_classifier.test()\n",
    "\n",
    "    # Make predictions on test dataset\n",
    "    pet_image_classifier.predict()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122eea0f-19cd-4f19-98af-c0fdeb314c02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TensorFlow-2.1",
   "language": "python",
   "name": "tensorflow-2.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
